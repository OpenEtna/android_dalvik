diff --git a/libcore/dalvik/src/main/java/dalvik/system/VMDebug.java b/libcore/dalvik/src/main/java/dalvik/system/VMDebug.java
index 365388a..31e82ec 100644
--- a/libcore/dalvik/src/main/java/dalvik/system/VMDebug.java
+++ b/libcore/dalvik/src/main/java/dalvik/system/VMDebug.java
@@ -34,6 +34,8 @@ import java.io.IOException;
 public final class VMDebug {
     /**
      * Specifies the default method trace data file name.
+     *
+     * @deprecated only used in one place, which is unused and deprecated
      */
     static public final String DEFAULT_METHOD_TRACE_FILE_NAME = "/sdcard/dmtrace.trace";
 
@@ -44,11 +46,13 @@ public final class VMDebug {
     public static final int TRACE_COUNT_ALLOCS = 1;
 
     /* constants for getAllocCount */
-    private static final int KIND_ALLOCATED_OBJECTS = 1<<0;
-    private static final int KIND_ALLOCATED_BYTES   = 1<<1;
-    private static final int KIND_FREED_OBJECTS     = 1<<2;
-    private static final int KIND_FREED_BYTES       = 1<<3;
-    private static final int KIND_GC_INVOCATIONS    = 1<<4;
+    private static final int KIND_ALLOCATED_OBJECTS     = 1<<0;
+    private static final int KIND_ALLOCATED_BYTES       = 1<<1;
+    private static final int KIND_FREED_OBJECTS         = 1<<2;
+    private static final int KIND_FREED_BYTES           = 1<<3;
+    private static final int KIND_GC_INVOCATIONS        = 1<<4;
+    private static final int KIND_CLASS_INIT_COUNT      = 1<<5;
+    private static final int KIND_CLASS_INIT_TIME       = 1<<6;
     private static final int KIND_EXT_ALLOCATED_OBJECTS = 1<<12;
     private static final int KIND_EXT_ALLOCATED_BYTES   = 1<<13;
     private static final int KIND_EXT_FREED_OBJECTS     = 1<<14;
@@ -64,6 +68,10 @@ public final class VMDebug {
         KIND_FREED_BYTES;
     public static final int KIND_GLOBAL_GC_INVOCATIONS =
         KIND_GC_INVOCATIONS;
+    public static final int KIND_GLOBAL_CLASS_INIT_COUNT =
+        KIND_CLASS_INIT_COUNT;
+    public static final int KIND_GLOBAL_CLASS_INIT_TIME =
+        KIND_CLASS_INIT_TIME;
     public static final int KIND_GLOBAL_EXT_ALLOCATED_OBJECTS =
         KIND_EXT_ALLOCATED_OBJECTS;
     public static final int KIND_GLOBAL_EXT_ALLOCATED_BYTES =
@@ -83,6 +91,10 @@ public final class VMDebug {
         KIND_FREED_BYTES << 16;
     public static final int KIND_THREAD_GC_INVOCATIONS =
         KIND_GC_INVOCATIONS << 16;
+    public static final int KIND_THREAD_CLASS_INIT_COUNT =
+        KIND_CLASS_INIT_COUNT << 16;
+    public static final int KIND_THREAD_CLASS_INIT_TIME =
+        KIND_CLASS_INIT_TIME << 16;
     public static final int KIND_THREAD_EXT_ALLOCATED_OBJECTS =
         KIND_EXT_ALLOCATED_OBJECTS << 16;
     public static final int KIND_THREAD_EXT_ALLOCATED_BYTES =
@@ -120,14 +132,19 @@ public final class VMDebug {
     public static native boolean isDebuggerConnected();
 
     /**
-     * Enable object allocation count logging and reporting.  Call with
-     * a depth of zero to disable.  This produces "top N" lists on every GC.
+     * Returns an array of strings that identify VM features.  This is
+     * used by DDMS to determine what sorts of operations the VM can
+     * perform.
+     *
+     * @hide
      */
-    //public static native void enableTopAllocCounts(int depth);
-    
+    public static native String[] getVmFeatureList();
+
     /**
      * Start method tracing with default name, size, and with <code>0</code>
      * flags.
+     *
+     * @deprecated not used, not needed
      */
     public static void startMethodTracing() {
         startMethodTracing(DEFAULT_METHOD_TRACE_FILE_NAME, 0, 0);
@@ -153,7 +170,12 @@ public final class VMDebug {
      */
     public static void startMethodTracing(String traceFileName,
         int bufferSize, int flags) {
-        startMethodTracing(traceFileName, null, bufferSize, flags);
+
+        if (traceFileName == null) {
+            throw new NullPointerException();
+        }
+
+        startMethodTracingNative(traceFileName, null, bufferSize, flags);
     }
 
     /**
@@ -165,7 +187,33 @@ public final class VMDebug {
      * this and find it would be useful.
      * @hide
      */
-    public static native void startMethodTracing(String traceFileName,
+    public static void startMethodTracing(String traceFileName,
+        FileDescriptor fd, int bufferSize, int flags)
+    {
+        if (traceFileName == null || fd == null) {
+            throw new NullPointerException();
+        }
+
+        startMethodTracingNative(traceFileName, fd, bufferSize, flags);
+    }
+
+    /**
+     * Starts method tracing without a backing file.  When stopMethodTracing
+     * is called, the result is sent directly to DDMS.  (If DDMS is not
+     * attached when tracing ends, the profiling data will be discarded.)
+     *
+     * @hide
+     */
+    public static void startMethodTracingDdms(int bufferSize, int flags) {
+        startMethodTracingNative(null, null, bufferSize, flags);
+    }
+
+    /**
+     * Implements all startMethodTracing variants.
+     *
+     * @hide
+     */
+    private static native void startMethodTracingNative(String traceFileName,
         FileDescriptor fd, int bufferSize, int flags);
 
     /**
@@ -277,6 +325,16 @@ public final class VMDebug {
     public static native void dumpHprofData(String fileName) throws IOException;
 
     /**
+     * Collect "hprof" and send it to DDMS.  This will cause a GC.
+     *
+     * @throws UnsupportedOperationException if the VM was built without
+     *         HPROF support.
+     *
+     * @hide
+     */
+    public static native void dumpHprofDataDdms();
+
+    /**
      * Primes the register map cache.
      *
      * @hide
@@ -284,15 +342,31 @@ public final class VMDebug {
     public static native boolean cacheRegisterMap(String classAndMethodDesc);
 
     /**
-     * Crashes the VM.  Seriously.  Dumps the stack trace for the current
-     * thread and then aborts the VM so you can see the native stack trace.
-     * Useful for figuring out how you got somewhere when lots of native
-     * code is involved.
+     * Dumps the contents of the VM reference tables (e.g. JNI locals and
+     * globals) to the log file.
+     *
+     * @hide
+     */
+    public static native void dumpReferenceTables();
+
+    /**
+     * Crashes the VM.  Seriously.  Dumps the interpreter stack trace for
+     * the current thread and then aborts the VM so you can see the native
+     * stack trace.  Useful for figuring out how you got somewhere when
+     * lots of native code is involved.
      *
      * @hide
      */
     public static native void crash();
 
+    /**
+     * Together with gdb, provide a handy way to stop the VM at user-tagged
+     * locations.
+     *
+     * @hide
+     */
+    public static native void infopoint(int id);
+
     /*
      * Fake method, inserted into dmtrace output when the garbage collector
      * runs.  Not actually called.
diff --git a/libdex/CmdUtils.c b/libdex/CmdUtils.c
index 7dfee87..102664c 100644
--- a/libdex/CmdUtils.c
+++ b/libdex/CmdUtils.c
@@ -162,7 +162,7 @@ UnzipToFileResult dexOpenAndMap(const char* fileName, const char* tempFileName,
         goto bail;
     }
 
-    if (sysMapFileInShmem(fd, pMap) != 0) {
+    if (sysMapFileInShmemReadOnly(fd, pMap) != 0) {
         fprintf(stderr, "ERROR: Unable to map %s\n", fileName);
         close(fd);
         goto bail;
diff --git a/libdex/DexFile.c b/libdex/DexFile.c
index 99b38c9..b139746 100644
--- a/libdex/DexFile.c
+++ b/libdex/DexFile.c
@@ -33,6 +33,10 @@
 #include <fcntl.h>
 #include <errno.h>
 
+// fwd
+static u4 dexComputeOptChecksum(const DexOptHeader* pOptHeader);
+
+
 /*
  * Verifying checksums is good, but it slows things down and causes us to
  * touch every page.  In the "optimized" world, it doesn't work at all,
@@ -753,8 +757,8 @@ DexFile* dexFileParse(const u1* data, size_t length, int flags)
     }
 
     /*
-     * Verify the checksum.  This is reasonably quick, but does require
-     * touching every byte in the DEX file.  The checksum changes after
+     * Verify the checksum(s).  This is reasonably quick, but does require
+     * touching every byte in the DEX file.  The base checksum changes after
      * byte-swapping and DEX optimization.
      */
     if (flags & kDexParseVerifyChecksum) {
@@ -767,14 +771,26 @@ DexFile* dexFileParse(const u1* data, size_t length, int flags)
         } else {
             LOGV("+++ adler32 checksum (%08x) verified\n", adler);
         }
+
+        const DexOptHeader* pOptHeader = pDexFile->pOptHeader;
+        if (pOptHeader != NULL) {
+            adler = dexComputeOptChecksum(pOptHeader);
+            if (adler != pOptHeader->checksum) {
+                LOGE("ERROR: bad opt checksum (%08x vs %08x)\n",
+                    adler, pOptHeader->checksum);
+                if (!(flags & kDexParseContinueOnError))
+                    goto bail;
+            } else {
+                LOGV("+++ adler32 opt checksum (%08x) verified\n", adler);
+            }
+        }
     }
 
     /*
      * Verify the SHA-1 digest.  (Normally we don't want to do this --
-     * the digest is used to uniquely identify a DEX file, and can't be
-     * computed post-optimization.)
-     *
-     * The digest will be invalid after byte swapping and DEX optimization.
+     * the digest is used to uniquely identify the original DEX file, and
+     * can't be computed for verification after the DEX is byte-swapped
+     * and optimized.)
      */
     if (kVerifySignature) {
         unsigned char sha1Digest[kSHA1DigestLen];
@@ -887,6 +903,20 @@ u4 dexComputeChecksum(const DexHeader* pHeader)
     return (u4) adler32(adler, start + nonSum, pHeader->fileSize - nonSum);
 }
 
+/*
+ * Compute the checksum on the data appended to the DEX file by dexopt.
+ */
+static u4 dexComputeOptChecksum(const DexOptHeader* pOptHeader)
+{
+    const u1* start = (const u1*) pOptHeader + pOptHeader->depsOffset;
+    const u1* end = (const u1*) pOptHeader +
+        pOptHeader->auxOffset + pOptHeader->auxLength;
+
+    uLong adler = adler32(0L, Z_NULL, 0);
+
+    return (u4) adler32(adler, start, end - start);
+}
+
 
 /*
  * Compute the size, in bytes, of a DexCode.
diff --git a/libdex/DexFile.h b/libdex/DexFile.h
index 4b5fe7c..a10aaf5 100644
--- a/libdex/DexFile.h
+++ b/libdex/DexFile.h
@@ -52,7 +52,7 @@
 
 /* same, but for optimized DEX header */
 #define DEX_OPT_MAGIC   "dey\n"
-#define DEX_OPT_MAGIC_VERS  "035\0"
+#define DEX_OPT_MAGIC_VERS  "036\0"
 
 #define DEX_DEP_MAGIC   "deps"
 
@@ -484,8 +484,9 @@ typedef struct DexOptHeader {
     u4  auxLength;
 
     u4  flags;              /* some info flags */
+    u4  checksum;           /* adler32 checksum covering deps/aux */
 
-    u4  padding;            /* induce 64-bit alignment */
+    /* pad for 64-bit alignment if necessary */
 } DexOptHeader;
 
 #define DEX_FLAG_VERIFIED           (1)     /* tried to verify all classes */
diff --git a/libdex/DexSwapVerify.c b/libdex/DexSwapVerify.c
index bc6f51f..400e766 100644
--- a/libdex/DexSwapVerify.c
+++ b/libdex/DexSwapVerify.c
@@ -69,11 +69,19 @@ static u8 endianSwapU8(u8 value) {
  */
 typedef struct CheckState {
     const DexHeader*  pHeader;
-    const u1*         fileStart; 
+    const u1*         fileStart;
     const u1*         fileEnd;      // points to fileStart + fileLen
     u4                fileLen;
     DexDataMap*       pDataMap;     // set after map verification
     const DexFile*    pDexFile;     // set after intraitem verification
+
+    /*
+     * bitmap of type_id indices that have been used to define classes;
+     * initialized immediately before class_def cross-verification, and
+     * freed immediately after it
+     */
+    u4*               pDefinedClassBits;
+
     const void*       previousItem; // set during section iteration
 } CheckState;
 
@@ -235,12 +243,36 @@ static bool verifyMethodDefiner(const CheckState* state, u4 definingClass,
 }
 
 /*
+ * Calculate the required size (in elements) of the array pointed at by
+ * pDefinedClassBits.
+ */
+static size_t calcDefinedClassBitsSize(const CheckState* state)
+{
+    // Divide typeIdsSize by 32 (0x20), rounding up.
+    return (state->pHeader->typeIdsSize + 0x1f) >> 5;
+}
+
+/*
+ * Set the given bit in pDefinedClassBits, returning its former value.
+ */
+static bool setDefinedClassBit(const CheckState* state, u4 typeIdx) {
+    u4 arrayIdx = typeIdx >> 5;
+    u4 bit = 1 << (typeIdx & 0x1f);
+    u4* element = &state->pDefinedClassBits[arrayIdx];
+    bool result = (*element & bit) != 0;
+
+    *element |= bit;
+
+    return result;
+}
+
+/*
  * Swap the header_item.
  */
 static bool swapDexHeader(const CheckState* state, DexHeader* pHeader)
 {
     CHECK_PTR_RANGE(pHeader, pHeader + 1);
-    
+
     // magic is ok
     SWAP_FIELD4(pHeader->checksum);
     // signature is ok
@@ -371,7 +403,7 @@ static bool swapMap(CheckState* state, DexMapList* pMap)
 
     SWAP_FIELD4(pMap->size);
     count = pMap->size;
-    
+
     CHECK_LIST_SIZE(item, count, sizeof(DexMapItem));
 
     while (count--) {
@@ -485,7 +517,7 @@ static bool swapMap(CheckState* state, DexMapList* pMap)
         LOGE("Unable to allocate data map (size 0x%x)\n", dataItemCount);
         return false;
     }
-    
+
     return true;
 }
 
@@ -644,7 +676,7 @@ static void* crossVerifyProtoIdItem(const CheckState* state, void* ptr) {
                     item->parametersOff, kDexTypeTypeList)) {
         return NULL;
     }
-    
+
     if (!shortyDescMatch(*shorty,
                     dexStringByTypeIdx(state->pDexFile, item->returnTypeIdx),
                     true)) {
@@ -681,7 +713,7 @@ static void* crossVerifyProtoIdItem(const CheckState* state, void* ptr) {
         LOGE("Shorty is too long\n");
         return NULL;
     }
-    
+
     const DexProtoId* item0 = state->previousItem;
     if (item0 != NULL) {
         // Check ordering. This relies on type_ids being in order.
@@ -699,7 +731,7 @@ static void* crossVerifyProtoIdItem(const CheckState* state, void* ptr) {
             for (;;) {
                 u4 idx0 = dexParameterIteratorNextIndex(&iterator0);
                 u4 idx1 = dexParameterIteratorNextIndex(&iterator);
-                
+
                 if (idx1 == kDexNoIndex) {
                     badOrder = true;
                     break;
@@ -743,7 +775,7 @@ static void* swapFieldIdItem(const CheckState* state, void* ptr) {
 static void* crossVerifyFieldIdItem(const CheckState* state, void* ptr) {
     const DexFieldId* item = ptr;
     const char* s;
-    
+
     s = dexStringByTypeIdx(state->pDexFile, item->classIdx);
     if (!dexIsClassDescriptor(s)) {
         LOGE("Invalid descriptor for class_idx: '%s'\n", s);
@@ -802,7 +834,7 @@ static void* crossVerifyFieldIdItem(const CheckState* state, void* ptr) {
 /* Perform byte-swapping and intra-item verification on method_id_item. */
 static void* swapMethodIdItem(const CheckState* state, void* ptr) {
     DexMethodId* item = ptr;
-    
+
     CHECK_PTR_RANGE(item, item + 1);
     SWAP_INDEX2(item->classIdx, state->pHeader->typeIdsSize);
     SWAP_INDEX2(item->protoIdx, state->pHeader->protoIdsSize);
@@ -910,7 +942,7 @@ static bool verifyClassDataIsForDef(const CheckState* state, u4 offset,
      */
     u4 dataDefiner = findFirstClassDataDefiner(state, classData);
     bool result = (dataDefiner == definerIdx) || (dataDefiner == kDexNoIndex);
-    
+
     free(classData);
     return result;
 }
@@ -933,14 +965,19 @@ static bool verifyAnnotationsDirectoryIsForDef(const CheckState* state,
 /* Perform cross-item verification of class_def_item. */
 static void* crossVerifyClassDefItem(const CheckState* state, void* ptr) {
     const DexClassDef* item = ptr;
-    const char* descriptor =
-        dexStringByTypeIdx(state->pDexFile, item->classIdx);
+    u4 classIdx = item->classIdx;
+    const char* descriptor = dexStringByTypeIdx(state->pDexFile, classIdx);
 
     if (!dexIsClassDescriptor(descriptor)) {
         LOGE("Invalid class: '%s'\n", descriptor);
         return NULL;
     }
 
+    if (setDefinedClassBit(state, classIdx)) {
+        LOGE("Duplicate class definition: '%s'\n", descriptor);
+        return NULL;
+    }
+
     bool okay =
         dexDataMapVerify0Ok(state->pDataMap,
                 item->interfacesOff, kDexTypeTypeList)
@@ -1209,7 +1246,7 @@ static const u1* crossVerifyParameterAnnotations(const CheckState* state,
 static u4 findFirstAnnotationsDirectoryDefiner(const CheckState* state,
         const DexAnnotationsDirectoryItem* dir) {
     if (dir->fieldsSize != 0) {
-        const DexFieldAnnotationsItem* fields = 
+        const DexFieldAnnotationsItem* fields =
             dexGetFieldAnnotations(state->pDexFile, dir);
         const DexFieldId* field =
             dexGetFieldId(state->pDexFile, fields[0].fieldIdx);
@@ -1217,7 +1254,7 @@ static u4 findFirstAnnotationsDirectoryDefiner(const CheckState* state,
     }
 
     if (dir->methodsSize != 0) {
-        const DexMethodAnnotationsItem* methods = 
+        const DexMethodAnnotationsItem* methods =
             dexGetMethodAnnotations(state->pDexFile, dir);
         const DexMethodId* method =
             dexGetMethodId(state->pDexFile, methods[0].methodIdx);
@@ -1225,7 +1262,7 @@ static u4 findFirstAnnotationsDirectoryDefiner(const CheckState* state,
     }
 
     if (dir->parametersSize != 0) {
-        const DexParameterAnnotationsItem* parameters = 
+        const DexParameterAnnotationsItem* parameters =
             dexGetParameterAnnotations(state->pDexFile, dir);
         const DexMethodId* method =
             dexGetMethodId(state->pDexFile, parameters[0].methodIdx);
@@ -1362,7 +1399,7 @@ static u4 annotationItemTypeIdx(const DexAnnotationItem* item) {
     const u1* data = item->annotation;
     return readUnsignedLeb128(&data);
 }
-  
+
 /* Perform cross-item verification of annotation_set_item. */
 static void* crossVerifyAnnotationSetItem(const CheckState* state, void* ptr) {
     const DexAnnotationSetItem* set = ptr;
@@ -1376,7 +1413,7 @@ static void* crossVerifyAnnotationSetItem(const CheckState* state, void* ptr) {
                         dexGetAnnotationOff(set, i), kDexTypeAnnotationItem)) {
             return NULL;
         }
-        
+
         const DexAnnotationItem* annotation =
             dexGetAnnotationItem(state->pDexFile, set, i);
         u4 idx = annotationItemTypeIdx(annotation);
@@ -1443,7 +1480,7 @@ static bool verifyMethods(const CheckState* state, u4 size,
             return false;
         }
 
-        if (((accessFlags & ~ACC_METHOD_MASK) != 0) 
+        if (((accessFlags & ~ACC_METHOD_MASK) != 0)
                 || (isSynchronized && !allowSynchronized)) {
             LOGE("Bogus method access flags %x @ %d\n", accessFlags, i);
             return false;
@@ -1469,7 +1506,7 @@ static bool verifyMethods(const CheckState* state, u4 size,
 static bool verifyClassDataItem0(const CheckState* state,
         DexClassData* classData) {
     bool okay;
-    
+
     okay = verifyFields(state, classData->header.staticFieldsSize,
             classData->staticFields, true);
 
@@ -1477,7 +1514,7 @@ static bool verifyClassDataItem0(const CheckState* state,
         LOGE("Trouble with static fields\n");
         return false;
     }
-    
+
     verifyFields(state, classData->header.instanceFieldsSize,
             classData->instanceFields, false);
 
@@ -1488,7 +1525,7 @@ static bool verifyClassDataItem0(const CheckState* state,
 
     okay = verifyMethods(state, classData->header.directMethodsSize,
             classData->directMethods, true);
-    
+
     if (!okay) {
         LOGE("Trouble with direct methods\n");
         return false;
@@ -1496,7 +1533,7 @@ static bool verifyClassDataItem0(const CheckState* state,
 
     okay = verifyMethods(state, classData->header.virtualMethodsSize,
             classData->virtualMethods, false);
-    
+
     if (!okay) {
         LOGE("Trouble with virtual methods\n");
         return false;
@@ -1522,7 +1559,7 @@ static void* intraVerifyClassDataItem(const CheckState* state, void* ptr) {
     if (!okay) {
         return NULL;
     }
-    
+
     return (void*) data;
 }
 
@@ -1557,7 +1594,7 @@ static u4 findFirstClassDataDefiner(const CheckState* state,
 
     return kDexNoIndex;
 }
-   
+
 /* Perform cross-item verification of class_data_item. */
 static void* crossVerifyClassDataItem(const CheckState* state, void* ptr) {
     const u1* data = ptr;
@@ -1593,13 +1630,13 @@ static void* crossVerifyClassDataItem(const CheckState* state, void* ptr) {
                 kDexTypeCodeItem)
             && verifyMethodDefiner(state, definingClass, meth->methodIdx);
     }
-    
+
     free(classData);
 
     if (!okay) {
         return NULL;
     }
-    
+
     return (void*) data;
 }
 
@@ -1635,7 +1672,7 @@ static u4 setHandlerOffsAndVerify(const CheckState* state,
         } else {
             catchAll = false;
         }
-        
+
         handlerOffs[i] = offset;
 
         while (size-- > 0) {
@@ -1700,9 +1737,9 @@ static void* swapTriesAndCatches(const CheckState* state, DexCode* code) {
         LOGE("Invalid handlers_size: %d\n", handlersSize);
         return NULL;
     }
-    
+
     u4 handlerOffs[handlersSize]; // list of valid handlerOff values
-    u4 endOffset = setHandlerOffsAndVerify(state, code, 
+    u4 endOffset = setHandlerOffsAndVerify(state, code,
             encodedPtr - encodedHandlers,
             handlersSize, handlerOffs);
 
@@ -1747,11 +1784,11 @@ static void* swapTriesAndCatches(const CheckState* state, DexCode* code) {
         lastEnd = tries->startAddr + tries->insnCount;
 
         if (lastEnd > code->insnsSize) {
-            LOGE("Invalid insn_count: 0x%x (end addr 0x%x)\n", 
+            LOGE("Invalid insn_count: 0x%x (end addr 0x%x)\n",
                     tries->insnCount, lastEnd);
             return NULL;
         }
-        
+
         tries++;
     }
 
@@ -1816,7 +1853,7 @@ static void* intraVerifyStringDataItem(const CheckState* state, void* ptr) {
             LOGE("String data would go beyond end-of-file\n");
             return NULL;
         }
-        
+
         u1 byte1 = *(data++);
 
         // Switch on the high four bits.
@@ -2037,7 +2074,7 @@ static void* intraVerifyDebugInfoItem(const CheckState* state, void* ptr) {
             return NULL;
         }
     }
-    
+
     return (void*) data;
 }
 
@@ -2056,13 +2093,13 @@ static u4 readUnsignedLittleEndian(const CheckState* state, const u1** pData,
     u4 i;
 
     CHECK_PTR_RANGE(data, data + size);
-    
+
     for (i = 0; i < size; i++) {
         result |= ((u4) *(data++)) << (i * 8);
     }
 
     *pData = data;
-    return result;        
+    return result;
 }
 
 /* Helper for *VerifyAnnotationItem() and *VerifyEncodedArrayItem(), which
@@ -2093,7 +2130,7 @@ static const u1* verifyEncodedArray(const CheckState* state,
 static const u1* verifyEncodedValue(const CheckState* state,
         const u1* data, bool crossVerify) {
     CHECK_PTR_RANGE(data, data + 1);
-    
+
     u1 headerByte = *(data++);
     u4 valueType = headerByte & kDexAnnotationValueTypeMask;
     u4 valueArg = headerByte >> kDexAnnotationValueArgShift;
@@ -2230,7 +2267,7 @@ static const u1* verifyEncodedAnnotation(const CheckState* state,
             return NULL;
         }
     }
-    
+
     u4 size = readAndVerifyUnsignedLeb128(&data, fileEnd, &okay);
     u4 lastIdx = 0;
     bool first = true;
@@ -2242,7 +2279,7 @@ static const u1* verifyEncodedAnnotation(const CheckState* state,
 
     while (size--) {
         idx = readAndVerifyUnsignedLeb128(&data, fileEnd, &okay);
-        
+
         if (!okay) {
             LOGE("Bogus encoded_annotation name_idx\n");
             return NULL;
@@ -2287,7 +2324,7 @@ static void* intraVerifyAnnotationItem(const CheckState* state, void* ptr) {
     const u1* data = ptr;
 
     CHECK_PTR_RANGE(data, data + 1);
-    
+
     switch (*(data++)) {
         case kDexVisibilityBuild:
         case kDexVisibilityRuntime:
@@ -2333,11 +2370,11 @@ static bool iterateSectionWithOptionalUpdate(CheckState* state,
     u4 i;
 
     state->previousItem = NULL;
-    
+
     for (i = 0; i < count; i++) {
         u4 newOffset = (offset + alignmentMask) & ~alignmentMask;
         u1* ptr = filePointer(state, newOffset);
-        
+
         if (offset < newOffset) {
             ptr = filePointer(state, offset);
             if (offset < newOffset) {
@@ -2540,7 +2577,7 @@ static bool swapEverythingButHeaderAndMap(CheckState* state,
                 break;
             }
             case kDexTypeMapList: {
-                /* 
+                /*
                  * The map section was swapped early on, but do some
                  * additional sanity checking here.
                  */
@@ -2674,8 +2711,16 @@ static bool crossVerifyEverything(CheckState* state, DexMapList* pMap)
                 break;
             }
             case kDexTypeClassDefItem: {
+                // Allocate (on the stack) the "observed class_def" bits.
+                size_t arraySize = calcDefinedClassBitsSize(state);
+                u4 definedClassBits[arraySize];
+                memset(definedClassBits, 0, arraySize * sizeof(u4));
+                state->pDefinedClassBits = definedClassBits;
+
                 okay = iterateSection(state, sectionOffset, sectionCount,
                         crossVerifyClassDefItem, sizeof(u4), NULL);
+
+                state->pDefinedClassBits = NULL;
                 break;
             }
             case kDexTypeAnnotationSetRefList: {
@@ -2799,6 +2844,7 @@ int dexFixByteOrdering(u1* addr, int len)
         state.fileLen = len;
         state.pDexFile = NULL;
         state.pDataMap = NULL;
+        state.pDefinedClassBits = NULL;
         state.previousItem = NULL;
 
         /*
@@ -2850,6 +2896,6 @@ int dexFixByteOrdering(u1* addr, int len)
     if (state.pDataMap != NULL) {
         dexDataMapFree(state.pDataMap);
     }
-    
+
     return !okay;       // 0 == success
 }
diff --git a/libdex/InstrUtils.c b/libdex/InstrUtils.c
index 93e1f00..d1ebeec 100644
--- a/libdex/InstrUtils.c
+++ b/libdex/InstrUtils.c
@@ -302,11 +302,12 @@ InstructionWidth* dexCreateInstrWidthTable(void)
         case OP_INVOKE_SUPER_QUICK:
         case OP_INVOKE_SUPER_QUICK_RANGE:
         case OP_EXECUTE_INLINE:
+        case OP_EXECUTE_INLINE_RANGE:
         case OP_INVOKE_DIRECT_EMPTY:
             width = -3;
             break;
 
-        /* these should never appear */
+        /* these should never appear when scanning bytecode */
         case OP_UNUSED_3E:
         case OP_UNUSED_3F:
         case OP_UNUSED_40:
@@ -325,8 +326,7 @@ InstructionWidth* dexCreateInstrWidthTable(void)
         case OP_UNUSED_E9:
         case OP_UNUSED_EA:
         case OP_UNUSED_EB:
-        case OP_UNUSED_EC:
-        case OP_UNUSED_EF:
+        case OP_BREAKPOINT:
         case OP_UNUSED_F1:
         case OP_UNUSED_FC:
         case OP_UNUSED_FD:
@@ -616,7 +616,8 @@ InstructionFlags* dexCreateInstrFlagsTable(void)
             flags = kInstrCanThrow;
             break;
         case OP_EXECUTE_INLINE:
-            flags = kInstrCanContinue;
+        case OP_EXECUTE_INLINE_RANGE:
+            flags = kInstrCanContinue | kInstrCanThrow;
             break;
         case OP_IGET_QUICK:
         case OP_IGET_WIDE_QUICK:
@@ -635,7 +636,7 @@ InstructionFlags* dexCreateInstrFlagsTable(void)
             flags = kInstrCanContinue | kInstrCanThrow | kInstrInvoke;
             break;
 
-        /* these should never appear */
+        /* these should never appear when scanning code */
         case OP_UNUSED_3E:
         case OP_UNUSED_3F:
         case OP_UNUSED_40:
@@ -654,8 +655,7 @@ InstructionFlags* dexCreateInstrFlagsTable(void)
         case OP_UNUSED_E9:
         case OP_UNUSED_EA:
         case OP_UNUSED_EB:
-        case OP_UNUSED_EC:
-        case OP_UNUSED_EF:
+        case OP_BREAKPOINT:
         case OP_UNUSED_F1:
         case OP_UNUSED_FC:
         case OP_UNUSED_FD:
@@ -985,11 +985,14 @@ InstructionFormat* dexCreateInstrFormatTable(void)
         case OP_EXECUTE_INLINE:
             fmt = kFmt3inline;
             break;
+        case OP_EXECUTE_INLINE_RANGE:
+            fmt = kFmt3rinline;
+            break;
         case OP_INVOKE_DIRECT_EMPTY:
             fmt = kFmt35c;
             break;
 
-        /* these should never appear */
+        /* these should never appear when scanning code */
         case OP_UNUSED_3E:
         case OP_UNUSED_3F:
         case OP_UNUSED_40:
@@ -1008,8 +1011,7 @@ InstructionFormat* dexCreateInstrFormatTable(void)
         case OP_UNUSED_E9:
         case OP_UNUSED_EA:
         case OP_UNUSED_EB:
-        case OP_UNUSED_EC:
-        case OP_UNUSED_EF:
+        case OP_BREAKPOINT:
         case OP_UNUSED_F1:
         case OP_UNUSED_FC:
         case OP_UNUSED_FD:
@@ -1201,6 +1203,7 @@ void dexDecodeInstruction(const InstructionFormat* fmts, const u2* insns,
         break;
     case kFmt3rc:       // op {vCCCC .. v(CCCC+AA-1)}, meth@BBBB
     case kFmt3rms:      // [opt] invoke-virtual+super/range
+    case kFmt3rinline:  // [opt] execute-inline/range
         pDec->vA = INST_AA(inst);
         pDec->vB = FETCH(1);
         pDec->vC = FETCH(2);
diff --git a/libdex/InstrUtils.h b/libdex/InstrUtils.h
index 9728cd4..8449ae5 100644
--- a/libdex/InstrUtils.h
+++ b/libdex/InstrUtils.h
@@ -68,6 +68,7 @@ enum InstructionFormat {
     kFmt3rms,       // [opt] invoke-virtual+super/range
     kFmt3rfs,       // [opt] invoke-interface/range
     kFmt3inline,    // [opt] inline invoke
+    kFmt3rinline,   // [opt] inline invoke/range
     kFmt51l,        // op vAA, #+BBBBBBBBBBBBBBBB
 };
 
diff --git a/libdex/OpCode.h b/libdex/OpCode.h
index 1272231..58d1702 100644
--- a/libdex/OpCode.h
+++ b/libdex/OpCode.h
@@ -330,12 +330,20 @@ typedef enum OpCode {
     OP_UNUSED_E9                    = 0xe9,
     OP_UNUSED_EA                    = 0xea,
     OP_UNUSED_EB                    = 0xeb,
-    OP_UNUSED_EC                    = 0xec,
+
+    /*
+     * The "breakpoint" instruction is special, in that it should never
+     * be seen by anything but the debug interpreter.  During debugging
+     * it takes the place of an arbitrary opcode, which means operations
+     * like "tell me the opcode width so I can find the next instruction"
+     * aren't possible.  (This is correctable, but probably not useful.)
+     */
+    OP_BREAKPOINT                   = 0xec,
 
     /* optimizer output -- these are never generated by "dx" */
     OP_THROW_VERIFICATION_ERROR     = 0xed,
     OP_EXECUTE_INLINE               = 0xee,
-    OP_UNUSED_EF                    = 0xef, /* OP_EXECUTE_INLINE_RANGE? */
+    OP_EXECUTE_INLINE_RANGE         = 0xef,
 
     OP_INVOKE_DIRECT_EMPTY          = 0xf0,
     OP_UNUSED_F1                    = 0xf1, /* OP_INVOKE_DIRECT_EMPTY_RANGE? */
@@ -358,6 +366,7 @@ typedef enum OpCode {
 
 #define kNumDalvikInstructions 256
 
+
 /*
  * Switch-statement signatures are a "NOP" followed by a code.  (A true NOP
  * is 0x0000.)
@@ -627,10 +636,10 @@ typedef enum OpCode {
         H(OP_UNUSED_E9),                                                    \
         H(OP_UNUSED_EA),                                                    \
         H(OP_UNUSED_EB),                                                    \
-        H(OP_UNUSED_EC),                                                    \
+        H(OP_BREAKPOINT),                                                   \
         H(OP_THROW_VERIFICATION_ERROR),                                     \
         H(OP_EXECUTE_INLINE),                                               \
-        H(OP_UNUSED_EF),                                                    \
+        H(OP_EXECUTE_INLINE_RANGE),                                         \
         /* f0..ff */                                                        \
         H(OP_INVOKE_DIRECT_EMPTY),                                          \
         H(OP_UNUSED_F1),                                                    \
diff --git a/libdex/SysUtil.c b/libdex/SysUtil.c
index bf1be88..7c6aaef 100644
--- a/libdex/SysUtil.c
+++ b/libdex/SysUtil.c
@@ -153,14 +153,44 @@ int sysLoadFileInShmem(int fd, MemMapping* pMap)
 #endif
 }
 
+#ifndef HAVE_POSIX_FILEMAP
+int sysFakeMapFile(int fd, MemMapping* pMap)
+{
+    /* No MMAP, just fake it by copying the bits.
+       For Win32 we could use MapViewOfFile if really necessary
+       (see libs/utils/FileMap.cpp).
+    */
+    off_t start;
+    size_t length;
+    void* memPtr;
+
+    assert(pMap != NULL);
+
+    if (getFileStartAndLength(fd, &start, &length) < 0)
+        return -1;
+
+    memPtr = malloc(length);
+    if (read(fd, memPtr, length) < 0) {
+        LOGW("read(fd=%d, start=%d, length=%d) failed: %s\n", (int) length,
+            fd, (int) start, strerror(errno));
+        return -1;
+    }
+
+    pMap->baseAddr = pMap->addr = memPtr;
+    pMap->baseLength = pMap->length = length;
+
+    return 0;
+}
+#endif
+
 /*
  * Map a file (from fd's current offset) into a shared, read-only memory
- * segment.  The file offset must be a multiple of the page size.
+ * segment.  The file offset must be a multiple of the system page size.
  *
  * On success, returns 0 and fills out "pMap".  On failure, returns a nonzero
  * value and does not disturb "pMap".
  */
-int sysMapFileInShmem(int fd, MemMapping* pMap)
+int sysMapFileInShmemReadOnly(int fd, MemMapping* pMap)
 {
 #ifdef HAVE_POSIX_FILEMAP
     off_t start;
@@ -174,7 +204,7 @@ int sysMapFileInShmem(int fd, MemMapping* pMap)
 
     memPtr = mmap(NULL, length, PROT_READ, MAP_FILE | MAP_SHARED, fd, start);
     if (memPtr == MAP_FAILED) {
-        LOGW("mmap(%d, R, FILE|SHARED, %d, %d) failed: %s\n", (int) length,
+        LOGW("mmap(%d, RO, FILE|SHARED, %d, %d) failed: %s\n", (int) length,
             fd, (int) start, strerror(errno));
         return -1;
     }
@@ -184,10 +214,24 @@ int sysMapFileInShmem(int fd, MemMapping* pMap)
 
     return 0;
 #else
-    /* No MMAP, just fake it by copying the bits.
-       For Win32 we could use MapViewOfFile if really necessary
-       (see libs/utils/FileMap.cpp).
-    */
+    return sysFakeMapFile(fd, pMap);
+#endif
+}
+
+/*
+ * Map a file (from fd's current offset) into a private, read-write memory
+ * segment that will be marked read-only (a/k/a "writable read-only").  The
+ * file offset must be a multiple of the system page size.
+ *
+ * In some cases the mapping will be fully writable (e.g. for files on
+ * FAT filesystems).
+ *
+ * On success, returns 0 and fills out "pMap".  On failure, returns a nonzero
+ * value and does not disturb "pMap".
+ */
+int sysMapFileInShmemWritableReadOnly(int fd, MemMapping* pMap)
+{
+#ifdef HAVE_POSIX_FILEMAP
     off_t start;
     size_t length;
     void* memPtr;
@@ -197,17 +241,27 @@ int sysMapFileInShmem(int fd, MemMapping* pMap)
     if (getFileStartAndLength(fd, &start, &length) < 0)
         return -1;
 
-    memPtr = malloc(length);
-    if (read(fd, memPtr, length) < 0) {
-        LOGW("read(fd=%d, start=%d, length=%d) failed: %s\n", (int) length,
+    memPtr = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_FILE | MAP_PRIVATE,
+            fd, start);
+    if (memPtr == MAP_FAILED) {
+        LOGW("mmap(%d, R/W, FILE|PRIVATE, %d, %d) failed: %s\n", (int) length,
             fd, (int) start, strerror(errno));
         return -1;
     }
+    if (mprotect(memPtr, length, PROT_READ) < 0) {
+        /* this fails with EACCESS on FAT filesystems, e.g. /sdcard */
+        int err = errno;
+        LOGV("mprotect(%p, %d, PROT_READ) failed: %s\n",
+            memPtr, length, strerror(err));
+        LOGD("mprotect(RO) failed (%d), file will remain read-write\n", err);
+    }
 
     pMap->baseAddr = pMap->addr = memPtr;
     pMap->baseLength = pMap->length = length;
 
     return 0;
+#else
+    return sysFakeMapFile(fd, pMap);
 #endif
 }
 
@@ -270,6 +324,48 @@ int sysMapFileSegmentInShmem(int fd, off_t start, long length,
 }
 
 /*
+ * Change the access rights on one or more pages to read-only or read-write.
+ *
+ * Returns 0 on success.
+ */
+int sysChangeMapAccess(void* addr, size_t length, int wantReadWrite,
+    MemMapping* pMap)
+{
+#ifdef HAVE_POSIX_FILEMAP
+    /*
+     * Verify that "addr" is part of this mapping file.
+     */
+    if (addr < pMap->baseAddr ||
+        (u1*)addr >= (u1*)pMap->baseAddr + pMap->baseLength)
+    {
+        LOGE("Attempted to change %p; map is %p - %p\n",
+            addr, pMap->baseAddr, (u1*)pMap->baseAddr + pMap->baseLength);
+        return -1;
+    }
+
+    /*
+     * Align "addr" to a page boundary and adjust "length" appropriately.
+     * (The address must be page-aligned, the length doesn't need to be,
+     * but we do need to ensure we cover the same range.)
+     */
+    u1* alignAddr = (u1*) ((int) addr & ~(SYSTEM_PAGE_SIZE-1));
+    size_t alignLength = length + ((u1*) addr - alignAddr);
+
+    //LOGI("%p/%zd --> %p/%zd\n", addr, length, alignAddr, alignLength);
+    int prot = wantReadWrite ? (PROT_READ|PROT_WRITE) : (PROT_READ);
+    if (mprotect(alignAddr, alignLength, prot) != 0) {
+        int err = errno;
+        LOGV("mprotect (%p,%zd,%d) failed: %s\n",
+            alignAddr, alignLength, prot, strerror(errno));
+        return (errno != 0) ? errno : -1;
+    }
+#endif
+
+    /* for "fake" mapping, no need to do anything */
+    return 0;
+}
+
+/*
  * Release a memory mapping.
  */
 void sysReleaseShmem(MemMapping* pMap)
diff --git a/libdex/SysUtil.h b/libdex/SysUtil.h
index 8b80503..b300a7b 100644
--- a/libdex/SysUtil.h
+++ b/libdex/SysUtil.h
@@ -23,6 +23,17 @@
 #include <sys/types.h>
 
 /*
+ * System page size.  Normally you're expected to get this from
+ * sysconf(_SC_PAGESIZE) or some system-specific define (usually PAGESIZE
+ * or PAGE_SIZE).  If we use a simple #define the compiler can generate
+ * appropriate masks directly, so we define it here and verify it as the
+ * VM is starting up.
+ *
+ * Must be a power of 2.
+ */
+#define SYSTEM_PAGE_SIZE        4096
+
+/*
  * Use this to keep track of mapped segments.
  */
 typedef struct MemMapping {
@@ -55,10 +66,19 @@ int sysLoadFileInShmem(int fd, MemMapping* pMap);
  *
  * On success, "pMap" is filled in, and zero is returned.
  */
-int sysMapFileInShmem(int fd, MemMapping* pMap);
+int sysMapFileInShmemReadOnly(int fd, MemMapping* pMap);
 
 /*
- * Like sysMapFileInShmem, but on only part of a file.
+ * Map a file (from fd's current offset) into a shared, read-only memory
+ * segment that can be made writable.  (In some cases, such as when
+ * mapping a file on a FAT filesystem, the result may be fully writable.)
+ *
+ * On success, "pMap" is filled in, and zero is returned.
+ */
+int sysMapFileInShmemWritableReadOnly(int fd, MemMapping* pMap);
+
+/*
+ * Like sysMapFileInShmemReadOnly, but on only part of a file.
  */
 int sysMapFileSegmentInShmem(int fd, off_t start, long length,
     MemMapping* pMap);
@@ -71,6 +91,15 @@ int sysMapFileSegmentInShmem(int fd, off_t start, long length,
 int sysCreatePrivateMap(size_t length, MemMapping* pMap);
 
 /*
+ * Change the access rights on one or more pages.  If "wantReadWrite" is
+ * zero, the pages will be made read-only; otherwise they will be read-write.
+ *
+ * Returns 0 on success.
+ */
+int sysChangeMapAccess(void* addr, size_t length, int wantReadWrite,
+    MemMapping* pmap);
+
+/*
  * Release the pages associated with a shared memory segment.
  *
  * This does not free "pMap"; it just releases the memory.
diff --git a/libdex/ZipArchive.c b/libdex/ZipArchive.c
index 3f88e7d..7c7e18e 100644
--- a/libdex/ZipArchive.c
+++ b/libdex/ZipArchive.c
@@ -310,7 +310,7 @@ int dexZipPrepArchive(int fd, const char* debugFileName, ZipArchive* pArchive)
 
     pArchive->mFd = fd;
 
-    if (sysMapFileInShmem(pArchive->mFd, &map) != 0) {
+    if (sysMapFileInShmemReadOnly(pArchive->mFd, &map) != 0) {
         err = -1;
         LOGW("Map of '%s' failed\n", debugFileName);
         goto bail;
diff --git a/libnativehelper/JNIHelp.c b/libnativehelper/JNIHelp.c
index 748d8ff..e16843a 100644
--- a/libnativehelper/JNIHelp.c
+++ b/libnativehelper/JNIHelp.c
@@ -23,12 +23,12 @@ int jniRegisterNativeMethods(JNIEnv* env, const char* className,
     LOGV("Registering %s natives\n", className);
     clazz = (*env)->FindClass(env, className);
     if (clazz == NULL) {
-        LOGE("Native registration unable to find class '%s'\n", className);
-        return -1;
+        LOGW("Native registration unable to find class '%s'\n", className);
+        return 1;
     }
     if ((*env)->RegisterNatives(env, clazz, gMethods, numMethods) < 0) {
-        LOGE("RegisterNatives failed for '%s'\n", className);
-        return -1;
+        LOGW("RegisterNatives failed for '%s'\n", className);
+        return 1;
     }
     return 0;
 }
@@ -42,13 +42,13 @@ int jniThrowException(JNIEnv* env, const char* className, const char* msg)
 
     exceptionClass = (*env)->FindClass(env, className);
     if (exceptionClass == NULL) {
-        LOGE("Unable to find exception class %s\n", className);
+        LOGW("Unable to find exception class %s\n", className);
         assert(0);      /* fatal during dev; should always be fatal? */
-        return -1;
+        return 1;
     }
 
     if ((*env)->ThrowNew(env, exceptionClass, msg) != JNI_OK) {
-        LOGE("Failed throwing '%s' '%s'\n", className, msg);
+        LOGW("Failed throwing '%s' '%s'\n", className, msg);
         assert(!"failed to throw");
     }
     return 0;
diff --git a/vm/AllocTracker.c b/vm/AllocTracker.c
index 9649e68..9fb1c4d 100644
--- a/vm/AllocTracker.c
+++ b/vm/AllocTracker.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Allocation tracking and reporting.  We maintain a circular buffer with
  * the most recent allocations.  The data can be viewed through DDMS.
@@ -37,10 +38,12 @@
  *
  * TODO: consider making the parameters configurable, so DDMS can decide
  * how many allocations it wants to see and what the stack depth should be.
+ * Changing the window size is easy, changing the max stack depth is harder
+ * because we go from an array of fixed-size structs to variable-sized data.
  */
 #include "Dalvik.h"
 
-#define kMaxAllocRecordStackDepth   8       /* max 255 */
+#define kMaxAllocRecordStackDepth   16      /* max 255 */
 #define kNumAllocRecords            512     /* MUST be power of 2 */
 
 /*
@@ -108,8 +111,9 @@ bool dvmEnableAllocTracker(void)
     dvmLockMutex(&gDvm.allocTrackerLock);
 
     if (gDvm.allocRecords == NULL) {
-        LOGI("Enabling alloc tracker (%d entries / %d bytes)\n",
-            kNumAllocRecords, sizeof(AllocRecord) * kNumAllocRecords);
+        LOGI("Enabling alloc tracker (%d entries, %d frames --> %d bytes)\n",
+            kNumAllocRecords, kMaxAllocRecordStackDepth,
+            sizeof(AllocRecord) * kNumAllocRecords);
         gDvm.allocRecordHead = gDvm.allocRecordCount = 0;
         gDvm.allocRecords =
             (AllocRecord*) malloc(sizeof(AllocRecord) * kNumAllocRecords);
diff --git a/vm/Android.mk b/vm/Android.mk
index 09f5b1d..3cf641f 100644
--- a/vm/Android.mk
+++ b/vm/Android.mk
@@ -12,7 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 #
 # Android.mk for Dalvik VM.
 #
@@ -20,8 +19,11 @@
 # swath of common definitions are factored out into a separate file to
 # minimize duplication.
 #
-# Also, if you enable or disable optional features here (or Dvm.mk),
-# rebuild the VM with "make clean-libdvm && make -j4 libdvm".
+# If you enable or disable optional features here (or in Dvm.mk),
+# rebuild the VM with:
+#
+#  make clean-libdvm clean-libdvm_assert clean-libdvm_sv clean-libdvm_interp
+#  make -j4 libdvm
 #
 
 LOCAL_PATH:= $(call my-dir)
@@ -30,25 +32,56 @@ LOCAL_PATH:= $(call my-dir)
 # Build for the target (device).
 #
 
-include $(CLEAR_VARS)
+#ifeq ($(TARGET_ARCH_VARIANT),armv5te)
+#    WITH_JIT := false
+#endif
 
-# Variables used in the included Dvm.mk.
-dvm_os := $(TARGET_OS)
-dvm_arch := $(TARGET_ARCH)
-dvm_arch_variant := $(TARGET_ARCH_VARIANT)
-dvm_simulator := $(TARGET_SIMULATOR)
+# Build the installed version (libdvm.so) first
+include $(LOCAL_PATH)/ReconfigureDvm.mk
 
-DEBUG_DALVIK_VM := true
-include $(LOCAL_PATH)/Dvm.mk
+# Overwrite default settings
+ifneq ($(TARGET_ARCH),x86)
+ifeq ($(TARGET_SIMULATOR),false)
+    LOCAL_PRELINK_MODULE := true
+endif
+endif
+LOCAL_MODULE_TAGS := user
+LOCAL_MODULE := libdvm
+include $(BUILD_SHARED_LIBRARY)
 
-# liblog and libcutils are shared for target.
-LOCAL_SHARED_LIBRARIES += \
-	liblog libcutils
+# If WITH_JIT is configured, build multiple versions of libdvm.so to facilitate
+# correctness/performance bugs triage
+ifeq ($(WITH_JIT),true)
 
-LOCAL_MODULE := libdvm
+    # Derivation #1
+    # Enable assert and JIT tuning
+    include $(LOCAL_PATH)/ReconfigureDvm.mk
 
-include $(BUILD_SHARED_LIBRARY)
+    # Enable assertions and JIT-tuning
+    LOCAL_CFLAGS += -UNDEBUG -DDEBUG=1 -DLOG_NDEBUG=1 -DWITH_DALVIK_ASSERT \
+				    -DWITH_JIT_TUNING -DJIT_STATS
+    LOCAL_MODULE := libdvm_assert
+    include $(BUILD_SHARED_LIBRARY)
 
+    # Derivation #2
+    # Enable assert and self-verification
+    include $(LOCAL_PATH)/ReconfigureDvm.mk
+
+    # Enable assertions and JIT self-verification
+    LOCAL_CFLAGS += -UNDEBUG -DDEBUG=1 -DLOG_NDEBUG=1 -DWITH_DALVIK_ASSERT \
+					-DWITH_SELF_VERIFICATION
+    LOCAL_MODULE := libdvm_sv
+    include $(BUILD_SHARED_LIBRARY)
+
+    # Devivation #3
+    # Compile out the JIT
+    WITH_JIT := false
+    include $(LOCAL_PATH)/ReconfigureDvm.mk
+
+    LOCAL_MODULE := libdvm_interp
+    include $(BUILD_SHARED_LIBRARY)
+
+endif
 
 #
 # Build for the host.
@@ -63,21 +96,31 @@ ifeq ($(WITH_HOST_DALVIK),true)
     dvm_arch := $(HOST_ARCH)
     dvm_arch_variant := $(HOST_ARCH_VARIANT)
     dvm_simulator := false
-
+    DEBUG_DALVIK_VM := true
     include $(LOCAL_PATH)/Dvm.mk
 
-    # liblog and libcutils are static for host.
-    LOCAL_STATIC_LIBRARIES += \
-        liblog libcutils
-
-    # libffi is called libffi-host on the host. Similarly libnativehelper.
-    LOCAL_SHARED_LIBRARIES := \
-        $(patsubst libffi,libffi-host,$(LOCAL_SHARED_LIBRARIES))
-    LOCAL_SHARED_LIBRARIES := \
-        $(patsubst libnativehelper,libnativehelper-host,$(LOCAL_SHARED_LIBRARIES))
+    # We need to include all of these libraries. The end result of this
+    # section is a static library, but LOCAL_STATIC_LIBRARIES doesn't
+    # actually cause any code from the specified libraries to be included,
+    # whereas LOCAL_WHOLE_STATIC_LIBRARIES does. No, I (danfuzz) am not
+    # entirely sure what LOCAL_STATIC_LIBRARIES is even supposed to mean
+    # in this context, but it is in (apparently) meaningfully used in
+    # other parts of the build.
+    LOCAL_WHOLE_STATIC_LIBRARIES += \
+	libnativehelper-host libdex liblog libcutils
+
+    # The libffi from the source tree should never be used by host builds.
+    # The recommendation is that host builds should always either
+    # have sufficient custom code so that libffi isn't needed at all,
+    # or they should use the platform's provided libffi. So, if the common
+    # build rules decided to include it, axe it back out here.
+    ifneq (,$(findstring libffi,$(LOCAL_SHARED_LIBRARIES)))
+        LOCAL_SHARED_LIBRARIES := \
+            $(patsubst libffi, ,$(LOCAL_SHARED_LIBRARIES))
+    endif
 
     LOCAL_MODULE := libdvm-host
 
-    include $(BUILD_HOST_SHARED_LIBRARY)
+    include $(BUILD_HOST_STATIC_LIBRARY)
 
 endif
diff --git a/vm/CheckJni.c b/vm/CheckJni.c
index bc90527..2b7a61a 100644
--- a/vm/CheckJni.c
+++ b/vm/CheckJni.c
@@ -60,7 +60,7 @@ static void abortMaybe(void);       // fwd
  *
  * At this point, pResult->l has already been converted to an object pointer.
  */
-static void checkCallCommon(const u4* args, JValue* pResult,
+static void checkCallResultCommon(const u4* args, JValue* pResult,
     const Method* method, Thread* self)
 {
     assert(pResult->l != NULL);
@@ -85,12 +85,13 @@ static void checkCallCommon(const u4* args, JValue* pResult,
          *
          * Since we're returning an instance of declType, it's safe to
          * assume that it has been loaded and initialized (or, for the case
-         * of an array, generated), so we can just look for it in the
-         * loaded-classes list.
+         * of an array, generated).  However, the current class loader may
+         * not be listed as an initiating loader, so we can't just look for
+         * it in the loaded-classes list.
          */
         ClassObject* declClazz;
 
-        declClazz = dvmLookupClass(declType, method->clazz->classLoader, false);
+        declClazz = dvmFindClassNoInit(declType, method->clazz->classLoader);
         if (declClazz == NULL) {
             LOGW("JNI WARNING: method declared to return '%s' returned '%s'\n",
                 declType, objType);
@@ -116,7 +117,7 @@ static void checkCallCommon(const u4* args, JValue* pResult,
 /*
  * Determine if we need to check the return type coming out of the call.
  *
- * (We don't do this at the top of checkCallCommon() because this is on
+ * (We don't do this at the top of checkCallResultCommon() because this is on
  * the critical path for native method calls.)
  */
 static inline bool callNeedsCheck(const u4* args, JValue* pResult,
@@ -134,7 +135,7 @@ void dvmCheckCallJNIMethod_general(const u4* args, JValue* pResult,
 {
     dvmCallJNIMethod_general(args, pResult, method, self);
     if (callNeedsCheck(args, pResult, method, self))
-        checkCallCommon(args, pResult, method, self);
+        checkCallResultCommon(args, pResult, method, self);
 }
 
 /*
@@ -145,7 +146,7 @@ void dvmCheckCallJNIMethod_synchronized(const u4* args, JValue* pResult,
 {
     dvmCallJNIMethod_synchronized(args, pResult, method, self);
     if (callNeedsCheck(args, pResult, method, self))
-        checkCallCommon(args, pResult, method, self);
+        checkCallResultCommon(args, pResult, method, self);
 }
 
 /*
@@ -156,7 +157,7 @@ void dvmCheckCallJNIMethod_virtualNoRef(const u4* args, JValue* pResult,
 {
     dvmCallJNIMethod_virtualNoRef(args, pResult, method, self);
     if (callNeedsCheck(args, pResult, method, self))
-        checkCallCommon(args, pResult, method, self);
+        checkCallResultCommon(args, pResult, method, self);
 }
 
 /*
@@ -167,7 +168,7 @@ void dvmCheckCallJNIMethod_staticNoRef(const u4* args, JValue* pResult,
 {
     dvmCallJNIMethod_staticNoRef(args, pResult, method, self);
     if (callNeedsCheck(args, pResult, method, self))
-        checkCallCommon(args, pResult, method, self);
+        checkCallResultCommon(args, pResult, method, self);
 }
 
 
@@ -248,9 +249,12 @@ void dvmCheckCallJNIMethod_staticNoRef(const u4* args, JValue* pResult,
     checkLengthPositive(_env, _length, __FUNCTION__)
 #define CHECK_NON_NULL(_env, _ptr)                                          \
     checkNonNull(_env, _ptr, __FUNCTION__)
-
 #define CHECK_SIG(_env, _methid, _sigbyte, _isstatic)                       \
     checkSig(_env, _methid, _sigbyte, _isstatic, __FUNCTION__)
+#define CHECK_METHOD_ARGS_A(_env, _methid, _args)                           \
+    checkMethodArgsA(_env, _methid, _args, __FUNCTION__)
+#define CHECK_METHOD_ARGS_V(_env, _methid, _args)                           \
+    checkMethodArgsV(_env, _methid, _args, __FUNCTION__)
 
 /*
  * Print trace message when both "checkJNI" and "verbose:jni" are enabled.
@@ -290,7 +294,7 @@ static void showLocation(const Method* meth, const char* func)
 static void abortMaybe(void)
 {
     JavaVMExt* vm = (JavaVMExt*) gDvm.vmList;
-    if (vm->warnError) {
+    if (/*vm->warnError*/false) {
         dvmDumpThread(dvmThreadSelf(), false);
         dvmAbort();
     }
@@ -460,9 +464,9 @@ static void checkFieldType(JNIEnv* env, jobject jobj, jfieldID fieldID,
  * Verify that "jobj" is a valid object, and that it's an object that JNI
  * is allowed to know about.  We allow NULL references.
  *
- * Switches to "running" mode before performing checks.
+ * Must be in "running" mode before calling here.
  */
-static void checkObject(JNIEnv* env, jobject jobj, const char* func)
+static void checkObject0(JNIEnv* env, jobject jobj, const char* func)
 {
     UNUSED_PARAMETER(env);
     bool printWarn = false;
@@ -470,7 +474,13 @@ static void checkObject(JNIEnv* env, jobject jobj, const char* func)
     if (jobj == NULL)
         return;
 
-    JNI_ENTER();
+    if (dvmIsWeakGlobalRef(jobj)) {
+        /*
+         * Normalize and continue.  This will tell us if the PhantomReference
+         * object is valid.
+         */
+        jobj = dvmNormalizeWeakGlobalRef((jweak) jobj);
+    }
 
     if (dvmGetJNIRefType(env, jobj) == JNIInvalidRefType) {
         LOGW("JNI WARNING: %p is not a valid JNI reference\n", jobj);
@@ -489,7 +499,18 @@ static void checkObject(JNIEnv* env, jobject jobj, const char* func)
         showLocation(dvmGetCurrentJNIMethod(), func);
         abortMaybe();
     }
+}
 
+/*
+ * Verify that "jobj" is a valid object, and that it's an object that JNI
+ * is allowed to know about.  We allow NULL references.
+ *
+ * Switches to "running" mode before performing checks.
+ */
+static void checkObject(JNIEnv* env, jobject jobj, const char* func)
+{
+    JNI_ENTER();
+    checkObject0(env, jobj, func);
     JNI_EXIT();
 }
 
@@ -662,10 +683,12 @@ static void checkClassName(JNIEnv* env, const char* className, const char* func)
     /* quick check for illegal chars */
     cp = className;
     while (*cp != '\0') {
-        if (*cp == '.')
+        if (*cp == '.')     /* catch "java.lang.String" */
             goto fail;
         cp++;
     }
+    if (*(cp-1) == ';' && *className == 'L')
+        goto fail;         /* catch "Ljava/lang/String;" */
 
     // TODO: need a more rigorous check here
 
@@ -673,6 +696,7 @@ static void checkClassName(JNIEnv* env, const char* className, const char* func)
 
 fail:
     LOGW("JNI WARNING: illegal class name '%s' (%s)\n", className, func);
+    LOGW("             (should be formed like 'java/lang/String')\n");
     abortMaybe();
 }
 
@@ -832,6 +856,87 @@ bail:
     JNI_EXIT();
 }
 
+/*
+ * Verify that the reference arguments being passed in are appropriate for
+ * this method.
+ *
+ * At a minimum we want to make sure that the argument is a valid
+ * reference.  We can also do a class lookup on the method signature
+ * and verify that the object is an instance of the appropriate class,
+ * but that's more expensive.
+ *
+ * The basic tests are redundant when indirect references are enabled,
+ * since reference arguments must always be converted explicitly.  An
+ * instanceof test would not be redundant, but we're not doing that at
+ * this time.
+ */
+static void checkMethodArgsV(JNIEnv* env, jmethodID methodID, va_list args,
+    const char* func)
+{
+#ifndef USE_INDIRECT_REF
+    JNI_ENTER();
+
+    const Method* meth = (const Method*) methodID;
+    const char* desc = meth->shorty;
+    ClassObject* clazz;
+
+    LOGV("V-checking %s.%s:%s...\n", meth->clazz->descriptor, meth->name, desc);
+
+    while (*++desc != '\0') {       /* pre-incr to skip return type */
+        switch (*desc) {
+        case 'L':
+            {     /* 'shorty' descr uses L for all refs, incl array */
+                jobject argObj = va_arg(args, jobject);
+                checkObject0(env, argObj, func);
+            }
+            break;
+        case 'D':       /* 8-byte double */
+        case 'J':       /* 8-byte long */
+        case 'F':       /* floats normalized to doubles */
+            (void) va_arg(args, u8);
+            break;
+        default:        /* Z B C S I -- all passed as 32-bit integers */
+            (void) va_arg(args, u4);
+            break;
+        }
+    }
+
+bail:
+    JNI_EXIT();
+#endif
+}
+
+/*
+ * Same purpose as checkMethodArgsV, but with arguments in an array of
+ * jvalue structs.
+ */
+static void checkMethodArgsA(JNIEnv* env, jmethodID methodID, jvalue* args,
+    const char* func)
+{
+#ifndef USE_INDIRECT_REF
+    JNI_ENTER();
+
+    const Method* meth = (const Method*) methodID;
+    const char* desc = meth->shorty;
+    ClassObject* clazz;
+    int idx = 0;
+
+    LOGV("A-checking %s.%s:%s...\n", meth->clazz->descriptor, meth->name, desc);
+
+    while (*++desc != '\0') {       /* pre-incr to skip return type */
+        if (*desc == 'L') {
+            jobject argObj = args[idx].l;
+            checkObject0(env, argObj, func);
+        }
+
+        idx++;
+    }
+
+bail:
+    JNI_EXIT();
+#endif
+}
+
 
 /*
  * ===========================================================================
@@ -1384,9 +1489,14 @@ static jobject Check_NewObject(JNIEnv* env, jclass clazz, jmethodID methodID,
     CHECK_ENTER(env, kFlag_Default);
     CHECK_CLASS(env, clazz);
     jobject result;
-    va_list args;
+    va_list args, tmpArgs;
 
     va_start(args, methodID);
+
+    va_copy(tmpArgs, args);
+    CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);
+    va_end(tmpArgs);
+
     result = BASE_ENV(env)->NewObjectV(env, clazz, methodID, args);
     va_end(args);
 
@@ -1399,6 +1509,12 @@ static jobject Check_NewObjectV(JNIEnv* env, jclass clazz, jmethodID methodID,
     CHECK_ENTER(env, kFlag_Default);
     CHECK_CLASS(env, clazz);
     jobject result;
+
+    va_list tmpArgs;
+    va_copy(tmpArgs, args);
+    CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);
+    va_end(tmpArgs);
+
     result = BASE_ENV(env)->NewObjectV(env, clazz, methodID, args);
     CHECK_EXIT(env);
     return result;
@@ -1409,6 +1525,8 @@ static jobject Check_NewObjectA(JNIEnv* env, jclass clazz, jmethodID methodID,
     CHECK_ENTER(env, kFlag_Default);
     CHECK_CLASS(env, clazz);
     jobject result;
+
+    CHECK_METHOD_ARGS_A(env, methodID, args);
     result = BASE_ENV(env)->NewObjectA(env, clazz, methodID, args);
     CHECK_EXIT(env);
     return result;
@@ -1585,8 +1703,11 @@ SET_TYPE_FIELD(jdouble, Double, PRIM_DOUBLE);
         CHECK_OBJECT(env, obj);                                             \
         CHECK_SIG(env, methodID, _retsig, false);                           \
         _retdecl;                                                           \
-        va_list args;                                                       \
+        va_list args, tmpArgs;                                              \
         va_start(args, methodID);                                           \
+        va_copy(tmpArgs, args);                                             \
+        CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);                        \
+        va_end(tmpArgs);                                                    \
         _retasgn BASE_ENV(env)->Call##_jname##MethodV(env, obj, methodID,   \
             args);                                                          \
         va_end(args);                                                       \
@@ -1600,6 +1721,10 @@ SET_TYPE_FIELD(jdouble, Double, PRIM_DOUBLE);
         CHECK_OBJECT(env, obj);                                             \
         CHECK_SIG(env, methodID, _retsig, false);                           \
         _retdecl;                                                           \
+        va_list tmpArgs;                                                    \
+        va_copy(tmpArgs, args);                                             \
+        CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);                        \
+        va_end(tmpArgs);                                                    \
         _retasgn BASE_ENV(env)->Call##_jname##MethodV(env, obj, methodID,   \
             args);                                                          \
         CHECK_EXIT(env);                                                    \
@@ -1612,6 +1737,7 @@ SET_TYPE_FIELD(jdouble, Double, PRIM_DOUBLE);
         CHECK_OBJECT(env, obj);                                             \
         CHECK_SIG(env, methodID, _retsig, false);                           \
         _retdecl;                                                           \
+        CHECK_METHOD_ARGS_A(env, methodID, args);                           \
         _retasgn BASE_ENV(env)->Call##_jname##MethodA(env, obj, methodID,   \
             args);                                                          \
         CHECK_EXIT(env);                                                    \
@@ -1638,8 +1764,11 @@ CALL_VIRTUAL(void, Void, , , , 'V');
         CHECK_OBJECT(env, obj);                                             \
         CHECK_SIG(env, methodID, _retsig, false);                           \
         _retdecl;                                                           \
-        va_list args;                                                       \
+        va_list args, tmpArgs;                                              \
         va_start(args, methodID);                                           \
+        va_copy(tmpArgs, args);                                             \
+        CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);                        \
+        va_end(tmpArgs);                                                    \
         _retasgn BASE_ENV(env)->CallNonvirtual##_jname##MethodV(env, obj,   \
             clazz, methodID, args);                                         \
         va_end(args);                                                       \
@@ -1654,6 +1783,10 @@ CALL_VIRTUAL(void, Void, , , , 'V');
         CHECK_OBJECT(env, obj);                                             \
         CHECK_SIG(env, methodID, _retsig, false);                           \
         _retdecl;                                                           \
+        va_list tmpArgs;                                                    \
+        va_copy(tmpArgs, args);                                             \
+        CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);                        \
+        va_end(tmpArgs);                                                    \
         _retasgn BASE_ENV(env)->CallNonvirtual##_jname##MethodV(env, obj,   \
             clazz, methodID, args);                                         \
         CHECK_EXIT(env);                                                    \
@@ -1667,6 +1800,7 @@ CALL_VIRTUAL(void, Void, , , , 'V');
         CHECK_OBJECT(env, obj);                                             \
         CHECK_SIG(env, methodID, _retsig, false);                           \
         _retdecl;                                                           \
+        CHECK_METHOD_ARGS_A(env, methodID, args);                           \
         _retasgn BASE_ENV(env)->CallNonvirtual##_jname##MethodA(env, obj,   \
             clazz, methodID, args);                                         \
         CHECK_EXIT(env);                                                    \
@@ -1692,8 +1826,11 @@ CALL_NONVIRTUAL(void, Void, , , , 'V');
         CHECK_CLASS(env, clazz);                                            \
         CHECK_SIG(env, methodID, _retsig, true);                            \
         _retdecl;                                                           \
-        va_list args;                                                       \
+        va_list args, tmpArgs;                                              \
         va_start(args, methodID);                                           \
+        va_copy(tmpArgs, args);                                             \
+        CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);                        \
+        va_end(tmpArgs);                                                    \
         _retasgn BASE_ENV(env)->CallStatic##_jname##MethodV(env, clazz,     \
             methodID, args);                                                \
         va_end(args);                                                       \
@@ -1707,6 +1844,10 @@ CALL_NONVIRTUAL(void, Void, , , , 'V');
         CHECK_CLASS(env, clazz);                                            \
         CHECK_SIG(env, methodID, _retsig, true);                            \
         _retdecl;                                                           \
+        va_list tmpArgs;                                                    \
+        va_copy(tmpArgs, args);                                             \
+        CHECK_METHOD_ARGS_V(env, methodID, tmpArgs);                        \
+        va_end(tmpArgs);                                                    \
         _retasgn BASE_ENV(env)->CallStatic##_jname##MethodV(env, clazz,     \
             methodID, args);                                                \
         CHECK_EXIT(env);                                                    \
@@ -1719,6 +1860,7 @@ CALL_NONVIRTUAL(void, Void, , , , 'V');
         CHECK_CLASS(env, clazz);                                            \
         CHECK_SIG(env, methodID, _retsig, true);                            \
         _retdecl;                                                           \
+        CHECK_METHOD_ARGS_A(env, methodID, args);                           \
         _retasgn BASE_ENV(env)->CallStatic##_jname##MethodA(env, clazz,     \
             methodID, args);                                                \
         CHECK_EXIT(env);                                                    \
diff --git a/vm/DalvikVersion.h b/vm/DalvikVersion.h
index efbb393..2e00ac4 100644
--- a/vm/DalvikVersion.h
+++ b/vm/DalvikVersion.h
@@ -24,7 +24,7 @@
  * The version we show to tourists.
  */
 #define DALVIK_MAJOR_VERSION    1
-#define DALVIK_MINOR_VERSION    1
+#define DALVIK_MINOR_VERSION    2
 #define DALVIK_BUG_VERSION      0
 
 /*
@@ -32,6 +32,6 @@
  * way classes load changes, e.g. field ordering or vtable layout.  Changing
  * this guarantees that the optimized form of the DEX file is regenerated.
  */
-#define DALVIK_VM_BUILD         17
+#define DALVIK_VM_BUILD         19
 
 #endif /*_DALVIK_VERSION*/
diff --git a/vm/Debugger.c b/vm/Debugger.c
index 4ddf25c..2f57046 100644
--- a/vm/Debugger.c
+++ b/vm/Debugger.c
@@ -97,6 +97,9 @@ same time.
  */
 bool dvmDebuggerStartup(void)
 {
+    if (!dvmBreakpointStartup())
+        return false;
+
     gDvm.dbgRegistry = dvmHashTableCreate(1000, NULL);
     return (gDvm.dbgRegistry != NULL);
 }
@@ -108,6 +111,7 @@ void dvmDebuggerShutdown(void)
 {
     dvmHashTableFree(gDvm.dbgRegistry);
     gDvm.dbgRegistry = NULL;
+    dvmBreakpointShutdown();
 }
 
 
@@ -317,6 +321,20 @@ static Object* objectIdToObject(ObjectId id)
 }
 
 /*
+ * Register an object ID that might not have been registered previously.
+ *
+ * Normally this wouldn't happen -- the conversion to an ObjectId would
+ * have added the object to the registry -- but in some cases (e.g.
+ * throwing exceptions) we really want to do the registration late.
+ */
+void dvmDbgRegisterObjectId(ObjectId id)
+{
+    Object* obj = (Object*)(u4) id;
+    LOGV("+++ registering %p (%s)\n", obj, obj->clazz->descriptor);
+    registerObject(obj, kObjectId, true);
+}
+
+/*
  * Convert to/from a MethodId.
  *
  * These IDs are only guaranteed unique within a class, so they could be
@@ -400,6 +418,9 @@ void dvmDbgActive(void)
     LOGI("Debugger is active\n");
     dvmInitBreakpoints();
     gDvm.debuggerActive = true;
+#if defined(WITH_JIT)
+    dvmCompilerStateRefresh();
+#endif
 }
 
 /*
@@ -419,7 +440,7 @@ void dvmDbgDisconnected(void)
     dvmHashTableLock(gDvm.dbgRegistry);
     gDvm.debuggerConnected = false;
 
-    LOGI("Debugger has detached; object registry had %d entries\n",
+    LOGD("Debugger has detached; object registry had %d entries\n",
         dvmHashTableNumEntries(gDvm.dbgRegistry));
     //int i;
     //for (i = 0; i < gDvm.dbgRegistryNext; i++)
@@ -427,6 +448,9 @@ void dvmDbgDisconnected(void)
 
     dvmHashTableClear(gDvm.dbgRegistry);
     dvmHashTableUnlock(gDvm.dbgRegistry);
+#if defined(WITH_JIT)
+    dvmCompilerStateRefresh();
+#endif
 }
 
 /*
@@ -512,6 +536,15 @@ const char* dvmDbgGetClassDescriptor(RefTypeId id)
 }
 
 /*
+ * Convert a RefTypeId to an ObjectId.
+ */
+ObjectId dvmDbgGetClassObject(RefTypeId id)
+{
+    ClassObject* clazz = refTypeIdToClassObject(id);
+    return objectToObjectId((Object*) clazz);
+}
+
+/*
  * Return the superclass of a class (will be NULL for java/lang/Object).
  */
 RefTypeId dvmDbgGetSuperclass(RefTypeId id)
@@ -1117,6 +1150,19 @@ ObjectId dvmDbgCreateString(const char* str)
 }
 
 /*
+ * Allocate a new object of the specified type.
+ *
+ * Add it to the registry to prevent it from being GCed.
+ */
+ObjectId dvmDbgCreateObject(RefTypeId classId)
+{
+    ClassObject* clazz = refTypeIdToClassObject(classId);
+    Object* newObj = dvmAllocObject(clazz, ALLOC_DEFAULT);
+    dvmReleaseTrackedAlloc(newObj, NULL);
+    return objectToObjectId(newObj);
+}
+
+/*
  * Determine if "instClassId" is an instance of "classId".
  */
 bool dvmDbgMatchType(RefTypeId instClassId, RefTypeId classId)
@@ -2470,7 +2516,7 @@ void dvmDbgPostLocationEvent(const Method* method, int pcOffset,
 
     /*
      * Note we use "NoReg" so we don't keep track of references that are
-     * never actually sent to the debugger.  The "thisPtr" is used to
+     * never actually sent to the debugger.  The "thisPtr" is only used to
      * compare against registered events.
      */
 
@@ -2517,7 +2563,17 @@ void dvmDbgPostException(void* throwFp, int throwRelPc, void* catchFp,
     /* need this for InstanceOnly filters */
     Object* thisObj = getThisObject(throwFp);
 
-    dvmJdwpPostException(gDvm.jdwpState, &throwLoc, objectToObjectId(exception),
+    /*
+     * Hand the event to the JDWP exception handler.  Note we're using the
+     * "NoReg" objectID on the exception, which is not strictly correct --
+     * the exception object WILL be passed up to the debugger if the
+     * debugger is interested in the event.  We do this because the current
+     * implementation of the debugger object registry never throws anything
+     * away, and some people were experiencing a fatal build up of exception
+     * objects when dealing with certain libraries.
+     */
+    dvmJdwpPostException(gDvm.jdwpState, &throwLoc,
+        objectToObjectIdNoReg(exception),
         classObjectToRefTypeId(exception->clazz), &catchLoc,
         objectToObjectId(thisObj));
 }
@@ -2676,6 +2732,30 @@ JdwpError dvmDbgInvokeMethod(ObjectId threadId, ObjectId objectId,
     }
 
     /*
+     * We currently have a bug where we don't successfully resume the
+     * target thread if the suspend count is too deep.  We're expected to
+     * require one "resume" for each "suspend", but when asked to execute
+     * a method we have to resume fully and then re-suspend it back to the
+     * same level.  (The easiest way to cause this is to type "suspend"
+     * multiple times in jdb.)
+     *
+     * It's unclear what this means when the event specifies "resume all"
+     * and some threads are suspended more deeply than others.  This is
+     * a rare problem, so for now we just prevent it from hanging forever
+     * by rejecting the method invocation request.  Without this, we will
+     * be stuck waiting on a suspended thread.
+     */
+    if (targetThread->suspendCount > 1) {
+        LOGW("threadid=%d: suspend count on threadid=%d is %d, too deep "
+             "for method exec\n",
+            dvmThreadSelf()->threadId, targetThread->threadId,
+            targetThread->suspendCount);
+        err = ERR_THREAD_SUSPENDED;     /* probably not expected here */
+        dvmUnlockThreadList();
+        goto bail;
+    }
+
+    /*
      * TODO: ought to screen the various IDs, and verify that the argument
      * list is valid.
      */
@@ -2796,9 +2876,11 @@ void dvmDbgExecuteMethod(DebugInvokeReq* pReq)
 
     /*
      * Translate the method through the vtable, unless we're calling a
-     * static method or the debugger wants to suppress it.
+     * direct method or the debugger wants to suppress it.
      */
-    if ((pReq->options & INVOKE_NONVIRTUAL) != 0 || pReq->obj == NULL) {
+    if ((pReq->options & INVOKE_NONVIRTUAL) != 0 || pReq->obj == NULL ||
+        dvmIsDirectMethod(pReq->method))
+    {
         meth = pReq->method;
     } else {
         meth = dvmGetVirtualizedMethod(pReq->clazz, pReq->method);
@@ -2809,8 +2891,8 @@ void dvmDbgExecuteMethod(DebugInvokeReq* pReq)
 
     IF_LOGV() {
         char* desc = dexProtoCopyMethodDescriptor(&meth->prototype);
-        LOGV("JDWP invoking method %s.%s %s\n",
-            meth->clazz->descriptor, meth->name, desc);
+        LOGV("JDWP invoking method %p/%p %s.%s:%s\n",
+            pReq->method, meth, meth->clazz->descriptor, meth->name, desc);
         free(desc);
     }
 
@@ -2819,8 +2901,10 @@ void dvmDbgExecuteMethod(DebugInvokeReq* pReq)
     pReq->exceptObj = objectToObjectId(dvmGetException(self));
     pReq->resultTag = resultTagFromSignature(meth);
     if (pReq->exceptObj != 0) {
-        LOGD("  JDWP invocation returning with exceptObj=%p\n",
-            dvmGetException(self));
+        Object* exc = dvmGetException(self);
+        LOGD("  JDWP invocation returning with exceptObj=%p (%s)\n",
+            exc, exc->clazz->descriptor);
+        //dvmLogExceptionStackTrace();
         dvmClearException(self);
         /*
          * Nothing should try to use this, but it looks like something is.
@@ -2946,14 +3030,24 @@ void dvmDbgDdmDisconnected(void)
 /*
  * Send up a JDWP event packet with a DDM chunk in it.
  */
-void dvmDbgDdmSendChunk(int type, int len, const u1* buf)
+void dvmDbgDdmSendChunk(int type, size_t len, const u1* buf)
+{
+    assert(buf != NULL);
+    struct iovec vec[1] = { {(void*)buf, len} };
+    dvmDbgDdmSendChunkV(type, vec, 1);
+}
+
+/*
+ * Send up a JDWP event packet with a DDM chunk in it.  The chunk is
+ * concatenated from multiple source buffers.
+ */
+void dvmDbgDdmSendChunkV(int type, const struct iovec* iov, int iovcnt)
 {
     if (gDvm.jdwpState == NULL) {
-        LOGI("Debugger thread not active, ignoring DDM send (t=0x%08x l=%d)\n",
+        LOGV("Debugger thread not active, ignoring DDM send (t=0x%08x l=%d)\n",
             type, len);
         return;
     }
 
-    dvmJdwpDdmSendChunk(gDvm.jdwpState, type, len, buf);
+    dvmJdwpDdmSendChunkV(gDvm.jdwpState, type, iov, iovcnt);
 }
-
diff --git a/vm/Debugger.h b/vm/Debugger.h
index fcf07c7..04477fb 100644
--- a/vm/Debugger.h
+++ b/vm/Debugger.h
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Dalvik-specific side of debugger support.  (The JDWP code is intended to
  * be relatively generic.)
@@ -32,7 +33,7 @@ struct Method;
 struct Thread;
 
 /*
- * used by StepControl to track a set of addresses associated with
+ * Used by StepControl to track a set of addresses associated with
  * a single line.
  */
 typedef struct AddressSet {
@@ -159,6 +160,7 @@ void dvmDbgExit(int status);
  * Class, Object, Array
  */
 const char* dvmDbgGetClassDescriptor(RefTypeId id);
+ObjectId dvmDbgGetClassObject(RefTypeId id);
 RefTypeId dvmDbgGetSuperclass(RefTypeId id);
 ObjectId dvmDbgGetClassLoader(RefTypeId id);
 u4 dvmDbgGetAccessFlags(RefTypeId id);
@@ -188,6 +190,7 @@ bool dvmDbgSetArrayElements(ObjectId arrayId, int firstIndex, int count,
     const u1* buf);
 
 ObjectId dvmDbgCreateString(const char* str);
+ObjectId dvmDbgCreateObject(RefTypeId classId);
 
 bool dvmDbgMatchType(RefTypeId instClassId, RefTypeId classId);
 
@@ -288,6 +291,9 @@ void dvmDbgExecuteMethod(DebugInvokeReq* pReq);
 /* Make an AddressSet for a line, for single stepping */
 const AddressSet *dvmAddressSetForLine(const struct Method* method, int line);
 
+/* perform "late registration" of an object ID */
+void dvmDbgRegisterObjectId(ObjectId id);
+
 /*
  * DDM support.
  */
@@ -295,7 +301,8 @@ bool dvmDbgDdmHandlePacket(const u1* buf, int dataLen, u1** pReplyBuf,
     int* pReplyLen);
 void dvmDbgDdmConnected(void);
 void dvmDbgDdmDisconnected(void);
-void dvmDbgDdmSendChunk(int type, int len, const u1* buf);
+void dvmDbgDdmSendChunk(int type, size_t len, const u1* buf);
+void dvmDbgDdmSendChunkV(int type, const struct iovec* iov, int iovcnt);
 
 #define CHUNK_TYPE(_name) \
     ((_name)[0] << 24 | (_name)[1] << 16 | (_name)[2] << 8 | (_name)[3])
diff --git a/vm/Dvm.mk b/vm/Dvm.mk
index 96f6dac..baf41c6 100644
--- a/vm/Dvm.mk
+++ b/vm/Dvm.mk
@@ -15,8 +15,9 @@
 #
 # Common definitions for host or target builds of libdvm.
 #
-# If you enable or disable optional features here,
-# rebuild the VM with "make clean-libdvm && make -j4 libdvm".
+# If you enable or disable optional features here, make sure you do
+# a "clean" build -- not everything depends on Dalvik.h.  (See Android.mk
+# for the exact command.)
 #
 
 
@@ -122,6 +123,7 @@ LOCAL_SRC_FILES := \
 	SignalCatcher.c \
 	StdioConverter.c \
 	Sync.c \
+	TestCompability.c \
 	Thread.c \
 	UtfString.c \
 	alloc/clz.c.arm \
@@ -194,11 +196,7 @@ LOCAL_SRC_FILES := \
 	test/TestHash.c \
 	test/TestIndirectRefTable.c
 
-ifeq ($(WITH_JIT_TUNING),true)
-  LOCAL_CFLAGS += -DWITH_JIT_TUNING
-  # NOTE: Turn on assertion for JIT for now
-  LOCAL_CFLAGS += -UNDEBUG -DDEBUG=1 -DLOG_NDEBUG=1 -DWITH_DALVIK_ASSERT
-endif
+WITH_JIT := $(strip $(WITH_JIT))
 
 ifeq ($(WITH_JIT),true)
   LOCAL_CFLAGS += -DWITH_JIT
@@ -208,6 +206,9 @@ ifeq ($(WITH_JIT),true)
 	compiler/Frontend.c \
 	compiler/Utility.c \
 	compiler/IntermediateRep.c \
+	compiler/Dataflow.c \
+	compiler/Loop.c \
+	compiler/Ralloc.c \
 	interp/Jit.c
 endif
 
@@ -276,7 +277,9 @@ ifeq ($(dvm_arch),arm)
 
   ifeq ($(WITH_JIT),true)
     LOCAL_SRC_FILES += \
-		compiler/codegen/arm/Codegen-$(dvm_arch_variant).c \
+		compiler/codegen/arm/RallocUtil.c \
+		compiler/codegen/arm/$(dvm_arch_variant)/Codegen.c \
+		compiler/codegen/arm/$(dvm_arch_variant)/CallingConvention.S \
 		compiler/codegen/arm/Assemble.c \
 		compiler/codegen/arm/ArchUtility.c \
 		compiler/codegen/arm/LocalOptimizations.c \
@@ -289,10 +292,10 @@ ifeq ($(dvm_arch),x86)
   ifeq ($(dvm_os),linux)
     MTERP_ARCH_KNOWN := true
     LOCAL_SRC_FILES += \
-		arch/x86/Call386ABI.S \
-		arch/x86/Hints386ABI.c \
-		mterp/out/InterpC-x86.c \
-		mterp/out/InterpAsm-x86.S
+		arch/$(dvm_arch_variant)/Call386ABI.S \
+		arch/$(dvm_arch_variant)/Hints386ABI.c \
+		mterp/out/InterpC-$(dvm_arch_variant).c \
+		mterp/out/InterpAsm-$(dvm_arch_variant).S
   endif
 endif
 
@@ -308,7 +311,13 @@ endif
 ifeq ($(MTERP_ARCH_KNOWN),false)
   # unknown architecture, try to use FFI
   LOCAL_C_INCLUDES += external/libffi/$(dvm_os)-$(dvm_arch)
-  LOCAL_SHARED_LIBRARIES += libffi
+
+  ifeq ($(dvm_os)-$(dvm_arch),darwin-x86)
+      # OSX includes libffi, so just make the linker aware of it directly.
+      LOCAL_LDLIBS += -lffi
+  else
+      LOCAL_SHARED_LIBRARIES += libffi
+  endif
 
   LOCAL_SRC_FILES += \
 		arch/generic/Call.c \
@@ -323,9 +332,6 @@ ifeq ($(MTERP_ARCH_KNOWN),false)
 	-DdvmAsmSisterStart=0 -DdvmAsmSisterEnd=0 -DDVM_NO_ASM_INTERP=1
 endif
 
-LOCAL_SHARED_LIBRARIES += \
-	libnativehelper \
-	libz
-
-LOCAL_STATIC_LIBRARIES += \
-	libdex
+ifeq ($(TEST_VM_IN_ECLAIR),true)
+  LOCAL_CFLAGS += -DTEST_VM_IN_ECLAIR
+endif
diff --git a/vm/DvmDex.c b/vm/DvmDex.c
index 6740632..258d768 100644
--- a/vm/DvmDex.c
+++ b/vm/DvmDex.c
@@ -13,11 +13,13 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * VM-specific state associated with a DEX file.
  */
 #include "Dalvik.h"
 
+
 /*
  * Create auxillary data structures.
  *
@@ -128,7 +130,7 @@ int dvmDexFileOpenFromFd(int fd, DvmDex** ppDvmDex)
         goto bail;
     }
 
-    if (sysMapFileInShmem(fd, &memMap) != 0) {
+    if (sysMapFileInShmemWritableReadOnly(fd, &memMap) != 0) {
         LOGE("Unable to map file\n");
         goto bail;
     }
@@ -217,3 +219,73 @@ void dvmDexFileFree(DvmDex* pDvmDex)
     free(pDvmDex);
 }
 
+
+/*
+ * Change the byte at the specified address to a new value.  If the location
+ * already has the new value, do nothing.
+ *
+ * This requires changing the access permissions to read-write, updating
+ * the value, and then resetting the permissions.
+ *
+ * This does not make any synchronization guarantees.  It's important for the
+ * caller(s) to work out mutual exclusion, at least on a page granularity,
+ * to avoid a race where one threads sets read-write, another thread sets
+ * read-only, and then the first thread does a write.
+ *
+ * TODO: if we're back to the original state of the page, use
+ * madvise(MADV_DONTNEED) to release the private/dirty copy.
+ *
+ * Returns "true" on success.
+ */
+bool dvmDexChangeDex1(DvmDex* pDvmDex, u1* addr, u1 newVal)
+{
+    if (*addr == newVal) {
+        LOGV("+++ byte at %p is already 0x%02x\n", addr, newVal);
+        return true;
+    }
+
+    LOGV("+++ change byte at %p from 0x%02x to 0x%02x\n", addr, *addr, newVal);
+    if (sysChangeMapAccess(addr, 1, true, &pDvmDex->memMap) != 0) {
+        LOGD("NOTE: DEX page access change (->RW) failed\n");
+        /* expected on files mounted from FAT; keep going (may crash) */
+    }
+
+    *addr = newVal;
+
+    if (sysChangeMapAccess(addr, 1, false, &pDvmDex->memMap) != 0) {
+        LOGD("NOTE: DEX page access change (->RO) failed\n");
+        /* expected on files mounted from FAT; keep going */
+    }
+
+    return true;
+}
+
+/*
+ * Change the 2-byte value at the specified address to a new value.  If the
+ * location already has the new value, do nothing.
+ *
+ * Otherwise works like dvmDexChangeDex1.
+ */
+bool dvmDexChangeDex2(DvmDex* pDvmDex, u2* addr, u2 newVal)
+{
+    if (*addr == newVal) {
+        LOGV("+++ value at %p is already 0x%04x\n", addr, newVal);
+        return true;
+    }
+
+    LOGV("+++ change 2byte at %p from 0x%04x to 0x%04x\n", addr, *addr, newVal);
+    if (sysChangeMapAccess(addr, 2, true, &pDvmDex->memMap) != 0) {
+        LOGD("NOTE: DEX page access change (->RW) failed\n");
+        /* expected on files mounted from FAT; keep going (may crash) */
+    }
+
+    *addr = newVal;
+
+    if (sysChangeMapAccess(addr, 2, false, &pDvmDex->memMap) != 0) {
+        LOGD("NOTE: DEX page access change (->RO) failed\n");
+        /* expected on files mounted from FAT; keep going */
+    }
+
+    return true;
+}
+
diff --git a/vm/DvmDex.h b/vm/DvmDex.h
index be31af3..9f3903a 100644
--- a/vm/DvmDex.h
+++ b/vm/DvmDex.h
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * The VM wraps some additional data structures around the DexFile.  These
  * are defined here.
@@ -81,6 +82,21 @@ int dvmDexFileOpenPartial(const void* addr, int len, DvmDex** ppDvmDex);
 void dvmDexFileFree(DvmDex* pDvmDex);
 
 
+/*
+ * Change the 1- or 2-byte value at the specified address to a new value.  If
+ * the location already has the new value, do nothing.
+ *
+ * This does not make any synchronization guarantees.  The caller must
+ * ensure exclusivity vs. other callers.
+ *
+ * For the 2-byte call, the pointer should have 16-bit alignment.
+ *
+ * Returns "true" on success.
+ */
+bool dvmDexChangeDex1(DvmDex* pDvmDex, u1* addr, u1 newVal);
+bool dvmDexChangeDex2(DvmDex* pDvmDex, u2* addr, u2 newVal);
+
+
 #if DVM_RESOLVER_CACHE == DVM_RC_DISABLED
 /* 1:1 mapping */
 
diff --git a/vm/Exception.c b/vm/Exception.c
index 808b0b2..13b051e 100644
--- a/vm/Exception.c
+++ b/vm/Exception.c
@@ -112,6 +112,8 @@ bool dvmExceptionStartup(void)
         dvmFindSystemClassNoInit("Ljava/lang/Throwable;");
     gDvm.classJavaLangRuntimeException =
         dvmFindSystemClassNoInit("Ljava/lang/RuntimeException;");
+    gDvm.classJavaLangStackOverflowError =
+        dvmFindSystemClassNoInit("Ljava/lang/StackOverflowError;");
     gDvm.classJavaLangError =
         dvmFindSystemClassNoInit("Ljava/lang/Error;");
     gDvm.classJavaLangStackTraceElement =
@@ -181,6 +183,18 @@ void dvmExceptionShutdown(void)
 
 
 /*
+ * Format the message into a small buffer and pass it along.
+ */
+void dvmThrowExceptionFmtV(const char* exceptionDescriptor, const char* fmt,
+    va_list args)
+{
+    char msgBuf[512];
+
+    vsnprintf(msgBuf, sizeof(msgBuf), fmt, args);
+    dvmThrowChainedException(exceptionDescriptor, msgBuf, NULL);
+}
+
+/*
  * Create a Throwable and throw an exception in the current thread (where
  * "throwing" just means "set the thread's exception pointer").
  *
diff --git a/vm/Exception.h b/vm/Exception.h
index 4044345..b812f73 100644
--- a/vm/Exception.h
+++ b/vm/Exception.h
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Exception handling.
  */
@@ -35,6 +36,25 @@ INLINE void dvmThrowException(const char* exceptionDescriptor,
 }
 
 /*
+ * Like dvmThrowChainedException, but takes printf-style args for the message.
+ */
+void dvmThrowExceptionFmtV(const char* exceptionDescriptor, const char* fmt,
+    va_list args);
+void dvmThrowExceptionFmt(const char* exceptionDescriptor, const char* fmt, ...)
+#if defined(__GNUC__)
+    __attribute__ ((format(printf, 2, 3)))
+#endif
+    ;
+INLINE void dvmThrowExceptionFmt(const char* exceptionDescriptor,
+    const char* fmt, ...)
+{
+    va_list args;
+    va_start(args, fmt);
+    dvmThrowExceptionFmtV(exceptionDescriptor, fmt, args);
+    va_end(args);
+}
+
+/*
  * Throw an exception in the current thread, by class object.
  */
 void dvmThrowChainedExceptionByClass(ClassObject* exceptionClass,
diff --git a/vm/Globals.h b/vm/Globals.h
index 9b12d84..fb2518d 100644
--- a/vm/Globals.h
+++ b/vm/Globals.h
@@ -34,8 +34,9 @@
 
 #define MAX_BREAKPOINTS 20      /* used for a debugger optimization */
 
-// fwd
-typedef struct GcHeap GcHeap;   /* heap internal structure */
+/* private structures */
+typedef struct GcHeap GcHeap;
+typedef struct BreakpointSet BreakpointSet;
 
 /*
  * One of these for each -ea/-da/-esa/-dsa on the command line.
@@ -79,6 +80,7 @@ struct DvmGlobals {
     bool        verboseGc;
     bool        verboseJni;
     bool        verboseClass;
+    bool        verboseShutdown;
 
     bool        jdwpAllowed;        // debugging allowed for this process?
     bool        jdwpConfigured;     // has debugging info been provided?
@@ -88,6 +90,14 @@ struct DvmGlobals {
     int         jdwpPort;
     bool        jdwpSuspend;
 
+    /*
+     * Lock profiling threshold value in milliseconds.  Acquires that
+     * exceed threshold are logged.  Acquires within the threshold are
+     * logged with a probability of $\frac{time}{threshold}$ .  If the
+     * threshold is unset no additional logging occurs.
+     */
+    u4          lockProfThreshold;
+
     int         (*vfprintfHook)(FILE*, const char*, va_list);
     void        (*exitHook)(int);
     void        (*abortHook)(void);
@@ -174,6 +184,7 @@ struct DvmGlobals {
     ClassObject* classJavaLangVMThread;
     ClassObject* classJavaLangThreadGroup;
     ClassObject* classJavaLangThrowable;
+    ClassObject* classJavaLangStackOverflowError;
     ClassObject* classJavaLangStackTraceElement;
     ClassObject* classJavaLangStackTraceElementArray;
     ClassObject* classJavaLangAnnotationAnnotationArray;
@@ -187,6 +198,7 @@ struct DvmGlobals {
     ClassObject* classJavaLangReflectMethodArray;
     ClassObject* classJavaLangReflectProxy;
     ClassObject* classJavaLangExceptionInInitializerError;
+    ClassObject* classJavaLangRefPhantomReference;
     ClassObject* classJavaLangRefReference;
     ClassObject* classJavaNioReadWriteDirectByteBuffer;
     ClassObject* classJavaSecurityAccessController;
@@ -259,14 +271,8 @@ struct DvmGlobals {
     int         offJavaLangRefReference_queueNext;
     int         offJavaLangRefReference_vmData;
 
-#if FANCY_REFERENCE_SUBCLASS
-    /* method offsets - java.lang.ref.Reference */
-    int         voffJavaLangRefReference_clear;
-    int         voffJavaLangRefReference_enqueue;
-#else
     /* method pointers - java.lang.ref.Reference */
     Method*     methJavaLangRefReference_enqueueInternal;
-#endif
 
     /* field offsets - java.nio.Buffer and java.nio.DirectByteBufferImpl */
     //int         offJavaNioBuffer_capacity;
@@ -279,6 +285,7 @@ struct DvmGlobals {
     /* constructor method pointers; no vtable involved, so use Method* */
     Method*     methJavaLangStackTraceElement_init;
     Method*     methJavaLangExceptionInInitializerError_init;
+    Method*     methJavaLangRefPhantomReference_init;
     Method*     methJavaLangReflectConstructor_init;
     Method*     methJavaLangReflectField_init;
     Method*     methJavaLangReflectMethod_init;
@@ -425,6 +432,9 @@ struct DvmGlobals {
     ReferenceTable  jniPinRefTable;
     pthread_mutex_t jniPinRefLock;
 
+    /* special ReferenceQueue for JNI weak globals */
+    Object*     jniWeakGlobalRefQueue;
+
     /*
      * Native shared library table.
      */
@@ -523,12 +533,9 @@ struct DvmGlobals {
     HashTable*  dbgRegistry;
 
     /*
-     * Breakpoint optimization table.  This is global and NOT explicitly
-     * synchronized, but all operations that modify the table are made
-     * from relatively-synchronized functions.  False-positives are
-     * possible, false-negatives (i.e. missing a breakpoint) should not be.
+     * Debugger breakpoint table.
      */
-    const u2*   debugBreakAddr[MAX_BREAKPOINTS];
+    BreakpointSet*  breakpointSet;
 
     /*
      * Single-step control struct.  We currently only allow one thread to
@@ -649,6 +656,19 @@ extern struct DvmGlobals gDvm;
 #if defined(WITH_JIT)
 
 /*
+ * Exiting the compiled code w/o chaining will incur overhead to look up the
+ * target in the code cache which is extra work only when JIT is enabled. So
+ * we want to monitor it closely to make sure we don't have performance bugs.
+ */
+typedef enum NoChainExits {
+    kInlineCacheMiss = 0,
+    kCallsiteInterpreted,
+    kSwitchOverflow,
+    kHeavyweightMonitor,
+    kNoChainExitLast,
+} NoChainExits;
+
+/*
  * JIT-specific global state
  */
 struct DvmJitGlobals {
@@ -675,6 +695,8 @@ struct DvmJitGlobals {
 
     /* Array of profile threshold counters */
     unsigned char *pProfTable;
+
+    /* Copy of pProfTable used for temporarily disabling the Jit */
     unsigned char *pProfTableCopy;
 
     /* Size of JIT hash table in entries.  Must be a power of 2 */
@@ -686,6 +708,9 @@ struct DvmJitGlobals {
     /* How many entries in the JitEntryTable are in use */
     unsigned int jitTableEntriesUsed;
 
+    /* Bytes allocated for the code cache */
+    unsigned int codeCacheSize;
+
     /* Trigger for trace selection */
     unsigned short threshold;
 
@@ -694,26 +719,31 @@ struct DvmJitGlobals {
     bool               blockingMode;
     pthread_t          compilerHandle;
     pthread_mutex_t    compilerLock;
+    pthread_mutex_t    compilerICPatchLock;
     pthread_cond_t     compilerQueueActivity;
     pthread_cond_t     compilerQueueEmpty;
-    int                compilerQueueLength;
+    volatile int       compilerQueueLength;
     int                compilerHighWater;
     int                compilerWorkEnqueueIndex;
     int                compilerWorkDequeueIndex;
-    CompilerWorkOrder  compilerWorkQueue[COMPILER_WORK_QUEUE_SIZE];
+    int                compilerICPatchIndex;
 
     /* JIT internal stats */
     int                compilerMaxQueued;
     int                addrLookupsFound;
     int                addrLookupsNotFound;
-    int                noChainExit;
+    int                noChainExit[kNoChainExitLast];
     int                normalExit;
     int                puntExit;
     int                translationChains;
-    int                invokeChain;
-    int                invokePredictedChain;
+    int                invokeMonomorphic;
+    int                invokePolymorphic;
     int                invokeNative;
     int                returnOp;
+    int                icPatchFast;
+    int                icPatchQueued;
+    int                icPatchDropped;
+    u8                 jitTime;
 
     /* Compiled code cache */
     void* codeCache;
@@ -730,6 +760,12 @@ struct DvmJitGlobals {
     /* Flag to indicate that the code cache is full */
     bool codeCacheFull;
 
+    /* Number of times that the code cache has been reset */
+    int numCodeCacheReset;
+
+    /* Number of times that the code cache reset request has been delayed */
+    int numCodeCacheResetDelayed;
+
     /* true/false: compile/reject opcodes specified in the -Xjitop list */
     bool includeSelectedOp;
 
@@ -748,8 +784,48 @@ struct DvmJitGlobals {
     /* Flag to count trace execution */
     bool profile;
 
+    /* Vector to disable selected optimizations */
+    int disableOpt;
+
+    /* Code address of special interpret-only pseudo-translation */
+    void *interpretTemplate;
+
     /* Table to track the overall and trace statistics of hot methods */
     HashTable*  methodStatsTable;
+
+    /* Filter method compilation blacklist with call-graph information */
+    bool checkCallGraph;
+
+    /* New translation chain has been set up */
+    volatile bool hasNewChain;
+
+#if defined(WITH_SELF_VERIFICATION)
+    /* Spin when error is detected, volatile so GDB can reset it */
+    volatile bool selfVerificationSpin;
+#endif
+
+    /* Framework or stand-alone? */
+    bool runningInAndroidFramework;
+
+    /* Framework callback happened? */
+    bool alreadyEnabledViaFramework;
+
+    /* Framework requests to disable the JIT for good */
+    bool disableJit;
+
+#if defined(SIGNATURE_BREAKPOINT)
+    /* Signature breakpoint */
+    u4 signatureBreakpointSize;         // # of words
+    u4 *signatureBreakpoint;            // Signature content
+#endif
+
+    /* Place arrays at the end to ease the display in gdb sessions */
+
+    /* Work order queue for compilations */
+    CompilerWorkOrder compilerWorkQueue[COMPILER_WORK_QUEUE_SIZE];
+
+    /* Work order queue for predicted chain patching */
+    ICPatchWorkOrder compilerICPatchQueue[COMPILER_IC_PATCH_QUEUE_SIZE];
 };
 
 extern struct DvmJitGlobals gDvmJit;
diff --git a/vm/IndirectRefTable.c b/vm/IndirectRefTable.c
index f7c7647..bea0a0f 100644
--- a/vm/IndirectRefTable.c
+++ b/vm/IndirectRefTable.c
@@ -35,6 +35,12 @@ bool dvmInitIndirectRefTable(IndirectRefTable* pRef, int initialCount,
 #ifndef NDEBUG
     memset(pRef->table, 0xd1, initialCount * sizeof(Object*));
 #endif
+
+    pRef->slotData =
+        (IndirectRefSlot*) calloc(maxCount, sizeof(IndirectRefSlot));
+    if (pRef->slotData == NULL)
+        return false;
+
     pRef->segmentState.all = IRT_FIRST_SEGMENT;
     pRef->allocEntries = initialCount;
     pRef->maxEntries = maxCount;
@@ -79,8 +85,8 @@ bool dvmPopIndirectRefTableSegmentCheck(IndirectRefTable* pRef, u4 cookie)
         return false;
     }
 
-    LOGV("--- after pop, top=%d holes=%d\n",
-        sst.parts.topIndex, sst.parts.numHoles);
+    LOGV("IRT %p[%d]: pop, top=%d holes=%d\n",
+        pRef, pRef->kind, sst.parts.topIndex, sst.parts.numHoles);
 
     return true;
 }
@@ -91,15 +97,45 @@ bool dvmPopIndirectRefTableSegmentCheck(IndirectRefTable* pRef, u4 cookie)
 static bool checkEntry(IndirectRefTable* pRef, IndirectRef iref, int idx)
 {
     Object* obj = pRef->table[idx];
-    IndirectRef checkRef = dvmObjectToIndirectRef(obj, idx, pRef->kind);
+    IndirectRef checkRef = dvmObjectToIndirectRef(pRef, obj, idx, pRef->kind);
     if (checkRef != iref) {
-        LOGW("iref mismatch: %p vs %p\n", iref, checkRef);
+        LOGW("IRT %p[%d]: iref mismatch (req=%p vs cur=%p)\n",
+            pRef, pRef->kind, iref, checkRef);
         return false;
     }
     return true;
 }
 
 /*
+ * Update extended debug info when an entry is added.
+ *
+ * We advance the serial number, invalidating any outstanding references to
+ * this slot.
+ */
+static inline void updateSlotAdd(IndirectRefTable* pRef, Object* obj, int slot)
+{
+    if (pRef->slotData != NULL) {
+        IndirectRefSlot* pSlot = &pRef->slotData[slot];
+        pSlot->serial++;
+        //LOGI("+++ add [%d] slot %d (%p->%p), serial=%d\n",
+        //    pRef->kind, slot, obj, iref, pSlot->serial);
+        pSlot->previous[pSlot->serial % kIRTPrevCount] = obj;
+    }
+}
+
+/*
+ * Update extended debug info when an entry is removed.
+ */
+static inline void updateSlotRemove(IndirectRefTable* pRef, int slot)
+{
+    if (pRef->slotData != NULL) {
+        IndirectRefSlot* pSlot = &pRef->slotData[slot];
+        //LOGI("+++ remove [%d] slot %d, serial now %d\n",
+        //    pRef->kind, slot, pSlot->serial);
+    }
+}
+
+/*
  * Add "obj" to "pRef".
  */
 IndirectRef dvmAddToIndirectRefTable(IndirectRefTable* pRef, u4 cookie,
@@ -161,12 +197,15 @@ IndirectRef dvmAddToIndirectRefTable(IndirectRefTable* pRef, u4 cookie,
         while (*--pScan != NULL) {
             assert(pScan >= pRef->table + bottomIndex);
         }
-        result = dvmObjectToIndirectRef(obj, pScan - pRef->table, pRef->kind);
+        updateSlotAdd(pRef, obj, pScan - pRef->table);
+        result = dvmObjectToIndirectRef(pRef, obj, pScan - pRef->table,
+            pRef->kind);
         *pScan = obj;
         pRef->segmentState.parts.numHoles--;
     } else {
         /* add to the end */
-        result = dvmObjectToIndirectRef(obj, topIndex, pRef->kind);
+        updateSlotAdd(pRef, obj, topIndex);
+        result = dvmObjectToIndirectRef(pRef, obj, topIndex, pRef->kind);
         pRef->table[topIndex++] = obj;
         pRef->segmentState.parts.topIndex = topIndex;
     }
@@ -220,6 +259,9 @@ bool dvmGetFromIndirectRefTableCheck(IndirectRefTable* pRef, IndirectRef iref)
  * specified by the cookie, we don't remove anything.  This is the behavior
  * required by JNI's DeleteLocalRef function.
  *
+ * Note this is NOT called when a local frame is popped.  This is only used
+ * for explict single removals.
+ *
  * Returns "false" if nothing was removed.
  */
 bool dvmRemoveFromIndirectRefTable(IndirectRefTable* pRef, u4 cookie,
@@ -255,6 +297,7 @@ bool dvmRemoveFromIndirectRefTable(IndirectRefTable* pRef, u4 cookie,
          */
         if (!checkEntry(pRef, iref, idx))
             return false;
+        updateSlotRemove(pRef, idx);
 
 #ifndef NDEBUG
         pRef->table[idx] = (IndirectRef) 0xd3d3d3d3;
@@ -290,6 +333,7 @@ bool dvmRemoveFromIndirectRefTable(IndirectRefTable* pRef, u4 cookie,
         }
         if (!checkEntry(pRef, iref, idx))
             return false;
+        updateSlotRemove(pRef, idx);
 
         pRef->table[idx] = NULL;
         pRef->segmentState.parts.numHoles++;
@@ -408,7 +452,7 @@ void dvmDumpIndirectRefTable(const IndirectRefTable* pRef, const char* descr)
     qsort(tableCopy, count, sizeof(Object*), compareObject);
     refs = tableCopy;       // use sorted list
 
-    {
+    if (false) {
         int q;
         for (q = 0; q < count; q++)
             LOGI("%d %p\n", q, refs[q]);
diff --git a/vm/IndirectRefTable.h b/vm/IndirectRefTable.h
index c03353b..6a4db04 100644
--- a/vm/IndirectRefTable.h
+++ b/vm/IndirectRefTable.h
@@ -98,6 +98,16 @@ typedef enum IndirectRefKind {
 } IndirectRefKind;
 
 /*
+ * Extended debugging structure.  We keep a parallel array of these, one
+ * per slot in the table.
+ */
+#define kIRTPrevCount   4
+typedef struct IndirectRefSlot {
+    u4          serial;         /* slot serial */
+    Object*     previous[kIRTPrevCount];
+} IndirectRefSlot;
+
+/*
  * Table definition.
  *
  * For the global reference table, the expected common operations are
@@ -179,6 +189,7 @@ typedef struct IndirectRefTable {
     Object**        table;              /* bottom of the stack */
 
     /* private */
+    IndirectRefSlot* slotData;          /* extended debugging info */
     int             allocEntries;       /* #of entries we have space for */
     int             maxEntries;         /* max #of entries allowed */
     IndirectRefKind kind;               /* bit mask, ORed into all irefs */
@@ -198,12 +209,14 @@ typedef struct IndirectRefTable {
  * The object pointer itself is subject to relocation in some GC
  * implementations, so we shouldn't really be using it here.
  */
-INLINE IndirectRef dvmObjectToIndirectRef(Object* obj, u4 tableIndex,
-    IndirectRefKind kind)
+INLINE IndirectRef dvmObjectToIndirectRef(IndirectRefTable* pRef,
+    Object* obj, u4 tableIndex, IndirectRefKind kind)
 {
     assert(tableIndex < 65536);
-    u4 objChunk = (((u4) obj >> 3) ^ ((u4) obj >> 19)) & 0x3fff;
-    u4 uref = objChunk << 18 | (tableIndex << 2) | kind;
+    //u4 objChunk = (((u4) obj >> 3) ^ ((u4) obj >> 19)) & 0x3fff;
+    //u4 uref = objChunk << 18 | (tableIndex << 2) | kind;
+    u4 serialChunk = pRef->slotData[tableIndex].serial;
+    u4 uref = serialChunk << 20 | (tableIndex << 2) | kind;
     return (IndirectRef) uref;
 }
 
@@ -270,7 +283,7 @@ bool dvmPopIndirectRefTableSegmentCheck(IndirectRefTable* pRef, u4 cookie);
  *
  * IMPORTANT: this is implemented as a single instruction in mterp, rather
  * than a call here.  You can add debugging aids for the C-language
- * interpreters, but the basic implementation may not change.
+ * interpreters, but the basic implementation must not change.
  */
 INLINE void dvmPopIndirectRefTableSegment(IndirectRefTable* pRef, u4 cookie)
 {
@@ -323,7 +336,8 @@ INLINE IndirectRef dvmAppendToIndirectRefTable(IndirectRefTable* pRef,
         /* up against alloc or max limit, call the fancy version */
         return dvmAddToIndirectRefTable(pRef, cookie, obj);
     } else {
-        IndirectRef result = dvmObjectToIndirectRef(obj, topIndex, pRef->kind);
+        IndirectRef result = dvmObjectToIndirectRef(pRef, obj, topIndex,
+            pRef->kind);
         pRef->table[topIndex++] = obj;
         pRef->segmentState.parts.topIndex = topIndex;
         return result;
diff --git a/vm/Init.c b/vm/Init.c
index c46de25..6630395 100644
--- a/vm/Init.c
+++ b/vm/Init.c
@@ -114,12 +114,14 @@ static void dvmUsage(const char* progName)
     dvmFprintf(stderr, "  -Xjitop:hexopvalue[-endvalue]"
                        "[,hexopvalue[-endvalue]]*\n");
     dvmFprintf(stderr, "  -Xincludeselectedmethod\n");
-    dvmFprintf(stderr, "  -Xthreshold:decimalvalue\n");
-    dvmFprintf(stderr, "  -Xblocking\n");
-    dvmFprintf(stderr, "  -Xjitmethod:signture[,signature]* "
+    dvmFprintf(stderr, "  -Xjitthreshold:decimalvalue\n");
+    dvmFprintf(stderr, "  -Xjitblocking\n");
+    dvmFprintf(stderr, "  -Xjitmethod:signature[,signature]* "
                        "(eg Ljava/lang/String\\;replace)\n");
+    dvmFprintf(stderr, "  -Xjitcheckcg\n");
     dvmFprintf(stderr, "  -Xjitverbose\n");
     dvmFprintf(stderr, "  -Xjitprofile\n");
+    dvmFprintf(stderr, "  -Xjitdisableopt\n");
 #endif
     dvmFprintf(stderr, "\n");
     dvmFprintf(stderr, "Configured with:"
@@ -185,7 +187,10 @@ static void dvmUsage(const char* progName)
         " resolver_cache_disabled"
 #endif
 #if defined(WITH_JIT)
-        " with_jit"
+        " jit"
+#endif
+#if defined(WITH_SELF_VERIFICATION)
+        " self_verification"
 #endif
     );
 #ifdef DVM_SHOW_EXCEPTION
@@ -790,6 +795,8 @@ static int dvmProcessOptions(int argc, const char* const argv[],
             gDvm.verboseJni = true;
         } else if (strcmp(argv[i], "-verbose:gc") == 0) {
             gDvm.verboseGc = true;
+        } else if (strcmp(argv[i], "-verbose:shutdown") == 0) {
+            gDvm.verboseShutdown = true;
 
         } else if (strncmp(argv[i], "-enableassertions", 17) == 0) {
             enableAssertions(argv[i] + 17, true);
@@ -837,6 +844,9 @@ static int dvmProcessOptions(int argc, const char* const argv[],
             gDvm.noQuitHandler = true;
         } else if (strcmp(argv[i], "-Xzygote") == 0) {
             gDvm.zygote = true;
+#if defined(WITH_JIT)
+            gDvmJit.runningInAndroidFramework = true;
+#endif
         } else if (strncmp(argv[i], "-Xdexopt:", 9) == 0) {
             if (strcmp(argv[i] + 9, "none") == 0)
                 gDvm.dexOptMode = OPTIMIZE_MODE_NONE;
@@ -890,23 +900,38 @@ static int dvmProcessOptions(int argc, const char* const argv[],
                 /* disable JIT -- nothing to do here for now */
             }
 
+        } else if (strncmp(argv[i], "-Xlockprofthreshold:", 20) == 0) {
+            gDvm.lockProfThreshold = atoi(argv[i] + 20);
+
 #ifdef WITH_JIT
         } else if (strncmp(argv[i], "-Xjitop", 7) == 0) {
             processXjitop(argv[i]);
         } else if (strncmp(argv[i], "-Xjitmethod", 11) == 0) {
             processXjitmethod(argv[i]);
-        } else if (strncmp(argv[i], "-Xblocking", 10) == 0) {
+        } else if (strncmp(argv[i], "-Xjitblocking", 13) == 0) {
           gDvmJit.blockingMode = true;
-        } else if (strncmp(argv[i], "-Xthreshold:", 12) == 0) {
-          gDvmJit.threshold = atoi(argv[i] + 12);
+        } else if (strncmp(argv[i], "-Xjitthreshold:", 15) == 0) {
+          gDvmJit.threshold = atoi(argv[i] + 15);
         } else if (strncmp(argv[i], "-Xincludeselectedop", 19) == 0) {
           gDvmJit.includeSelectedOp = true;
         } else if (strncmp(argv[i], "-Xincludeselectedmethod", 23) == 0) {
           gDvmJit.includeSelectedMethod = true;
+        } else if (strncmp(argv[i], "-Xjitcheckcg", 12) == 0) {
+          gDvmJit.checkCallGraph = true;
+          /* Need to enable blocking mode due to stack crawling */
+          gDvmJit.blockingMode = true;
         } else if (strncmp(argv[i], "-Xjitverbose", 12) == 0) {
           gDvmJit.printMe = true;
         } else if (strncmp(argv[i], "-Xjitprofile", 12) == 0) {
           gDvmJit.profile = true;
+        } else if (strncmp(argv[i], "-Xjitdisableopt", 15) == 0) {
+          /* Disable selected optimizations */
+          if (argv[i][15] == ':') {
+              sscanf(argv[i] + 16, "%x", &gDvmJit.disableOpt);
+          /* Disable all optimizations */
+          } else {
+              gDvmJit.disableOpt = -1;
+          }
 #endif
 
         } else if (strncmp(argv[i], "-Xdeadlockpredict:", 18) == 0) {
@@ -943,7 +968,7 @@ static int dvmProcessOptions(int argc, const char* const argv[],
                 dvmFprintf(stderr, "Bad value for -Xgc");
                 return -1;
             }
-            LOGD("Precise GC configured %s\n", gDvm.preciseGc ? "ON" : "OFF");
+            LOGV("Precise GC configured %s\n", gDvm.preciseGc ? "ON" : "OFF");
 
         } else if (strcmp(argv[i], "-Xcheckdexsum") == 0) {
             gDvm.verifyDexChecksum = true;
@@ -1009,14 +1034,6 @@ static void setCommandLineDefaults()
      */
 #if defined(WITH_JIT)
     gDvm.executionMode = kExecutionModeJit;
-    /* 
-     * TODO - check system property and insert command-line options in 
-     *        frameworks/base/core/jni/AndroidRuntime.cpp
-     */
-    gDvmJit.blockingMode = false;
-    gDvmJit.jitTableSize = 512;
-    gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
-    gDvmJit.threshold = 200;
 #else
     gDvm.executionMode = kExecutionModeInterpFast;
 #endif
@@ -1129,6 +1146,13 @@ int dvmStartup(int argc, const char* const argv[], bool ignoreUnrecognized,
     if (!gDvm.reduceSignals)
         blockSignals();
 
+    /* verify system page size */
+    if (sysconf(_SC_PAGESIZE) != SYSTEM_PAGE_SIZE) {
+        LOGE("ERROR: expected page size %d, got %d\n",
+            SYSTEM_PAGE_SIZE, (int) sysconf(_SC_PAGESIZE));
+        goto fail;
+    }
+
     /* mterp setup */
     LOGV("Using executionMode %d\n", gDvm.executionMode);
     dvmCheckAsmConstants();
@@ -1231,6 +1255,16 @@ int dvmStartup(int argc, const char* const argv[], bool ignoreUnrecognized,
     if (!dvmPrepMainThread())
         goto fail;
 
+    /*
+     * Make sure we haven't accumulated any tracked references.  The main
+     * thread should be starting with a clean slate.
+     */
+    if (dvmReferenceTableEntries(&dvmThreadSelf()->internalLocalRefTable) != 0)
+    {
+        LOGW("Warning: tracked references remain post-initialization\n");
+        dvmDumpReferenceTable(&dvmThreadSelf()->internalLocalRefTable, "MAIN");
+    }
+
     /* general debugging setup */
     if (!dvmDebuggerStartup())
         goto fail;
@@ -1362,8 +1396,10 @@ bool dvmInitAfterZygote(void)
         (int)(endJdwp-startJdwp), (int)(endJdwp-startHeap));
 
 #ifdef WITH_JIT
-    if (!dvmJitStartup())
-        return false;
+    if (gDvm.executionMode == kExecutionModeJit) {
+        if (!dvmCompilerStartup())
+            return false;
+    }
 #endif
 
     return true;
@@ -1552,8 +1588,10 @@ void dvmShutdown(void)
     dvmStdioConverterShutdown();
 
 #ifdef WITH_JIT
-    /* tell the compiler to shut down if it was started */
-    dvmJitShutdown();
+    if (gDvm.executionMode == kExecutionModeJit) {
+        /* shut down the compiler thread */
+        dvmCompilerShutdown();
+    }
 #endif
 
     /*
@@ -1563,7 +1601,8 @@ void dvmShutdown(void)
      */
     dvmSlayDaemons();
 
-    LOGD("VM cleaning up\n");
+    if (gDvm.verboseShutdown)
+        LOGD("VM cleaning up\n");
 
     dvmDebuggerShutdown();
     dvmReflectShutdown();
diff --git a/vm/InlineNative.c b/vm/InlineNative.c
index f829360..ec8a1fb 100644
--- a/vm/InlineNative.c
+++ b/vm/InlineNative.c
@@ -129,14 +129,14 @@ static bool javaLangString_charAt(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
         return false;
 
     //LOGI("String.charAt this=0x%08x index=%d\n", arg0, arg1);
-    count = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_count);
+    count = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_COUNT);
     if ((s4) arg1 < 0 || (s4) arg1 >= count) {
         dvmThrowException("Ljava/lang/StringIndexOutOfBoundsException;", NULL);
         return false;
     } else {
-        offset = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_offset);
+        offset = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_OFFSET);
         chars = (ArrayObject*)
-            dvmGetFieldObject((Object*) arg0, gDvm.offJavaLangString_value);
+            dvmGetFieldObject((Object*) arg0, STRING_FIELDOFF_VALUE);
 
         pResult->i = ((const u2*) chars->contents)[arg1 + offset];
         return true;
@@ -157,17 +157,17 @@ static void badMatch(StringObject* thisStrObj, StringObject* compStrObj,
     int thisOffset, compOffset, thisCount, compCount;
 
     thisCount =
-        dvmGetFieldInt((Object*) thisStrObj, gDvm.offJavaLangString_count);
+        dvmGetFieldInt((Object*) thisStrObj, STRING_FIELDOFF_COUNT);
     compCount =
-        dvmGetFieldInt((Object*) compStrObj, gDvm.offJavaLangString_count);
+        dvmGetFieldInt((Object*) compStrObj, STRING_FIELDOFF_COUNT);
     thisOffset =
-        dvmGetFieldInt((Object*) thisStrObj, gDvm.offJavaLangString_offset);
+        dvmGetFieldInt((Object*) thisStrObj, STRING_FIELDOFF_OFFSET);
     compOffset =
-        dvmGetFieldInt((Object*) compStrObj, gDvm.offJavaLangString_offset);
+        dvmGetFieldInt((Object*) compStrObj, STRING_FIELDOFF_OFFSET);
     thisArray = (ArrayObject*)
-        dvmGetFieldObject((Object*) thisStrObj, gDvm.offJavaLangString_value);
+        dvmGetFieldObject((Object*) thisStrObj, STRING_FIELDOFF_VALUE);
     compArray = (ArrayObject*)
-        dvmGetFieldObject((Object*) compStrObj, gDvm.offJavaLangString_value);
+        dvmGetFieldObject((Object*) compStrObj, STRING_FIELDOFF_VALUE);
 
     thisStr = dvmCreateCstrFromString(thisStrObj);
     compStr = dvmCreateCstrFromString(compStrObj);
@@ -221,16 +221,16 @@ static bool javaLangString_compareTo(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
     const u2* compChars;
     int i, minCount, countDiff;
 
-    thisCount = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_count);
-    compCount = dvmGetFieldInt((Object*) arg1, gDvm.offJavaLangString_count);
+    thisCount = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_COUNT);
+    compCount = dvmGetFieldInt((Object*) arg1, STRING_FIELDOFF_COUNT);
     countDiff = thisCount - compCount;
     minCount = (countDiff < 0) ? thisCount : compCount;
-    thisOffset = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_offset);
-    compOffset = dvmGetFieldInt((Object*) arg1, gDvm.offJavaLangString_offset);
+    thisOffset = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_OFFSET);
+    compOffset = dvmGetFieldInt((Object*) arg1, STRING_FIELDOFF_OFFSET);
     thisArray = (ArrayObject*)
-        dvmGetFieldObject((Object*) arg0, gDvm.offJavaLangString_value);
+        dvmGetFieldObject((Object*) arg0, STRING_FIELDOFF_VALUE);
     compArray = (ArrayObject*)
-        dvmGetFieldObject((Object*) arg1, gDvm.offJavaLangString_value);
+        dvmGetFieldObject((Object*) arg1, STRING_FIELDOFF_VALUE);
     thisChars = ((const u2*) thisArray->contents) + thisOffset;
     compChars = ((const u2*) compArray->contents) + compOffset;
 
@@ -321,19 +321,19 @@ static bool javaLangString_equals(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
     int i;
 
     /* quick length check */
-    thisCount = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_count);
-    compCount = dvmGetFieldInt((Object*) arg1, gDvm.offJavaLangString_count);
+    thisCount = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_COUNT);
+    compCount = dvmGetFieldInt((Object*) arg1, STRING_FIELDOFF_COUNT);
     if (thisCount != compCount) {
         pResult->i = false;
         return true;
     }
 
-    thisOffset = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_offset);
-    compOffset = dvmGetFieldInt((Object*) arg1, gDvm.offJavaLangString_offset);
+    thisOffset = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_OFFSET);
+    compOffset = dvmGetFieldInt((Object*) arg1, STRING_FIELDOFF_OFFSET);
     thisArray = (ArrayObject*)
-        dvmGetFieldObject((Object*) arg0, gDvm.offJavaLangString_value);
+        dvmGetFieldObject((Object*) arg0, STRING_FIELDOFF_VALUE);
     compArray = (ArrayObject*)
-        dvmGetFieldObject((Object*) arg1, gDvm.offJavaLangString_value);
+        dvmGetFieldObject((Object*) arg1, STRING_FIELDOFF_VALUE);
     thisChars = ((const u2*) thisArray->contents) + thisOffset;
     compChars = ((const u2*) compArray->contents) + compOffset;
 
@@ -383,7 +383,90 @@ static bool javaLangString_length(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
     if (!dvmValidateObject((Object*) arg0))
         return false;
 
-    pResult->i = dvmGetFieldInt((Object*) arg0, gDvm.offJavaLangString_count);
+    pResult->i = dvmGetFieldInt((Object*) arg0, STRING_FIELDOFF_COUNT);
+    return true;
+}
+
+/*
+ * Determine the index of the first character matching "ch".  The string
+ * to search is described by "chars", "offset", and "count".
+ *
+ * The "ch" parameter is allowed to be > 0xffff.  Our Java-language
+ * implementation does not currently handle this, so neither do we.
+ *
+ * The "start" parameter must be clamped to [0..count].
+ *
+ * Returns -1 if no match is found.
+ */
+static inline int indexOfCommon(Object* strObj, int ch, int start)
+{
+    //if ((ch & 0xffff) != ch)        /* 32-bit code point */
+    //    return -1;
+
+    /* pull out the basic elements */
+    ArrayObject* charArray =
+        (ArrayObject*) dvmGetFieldObject(strObj, STRING_FIELDOFF_VALUE);
+    const u2* chars = (const u2*) charArray->contents;
+    int offset = dvmGetFieldInt(strObj, STRING_FIELDOFF_OFFSET);
+    int count = dvmGetFieldInt(strObj, STRING_FIELDOFF_COUNT);
+    //LOGI("String.indexOf(0x%08x, 0x%04x, %d) off=%d count=%d\n",
+    //    (u4) strObj, ch, start, offset, count);
+
+    /* factor out the offset */
+    chars += offset;
+
+    if (start < 0)
+        start = 0;
+
+#if 0
+    /* 16-bit loop, simple */
+    while (start < count) {
+        if (chars[start] == ch)
+            return start;
+        start++;
+    }
+#else
+    /* 16-bit loop, slightly better on ARM */
+    const u2* ptr = chars + start;
+    const u2* endPtr = chars + count;
+    while (ptr < endPtr) {
+        if (*ptr++ == ch)
+            return (ptr-1) - chars;
+    }
+#endif
+
+    return -1;
+}
+
+/*
+ * public int indexOf(int c)
+ *
+ * Scan forward through the string for a matching character.
+ */
+static bool javaLangString_indexOf_I(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
+    JValue* pResult)
+{
+    /* null reference check on "this" */
+    if (!dvmValidateObject((Object*) arg0))
+        return false;
+
+    pResult->i = indexOfCommon((Object*) arg0, arg1, 0);
+    return true;
+}
+
+/*
+ * public int indexOf(int c, int start)
+ *
+ * Scan forward through the string for a matching character.
+ */
+static bool javaLangString_indexOf_II(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
+    JValue* pResult)
+{
+    /* null reference check on "this" */
+    if (!dvmValidateObject((Object*) arg0))
+        return false;
+
+    pResult->i = indexOfCommon((Object*) arg0, arg1, arg2);
     return true;
 }
 
@@ -550,8 +633,11 @@ static bool javaLangMath_sin(u4 arg0, u4 arg1, u4 arg2, u4 arg3,
  * pointer field.
  *
  * IMPORTANT: you must update DALVIK_VM_BUILD in DalvikVersion.h if you make
- * changes to this table.  Must also be kept in sync with NativeInlineOps
- * enum in InlineNative.h.
+ * changes to this table.
+ *
+ * NOTE: If present, the JIT will also need to know about changes
+ * to this table.  Update the NativeInlineOps enum in InlineNative.h and
+ * the dispatch code in compiler/codegen/<target>/Codegen.c.
  */
 const InlineOperation gDvmInlineOpsTable[] = {
     { org_apache_harmony_dalvik_NativeTestTarget_emptyInlineMethod,
@@ -564,6 +650,10 @@ const InlineOperation gDvmInlineOpsTable[] = {
         "Ljava/lang/String;", "compareTo", "(Ljava/lang/String;)I" },
     { javaLangString_equals,
         "Ljava/lang/String;", "equals", "(Ljava/lang/Object;)Z" },
+    { javaLangString_indexOf_I,
+        "Ljava/lang/String;", "indexOf", "(I)I" },
+    { javaLangString_indexOf_II,
+        "Ljava/lang/String;", "indexOf", "(II)I" },
     { javaLangString_length,
         "Ljava/lang/String;", "length", "()I" },
 
@@ -587,7 +677,6 @@ const InlineOperation gDvmInlineOpsTable[] = {
         "Ljava/lang/Math;", "sin", "(D)D" },
 };
 
-
 /*
  * Allocate some tables.
  */
diff --git a/vm/InlineNative.h b/vm/InlineNative.h
index 98398da..300c94c 100644
--- a/vm/InlineNative.h
+++ b/vm/InlineNative.h
@@ -54,16 +54,18 @@ typedef enum NativeInlineOps {
     INLINE_STRING_CHARAT = 1,
     INLINE_STRING_COMPARETO = 2,
     INLINE_STRING_EQUALS = 3,
-    INLINE_STRING_LENGTH = 4,
-    INLINE_MATH_ABS_INT = 5,
-    INLINE_MATH_ABS_LONG = 6,
-    INLINE_MATH_ABS_FLOAT = 7,
-    INLINE_MATH_ABS_DOUBLE = 8,
-    INLINE_MATH_MIN_INT = 9,
-    INLINE_MATH_MAX_INT = 10,
-    INLINE_MATH_SQRT = 11,
-    INLINE_MATH_COS = 12,
-    INLINE_MATH_SIN = 13,
+    INLINE_STRING_INDEXOF_I = 4,
+    INLINE_STRING_INDEXOF_II = 5,
+    INLINE_STRING_LENGTH = 6,
+    INLINE_MATH_ABS_INT = 7,
+    INLINE_MATH_ABS_LONG = 8,
+    INLINE_MATH_ABS_FLOAT = 9,
+    INLINE_MATH_ABS_DOUBLE = 10,
+    INLINE_MATH_MIN_INT = 11,
+    INLINE_MATH_MAX_INT = 12,
+    INLINE_MATH_SQRT = 13,
+    INLINE_MATH_COS = 14,
+    INLINE_MATH_SIN = 15,
 } NativeInlineOps;
 
 /*
diff --git a/vm/Jni.c b/vm/Jni.c
index 2ef6b0c..9addb43 100644
--- a/vm/Jni.c
+++ b/vm/Jni.c
@@ -318,12 +318,30 @@ bool dvmJniStartup(void)
 
     dvmInitMutex(&gDvm.jniPinRefLock);
 
+    Method* meth;
+
+    /*
+     * Grab the PhantomReference constructor.
+     */
+    gDvm.classJavaLangRefPhantomReference =
+        dvmFindSystemClassNoInit("Ljava/lang/ref/PhantomReference;");
+    if (gDvm.classJavaLangRefPhantomReference == NULL) {
+        LOGE("Unable to find PhantomReference class\n");
+        return false;
+    }
+    meth= dvmFindDirectMethodByDescriptor(gDvm.classJavaLangRefPhantomReference,
+        "<init>", "(Ljava/lang/Object;Ljava/lang/ref/ReferenceQueue;)V");
+    if (meth == NULL) {
+        LOGE("Unable to find constructor for PhantomReference\n");
+        return false;
+    }
+    gDvm.methJavaLangRefPhantomReference_init = meth;
+
+
     /*
      * Look up and cache pointers to some direct buffer classes, fields,
      * and methods.
      */
-    Method* meth;
-
     ClassObject* platformAddressClass =
         dvmFindSystemClassNoInit("Lorg/apache/harmony/luni/platform/PlatformAddress;");
     ClassObject* platformAddressFactoryClass =
@@ -423,6 +441,7 @@ void dvmJniShutdown(void)
 #else
     dvmClearReferenceTable(&gDvm.jniGlobalRefTable);
 #endif
+    dvmClearReferenceTable(&gDvm.jniPinRefTable);
 }
 
 
@@ -573,6 +592,7 @@ Object* dvmDecodeIndirectRef(JNIEnv* env, jobject jobj)
         break;
     case kIndirectKindWeakGlobal:
         {
+            // TODO: implement
             LOGE("weak-global not yet supported\n");
             result = NULL;
             dvmAbort();
@@ -898,6 +918,143 @@ bail:
     dvmUnlockMutex(&gDvm.jniGlobalRefLock);
 }
 
+
+/*
+ * Get the "magic" JNI weak global ReferenceQueue.  It's allocated on
+ * first use.
+ *
+ * Returns NULL with an exception raised if allocation fails.
+ */
+static Object* getWeakGlobalRefQueue(void)
+{
+    /* use an indirect variable to avoid "type-punned pointer" complaints */
+    Object** pGlobalQ = &gDvm.jniWeakGlobalRefQueue;
+
+    if (*pGlobalQ != NULL)
+        return *pGlobalQ;
+
+    ClassObject* clazz = dvmFindSystemClass("Ljava/lang/ref/ReferenceQueue;");
+    if (clazz == NULL) {
+        LOGE("Unable to find java.lang.ref.ReferenceQueue");
+        dvmAbort();
+    }
+
+    /*
+     * Create an instance of ReferenceQueue.  The object is never actually
+     * used for anything, so we don't need to call a constructor.  (We could
+     * get away with using an instance of Object, but this is cleaner.)
+     */
+    Object* queue = dvmAllocObject(clazz, ALLOC_DEFAULT);
+    if (queue == NULL) {
+        LOGW("Failed allocating weak global ref queue\n");
+        assert(dvmCheckException(dvmThreadSelf()));
+        return NULL;
+    }
+    dvmReleaseTrackedAlloc(queue, NULL);
+
+    /*
+     * Save it, using atomic ops to ensure we don't double-up.  The gDvm
+     * field is known to the GC.
+     */
+    if (!ATOMIC_CMP_SWAP((int*) pGlobalQ, 0, (int) queue)) {
+        LOGD("WOW: lost race to create weak global ref queue\n");
+        queue = *pGlobalQ;
+    }
+
+    return queue;
+}
+
+
+/*
+ * We create a PhantomReference that references the object, add a
+ * global reference to it, and then flip some bits before returning it.
+ * The last step ensures that we detect it as special and that only
+ * appropriate calls will accept it.
+ *
+ * On failure, returns NULL with an exception pending.
+ */
+static jweak createWeakGlobalRef(JNIEnv* env, jobject jobj)
+{
+    if (jobj == NULL)
+        return NULL;
+
+    Thread* self = ((JNIEnvExt*)env)->self;
+    Object* obj = dvmDecodeIndirectRef(env, jobj);
+    Object* weakGlobalQueue = getWeakGlobalRefQueue();
+    Object* phantomObj;
+    jobject phantomRef;
+
+    /*
+     * Allocate a PhantomReference, then call the constructor to set
+     * the referent and the reference queue.
+     *
+     * We use a "magic" reference queue that the GC knows about; it behaves
+     * more like a queueless WeakReference, clearing the referent and
+     * not calling enqueue().
+     */
+    if (!dvmIsClassInitialized(gDvm.classJavaLangRefPhantomReference))
+        dvmInitClass(gDvm.classJavaLangRefPhantomReference);
+    phantomObj = dvmAllocObject(gDvm.classJavaLangRefPhantomReference,
+            ALLOC_DEFAULT);
+    if (phantomObj == NULL) {
+        assert(dvmCheckException(self));
+        LOGW("Failed on WeakGlobalRef alloc\n");
+        return NULL;
+    }
+
+    JValue unused;
+    dvmCallMethod(self, gDvm.methJavaLangRefPhantomReference_init, phantomObj,
+        &unused, jobj, weakGlobalQueue);
+    dvmReleaseTrackedAlloc(phantomObj, self);
+
+    if (dvmCheckException(self)) {
+        LOGW("PhantomReference init failed\n");
+        return NULL;
+    }
+
+    LOGV("+++ WGR: created phantom ref %p for object %p\n", phantomObj, obj);
+
+    /*
+     * Add it to the global reference table, and mangle the pointer.
+     */
+    phantomRef = addGlobalReference(phantomObj);
+    return dvmObfuscateWeakGlobalRef(phantomRef);
+}
+
+/*
+ * Delete the global reference that's keeping the PhantomReference around.
+ * The PhantomReference will eventually be discarded by the GC.
+ */
+static void deleteWeakGlobalRef(JNIEnv* env, jweak wref)
+{
+    if (wref == NULL)
+        return;
+
+    jobject phantomRef = dvmNormalizeWeakGlobalRef(wref);
+    deleteGlobalReference(phantomRef);
+}
+
+/*
+ * Extract the referent from a PhantomReference.  Used for weak global
+ * references.
+ *
+ * "jwobj" is a "mangled" WGR pointer.
+ */
+static Object* getPhantomReferent(JNIEnv* env, jweak jwobj)
+{
+    jobject jobj = dvmNormalizeWeakGlobalRef(jwobj);
+    Object* obj = dvmDecodeIndirectRef(env, jobj);
+
+    if (obj->clazz != gDvm.classJavaLangRefPhantomReference) {
+        LOGE("%p is not a phantom reference (%s)\n",
+            jwobj, obj->clazz->descriptor);
+        return NULL;
+    }
+
+    return dvmGetFieldObject(obj, gDvm.offJavaLangRefReference_referent);
+}
+
+
 /*
  * Objects don't currently move, so we just need to create a reference
  * that will ensure the array object isn't collected.
@@ -967,6 +1124,27 @@ bail:
 }
 
 /*
+ * Dump the contents of the JNI reference tables to the log file.
+ *
+ * We only dump the local refs associated with the current thread.
+ */
+void dvmDumpJniReferenceTables(void)
+{
+    Thread* self = dvmThreadSelf();
+    JNIEnv* env = self->jniEnv;
+    ReferenceTable* pLocalRefs = getLocalRefTable(env);
+
+#ifdef USE_INDIRECT_REF
+    dvmDumpIndirectRefTable(pLocalRefs, "JNI local");
+    dvmDumpIndirectRefTable(&gDvm.jniGlobalRefTable, "JNI global");
+#else
+    dvmDumpReferenceTable(pLocalRefs, "JNI local");
+    dvmDumpReferenceTable(&gDvm.jniGlobalRefTable, "JNI global");
+#endif
+    dvmDumpReferenceTable(&gDvm.jniPinRefTable, "JNI pinned array");
+}
+
+/*
  * GC helper function to mark all JNI global references.
  *
  * We're currently handling the "pin" table here too.
@@ -1121,6 +1299,10 @@ jobjectRefType dvmGetJNIRefType(JNIEnv* env, jobject jobj)
     //Object** top;
     Object** ptr;
 
+    if (dvmIsWeakGlobalRef(jobj)) {
+        return JNIWeakGlobalRefType;
+    }
+
     /* check args */
     if (findInArgList(self, jobj)) {
         //LOGI("--- REF found %p on stack\n", jobj);
@@ -1538,6 +1720,7 @@ void dvmCallJNIMethod_general(const u4* args, JValue* pResult,
         (jclass) method->clazz : NULL;
 #endif
 
+    //LOGW("dvmCallJNIMethod_staticNoRef(%d): %s.%s\n", gettid(), method->clazz->descriptor, method->name);
     oldStatus = dvmChangeStatus(self, THREAD_NATIVE);
 
     COMPUTE_STACK_SUM(self);
@@ -1631,7 +1814,6 @@ void dvmCallJNIMethod_staticNoRef(const u4* args, JValue* pResult,
     staticMethodClass = (jobject) method->clazz;
 #endif
 
-    //LOGW("dvmCallJNIMethod_staticNoRef(%d): %s.%s\n", gettid(), method->clazz->descriptor, method->name);
     oldStatus = dvmChangeStatus(self, THREAD_NATIVE);
 
     COMPUTE_STACK_SUM(self);
@@ -2031,8 +2213,13 @@ static jobject PopLocalFrame(JNIEnv* env, jobject jresult)
  */
 static jobject NewGlobalRef(JNIEnv* env, jobject jobj)
 {
+    Object* obj;
+
     JNI_ENTER();
-    Object* obj = dvmDecodeIndirectRef(env, jobj);
+    if (dvmIsWeakGlobalRef(jobj))
+        obj = getPhantomReferent(env, (jweak) jobj);
+    else
+        obj = dvmDecodeIndirectRef(env, jobj);
     jobject retval = addGlobalReference(obj);
     JNI_EXIT();
     return retval;
@@ -2052,10 +2239,15 @@ static void DeleteGlobalRef(JNIEnv* env, jobject jglobalRef)
 /*
  * Add a reference to the local list.
  */
-static jobject NewLocalRef(JNIEnv* env, jobject jref)
+static jobject NewLocalRef(JNIEnv* env, jobject jobj)
 {
+    Object* obj;
+
     JNI_ENTER();
-    Object* obj = dvmDecodeIndirectRef(env, jref);
+    if (dvmIsWeakGlobalRef(jobj))
+        obj = getPhantomReferent(env, (jweak) jobj);
+    else
+        obj = dvmDecodeIndirectRef(env, jobj);
     jobject retval = addLocalReference(env, obj);
     JNI_EXIT();
     return retval;
@@ -3156,7 +3348,7 @@ static jint RegisterNatives(JNIEnv* env, jclass jclazz,
     JNI_ENTER();
 
     ClassObject* clazz = (ClassObject*) dvmDecodeIndirectRef(env, jclazz);
-    jint retval;
+    jint retval = JNI_OK;
     int i;
 
     if (gDvm.verboseJni) {
@@ -3169,12 +3361,9 @@ static jint RegisterNatives(JNIEnv* env, jclass jclazz,
                 methods[i].signature, methods[i].fnPtr))
         {
             retval = JNI_ERR;
-            goto bail;
         }
     }
-    retval = JNI_OK;
 
-bail:
     JNI_EXIT();
     return retval;
 }
@@ -3352,23 +3541,18 @@ static void ReleaseStringCritical(JNIEnv* env, jstring jstr,
 static jweak NewWeakGlobalRef(JNIEnv* env, jobject obj)
 {
     JNI_ENTER();
-    // TODO - implement
-    jobject gref = NULL;
-    LOGE("JNI ERROR: NewWeakGlobalRef not implemented\n");
-    dvmAbort();
+    jweak wref = createWeakGlobalRef(env, obj);
     JNI_EXIT();
-    return gref;
+    return wref;
 }
 
 /*
  * Delete the specified weak global reference.
  */
-static void DeleteWeakGlobalRef(JNIEnv* env, jweak obj)
+static void DeleteWeakGlobalRef(JNIEnv* env, jweak wref)
 {
     JNI_ENTER();
-    // TODO - implement
-    LOGE("JNI ERROR: DeleteWeakGlobalRef not implemented\n");
-    dvmAbort();
+    deleteWeakGlobalRef(env, wref);
     JNI_EXIT();
 }
 
@@ -3415,6 +3599,11 @@ static jobject NewDirectByteBuffer(JNIEnv* env, void* address, jlong capacity)
     Object* platformAddress = NULL;
     JValue callResult;
     jobject result = NULL;
+    ClassObject* tmpClazz;
+
+    tmpClazz = gDvm.methOrgApacheHarmonyLuniPlatformPlatformAddress_on->clazz;
+    if (!dvmIsClassInitialized(tmpClazz) && !dvmInitClass(tmpClazz))
+        goto bail;
 
     /* get an instance of PlatformAddress that wraps the provided address */
     dvmCallMethod(self,
@@ -3429,10 +3618,10 @@ static jobject NewDirectByteBuffer(JNIEnv* env, void* address, jlong capacity)
     LOGV("tracking %p for address=%p\n", platformAddress, address);
 
     /* create an instance of java.nio.ReadWriteDirectByteBuffer */
-    ClassObject* clazz = gDvm.classJavaNioReadWriteDirectByteBuffer;
-    if (!dvmIsClassInitialized(clazz) && !dvmInitClass(clazz))
+    tmpClazz = gDvm.classJavaNioReadWriteDirectByteBuffer;
+    if (!dvmIsClassInitialized(tmpClazz) && !dvmInitClass(tmpClazz))
         goto bail;
-    Object* newObj = dvmAllocObject(clazz, ALLOC_DONT_TRACK);
+    Object* newObj = dvmAllocObject(tmpClazz, ALLOC_DONT_TRACK);
     if (newObj != NULL) {
         /* call the (PlatformAddress, int, int) constructor */
         result = addLocalReference(env, newObj);
@@ -3739,7 +3928,8 @@ static jint DestroyJavaVM(JavaVM* vm)
     if (ext == NULL)
         return JNI_ERR;
 
-    LOGD("DestroyJavaVM waiting for non-daemon threads to exit\n");
+    if (gDvm.verboseShutdown)
+        LOGD("DestroyJavaVM waiting for non-daemon threads to exit\n");
 
     /*
      * Sleep on a condition variable until it's okay to exit.
@@ -3771,7 +3961,8 @@ shutdown:
     // TODO: call System.exit() to run any registered shutdown hooks
     // (this may not return -- figure out how this should work)
 
-    LOGD("DestroyJavaVM shutting VM down\n");
+    if (gDvm.verboseShutdown)
+        LOGD("DestroyJavaVM shutting VM down\n");
     dvmShutdown();
 
     // TODO - free resources associated with JNI-attached daemon threads
@@ -4272,4 +4463,3 @@ bail:
     free(argv);
     return result;
 }
-
diff --git a/vm/JniInternal.h b/vm/JniInternal.h
index 37920ca..302dcb0 100644
--- a/vm/JniInternal.h
+++ b/vm/JniInternal.h
@@ -201,4 +201,48 @@ DalvikJniReturnType dvmGetArgInfoReturnType(int jniArgInfo);
  */
 void dvmReleaseJniMonitors(Thread* self);
 
+/*
+ * Dump the contents of the JNI reference tables to the log file.
+ *
+ * The local ref tables associated with other threads are not included.
+ */
+void dvmDumpJniReferenceTables(void);
+
+/*
+ * This mask is applied to weak global reference values returned to
+ * native code.  The goal is to create an invalid pointer that will cause
+ * a crash if misused.  The mmap region for the virtual heap is typically
+ * around 0x40xxxxxx.
+ *
+ * To make weak global references easily distinguishable from other kinds
+ * of references when !USE_INDIRECT_REF, we XOR the low bits.  Assuming >=
+ * 64-bit alignment of objects, this changes the low 3 bits from all clear
+ * to all set.
+ */
+#define WEAK_GLOBAL_XOR 0x9e0fffff
+
+/*
+ * "Obfuscate" a weak global reference pointer.
+ */
+INLINE jweak dvmObfuscateWeakGlobalRef(jobject jobj) {
+    return (jweak) ((u4) jobj ^ WEAK_GLOBAL_XOR);
+}
+
+/*
+ * Undo the obfuscation.
+ */
+INLINE jobject dvmNormalizeWeakGlobalRef(jweak ref) {
+    return (jobject) ((u4) ref ^ WEAK_GLOBAL_XOR);
+}
+
+/*
+ * Returns "true" if this looks like a weak global reference.
+ *
+ * Relies on the low 3 bits being set instead of clear (the latter is
+ * guaranteed by 64-bit alignment of objects).
+ */
+INLINE bool dvmIsWeakGlobalRef(jobject jobj) {
+    return (((u4) jobj & 0x07) == 0x07);
+}
+
 #endif /*_DALVIK_JNIINTERNAL*/
diff --git a/vm/LinearAlloc.c b/vm/LinearAlloc.c
index 8a18af3..bf89d50 100644
--- a/vm/LinearAlloc.c
+++ b/vm/LinearAlloc.c
@@ -79,11 +79,6 @@ guard the pages on debug builds.  Handy when tracking down corruption.
 #define LENGTHFLAG_RW      0x40000000
 #define LENGTHFLAG_MASK    (~(LENGTHFLAG_FREE|LENGTHFLAG_RW))
 
-/* in case limits.h doesn't have it; must be a power of 2 */
-#ifndef PAGESIZE
-# define PAGESIZE           4096
-#endif
-
 
 /* fwd */
 static void checkAllFree(Object* classLoader);
@@ -130,7 +125,8 @@ LinearAllocHdr* dvmLinearAllocCreate(Object* classLoader)
      * chunk of data will be properly aligned.
      */
     assert(BLOCK_ALIGN >= HEADER_EXTRA);
-    pHdr->curOffset = pHdr->firstOffset = (BLOCK_ALIGN-HEADER_EXTRA) + PAGESIZE;
+    pHdr->curOffset = pHdr->firstOffset =
+        (BLOCK_ALIGN-HEADER_EXTRA) + SYSTEM_PAGE_SIZE;
     pHdr->mapLength = DEFAULT_MAX_LENGTH;
 
 #ifdef USE_ASHMEM
@@ -168,7 +164,7 @@ LinearAllocHdr* dvmLinearAllocCreate(Object* classLoader)
 #endif /*USE_ASHMEM*/
 
     /* region expected to begin on a page boundary */
-    assert(((int) pHdr->mapAddr & (PAGESIZE-1)) == 0);
+    assert(((int) pHdr->mapAddr & (SYSTEM_PAGE_SIZE-1)) == 0);
 
     /* the system should initialize newly-mapped memory to zero */
     assert(*(u4*) (pHdr->mapAddr + pHdr->curOffset) == 0);
@@ -195,7 +191,7 @@ LinearAllocHdr* dvmLinearAllocCreate(Object* classLoader)
         free(pHdr);
         return NULL;
     }
-    if (mprotect(pHdr->mapAddr + PAGESIZE, PAGESIZE,
+    if (mprotect(pHdr->mapAddr + SYSTEM_PAGE_SIZE, SYSTEM_PAGE_SIZE,
             ENFORCE_READ_ONLY ? PROT_READ : PROT_READ|PROT_WRITE) != 0)
     {
         LOGW("LinearAlloc init mprotect #2 failed: %s\n", strerror(errno));
@@ -205,7 +201,7 @@ LinearAllocHdr* dvmLinearAllocCreate(Object* classLoader)
 
     if (ENFORCE_READ_ONLY) {
         /* allocate the per-page ref count */
-        int numPages = (pHdr->mapLength+PAGESIZE-1) / PAGESIZE;
+        int numPages = (pHdr->mapLength+SYSTEM_PAGE_SIZE-1) / SYSTEM_PAGE_SIZE;
         pHdr->writeRefCount = calloc(numPages, sizeof(short));
         if (pHdr->writeRefCount == NULL) {
             free(pHdr);
@@ -240,10 +236,12 @@ void dvmLinearAllocDestroy(Object* classLoader)
 
     //dvmLinearAllocDump(classLoader);
 
-    LOGV("Unmapping linear allocator base=%p\n", pHdr->mapAddr);
-    LOGD("LinearAlloc %p used %d of %d (%d%%)\n",
-        classLoader, pHdr->curOffset, pHdr->mapLength,
-        (pHdr->curOffset * 100) / pHdr->mapLength);
+    if (gDvm.verboseShutdown) {
+        LOGV("Unmapping linear allocator base=%p\n", pHdr->mapAddr);
+        LOGD("LinearAlloc %p used %d of %d (%d%%)\n",
+            classLoader, pHdr->curOffset, pHdr->mapLength,
+            (pHdr->curOffset * 100) / pHdr->mapLength);
+    }
 
     if (munmap(pHdr->mapAddr, pHdr->mapLength) != 0) {
         LOGW("LinearAlloc munmap(%p, %d) failed: %s\n",
@@ -332,7 +330,7 @@ void* dvmLinearAlloc(Object* classLoader, size_t size)
      * See if we are starting on or have crossed into a new page.  If so,
      * call mprotect on the page(s) we're about to write to.  We have to
      * page-align the start address, but don't have to make the length a
-     * PAGESIZE multiple (but we do it anyway).
+     * SYSTEM_PAGE_SIZE multiple (but we do it anyway).
      *
      * Note that "startOffset" is not the last *allocated* byte, but rather
      * the offset of the first *unallocated* byte (which we are about to
@@ -341,9 +339,9 @@ void* dvmLinearAlloc(Object* classLoader, size_t size)
      * If ENFORCE_READ_ONLY is enabled, we have to call mprotect even if
      * we've written to this page before, because it might be read-only.
      */
-    lastGoodOff = (startOffset-1) & ~(PAGESIZE-1);
-    firstWriteOff = startOffset & ~(PAGESIZE-1);
-    lastWriteOff = (nextOffset-1) & ~(PAGESIZE-1);
+    lastGoodOff = (startOffset-1) & ~(SYSTEM_PAGE_SIZE-1);
+    firstWriteOff = startOffset & ~(SYSTEM_PAGE_SIZE-1);
+    lastWriteOff = (nextOffset-1) & ~(SYSTEM_PAGE_SIZE-1);
     LOGVV("---  lastGood=0x%04x firstWrite=0x%04x lastWrite=0x%04x\n",
         lastGoodOff, firstWriteOff, lastWriteOff);
     if (lastGoodOff != lastWriteOff || ENFORCE_READ_ONLY) {
@@ -351,7 +349,7 @@ void* dvmLinearAlloc(Object* classLoader, size_t size)
 
         start = firstWriteOff;
         assert(start <= nextOffset);
-        len = (lastWriteOff - firstWriteOff) + PAGESIZE;
+        len = (lastWriteOff - firstWriteOff) + SYSTEM_PAGE_SIZE;
 
         LOGVV("---    calling mprotect(start=%d len=%d RW)\n", start, len);
         cc = mprotect(pHdr->mapAddr + start, len, PROT_READ | PROT_WRITE);
@@ -367,8 +365,8 @@ void* dvmLinearAlloc(Object* classLoader, size_t size)
     if (ENFORCE_READ_ONLY) {
         int i, start, end;
 
-        start = firstWriteOff / PAGESIZE;
-        end = lastWriteOff / PAGESIZE;
+        start = firstWriteOff / SYSTEM_PAGE_SIZE;
+        end = lastWriteOff / SYSTEM_PAGE_SIZE;
 
         LOGVV("---  marking pages %d-%d RW (alloc %d at %p)\n",
             start, end, size, pHdr->mapAddr + startOffset + HEADER_EXTRA);
@@ -465,8 +463,8 @@ static void updatePages(Object* classLoader, void* mem, int direction)
     u4 len = *pLen & LENGTHFLAG_MASK;
     int firstPage, lastPage;
 
-    firstPage = ((u1*)pLen - (u1*)pHdr->mapAddr) / PAGESIZE;
-    lastPage = ((u1*)mem - (u1*)pHdr->mapAddr + (len-1)) / PAGESIZE;
+    firstPage = ((u1*)pLen - (u1*)pHdr->mapAddr) / SYSTEM_PAGE_SIZE;
+    lastPage = ((u1*)mem - (u1*)pHdr->mapAddr + (len-1)) / SYSTEM_PAGE_SIZE;
     LOGVV("--- updating pages %d-%d (%d)\n", firstPage, lastPage, direction);
 
     int i, cc;
@@ -496,7 +494,8 @@ static void updatePages(Object* classLoader, void* mem, int direction)
             pHdr->writeRefCount[i]--;
             if (pHdr->writeRefCount[i] == 0) {
                 LOGVV("---  prot page %d RO\n", i);
-                cc = mprotect(pHdr->mapAddr + PAGESIZE * i, PAGESIZE, PROT_READ);
+                cc = mprotect(pHdr->mapAddr + SYSTEM_PAGE_SIZE * i,
+                        SYSTEM_PAGE_SIZE, PROT_READ);
                 assert(cc == 0);
             }
         } else {
@@ -509,8 +508,8 @@ static void updatePages(Object* classLoader, void* mem, int direction)
             }
             if (pHdr->writeRefCount[i] == 0) {
                 LOGVV("---  prot page %d RW\n", i);
-                cc = mprotect(pHdr->mapAddr + PAGESIZE * i, PAGESIZE,
-                        PROT_READ | PROT_WRITE);
+                cc = mprotect(pHdr->mapAddr + SYSTEM_PAGE_SIZE * i,
+                        SYSTEM_PAGE_SIZE, PROT_READ | PROT_WRITE);
                 assert(cc == 0);
             }
             pHdr->writeRefCount[i]++;
@@ -616,7 +615,7 @@ void dvmLinearAllocDump(Object* classLoader)
                     & ~(BLOCK_ALIGN-1));
 
         LOGI("  %p (%3d): %clen=%d%s\n", pHdr->mapAddr + off + HEADER_EXTRA,
-            (int) ((off + HEADER_EXTRA) / PAGESIZE),
+            (int) ((off + HEADER_EXTRA) / SYSTEM_PAGE_SIZE),
             (rawLen & LENGTHFLAG_FREE) != 0 ? '*' : ' ',
             rawLen & LENGTHFLAG_MASK,
             (rawLen & LENGTHFLAG_RW) != 0 ? " [RW]" : "");
@@ -627,7 +626,7 @@ void dvmLinearAllocDump(Object* classLoader)
     if (ENFORCE_READ_ONLY) {
         LOGI("writeRefCount map:\n");
 
-        int numPages = (pHdr->mapLength+PAGESIZE-1) / PAGESIZE;
+        int numPages = (pHdr->mapLength+SYSTEM_PAGE_SIZE-1) / SYSTEM_PAGE_SIZE;
         int zstart = 0;
         int i;
 
diff --git a/vm/LinearAlloc.h b/vm/LinearAlloc.h
index a390ee3..aa33fe1 100644
--- a/vm/LinearAlloc.h
+++ b/vm/LinearAlloc.h
@@ -22,7 +22,6 @@
 /*
  * If this is set, we create additional data structures and make many
  * additional mprotect() calls.
- * (this breaks the debugger because the debugBreakpointCount cannot be updated)
  */
 #define ENFORCE_READ_ONLY   false
 
diff --git a/vm/Misc.c b/vm/Misc.c
index f8f7256..87f4c81 100644
--- a/vm/Misc.c
+++ b/vm/Misc.c
@@ -21,6 +21,7 @@
 #include <stdlib.h>
 #include <stddef.h>
 #include <string.h>
+#include <strings.h>
 #include <ctype.h>
 #include <time.h>
 #include <sys/time.h>
@@ -345,6 +346,39 @@ int dvmCountSetBits(const BitVector* pBits)
 }
 
 /*
+ * Copy a whole vector to the other. Only do that when the both vectors have
+ * the same size and attribute.
+ */
+bool dvmCopyBitVector(BitVector *dest, const BitVector *src)
+{
+    if (dest->storageSize != src->storageSize ||
+        dest->expandable != src->expandable)
+        return false;
+    memcpy(dest->storage, src->storage, sizeof(u4) * dest->storageSize);
+    return true;
+}
+
+/*
+ * Intersect two bit vectores and merge the result on top of the pre-existing
+ * value in the dest vector.
+ */
+bool dvmIntersectBitVectors(BitVector *dest, const BitVector *src1,
+                            const BitVector *src2)
+{
+    if (dest->storageSize != src1->storageSize ||
+        dest->storageSize != src2->storageSize ||
+        dest->expandable != src1->expandable ||
+        dest->expandable != src2->expandable)
+        return false;
+
+    int i;
+    for (i = 0; i < dest->storageSize; i++) {
+        dest->storage[i] |= src1->storage[i] & src2->storage[i];
+    }
+    return true;
+}
+
+/*
  * Return a newly-allocated string in which all occurrences of '.' have
  * been changed to '/'.  If we find a '/' in the original string, NULL
  * is returned to avoid ambiguity.
@@ -354,6 +388,9 @@ char* dvmDotToSlash(const char* str)
     char* newStr = strdup(str);
     char* cp = newStr;
 
+    if (newStr == NULL)
+        return NULL;
+
     while (*cp != '\0') {
         if (*cp == '/') {
             assert(false);
@@ -384,6 +421,9 @@ char* dvmDescriptorToDot(const char* str)
     }
 
     newStr = malloc(at + 1); /* Add one for the '\0'. */
+    if (newStr == NULL)
+        return NULL;
+
     newStr[at] = '\0';
 
     while (at > 0) {
diff --git a/vm/Misc.h b/vm/Misc.h
index 14635a3..3e04f84 100644
--- a/vm/Misc.h
+++ b/vm/Misc.h
@@ -164,6 +164,16 @@ bool dvmIsBitSet(const BitVector* pBits, int num);
 /* count the number of bits that have been set */
 int dvmCountSetBits(const BitVector* pBits);
 
+/* copy one vector to the other compatible one */
+bool dvmCopyBitVector(BitVector *dest, const BitVector *src);
+
+/*
+ * Intersect two bit vectores and merge the result on top of the pre-existing
+ * value in the dest vector.
+ */
+bool dvmIntersectBitVectors(BitVector *dest, const BitVector *src1,
+                            const BitVector *src2);
+
 #define kBitVectorGrowth    4   /* increase by 4 u4s when limit hit */
 
 
diff --git a/vm/Native.c b/vm/Native.c
index 43d62cd..e59d087 100644
--- a/vm/Native.c
+++ b/vm/Native.c
@@ -445,8 +445,13 @@ bool dvmLoadNativeCode(const char* pathName, Object* classLoader)
 {
     SharedLib* pEntry;
     void* handle;
+    bool verbose;
 
-    LOGD("Trying to load lib %s %p\n", pathName, classLoader);
+    /* reduce noise by not chattering about system libraries */
+    verbose = strncmp(pathName, "/system", sizeof("/system")-1) != 0;
+
+    if (verbose)
+        LOGD("Trying to load lib %s %p\n", pathName, classLoader);
 
     /*
      * See if we've already loaded it.  If we have, and the class loader
@@ -459,8 +464,10 @@ bool dvmLoadNativeCode(const char* pathName, Object* classLoader)
                 pathName, pEntry->classLoader, classLoader);
             return false;
         }
-        LOGD("Shared lib '%s' already loaded in same CL %p\n",
-            pathName, classLoader);
+        if (verbose) {
+            LOGD("Shared lib '%s' already loaded in same CL %p\n",
+                pathName, classLoader);
+        }
         if (!checkOnLoadResult(pEntry))
             return false;
         return true;
@@ -474,11 +481,9 @@ bool dvmLoadNativeCode(const char* pathName, Object* classLoader)
      * Failures here are expected when java.library.path has several entries
      * and we have to hunt for the lib.
      *
-     * The current android-arm dynamic linker implementation tends to
-     * return "Cannot find library" from dlerror() regardless of the actual
-     * problem.  A more useful diagnostic may be sent to stdout/stderr if
-     * linker diagnostics are enabled, but that's not usually visible in
-     * Android apps.  Some things to try:
+     * The current version of the dynamic linker prints detailed information
+     * about dlopen() failures.  Some things to check if the message is
+     * cryptic:
      *   - make sure the library exists on the device
      *   - verify that the right path is being opened (the debug log message
      *     above can help with that)
@@ -524,7 +529,8 @@ bool dvmLoadNativeCode(const char* pathName, Object* classLoader)
         freeSharedLibEntry(pNewEntry);
         return checkOnLoadResult(pActualEntry);
     } else {
-        LOGD("Added shared lib %s %p\n", pathName, classLoader);
+        if (verbose)
+            LOGD("Added shared lib %s %p\n", pathName, classLoader);
 
         bool result = true;
         void* vonLoad;
@@ -532,7 +538,8 @@ bool dvmLoadNativeCode(const char* pathName, Object* classLoader)
 
         vonLoad = dlsym(handle, "JNI_OnLoad");
         if (vonLoad == NULL) {
-            LOGD("No JNI_OnLoad found in %s %p\n", pathName, classLoader);
+            LOGD("No JNI_OnLoad found in %s %p, skipping init\n",
+                pathName, classLoader);
         } else {
             /*
              * Call JNI_OnLoad.  We have to override the current class
@@ -747,7 +754,7 @@ static int findMethodInLib(void* vlib, void* vmethod)
     int len;
 
     if (meth->clazz->classLoader != pLib->classLoader) {
-        LOGD("+++ not scanning '%s' for '%s' (wrong CL)\n",
+        LOGV("+++ not scanning '%s' for '%s' (wrong CL)\n",
             pLib->pathName, meth->name);
         return 0;
     } else
diff --git a/vm/Profile.c b/vm/Profile.c
index 7f39f1a..b079988 100644
--- a/vm/Profile.c
+++ b/vm/Profile.c
@@ -31,11 +31,10 @@
 #include <errno.h>
 #include <fcntl.h>
 
+#include <cutils/open_memstream.h>
+
 #ifdef HAVE_ANDROID_OS
 # define UPDATE_MAGIC_PAGE      1
-# ifndef PAGESIZE
-#  define PAGESIZE              4096
-# endif
 #endif
 
 /*
@@ -183,7 +182,7 @@ bool dvmProfilingStartup(void)
     if (fd < 0) {
         LOGV("Unable to open /dev/qemu_trace\n");
     } else {
-        gDvm.emulatorTracePage = mmap(0, PAGESIZE, PROT_READ|PROT_WRITE,
+        gDvm.emulatorTracePage = mmap(0, SYSTEM_PAGE_SIZE, PROT_READ|PROT_WRITE,
                                       MAP_SHARED, fd, 0);
         close(fd);
         if (gDvm.emulatorTracePage == MAP_FAILED) {
@@ -207,7 +206,7 @@ void dvmProfilingShutdown(void)
 {
 #ifdef UPDATE_MAGIC_PAGE
     if (gDvm.emulatorTracePage != NULL)
-        munmap(gDvm.emulatorTracePage, PAGESIZE);
+        munmap(gDvm.emulatorTracePage, SYSTEM_PAGE_SIZE);
 #endif
     free(gDvm.executedInstrCounts);
 }
@@ -231,6 +230,9 @@ static void updateActiveProfilers(int count)
     } while (!ATOMIC_CMP_SWAP(&gDvm.activeProfilers, oldValue, newValue));
 
     LOGD("+++ active profiler count now %d\n", newValue);
+#if defined(WITH_JIT)
+    dvmCompilerStateRefresh();
+#endif
 }
 
 
@@ -322,27 +324,30 @@ static void dumpMethodList(FILE* fp)
 }
 
 /*
- * Start method tracing.  This opens the file (if an already open fd has not
- * been supplied) and allocates the buffer.
- * If any of these fail, we throw an exception and return.
+ * Start method tracing.  Method tracing is global to the VM (i.e. we
+ * trace all threads).
+ *
+ * This opens the output file (if an already open fd has not been supplied,
+ * and we're not going direct to DDMS) and allocates the data buffer.
  *
- * Method tracing is global to the VM.
+ * On failure, we throw an exception and return.
  */
 void dvmMethodTraceStart(const char* traceFileName, int traceFd, int bufferSize,
-        int flags)
+    int flags, bool directToDdms)
 {
     MethodTraceState* state = &gDvm.methodTrace;
 
     assert(bufferSize > 0);
 
-    if (state->traceEnabled != 0) {
+    dvmLockMutex(&state->startStopLock);
+    while (state->traceEnabled != 0) {
         LOGI("TRACE start requested, but already in progress; stopping\n");
+        dvmUnlockMutex(&state->startStopLock);
         dvmMethodTraceStop();
+        dvmLockMutex(&state->startStopLock);
     }
     updateActiveProfilers(1);
-    LOGI("TRACE STARTED: '%s' %dKB\n",
-        traceFileName, bufferSize / 1024);
-    dvmLockMutex(&state->startStopLock);
+    LOGI("TRACE STARTED: '%s' %dKB\n", traceFileName, bufferSize / 1024);
 
     /*
      * Allocate storage and open files.
@@ -355,19 +360,25 @@ void dvmMethodTraceStart(const char* traceFileName, int traceFd, int bufferSize,
         dvmThrowException("Ljava/lang/InternalError;", "buffer alloc failed");
         goto fail;
     }
-    if (traceFd < 0) {
-        state->traceFile = fopen(traceFileName, "w");
-    } else {
-        state->traceFile = fdopen(traceFd, "w");
-    }
-    if (state->traceFile == NULL) {
-        LOGE("Unable to open trace file '%s': %s\n",
-            traceFileName, strerror(errno));
-        dvmThrowException("Ljava/lang/RuntimeException;", "file open failed");
-        goto fail;
+    if (!directToDdms) {
+        if (traceFd < 0) {
+            state->traceFile = fopen(traceFileName, "w");
+        } else {
+            state->traceFile = fdopen(traceFd, "w");
+        }
+        if (state->traceFile == NULL) {
+            int err = errno;
+            LOGE("Unable to open trace file '%s': %s\n",
+                traceFileName, strerror(err));
+            dvmThrowExceptionFmt("Ljava/lang/RuntimeException;",
+                "Unable to open trace file '%s': %s",
+                traceFileName, strerror(err));
+            goto fail;
+        }
     }
     memset(state->buf, (char)FILL_PATTERN, bufferSize);
 
+    state->directToDdms = directToDdms;
     state->bufferSize = bufferSize;
     state->overflow = false;
 
@@ -572,6 +583,19 @@ void dvmMethodTraceStop(void)
 
     markTouchedMethods(finalCurOffset);
 
+    char* memStreamPtr;
+    size_t memStreamSize;
+    if (state->directToDdms) {
+        assert(state->traceFile == NULL);
+        state->traceFile = open_memstream(&memStreamPtr, &memStreamSize);
+        if (state->traceFile == NULL) {
+            /* not expected */
+            LOGE("Unable to open memstream\n");
+            dvmAbort();
+        }
+    }
+    assert(state->traceFile != NULL);
+
     fprintf(state->traceFile, "%cversion\n", TOKEN_CHAR);
     fprintf(state->traceFile, "%d\n", TRACE_VERSION);
     fprintf(state->traceFile, "data-file-overflow=%s\n",
@@ -600,18 +624,36 @@ void dvmMethodTraceStop(void)
     dumpMethodList(state->traceFile);
     fprintf(state->traceFile, "%cend\n", TOKEN_CHAR);
 
-    if (fwrite(state->buf, finalCurOffset, 1, state->traceFile) != 1) {
-        LOGE("trace fwrite(%d) failed, errno=%d\n", finalCurOffset, errno);
-        dvmThrowException("Ljava/lang/RuntimeException;", "data write failed");
-        goto bail;
+    if (state->directToDdms) {
+        /*
+         * Data is in two places: memStreamPtr and state->buf.  Send
+         * the whole thing to DDMS, wrapped in an MPSE packet.
+         */
+        fflush(state->traceFile);
+
+        struct iovec iov[2];
+        iov[0].iov_base = memStreamPtr;
+        iov[0].iov_len = memStreamSize;
+        iov[1].iov_base = state->buf;
+        iov[1].iov_len = finalCurOffset;
+        dvmDbgDdmSendChunkV(CHUNK_TYPE("MPSE"), iov, 2);
+    } else {
+        /* append the profiling data */
+        if (fwrite(state->buf, finalCurOffset, 1, state->traceFile) != 1) {
+            int err = errno;
+            LOGE("trace fwrite(%d) failed, errno=%d\n", finalCurOffset, err);
+            dvmThrowExceptionFmt("Ljava/lang/RuntimeException;",
+                "Trace data write failed: %s", strerror(err));
+        }
     }
 
-bail:
+    /* done! */
     free(state->buf);
     state->buf = NULL;
     fclose(state->traceFile);
     state->traceFile = NULL;
 
+    /* wake any threads that were waiting for profiling to complete */
     int cc = pthread_cond_broadcast(&state->threadExitCond);
     assert(cc == 0);
     dvmUnlockMutex(&state->startStopLock);
diff --git a/vm/Profile.h b/vm/Profile.h
index d5dbea2..a294f83 100644
--- a/vm/Profile.h
+++ b/vm/Profile.h
@@ -39,7 +39,6 @@ void dvmProfilingShutdown(void);
 /*
  * Method trace state.  This is currently global.  In theory we could make
  * most of this per-thread.
- *
  */
 typedef struct MethodTraceState {
     /* these are set during VM init */
@@ -50,6 +49,7 @@ typedef struct MethodTraceState {
     pthread_mutex_t startStopLock;
     pthread_cond_t  threadExitCond;
     FILE*   traceFile;
+    bool    directToDdms;
     int     bufferSize;
     int     flags;
 
@@ -80,6 +80,9 @@ typedef struct AllocProfState {
 
     int     gcCount;            // #of times an allocation triggered a GC
 
+    int     classInitCount;     // #of initialized classes
+    u8      classInitTime;      // cumulative time spent in class init (nsec)
+
 #if PROFILE_EXTERNAL_ALLOCATIONS
     int     externalAllocCount; // #of calls to dvmTrackExternalAllocation()
     int     externalAllocSize;  // #of bytes passed to ...ExternalAllocation()
@@ -97,7 +100,7 @@ typedef struct AllocProfState {
  * Start/stop method tracing.
  */
 void dvmMethodTraceStart(const char* traceFileName, int traceFd, int bufferSize,
-        int flags);
+        int flags, bool directToDdms);
 bool dvmIsMethodTraceActive(void);
 void dvmMethodTraceStop(void);
 
diff --git a/vm/ReferenceTable.c b/vm/ReferenceTable.c
index a8010a2..310669d 100644
--- a/vm/ReferenceTable.c
+++ b/vm/ReferenceTable.c
@@ -188,12 +188,15 @@ static void logObject(Object* obj, int size, int identical, int equiv)
         return;
     }
 
+    /* handle "raw" dvmMalloc case */
+    const char* descriptor =
+        (obj->clazz != NULL) ? obj->clazz->descriptor : "(raw)";
+
     if (identical + equiv != 0) {
         LOGW("%5d of %s %dB (%d unique)\n", identical + equiv +1,
-            obj->clazz->descriptor, size, equiv +1);
+            descriptor, size, equiv +1);
     } else {
-        LOGW("%5d of %s %dB\n", identical + equiv +1,
-            obj->clazz->descriptor, size);
+        LOGW("%5d of %s %dB\n", identical + equiv +1, descriptor, size);
     }
 }
 
@@ -213,7 +216,7 @@ void dvmDumpReferenceTable(const ReferenceTable* pRef, const char* descr)
     int i;
 
     if (count == 0) {
-        LOGW("Reference table has no entries\n");
+        LOGW("%s reference table has no entries\n", descr);
         return;
     }
     assert(count > 0);
@@ -236,6 +239,9 @@ void dvmDumpReferenceTable(const ReferenceTable* pRef, const char* descr)
             LOGW("%5d: %p cls=%s '%s' (%d bytes)\n", i, ref,
                 (refs[i] == NULL) ? "-" : ref->clazz->descriptor,
                 clazz->descriptor, size);
+        } else if (ref->clazz == NULL) {
+            /* should only be possible right after a plain dvmMalloc() */
+            LOGW("%5d: %p cls=(raw) (%d bytes)\n", i, ref, size);
         } else {
             LOGW("%5d: %p cls=%s (%d bytes)\n", i, ref,
                 (refs[i] == NULL) ? "-" : ref->clazz->descriptor, size);
@@ -282,7 +288,7 @@ void dvmDumpReferenceTable(const ReferenceTable* pRef, const char* descr)
     total += size;
     logObject(refs[count-1], size, identical, equiv);
 
-    LOGW("Memory held directly by native code is %d bytes\n", total);
+    LOGW("Memory held directly by tracked refs is %d bytes\n", total);
     free(tableCopy);
 }
 
diff --git a/vm/SignalCatcher.c b/vm/SignalCatcher.c
index e187aeb..2b994da 100644
--- a/vm/SignalCatcher.c
+++ b/vm/SignalCatcher.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * This is a thread that catches signals and does something useful.  For
  * example, when a SIGQUIT (Ctrl-\) arrives, suspend the VM and dump the
@@ -29,6 +30,8 @@
 #include <fcntl.h>
 #include <errno.h>
 
+#include <cutils/open_memstream.h>
+
 static void* signalCatcherThreadStart(void* arg);
 
 /*
@@ -93,87 +96,157 @@ bail:
 }
 
 /*
- * Dump the stack traces for all threads to the log or to a file.  If it's
- * to a file we have a little setup to do.
+ * Dump the stack traces for all threads to the supplied file, putting
+ * a timestamp header on it.
  */
-static void logThreadStacks(void)
+static void logThreadStacks(FILE* fp)
 {
     DebugOutputTarget target;
 
+    dvmCreateFileOutputTarget(&target, fp);
+
+    pid_t pid = getpid();
+    time_t now = time(NULL);
+    struct tm* ptm;
+#ifdef HAVE_LOCALTIME_R
+    struct tm tmbuf;
+    ptm = localtime_r(&now, &tmbuf);
+#else
+    ptm = localtime(&now);
+#endif
+    dvmPrintDebugMessage(&target,
+        "\n\n----- pid %d at %04d-%02d-%02d %02d:%02d:%02d -----\n",
+        pid, ptm->tm_year + 1900, ptm->tm_mon+1, ptm->tm_mday,
+        ptm->tm_hour, ptm->tm_min, ptm->tm_sec);
+    printProcessName(&target);
+    dvmPrintDebugMessage(&target, "\n");
+    dvmDumpAllThreadsEx(&target, true);
+    fprintf(fp, "----- end %d -----\n", pid);
+}
+
+
+/*
+ * Respond to a SIGQUIT by dumping the thread stacks.  Optionally dump
+ * a few other things while we're at it.
+ *
+ * Thread stacks can either go to the log or to a file designated for holding
+ * ANR traces.  If we're writing to a file, we want to do it in one shot,
+ * so we can use a single O_APPEND write instead of contending for exclusive
+ * access with flock().  There may be an advantage in resuming the VM
+ * before doing the file write, so we don't stall the VM if disk I/O is
+ * bottlenecked.
+ *
+ * If JIT tuning is compiled in, dump compiler stats as well.
+ */
+static void handleSigQuit(void)
+{
+    char* traceBuf = NULL;
+    size_t traceLen;
+
+    dvmSuspendAllThreads(SUSPEND_FOR_STACK_DUMP);
+
+    dvmDumpLoaderStats("sig");
+
     if (gDvm.stackTraceFile == NULL) {
-        /* just dump to log file */
+        /* just dump to log */
+        DebugOutputTarget target;
         dvmCreateLogOutputTarget(&target, ANDROID_LOG_INFO, LOG_TAG);
         dvmDumpAllThreadsEx(&target, true);
     } else {
-        FILE* fp = NULL;
-        int cc, fd;
+        /* write to memory buffer */
+        FILE* memfp = open_memstream(&traceBuf, &traceLen);
+        if (memfp == NULL) {
+            LOGE("Unable to create memstream for stack traces\n");
+            traceBuf = NULL;        /* make sure it didn't touch this */
+            /* continue on */
+        } else {
+            logThreadStacks(memfp);
+            fclose(memfp);
+        }
+    }
+
+#if defined(WITH_JIT) && defined(WITH_JIT_TUNING)
+    dvmCompilerDumpStats();
+#endif
+
+    if (false) {
+        dvmLockMutex(&gDvm.jniGlobalRefLock);
+        dvmDumpReferenceTable(&gDvm.jniGlobalRefTable, "JNI global");
+        dvmUnlockMutex(&gDvm.jniGlobalRefLock);
+    }
+    if (false) dvmDumpTrackedAllocations(true);
+
+    dvmResumeAllThreads(SUSPEND_FOR_STACK_DUMP);
+
+    if (traceBuf != NULL) {
+        /*
+         * We don't know how long it will take to do the disk I/O, so put us
+         * into VMWAIT for the duration.
+         */
+        int oldStatus = dvmChangeStatus(dvmThreadSelf(), THREAD_VMWAIT);
 
         /*
          * Open the stack trace output file, creating it if necessary.  It
          * needs to be world-writable so other processes can write to it.
          */
-        fd = open(gDvm.stackTraceFile, O_WRONLY | O_APPEND | O_CREAT, 0666);
+        int fd = open(gDvm.stackTraceFile, O_WRONLY | O_APPEND | O_CREAT, 0666);
         if (fd < 0) {
             LOGE("Unable to open stack trace file '%s': %s\n",
                 gDvm.stackTraceFile, strerror(errno));
-            return;
-        }
-
-        /* gain exclusive access to the file */
-        cc = flock(fd, LOCK_EX | LOCK_UN);
-        if (cc != 0) {
-            LOGV("Sleeping on flock(%s)\n", gDvm.stackTraceFile);
-            cc = flock(fd, LOCK_EX);
-        }
-        if (cc != 0) {
-            LOGE("Unable to lock stack trace file '%s': %s\n",
-                gDvm.stackTraceFile, strerror(errno));
-            close(fd);
-            return;
-        }
-
-        fp = fdopen(fd, "a");
-        if (fp == NULL) {
-            LOGE("Unable to fdopen '%s' (%d): %s\n",
-                gDvm.stackTraceFile, fd, strerror(errno));
-            flock(fd, LOCK_UN);
+        } else {
+            ssize_t actual = write(fd, traceBuf, traceLen);
+            if (actual != (ssize_t) traceLen) {
+                LOGE("Failed to write stack traces to %s (%d of %zd): %s\n",
+                    gDvm.stackTraceFile, (int) actual, traceLen,
+                    strerror(errno));
+            } else {
+                LOGI("Wrote stack traces to '%s'\n", gDvm.stackTraceFile);
+            }
             close(fd);
-            return;
         }
 
-        dvmCreateFileOutputTarget(&target, fp);
+        free(traceBuf);
+        dvmChangeStatus(dvmThreadSelf(), oldStatus);
+    }
+}
 
-        pid_t pid = getpid();
-        time_t now = time(NULL);
-        struct tm* ptm;
-#ifdef HAVE_LOCALTIME_R
-        struct tm tmbuf;
-        ptm = localtime_r(&now, &tmbuf);
+/*
+ * Respond to a SIGUSR1 by forcing a GC.  If we were built with HPROF
+ * support, generate an HPROF dump file.
+ *
+ * (The HPROF dump generation is not all that useful now that we have
+ * better ways to generate it.  Consider removing this in a future release.)
+ */
+static void handleSigUsr1(void)
+{
+#if WITH_HPROF
+    LOGI("SIGUSR1 forcing GC and HPROF dump\n");
+    hprofDumpHeap(NULL, false);
 #else
-        ptm = localtime(&now);
+    LOGI("SIGUSR1 forcing GC (no HPROF)\n");
+    dvmCollectGarbage(false);
 #endif
-        dvmPrintDebugMessage(&target,
-            "\n\n----- pid %d at %04d-%02d-%02d %02d:%02d:%02d -----\n",
-            pid, ptm->tm_year + 1900, ptm->tm_mon+1, ptm->tm_mday,
-            ptm->tm_hour, ptm->tm_min, ptm->tm_sec);
-        printProcessName(&target);
-        dvmPrintDebugMessage(&target, "\n");
-        fflush(fp);     /* emit at least the header if we crash during dump */
-        dvmDumpAllThreadsEx(&target, true);
-        fprintf(fp, "----- end %d -----\n", pid);
-
-        /*
-         * Unlock and close the file, flushing pending data before we unlock
-         * it.  The fclose() will close the underyling fd.
-         */
-        fflush(fp);
-        flock(fd, LOCK_UN);
-        fclose(fp);
+}
 
-        LOGI("Wrote stack trace to '%s'\n", gDvm.stackTraceFile);
+#if defined(WITH_JIT) && defined(WITH_JIT_TUNING)
+/*
+ * Respond to a SIGUSR2 by dumping some JIT stats and possibly resetting
+ * the code cache.
+ */
+static void handleSigUsr2(void)
+{
+    static int codeCacheResetCount = 0;
+    if ((--codeCacheResetCount & 7) == 0) {
+        gDvmJit.codeCacheFull = true;
+    } else {
+        dvmCompilerDumpStats();
+        /* Stress-test unchain all */
+        dvmJitUnchainAll();
+        LOGD("Send %d more signals to rest the code cache",
+             codeCacheResetCount & 7);
     }
 }
-
+#endif
 
 /*
  * Sleep in sigwait() until a signal arrives.
@@ -207,11 +280,8 @@ static void* signalCatcherThreadStart(void* arg)
          * is met.  When the signal hits, we wake up, without any signal
          * handlers being invoked.
          *
-         * We want to suspend all other threads, so that it's safe to
-         * traverse their stacks.
-         *
-         * When running under GDB we occasionally return with EINTR (e.g.
-         * when other threads exit).
+         * When running under GDB we occasionally return from sigwait()
+         * with EINTR (e.g. when other threads exit).
          */
 loop:
         cc = sigwait(&mask, &rcvd);
@@ -234,41 +304,21 @@ loop:
         if (gDvm.haltSignalCatcher)
             break;
 
-        if (rcvd == SIGQUIT) {
-            dvmSuspendAllThreads(SUSPEND_FOR_STACK_DUMP);
-            dvmDumpLoaderStats("sig");
-
-            logThreadStacks();
-
-#if defined(WITH_JIT) && defined(WITH_JIT_TUNING)
-            dvmCompilerDumpStats();
-#endif
-
-            if (false) {
-                dvmLockMutex(&gDvm.jniGlobalRefLock);
-                //dvmDumpReferenceTable(&gDvm.jniGlobalRefTable, "JNI global");
-                dvmUnlockMutex(&gDvm.jniGlobalRefLock);
-            }
-
-            //dvmDumpTrackedAllocations(true);
-            dvmResumeAllThreads(SUSPEND_FOR_STACK_DUMP);
-        } else if (rcvd == SIGUSR1) {
-#if WITH_HPROF
-            LOGI("SIGUSR1 forcing GC and HPROF dump\n");
-            hprofDumpHeap(NULL);
-#else
-            LOGI("SIGUSR1 forcing GC (no HPROF)\n");
-            dvmCollectGarbage(false);
-#endif
+        switch (rcvd) {
+        case SIGQUIT:
+            handleSigQuit();
+            break;
+        case SIGUSR1:
+            handleSigUsr1();
+            break;
 #if defined(WITH_JIT) && defined(WITH_JIT_TUNING)
-        } else if (rcvd == SIGUSR2) {
-            gDvmJit.printMe ^= true;
-            dvmCompilerDumpStats();
-            /* Stress-test unchain all */
-            dvmJitUnchainAll();
+        case SIGUSR2:
+            handleSigUsr2();
+            break;
 #endif
-        } else {
+        default:
             LOGE("unexpected signal %d\n", rcvd);
+            break;
         }
     }
 
diff --git a/vm/Sync.c b/vm/Sync.c
index 1f81349..ebf2ad0 100644
--- a/vm/Sync.c
+++ b/vm/Sync.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Fundamental synchronization mechanisms.
  *
@@ -29,24 +30,10 @@
  *  - using a pool of monitor objects, with some sort of recycling scheme
  *
  * TODO: recycle native-level monitors when objects are garbage collected.
- *
- * NOTE: if we broadcast a notify, and somebody sneaks in a Thread.interrupt
- * before the notify finishes (i.e. before all threads sleeping on the
- * condition variable have awoken), we could end up with a nonzero value for
- * "notifying" after everybody is gone because one of the notified threads
- * will actually exit via the "interrupted" path.  This can be detected as
- * (notifying + interrupting > waiting), i.e. the number of threads that need
- * to be woken is greater than the number waiting.  The fix is to test and
- * adjust "notifying" at the start of the wait() call.
- * -> This is probably not a problem if we notify less than the full set
- * before the interrupt comes in.  If we have four waiters, two pending
- * notifies, and an interrupt hits, we will interrupt one thread and notify
- * two others.  Doesn't matter if the interrupted thread would have been
- * one of the notified.  Count is only screwed up if we have two waiters,
- * in which case it's safe to fix it at the start of the next wait().
  */
 #include "Dalvik.h"
 
+#include <fcntl.h>
 #include <stdlib.h>
 #include <unistd.h>
 #include <pthread.h>
@@ -92,37 +79,26 @@ static void expandObjClear(ExpandingObjectList* pList);
  * (ACM 1998).  Things are even easier for us, though, because we have
  * a full 32 bits to work with.
  *
- * The two states that an Object's lock may have are referred to as
- * "thin" and "fat".  The lock may transition between the two states
- * for various reasons.
- *
- * The lock value itself is stored in Object.lock, which is a union of
- * the form:
+ * The two states of an Object's lock are referred to as "thin" and
+ * "fat".  A lock may transition from the "thin" state to the "fat"
+ * state and this transition is referred to as inflation.  Once a lock
+ * has been inflated it remains in the "fat" state indefinitely.
  *
- *     typedef union Lock {
- *         u4          thin;
- *         Monitor*    mon;
- *     } Lock;
+ * The lock value itself is stored in Object.lock.  The LSB of the
+ * lock encodes its state.  When cleared, the lock is in the "thin"
+ * state and its bits are formatted as follows:
  *
- * It is possible to tell the current state of the lock from the actual
- * value, so we do not need to store any additional state.  When the
- * lock is "thin", it has the form:
+ *    [31 ---- 19] [18 ---- 3] [2 ---- 1] [0]
+ *     lock count   thread id  hash state  0
  *
- *     [31 ---- 16] [15 ---- 1] [0]
- *      lock count   thread id   1
+ * When set, the lock is in the "fat" state and its bits are formatted
+ * as follows:
  *
- * When it is "fat", the field is simply a (Monitor *).  Since the pointer
- * will always be 4-byte-aligned, bits 1 and 0 will always be zero when
- * the field holds a pointer.  Hence, we can tell the current fat-vs-thin
- * state by checking the least-significant bit.
+ *    [31 ---- 3] [2 ---- 1] [0]
+ *      pointer   hash state  1
  *
  * For an in-depth description of the mechanics of thin-vs-fat locking,
  * read the paper referred to above.
- *
- * To reduce the amount of work when attempting a compare and exchange,
- * Thread.threadId is guaranteed to have bit 0 set, and all new Objects
- * have their lock fields initialized to the value 0x1, or
- * DVM_LOCK_INITIAL_THIN_VALUE, via DVM_OBJECT_INIT().
  */
 
 /*
@@ -141,12 +117,9 @@ struct Monitor {
     int         lockCount;      /* owner's recursive lock depth */
     Object*     obj;            /* what object are we part of [debug only] */
 
-    int         waiting;        /* total #of threads waiting on this */
-    int         notifying;      /* #of threads being notified */
-    int         interrupting;   /* #of threads being interrupted */
+    Thread*     waitSet;	/* threads currently waiting on this monitor */
 
     pthread_mutex_t lock;
-    pthread_cond_t  cond;
 
     Monitor*    next;
 
@@ -187,9 +160,12 @@ Monitor* dvmCreateMonitor(Object* obj)
         LOGE("Unable to allocate monitor\n");
         dvmAbort();
     }
+    if (((u4)mon & 7) != 0) {
+        LOGE("Misaligned monitor: %p\n", mon);
+        dvmAbort();
+    }
     mon->obj = obj;
     dvmInitMutex(&mon->lock);
-    pthread_cond_init(&mon->cond, NULL);
 
     /* replace the head of the list with the new monitor */
     do {
@@ -201,14 +177,6 @@ Monitor* dvmCreateMonitor(Object* obj)
 }
 
 /*
- * Release a Monitor.
- */
-static void releaseMonitor(Monitor* mon)
-{
-    // TODO
-}
-
-/*
  * Free the monitor list.  Only used when shutting the VM down.
  */
 void dvmFreeMonitorList(void)
@@ -269,6 +237,43 @@ Object* dvmGetMonitorObject(Monitor* mon)
 }
 
 /*
+ * Returns the thread id of the thread owning the given lock.
+ */
+static u4 lockOwner(Object* obj)
+{
+    Thread *owner;
+    u4 lock;
+
+    assert(obj != NULL);
+    /*
+     * Since we're reading the lock value multiple times, latch it so
+     * that it doesn't change out from under us if we get preempted.
+     */
+    lock = obj->lock;
+    if (LW_SHAPE(lock) == LW_SHAPE_THIN) {
+        return LW_LOCK_OWNER(lock);
+    } else {
+        owner = LW_MONITOR(lock)->owner;
+        return owner ? owner->threadId : 0;
+    }
+}
+
+/*
+ * Get the thread that holds the lock on the specified object.  The
+ * object may be unlocked, thin-locked, or fat-locked.
+ *
+ * The caller must lock the thread list before calling here.
+ */
+Thread* dvmGetObjectLockHolder(Object* obj)
+{
+    u4 threadId = lockOwner(obj);
+
+    if (threadId == 0)
+        return NULL;
+    return dvmGetThreadByThreadId(threadId);
+}
+
+/*
  * Checks whether the given thread holds the given
  * objects's lock.
  */
@@ -276,17 +281,8 @@ bool dvmHoldsLock(Thread* thread, Object* obj)
 {
     if (thread == NULL || obj == NULL) {
         return false;
-    }
-
-    /* Since we're reading the lock value multiple times,
-     * latch it so that it doesn't change out from under
-     * us if we get preempted.
-     */
-    Lock lock = obj->lock;
-    if (IS_LOCK_FAT(&lock)) {
-        return thread == lock.mon->owner;
     } else {
-        return thread->threadId == (lock.thin & 0xffff);
+        return thread->threadId == lockOwner(obj);
     }
 }
 
@@ -294,21 +290,19 @@ bool dvmHoldsLock(Thread* thread, Object* obj)
  * Free the monitor associated with an object and make the object's lock
  * thin again.  This is called during garbage collection.
  */
-void dvmFreeObjectMonitor_internal(Lock *lock)
+static void freeObjectMonitor(Object* obj)
 {
     Monitor *mon;
 
-    /* The macro that wraps this function checks IS_LOCK_FAT() first.
-     */
-    assert(IS_LOCK_FAT(lock));
+    assert(LW_SHAPE(obj->lock) == LW_SHAPE_FAT);
 
 #ifdef WITH_DEADLOCK_PREDICTION
     if (gDvm.deadlockPredictMode != kDPOff)
-        removeCollectedObject(lock->mon->obj);
+        removeCollectedObject(obj);
 #endif
 
-    mon = lock->mon;
-    lock->thin = DVM_LOCK_INITIAL_THIN_VALUE;
+    mon = LW_MONITOR(obj->lock);
+    obj->lock = DVM_LOCK_INITIAL_THIN_VALUE;
 
     /* This lock is associated with an object
      * that's being swept.  The only possible way
@@ -319,55 +313,157 @@ void dvmFreeObjectMonitor_internal(Lock *lock)
      */
     assert(pthread_mutex_trylock(&mon->lock) == 0);
     pthread_mutex_destroy(&mon->lock);
-    pthread_cond_destroy(&mon->cond);
-#if 1
-//TODO: unlink from the monitor list (would require a lock)
-// (might not -- the GC suspension may be enough)
-    {
-        Monitor *next;
-        next = mon->next;
 #ifdef WITH_DEADLOCK_PREDICTION
-        expandObjClear(&mon->historyChildren);
-        expandObjClear(&mon->historyParents);
-        free(mon->historyRawStackTrace);
+    expandObjClear(&mon->historyChildren);
+    expandObjClear(&mon->historyParents);
+    free(mon->historyRawStackTrace);
 #endif
-        memset(mon, 0, sizeof (*mon));
-        mon->next = next;
+    free(mon);
+}
+
+/*
+ * Frees monitor objects belonging to unmarked objects.
+ */
+void dvmSweepMonitorList(Monitor** mon, int (*isUnmarkedObject)(void*))
+{
+    Monitor handle;
+    Monitor *prev, *curr;
+    Object *obj;
+
+    assert(mon != NULL);
+    assert(*mon != NULL);
+    assert(isUnmarkedObject != NULL);
+    prev = &handle;
+    prev->next = curr = *mon;
+    while (curr != NULL) {
+        obj = curr->obj;
+        if (obj != NULL && (*isUnmarkedObject)(obj) != 0) {
+            prev->next = curr = curr->next;
+            freeObjectMonitor(obj);
+        } else {
+            prev = curr;
+            curr = curr->next;
+        }
     }
-//free(mon);
-#endif
+    *mon = handle.next;
+}
+
+static char *logWriteInt(char *dst, int value)
+{
+    *dst++ = EVENT_TYPE_INT;
+    set4LE((u1 *)dst, value);
+    return dst + 4;
+}
+
+static char *logWriteString(char *dst, const char *value, size_t len)
+{
+    *dst++ = EVENT_TYPE_STRING;
+    len = len < 32 ? len : 32;
+    set4LE((u1 *)dst, len);
+    dst += 4;
+    memcpy(dst, value, len);
+    return dst + len;
 }
 
+#define EVENT_LOG_TAG_dvm_lock_sample 20003
+
+static void logContentionEvent(Thread *self, u4 waitMs, u4 samplePercent)
+{
+    const StackSaveArea *saveArea;
+    const Method *meth;
+    u4 relativePc;
+    char eventBuffer[132];
+    const char *fileName;
+    char procName[33], *selfName, *ownerName;
+    char *cp;
+    size_t len;
+    int fd;
+
+    saveArea = SAVEAREA_FROM_FP(self->curFrame);
+    meth = saveArea->method;
+    cp = eventBuffer;
+
+    /* Emit the event list length, 1 byte. */
+    *cp++ = 7;
+
+    /* Emit the process name, <= 37 bytes. */
+    fd = open("/proc/self/cmdline", O_RDONLY);
+    memset(procName, 0, sizeof(procName));
+    read(fd, procName, sizeof(procName) - 1);
+    close(fd);
+    len = strlen(procName);
+    cp = logWriteString(cp, procName, len);
+
+    /* Emit the main thread status, 5 bytes. */
+    bool isMainThread = (self->systemTid == getpid());
+    cp = logWriteInt(cp, isMainThread);
+
+    /* Emit self thread name string, <= 37 bytes. */
+    selfName = dvmGetThreadName(self);
+    cp = logWriteString(cp, selfName, strlen(selfName));
+    free(selfName);
+
+    /* Emit the wait time, 5 bytes. */
+    cp = logWriteInt(cp, waitMs);
+
+    /* Emit the source code file name, <= 37 bytes. */
+    fileName = dvmGetMethodSourceFile(meth);
+    if (fileName == NULL) fileName = "";
+    cp = logWriteString(cp, fileName, strlen(fileName));
+
+    /* Emit the source code line number, 5 bytes. */
+    relativePc = saveArea->xtra.currentPc - saveArea->method->insns;
+    cp = logWriteInt(cp, dvmLineNumFromPC(meth, relativePc));
+
+    /* Emit the sample percentage, 5 bytes. */
+    cp = logWriteInt(cp, samplePercent);
+
+    assert((size_t)(cp - eventBuffer) <= sizeof(eventBuffer));
+    android_btWriteLog(EVENT_LOG_TAG_dvm_lock_sample,
+                       EVENT_TYPE_LIST,
+                       eventBuffer,
+                       (size_t)(cp - eventBuffer));
+}
 
 /*
  * Lock a monitor.
  */
 static void lockMonitor(Thread* self, Monitor* mon)
 {
-    int cc;
+    Thread *owner;
+    ThreadStatus oldStatus;
+    u4 waitThreshold, samplePercent;
+    u8 waitStart, waitEnd, waitMs;
 
     if (mon->owner == self) {
         mon->lockCount++;
-    } else {
-        ThreadStatus oldStatus;
-
-        if (pthread_mutex_trylock(&mon->lock) != 0) {
-            /* mutex is locked, switch to wait status and sleep on it */
-            oldStatus = dvmChangeStatus(self, THREAD_MONITOR);
-            cc = pthread_mutex_lock(&mon->lock);
-            assert(cc == 0);
-            dvmChangeStatus(self, oldStatus);
+        return;
+    }
+    if (pthread_mutex_trylock(&mon->lock) != 0) {
+        oldStatus = dvmChangeStatus(self, THREAD_MONITOR);
+        waitThreshold = gDvm.lockProfThreshold;
+        if (waitThreshold) {
+            waitStart = dvmGetRelativeTimeUsec();
+        }
+        dvmLockMutex(&mon->lock);
+        if (waitThreshold) {
+            waitEnd = dvmGetRelativeTimeUsec();
+        }
+        dvmChangeStatus(self, oldStatus);
+        if (waitThreshold) {
+            waitMs = (waitEnd - waitStart) / 1000;
+            if (waitMs >= waitThreshold) {
+                samplePercent = 100;
+            } else {
+                samplePercent = 100 * waitMs / waitThreshold;
+            }
+            if (samplePercent != 0 && ((u4)rand() % 100 < samplePercent)) {
+                logContentionEvent(self, waitMs, samplePercent);
+            }
         }
-
-        mon->owner = self;
-        assert(mon->lockCount == 0);
-
-        /*
-         * "waiting", "notifying", and "interrupting" could all be nonzero
-         * if we're locking an object on which other threads are waiting.
-         * Nothing worth assert()ing about here.
-         */
     }
+    mon->owner = self;
+    assert(mon->lockCount == 0);
 }
 
 /*
@@ -403,6 +499,7 @@ static bool tryLockMonitor(Thread* self, Monitor* mon)
  */
 static bool unlockMonitor(Thread* self, Monitor* mon)
 {
+    assert(self != NULL);
     assert(mon != NULL);        // can this happen?
 
     if (mon->owner == self) {
@@ -423,20 +520,138 @@ static bool unlockMonitor(Thread* self, Monitor* mon)
          * The JNI spec says that we should throw IllegalMonitorStateException
          * in this case.
          */
-        if (mon->owner == NULL) {
-            //LOGW("Unlock fat %p: not owned\n", mon->obj);
-        } else {
-            //LOGW("Unlock fat %p: id %d vs %d\n",
-            //    mon->obj, mon->owner->threadId, self->threadId);
-        }
         dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
-            "unlock of unowned monitor");
+                          "unlock of unowned monitor");
         return false;
     }
     return true;
 }
 
 /*
+ * Checks the wait set for circular structure.  Returns 0 if the list
+ * is not circular.  Otherwise, returns 1.  Used only by asserts.
+ */
+static int waitSetCheck(Monitor *mon)
+{
+    Thread *fast, *slow;
+    size_t n;
+
+    assert(mon != NULL);
+    fast = slow = mon->waitSet;
+    n = 0;
+    for (;;) {
+        if (fast == NULL) return 0;
+        if (fast->waitNext == NULL) return 0;
+        if (fast == slow && n > 0) return 1;
+        n += 2;
+        fast = fast->waitNext->waitNext;
+        slow = slow->waitNext;
+    }
+}
+
+/*
+ * Links a thread into a monitor's wait set.  The monitor lock must be
+ * held by the caller of this routine.
+ */
+static void waitSetAppend(Monitor *mon, Thread *thread)
+{
+    Thread *elt;
+
+    assert(mon != NULL);
+    assert(mon->owner == dvmThreadSelf());
+    assert(thread != NULL);
+    assert(thread->waitNext == NULL);
+    assert(waitSetCheck(mon) == 0);
+    if (mon->waitSet == NULL) {
+        mon->waitSet = thread;
+        return;
+    }
+    elt = mon->waitSet;
+    while (elt->waitNext != NULL) {
+        elt = elt->waitNext;
+    }
+    elt->waitNext = thread;
+}
+
+/*
+ * Unlinks a thread from a monitor's wait set.  The monitor lock must
+ * be held by the caller of this routine.
+ */
+static void waitSetRemove(Monitor *mon, Thread *thread)
+{
+    Thread *elt;
+
+    assert(mon != NULL);
+    assert(mon->owner == dvmThreadSelf());
+    assert(thread != NULL);
+    assert(waitSetCheck(mon) == 0);
+    if (mon->waitSet == NULL) {
+        return;
+    }
+    if (mon->waitSet == thread) {
+        mon->waitSet = thread->waitNext;
+        thread->waitNext = NULL;
+        return;
+    }
+    elt = mon->waitSet;
+    while (elt->waitNext != NULL) {
+        if (elt->waitNext == thread) {
+            elt->waitNext = thread->waitNext;
+            thread->waitNext = NULL;
+            return;
+        }
+        elt = elt->waitNext;
+    }
+}
+
+/*
+ * Converts the given relative waiting time into an absolute time.
+ */
+void absoluteTime(s8 msec, s4 nsec, struct timespec *ts)
+{
+    s8 endSec;
+
+#ifdef HAVE_TIMEDWAIT_MONOTONIC
+    clock_gettime(CLOCK_MONOTONIC, ts);
+#else
+    {
+        struct timeval tv;
+        gettimeofday(&tv, NULL);
+        ts->tv_sec = tv.tv_sec;
+        ts->tv_nsec = tv.tv_usec * 1000;
+    }
+#endif
+    endSec = ts->tv_sec + msec / 1000;
+    if (endSec >= 0x7fffffff) {
+        LOGV("NOTE: end time exceeds epoch\n");
+        endSec = 0x7ffffffe;
+    }
+    ts->tv_sec = endSec;
+    ts->tv_nsec = (ts->tv_nsec + (msec % 1000) * 1000000) + nsec;
+
+    /* catch rollover */
+    if (ts->tv_nsec >= 1000000000L) {
+        ts->tv_sec++;
+        ts->tv_nsec -= 1000000000L;
+    }
+}
+
+int dvmRelativeCondWait(pthread_cond_t* cond, pthread_mutex_t* mutex,
+                        s8 msec, s4 nsec)
+{
+    int ret;
+    struct timespec ts;
+    absoluteTime(msec, nsec, &ts);
+#if defined(HAVE_TIMEDWAIT_MONOTONIC)
+    ret = pthread_cond_timedwait_monotonic(cond, mutex, &ts);
+#else
+    ret = pthread_cond_timedwait(cond, mutex, &ts);
+#endif
+    assert(ret == 0 || ret == ETIMEDOUT);
+    return ret;
+}
+
+/*
  * Wait on a monitor until timeout, interrupt, or notification.  Used for
  * Object.wait() and (somewhat indirectly) Thread.sleep() and Thread.join().
  *
@@ -465,10 +680,13 @@ static void waitMonitor(Thread* self, Monitor* mon, s8 msec, s4 nsec,
     struct timespec ts;
     bool wasInterrupted = false;
     bool timed;
-    int cc;
+    int ret;
+
+    assert(self != NULL);
+    assert(mon != NULL);
 
-    /* Make sure that the lock is fat and that we hold it. */
-    if (mon == NULL || ((u4)mon & 1) != 0 || mon->owner != self) {
+    /* Make sure that we hold the lock. */
+    if (mon->owner != self) {
         dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
             "object not locked by thread before wait()");
         return;
@@ -489,65 +707,23 @@ static void waitMonitor(Thread* self, Monitor* mon, s8 msec, s4 nsec,
     if (msec == 0 && nsec == 0) {
         timed = false;
     } else {
-        s8 endSec;
-
-#ifdef HAVE_TIMEDWAIT_MONOTONIC
-        struct timespec now;
-        clock_gettime(CLOCK_MONOTONIC, &now);
-        endSec = now.tv_sec + msec / 1000;
-        if (endSec >= 0x7fffffff) {
-            LOGV("NOTE: end time exceeds epoch\n");
-            endSec = 0x7ffffffe;
-        }
-        ts.tv_sec = endSec;
-        ts.tv_nsec = (now.tv_nsec + (msec % 1000) * 1000 * 1000) + nsec;
-#else
-        struct timeval now;
-        gettimeofday(&now, NULL);
-        endSec = now.tv_sec + msec / 1000;
-        if (endSec >= 0x7fffffff) {
-            LOGV("NOTE: end time exceeds epoch\n");
-            endSec = 0x7ffffffe;
-        }
-        ts.tv_sec = endSec;
-        ts.tv_nsec = (now.tv_usec + (msec % 1000) * 1000) * 1000 + nsec;
-#endif
-
-        /* catch rollover */
-        if (ts.tv_nsec >= 1000000000L) {
-            ts.tv_sec++;
-            ts.tv_nsec -= 1000000000L;
-        }
+        absoluteTime(msec, nsec, &ts);
         timed = true;
     }
 
     /*
-     * Make sure "notifying" wasn't screwed up by earlier activity.  If this
-     * is wrong we could end up waking up too many people.  (This is a rare
-     * situation, but we need to handle it correctly.)
-     */
-    if (mon->notifying + mon->interrupting > mon->waiting) {
-        LOGD("threadid=%d: bogus mon %d+%d>%d; adjusting\n",
-            self->threadId, mon->notifying, mon->interrupting,
-            mon->waiting);
-
-        assert(mon->waiting >= mon->interrupting);
-        mon->notifying = mon->waiting - mon->interrupting;
-    }
-
-    /*
      * Add ourselves to the set of threads waiting on this monitor, and
      * release our hold.  We need to let it go even if we're a few levels
      * deep in a recursive lock, and we need to restore that later.
      *
-     * The order of operations here isn't significant, because we still
-     * hold the pthread mutex.
+     * We append to the wait set ahead of clearing the count and owner
+     * fields so the subroutine can check that the calling thread owns
+     * the monitor.  Aside from that, the order of member updates is
+     * not order sensitive as we hold the pthread mutex.
      */
-    int prevLockCount;
-
-    prevLockCount = mon->lockCount;
+    waitSetAppend(mon, self);
+    int prevLockCount = mon->lockCount;
     mon->lockCount = 0;
-    mon->waiting++;
     mon->owner = NULL;
 
     /*
@@ -560,12 +736,15 @@ static void waitMonitor(Thread* self, Monitor* mon, s8 msec, s4 nsec,
     else
         dvmChangeStatus(self, THREAD_WAIT);
 
+    ret = pthread_mutex_lock(&self->waitMutex);
+    assert(ret == 0);
+
     /*
-     * Tell the thread which monitor we're waiting on.  This is necessary
-     * so that Thread.interrupt() can wake us up.  Thread.interrupt needs
-     * to gain ownership of the monitor mutex before it can signal us, so
-     * we're still not worried about race conditions.
+     * Set waitMonitor to the monitor object we will be waiting on.
+     * When waitMonitor is non-NULL a notifying or interrupting thread
+     * must signal the thread's waitCond to wake it up.
      */
+    assert(self->waitMonitor == NULL);
     self->waitMonitor = mon;
 
     /*
@@ -574,82 +753,50 @@ static void waitMonitor(Thread* self, Monitor* mon, s8 msec, s4 nsec,
      */
     if (self->interrupted) {
         wasInterrupted = true;
+        self->waitMonitor = NULL;
+        pthread_mutex_unlock(&self->waitMutex);
         goto done;
     }
 
-    LOGVV("threadid=%d: waiting on %p\n", self->threadId, mon);
+    /*
+     * Release the monitor lock and wait for a notification or
+     * a timeout to occur.
+     */
+    pthread_mutex_unlock(&mon->lock);
 
-    while (true) {
-        if (!timed) {
-            cc = pthread_cond_wait(&mon->cond, &mon->lock);
-            assert(cc == 0);
-        } else {
+    if (!timed) {
+        ret = pthread_cond_wait(&self->waitCond, &self->waitMutex);
+        assert(ret == 0);
+    } else {
 #ifdef HAVE_TIMEDWAIT_MONOTONIC
-            cc = pthread_cond_timedwait_monotonic(&mon->cond, &mon->lock, &ts);
+        ret = pthread_cond_timedwait_monotonic(&self->waitCond, &self->waitMutex, &ts);
 #else
-            cc = pthread_cond_timedwait(&mon->cond, &mon->lock, &ts);
+        ret = pthread_cond_timedwait(&self->waitCond, &self->waitMutex, &ts);
 #endif
-            if (cc == ETIMEDOUT) {
-                LOGVV("threadid=%d wakeup: timeout\n", self->threadId);
-                break;
-            }
-        }
-
-        /*
-         * We woke up because of an interrupt (which does a broadcast) or
-         * a notification (which might be a signal or a broadcast).  Figure
-         * out what we need to do.
-         */
-        if (self->interruptingWait) {
-            /*
-             * The other thread successfully gained the monitor lock, and
-             * has confirmed that we were waiting on it.  If this is an
-             * interruptible wait, we bail out immediately.  If not, we
-             * continue on.
-             */
-            self->interruptingWait = false;
-            mon->interrupting--;
-            assert(self->interrupted);
-            if (interruptShouldThrow) {
-                wasInterrupted = true;
-                LOGD("threadid=%d wakeup: interrupted\n", self->threadId);
-                break;
-            } else {
-                LOGD("threadid=%d wakeup: not interruptible\n", self->threadId);
-            }
-        }
-        if (mon->notifying) {
-            /*
-             * One or more threads are being notified.  Remove ourselves
-             * from the set.
-             */
-            mon->notifying--;
-            LOGVV("threadid=%d wakeup: notified\n", self->threadId);
-            break;
-        } else {
-            /*
-             * Looks like we were woken unnecessarily, probably as a
-             * result of another thread being interrupted.  Go back to
-             * sleep.
-             */
-            LOGVV("threadid=%d wakeup: going back to sleep\n", self->threadId);
-        }
+        assert(ret == 0 || ret == ETIMEDOUT);
+    }
+    if (self->interrupted) {
+        wasInterrupted = true;
     }
 
-done:
-    //if (wasInterrupted) {
-    //    LOGW("threadid=%d: throwing InterruptedException:\n", self->threadId);
-    //    dvmDumpThread(self, false);
-    //}
+    self->interrupted = false;
+    self->waitMonitor = NULL;
 
+    pthread_mutex_unlock(&self->waitMutex);
+
+    /* Reacquire the monitor lock. */
+    lockMonitor(self, mon);
+
+done:
     /*
-     * Put everything back.  Again, we hold the pthread mutex, so the order
-     * here isn't significant.
+     * We remove our thread from wait set after restoring the count
+     * and owner fields so the subroutine can check that the calling
+     * thread owns the monitor. Aside from that, the order of member
+     * updates is not order sensitive as we hold the pthread mutex.
      */
-    self->waitMonitor = NULL;
     mon->owner = self;
-    mon->waiting--;
     mon->lockCount = prevLockCount;
+    waitSetRemove(mon, self);
 
     /* set self->status back to THREAD_RUNNING, and self-suspend if needed */
     dvmChangeStatus(self, THREAD_RUNNING);
@@ -657,7 +804,7 @@ done:
     if (wasInterrupted) {
         /*
          * We were interrupted while waiting, or somebody interrupted an
-         * un-interruptable thread earlier and we're bailing out immediately.
+         * un-interruptible thread earlier and we're bailing out immediately.
          *
          * The doc sayeth: "The interrupted status of the current thread is
          * cleared when this exception is thrown."
@@ -673,66 +820,63 @@ done:
  */
 static void notifyMonitor(Thread* self, Monitor* mon)
 {
-    /* Make sure that the lock is fat and that we hold it. */
-    if (mon == NULL || ((u4)mon & 1) != 0 || mon->owner != self) {
+    Thread* thread;
+
+    assert(self != NULL);
+    assert(mon != NULL);
+
+    /* Make sure that we hold the lock. */
+    if (mon->owner != self) {
         dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
             "object not locked by thread before notify()");
         return;
     }
-
-    /*
-     * Check to see if anybody is there to notify.  We subtract off
-     * threads that are being interrupted and anything that has
-     * potentially already been notified.
-     */
-    if (mon->notifying + mon->interrupting < mon->waiting) {
-        /* wake up one thread */
-        int cc;
-
-        LOGVV("threadid=%d: signaling on %p\n", self->threadId, mon);
-
-        mon->notifying++;
-        cc = pthread_cond_signal(&mon->cond);
-        assert(cc == 0);
-    } else {
-        LOGVV("threadid=%d: nobody to signal on %p\n", self->threadId, mon);
+    /* Signal the first waiting thread in the wait set. */
+    while (mon->waitSet != NULL) {
+        thread = mon->waitSet;
+        mon->waitSet = thread->waitNext;
+        thread->waitNext = NULL;
+        pthread_mutex_lock(&thread->waitMutex);
+        /* Check to see if the thread is still waiting. */
+        if (thread->waitMonitor != NULL) {
+            pthread_cond_signal(&thread->waitCond);
+            pthread_mutex_unlock(&thread->waitMutex);
+            return;
+        }
+        pthread_mutex_unlock(&thread->waitMutex);
     }
 }
 
 /*
  * Notify all threads waiting on this monitor.
- *
- * We keep a count of how many threads we notified, so that our various
- * counts remain accurate.
  */
 static void notifyAllMonitor(Thread* self, Monitor* mon)
 {
-    /* Make sure that the lock is fat and that we hold it. */
-    if (mon == NULL || ((u4)mon & 1) != 0 || mon->owner != self) {
+    Thread* thread;
+
+    assert(self != NULL);
+    assert(mon != NULL);
+
+    /* Make sure that we hold the lock. */
+    if (mon->owner != self) {
         dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
             "object not locked by thread before notifyAll()");
         return;
     }
-
-    mon->notifying = mon->waiting - mon->interrupting;
-    if (mon->notifying > 0) {
-        int cc;
-
-        LOGVV("threadid=%d: broadcasting to %d threads on %p\n",
-            self->threadId, mon->notifying, mon);
-
-        cc = pthread_cond_broadcast(&mon->cond);
-        assert(cc == 0);
-    } else {
-        LOGVV("threadid=%d: nobody to broadcast to on %p\n", self->threadId,mon);
+    /* Signal all threads in the wait set. */
+    while (mon->waitSet != NULL) {
+        thread = mon->waitSet;
+        mon->waitSet = thread->waitNext;
+        thread->waitNext = NULL;
+        pthread_mutex_lock(&thread->waitMutex);
+        /* Check to see if the thread is still waiting. */
+        if (thread->waitMonitor != NULL) {
+            pthread_cond_signal(&thread->waitCond);
+        }
+        pthread_mutex_unlock(&thread->waitMutex);
     }
 }
 
-#if THIN_LOCKING
-/*
- * Thin locking support
- */
-
 /*
  * Implements monitorenter for "synchronized" stuff.
  *
@@ -741,65 +885,86 @@ static void notifyAllMonitor(Thread* self, Monitor* mon)
  */
 void dvmLockObject(Thread* self, Object *obj)
 {
-    volatile u4 *thinp = &obj->lock.thin;
-    u4 threadId = self->threadId;
-
-    /* First, try to grab the lock as if it's thin;
-     * this is the common case and will usually succeed.
-     */
-    if (!ATOMIC_CMP_SWAP((int32_t *)thinp,
-                         (int32_t)DVM_LOCK_INITIAL_THIN_VALUE,
-                         (int32_t)threadId)) {
-        /* The lock is either a thin lock held by someone (possibly 'self'),
-         * or a fat lock.
+    volatile u4 *thinp;
+    Monitor *mon;
+    ThreadStatus oldStatus;
+    useconds_t sleepDelay;
+    const useconds_t maxSleepDelay = 1 << 20;
+    u4 thin, newThin, threadId;
+
+    assert(self != NULL);
+    assert(obj != NULL);
+    threadId = self->threadId;
+    thinp = &obj->lock;
+retry:
+    thin = *thinp;
+    if (LW_SHAPE(thin) == LW_SHAPE_THIN) {
+        /*
+         * The lock is a thin lock.  The owner field is used to
+         * determine the acquire method, ordered by cost.
          */
-        if ((*thinp & 0xffff) == threadId) {
-            /* 'self' is already holding the thin lock; we can just
-             * bump the count.  Atomic operations are not necessary
-             * because only the thread holding the lock is allowed
-             * to modify the Lock field.
+        if (LW_LOCK_OWNER(thin) == threadId) {
+            /*
+             * The calling thread owns the lock.  Increment the
+             * value of the recursion count field.
              */
-            *thinp += 1<<16;
-        } else {
-            /* If this is a thin lock we need to spin on it, if it's fat
-             * we need to acquire the monitor.
+            obj->lock += 1 << LW_LOCK_COUNT_SHIFT;
+        } else if (LW_LOCK_OWNER(thin) == 0) {
+            /*
+             * The lock is unowned.  Install the thread id of the
+             * calling thread into the owner field.  This is the
+             * common case.  In performance critical code the JIT
+             * will have tried this before calling out to the VM.
              */
-            if ((*thinp & 1) != 0) {
-                ThreadStatus oldStatus;
-                static const unsigned long maxSleepDelay = 1 * 1024 * 1024;
-                unsigned long sleepDelay;
-
-                LOG_THIN("(%d) spin on lock 0x%08x: 0x%08x (0x%08x) 0x%08x\n",
-                         threadId, (uint)&obj->lock,
-                         DVM_LOCK_INITIAL_THIN_VALUE, *thinp, threadId);
-
-                /* The lock is still thin, but some other thread is
-                 * holding it.  Let the VM know that we're about
-                 * to wait on another thread.
+            newThin = thin | (threadId << LW_LOCK_OWNER_SHIFT);
+            if (!ATOMIC_CMP_SWAP((int32_t *)thinp, thin, newThin)) {
+                /*
+                 * The acquire failed.  Try again.
                  */
-                oldStatus = dvmChangeStatus(self, THREAD_MONITOR);
-
-                /* Spin until the other thread lets go.
+                goto retry;
+            }
+        } else {
+            LOG_THIN("(%d) spin on lock %p: %#x (%#x) %#x",
+                     threadId, &obj->lock, 0, *thinp, thin);
+            /*
+             * The lock is owned by another thread.  Notify the VM
+             * that we are about to wait.
+             */
+            oldStatus = dvmChangeStatus(self, THREAD_MONITOR);
+            /*
+             * Spin until the thin lock is released or inflated.
+             */
+            sleepDelay = 0;
+            for (;;) {
+                thin = *thinp;
+                /*
+                 * Check the shape of the lock word.  Another thread
+                 * may have inflated the lock while we were waiting.
                  */
-                sleepDelay = 0;
-                do {
-                    /* In addition to looking for an unlock,
-                     * we need to watch out for some other thread
-                     * fattening the lock behind our back.
-                     */
-                    while (*thinp != DVM_LOCK_INITIAL_THIN_VALUE) {
-                        if ((*thinp & 1) == 0) {
-                            /* The lock has been fattened already.
+                if (LW_SHAPE(thin) == LW_SHAPE_THIN) {
+                    if (LW_LOCK_OWNER(thin) == 0) {
+                        /*
+                         * The lock has been released.  Install the
+                         * thread id of the calling thread into the
+                         * owner field.
+                         */
+                        newThin = thin | (threadId << LW_LOCK_OWNER_SHIFT);
+                        if (ATOMIC_CMP_SWAP((int32_t *)thinp,
+                                            thin, newThin)) {
+                            /*
+                             * The acquire succeed.  Break out of the
+                             * loop and proceed to inflate the lock.
                              */
-                            LOG_THIN("(%d) lock 0x%08x surprise-fattened\n",
-                                     threadId, (uint)&obj->lock);
-                            dvmChangeStatus(self, oldStatus);
-                            goto fat_lock;
+                            break;
                         }
-
+                    } else {
+                        /*
+                         * The lock has not been released.  Yield so
+                         * the owning thread can run.
+                         */
                         if (sleepDelay == 0) {
                             sched_yield();
-                            sleepDelay = 1 * 1000;
+                            sleepDelay = 1000;
                         } else {
                             usleep(sleepDelay);
                             if (sleepDelay < maxSleepDelay / 2) {
@@ -807,41 +972,44 @@ void dvmLockObject(Thread* self, Object *obj)
                             }
                         }
                     }
-                } while (!ATOMIC_CMP_SWAP((int32_t *)thinp,
-                                          (int32_t)DVM_LOCK_INITIAL_THIN_VALUE,
-                                          (int32_t)threadId));
-                LOG_THIN("(%d) spin on lock done 0x%08x: "
-                         "0x%08x (0x%08x) 0x%08x\n",
-                         threadId, (uint)&obj->lock,
-                         DVM_LOCK_INITIAL_THIN_VALUE, *thinp, threadId);
-
-                /* We've got the thin lock; let the VM know that we're
-                 * done waiting.
-                 */
-                dvmChangeStatus(self, oldStatus);
-
-                /* Fatten the lock.  Note this relinquishes ownership.
-                 * We could also create the monitor in an "owned" state
-                 * to avoid "re-locking" it in fat_lock.
-                 */
-                obj->lock.mon = dvmCreateMonitor(obj);
-                LOG_THIN("(%d) lock 0x%08x fattened\n",
-                         threadId, (uint)&obj->lock);
-
-                /* Fall through to acquire the newly fat lock.
-                 */
+                } else {
+                    /*
+                     * The thin lock was inflated by another thread.
+                     * Let the VM know we are no longer waiting and
+                     * try again.
+                     */
+                    LOG_THIN("(%d) lock %p surprise-fattened",
+                             threadId, &obj->lock);
+                    dvmChangeStatus(self, oldStatus);
+                    goto retry;
+                }
             }
-
-            /* The lock is already fat, which means
-             * that obj->lock.mon is a regular (Monitor *).
+            LOG_THIN("(%d) spin on lock done %p: %#x (%#x) %#x",
+                     threadId, &obj->lock, 0, *thinp, thin);
+            /*
+             * We have acquired the thin lock.  Let the VM know that
+             * we are no longer waiting.
+             */
+            dvmChangeStatus(self, oldStatus);
+            /*
+             * Fatten the lock.
              */
-        fat_lock:
-            assert(obj->lock.mon != NULL);
-            lockMonitor(self, obj->lock.mon);
+            mon = dvmCreateMonitor(obj);
+            lockMonitor(self, mon);
+            thin = *thinp;
+            thin &= LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT;
+            thin |= (u4)mon | LW_SHAPE_FAT;
+            MEM_BARRIER();
+            obj->lock = thin;
+            LOG_THIN("(%d) lock %p fattened", threadId, &obj->lock);
         }
+    } else {
+        /*
+         * The lock is a fat lock.
+         */
+        assert(LW_MONITOR(obj->lock) != NULL);
+        lockMonitor(self, LW_MONITOR(obj->lock));
     }
-    // else, the lock was acquired with the ATOMIC_CMP_SWAP().
-
 #ifdef WITH_DEADLOCK_PREDICTION
     /*
      * See if we were allowed to grab the lock at this time.  We do it
@@ -901,41 +1069,59 @@ void dvmLockObject(Thread* self, Object *obj)
  */
 bool dvmUnlockObject(Thread* self, Object *obj)
 {
-    volatile u4 *thinp = &obj->lock.thin;
-    u4 threadId = self->threadId;
+    u4 thin;
 
-    /* Check the common case, where 'self' has locked 'obj' once, first.
+    assert(self != NULL);
+    assert(self->status == THREAD_RUNNING);
+    assert(obj != NULL);
+    /*
+     * Cache the lock word as its value can change while we are
+     * examining its state.
      */
-    if (*thinp == threadId) {
-        /* Unlock 'obj' by clearing our threadId from 'thin'.
-         * The lock protects the lock field itself, so it's
-         * safe to update non-atomically.
-         */
-        *thinp = DVM_LOCK_INITIAL_THIN_VALUE;
-    } else if ((*thinp & 1) != 0) {
-        /* If the object is locked, it had better be locked by us.
+    thin = obj->lock;
+    if (LW_SHAPE(thin) == LW_SHAPE_THIN) {
+        /*
+         * The lock is thin.  We must ensure that the lock is owned
+         * by the given thread before unlocking it.
          */
-        if ((*thinp & 0xffff) != threadId) {
-            /* The JNI spec says that we should throw an exception
-             * in this case.
+        if (LW_LOCK_OWNER(thin) == self->threadId) {
+            /*
+             * We are the lock owner.  It is safe to update the lock
+             * without CAS as lock ownership guards the lock itself.
+             */
+            if (LW_LOCK_COUNT(thin) == 0) {
+                /*
+                 * The lock was not recursively acquired, the common
+                 * case.  Unlock by clearing all bits except for the
+                 * hash state.
+                 */
+                obj->lock &= (LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT);
+            } else {
+                /*
+                 * The object was recursively acquired.  Decrement the
+                 * lock recursion count field.
+                 */
+                obj->lock -= 1 << LW_LOCK_COUNT_SHIFT;
+            }
+        } else {
+            /*
+             * We do not own the lock.  The JVM spec requires that we
+             * throw an exception in this case.
              */
-            //LOGW("Unlock thin %p: id %d vs %d\n",
-            //    obj, (*thinp & 0xfff), threadId);
             dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
-                "unlock of unowned monitor");
+                              "unlock of unowned monitor");
             return false;
         }
-
-        /* It's a thin lock, but 'self' has locked 'obj'
-         * more than once.  Decrement the count.
-         */
-        *thinp -= 1<<16;
     } else {
-        /* It's a fat lock.
+        /*
+         * The lock is fat.  We must check to see if unlockMonitor has
+         * raised any exceptions before continuing.
          */
-        assert(obj->lock.mon != NULL);
-        if (!unlockMonitor(self, obj->lock.mon)) {
-            /* exception has been raised */
+        assert(LW_MONITOR(obj->lock) != NULL);
+        if (!unlockMonitor(self, LW_MONITOR(obj->lock))) {
+            /*
+             * An exception has been raised.  Do not fall through.
+             */
             return false;
         }
     }
@@ -956,15 +1142,16 @@ bool dvmUnlockObject(Thread* self, Object *obj)
 void dvmObjectWait(Thread* self, Object *obj, s8 msec, s4 nsec,
     bool interruptShouldThrow)
 {
-    Monitor* mon = obj->lock.mon;
-    u4 thin = obj->lock.thin;
+    Monitor* mon = LW_MONITOR(obj->lock);
+    u4 hashState;
+    u4 thin = obj->lock;
 
     /* If the lock is still thin, we need to fatten it.
      */
-    if ((thin & 1) != 0) {
+    if (LW_SHAPE(thin) == LW_SHAPE_THIN) {
         /* Make sure that 'self' holds the lock.
          */
-        if ((thin & 0xffff) != self->threadId) {
+        if (LW_LOCK_OWNER(thin) != self->threadId) {
             dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
                 "object not locked by thread before wait()");
             return;
@@ -981,13 +1168,17 @@ void dvmObjectWait(Thread* self, Object *obj, s8 msec, s4 nsec,
          * make sure that the monitor reflects this.
          */
         lockMonitor(self, mon);
-        mon->lockCount = thin >> 16;
+        mon->lockCount = LW_LOCK_COUNT(thin);
         LOG_THIN("(%d) lock 0x%08x fattened by wait() to count %d\n",
                  self->threadId, (uint)&obj->lock, mon->lockCount);
 
+
         /* Make the monitor public now that it's in the right state.
          */
-        obj->lock.mon = mon;
+        thin &= LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT;
+        thin |= (u4)mon | LW_SHAPE_FAT;
+        MEM_BARRIER();
+        obj->lock = thin;
     }
 
     waitMonitor(self, mon, msec, nsec, interruptShouldThrow);
@@ -998,16 +1189,15 @@ void dvmObjectWait(Thread* self, Object *obj, s8 msec, s4 nsec,
  */
 void dvmObjectNotify(Thread* self, Object *obj)
 {
-    Monitor* mon = obj->lock.mon;
-    u4 thin = obj->lock.thin;
+    u4 thin = obj->lock;
 
     /* If the lock is still thin, there aren't any waiters;
      * waiting on an object forces lock fattening.
      */
-    if ((thin & 1) != 0) {
+    if (LW_SHAPE(thin) == LW_SHAPE_THIN) {
         /* Make sure that 'self' holds the lock.
          */
-        if ((thin & 0xffff) != self->threadId) {
+        if (LW_LOCK_OWNER(thin) != self->threadId) {
             dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
                 "object not locked by thread before notify()");
             return;
@@ -1018,7 +1208,7 @@ void dvmObjectNotify(Thread* self, Object *obj)
     } else {
         /* It's a fat lock.
          */
-        notifyMonitor(self, mon);
+        notifyMonitor(self, LW_MONITOR(thin));
     }
 }
 
@@ -1027,15 +1217,15 @@ void dvmObjectNotify(Thread* self, Object *obj)
  */
 void dvmObjectNotifyAll(Thread* self, Object *obj)
 {
-    u4 thin = obj->lock.thin;
+    u4 thin = obj->lock;
 
     /* If the lock is still thin, there aren't any waiters;
      * waiting on an object forces lock fattening.
      */
-    if ((thin & 1) != 0) {
+    if (LW_SHAPE(thin) == LW_SHAPE_THIN) {
         /* Make sure that 'self' holds the lock.
          */
-        if ((thin & 0xffff) != self->threadId) {
+        if (LW_LOCK_OWNER(thin) != self->threadId) {
             dvmThrowException("Ljava/lang/IllegalMonitorStateException;",
                 "object not locked by thread before notifyAll()");
             return;
@@ -1044,83 +1234,13 @@ void dvmObjectNotifyAll(Thread* self, Object *obj)
         /* no-op;  there are no waiters to notify.
          */
     } else {
-        Monitor* mon = obj->lock.mon;
-
         /* It's a fat lock.
          */
-        notifyAllMonitor(self, mon);
-    }
-}
-
-#else  // not THIN_LOCKING
-
-/*
- * Implements monitorenter for "synchronized" stuff.
- *
- * This does not fail or throw an exception.
- */
-void dvmLockObject(Thread* self, Object* obj)
-{
-    Monitor* mon = obj->lock.mon;
-
-    if (mon == NULL) {
-        mon = dvmCreateMonitor(obj);
-        if (!ATOMIC_CMP_SWAP((int32_t *)&obj->lock.mon,
-                             (int32_t)NULL, (int32_t)mon)) {
-            /* somebody else beat us to it */
-            releaseMonitor(mon);
-            mon = obj->lock.mon;
-        }
+        notifyAllMonitor(self, LW_MONITOR(thin));
     }
-
-    lockMonitor(self, mon);
 }
 
 /*
- * Implements monitorexit for "synchronized" stuff.
- */
-bool dvmUnlockObject(Thread* self, Object* obj)
-{
-    Monitor* mon = obj->lock.mon;
-
-    return unlockMonitor(self, mon);
-}
-
-
-/*
- * Object.wait().
- */
-void dvmObjectWait(Thread* self, Object* obj, u8 msec, u4 nsec)
-{
-    Monitor* mon = obj->lock.mon;
-
-    waitMonitor(self, mon, msec, nsec);
-}
-
-/*
- * Object.notify().
- */
-void dvmObjectNotify(Thread* self, Object* obj)
-{
-    Monitor* mon = obj->lock.mon;
-
-    notifyMonitor(self, mon);
-}
-
-/*
- * Object.notifyAll().
- */
-void dvmObjectNotifyAll(Thread* self, Object* obj)
-{
-    Monitor* mon = obj->lock.mon;
-
-    notifyAllMonitor(self, mon);
-}
-
-#endif  // not THIN_LOCKING
-
-
-/*
  * This implements java.lang.Thread.sleep(long msec, int nsec).
  *
  * The sleep is interruptible by other threads, which means we can't just
@@ -1150,20 +1270,21 @@ void dvmThreadSleep(u8 msec, u4 nsec)
 
 /*
  * Implement java.lang.Thread.interrupt().
- *
- * We need to increment the monitor's "interrupting" count, and set the
- * interrupted status for the thread in question.  Doing so requires
- * gaining the monitor's lock, which may not happen in a timely fashion.
- * We are left with a decision between failing to interrupt the thread
- * and stalling the interrupting thread.
- *
- * We must take some care to ensure that we don't try to interrupt the same
- * thread on the same mutex twice.  Doing so would leave us with an
- * incorrect value for Monitor.interrupting.
  */
-void dvmThreadInterrupt(volatile Thread* thread)
+void dvmThreadInterrupt(Thread* thread)
 {
-    Monitor* mon;
+    assert(thread != NULL);
+
+    pthread_mutex_lock(&thread->waitMutex);
+
+    /*
+     * If the interrupted flag is already set no additional action is
+     * required.
+     */
+    if (thread->interrupted == true) {
+        pthread_mutex_unlock(&thread->waitMutex);
+        return;
+    }
 
     /*
      * Raise the "interrupted" flag.  This will cause it to bail early out
@@ -1180,81 +1301,203 @@ void dvmThreadInterrupt(volatile Thread* thread)
      * is only set when a thread actually waits on a monitor,
      * which implies that the monitor has already been fattened.
      */
-    mon = thread->waitMonitor;
-    if (mon == NULL)
-        return;
+    if (thread->waitMonitor != NULL) {
+        pthread_cond_signal(&thread->waitCond);
+    }
 
-    /*
-     * Try to acquire the monitor, if we don't already own it.  We need
-     * to hold the same mutex as the thread in order to signal the
-     * condition it's waiting on.  When the thread goes to sleep it will
-     * release the monitor's mutex, allowing us to signal it.
-     *
-     * TODO: we may be able to get rid of the explicit lock by coordinating
-     * this more closely with waitMonitor.
-     */
-    Thread* self = dvmThreadSelf();
-    if (!tryLockMonitor(self, mon)) {
+    pthread_mutex_unlock(&thread->waitMutex);
+}
+
+#ifndef WITH_COPYING_GC
+u4 dvmIdentityHashCode(Object *obj)
+{
+    return (u4)obj;
+}
+#else
+static size_t arrayElementWidth(const ArrayObject *array)
+{
+    const char *descriptor;
+
+    if (dvmIsObjectArray(array)) {
+	return sizeof(Object *);
+    } else {
+	descriptor = array->obj.clazz->descriptor;
+        switch (descriptor[1]) {
+        case 'B': return 1;  /* byte */
+        case 'C': return 2;  /* char */
+        case 'D': return 8;  /* double */
+        case 'F': return 4;  /* float */
+        case 'I': return 4;  /* int */
+        case 'J': return 8;  /* long */
+        case 'S': return 2;  /* short */
+        case 'Z': return 1;  /* boolean */
+        }
+    }
+    LOGE("object %p has an unhandled descriptor '%s'", array, descriptor);
+    dvmDumpThread(dvmThreadSelf(), false);
+    dvmAbort();
+    return 0;  /* Quiet the compiler. */
+}
+
+static size_t arrayObjectLength(const ArrayObject *array)
+{
+    size_t length;
+
+    length = offsetof(ArrayObject, contents);
+    length += array->length * arrayElementWidth(array);
+    return length;
+}
+
+/*
+ * Returns the identity hash code of the given object.
+ */
+u4 dvmIdentityHashCode(Object *obj)
+{
+    Thread *self, *thread;
+    volatile u4 *lw;
+    size_t length;
+    u4 lock, owner, hashState;
+
+    if (obj == NULL) {
         /*
-         * Failed to get the monitor the thread is waiting on; most likely
-         * the other thread is in the middle of doing something.
+         * Null is defined to have an identity hash code of 0.
          */
-        const int kSpinSleepTime = 500*1000;        /* 0.5s */
-        u8 startWhen = dvmGetRelativeTimeUsec();
-        int sleepIter = 0;
-
-        while (dvmIterativeSleep(sleepIter++, kSpinSleepTime, startWhen)) {
+        return 0;
+    }
+    lw = &obj->lock;
+retry:
+    hashState = LW_HASH_STATE(*lw);
+    if (hashState == LW_HASH_STATE_HASHED) {
+        /*
+         * The object has been hashed but has not had its hash code
+         * relocated by the garbage collector.  Use the raw object
+         * address.
+         */
+        return (u4)obj >> 3;
+    } else if (hashState == LW_HASH_STATE_HASHED_AND_MOVED) {
+        /*
+         * The object has been hashed and its hash code has been
+         * relocated by the collector.  Use the value of the naturally
+         * aligned word following the instance data.
+         */
+        if (IS_CLASS_FLAG_SET(obj->clazz, CLASS_ISARRAY)) {
+            length = arrayObjectLength((ArrayObject *)obj);
+            length = (length + 3) & ~3;
+        } else {
+            length = obj->clazz->objectSize;
+        }
+        return *(u4 *)(((char *)obj) + length);
+    } else if (hashState == LW_HASH_STATE_UNHASHED) {
+        /*
+         * The object has never been hashed.  Change the hash state to
+         * hashed and use the raw object address.
+         */
+        self = dvmThreadSelf();
+        if (self->threadId == lockOwner(obj)) {
             /*
-             * Still time left on the clock, try to grab it again.
+             * We already own the lock so we can update the hash state
+             * directly.
              */
-            if (tryLockMonitor(self, mon))
-                goto gotit;
-
+            *lw |= (LW_HASH_STATE_HASHED << LW_HASH_STATE_SHIFT);
+            return (u4)obj >> 3;
+        }
+        /*
+         * We do not own the lock.  Try acquiring the lock.  Should
+         * this fail, we must suspend the owning thread.
+         */
+        if (LW_SHAPE(*lw) == LW_SHAPE_THIN) {
             /*
-             * If the target thread is no longer waiting on the same monitor,
-             * the "interrupted" flag we set earlier will have caused the
-             * interrupt when the thread woke up, so we can stop now.
+             * If the lock is thin assume it is unowned.  We simulate
+             * an acquire, update, and release with a single CAS.
              */
-            if (thread->waitMonitor != mon)
-                return;
+            lock = DVM_LOCK_INITIAL_THIN_VALUE;
+            lock |= (LW_HASH_STATE_HASHED << LW_HASH_STATE_SHIFT);
+            if (ATOMIC_CMP_SWAP((int32_t *)lw,
+                                (int32_t)DVM_LOCK_INITIAL_THIN_VALUE,
+                                (int32_t)lock)) {
+                /*
+                 * A new lockword has been installed with a hash state
+                 * of hashed.  Use the raw object address.
+                 */
+                return (u4)obj >> 3;
+            }
+        } else {
+            if (tryLockMonitor(self, LW_MONITOR(*lw))) {
+                /*
+                 * The monitor lock has been acquired.  Change the
+                 * hash state to hashed and use the raw object
+                 * address.
+                 */
+                *lw |= (LW_HASH_STATE_HASHED << LW_HASH_STATE_SHIFT);
+                unlockMonitor(self, LW_MONITOR(*lw));
+                return (u4)obj >> 3;
+            }
         }
-
         /*
-         * We have to give up or risk deadlock.
+         * At this point we have failed to acquire the lock.  We must
+         * identify the owning thread and suspend it.
          */
-        LOGW("threadid=%d: unable to interrupt threadid=%d\n",
-            self->threadId, thread->threadId);
-        return;
-    }
-
-gotit:
-    /*
-     * We've got the monitor lock, which means nobody can be added or
-     * removed from the wait list.  This also means that the Thread's
-     * waitMonitor/interruptingWait fields can't be modified by anyone
-     * else.
-     *
-     * If things look good, raise flags and wake the threads sleeping
-     * on the monitor's condition variable.
-     */
-    if (thread->waitMonitor == mon &&       // still on same monitor?
-        thread->interrupted &&              // interrupt still pending?
-        !thread->interruptingWait)          // nobody else is interrupting too?
-    {
-        int cc;
-
-        LOGVV("threadid=%d: interrupting threadid=%d waiting on %p\n",
-            self->threadId, thread->threadId, mon);
-
-        thread->interruptingWait = true;    // prevent re-interrupt...
-        mon->interrupting++;                // ...so we only do this once
-        cc = pthread_cond_broadcast(&mon->cond);
-        assert(cc == 0);
+        dvmLockThreadList(self);
+        /*
+         * Cache the lock word as its value can change between
+         * determining its shape and retrieving its owner.
+         */
+        lock = *lw;
+        if (LW_SHAPE(lock) == LW_SHAPE_THIN) {
+            /*
+             * Find the thread with the corresponding thread id.
+             */
+            owner = LW_LOCK_OWNER(lock);
+            assert(owner != self->threadId);
+            /*
+             * If the lock has no owner do not bother scanning the
+             * thread list and fall through to the failure handler.
+             */
+            thread = owner ? gDvm.threadList : NULL;
+            while (thread != NULL) {
+                if (thread->threadId == owner) {
+                    break;
+                }
+                thread = thread->next;
+            }
+        } else {
+            thread = LW_MONITOR(lock)->owner;
+        }
+        /*
+         * If thread is NULL the object has been released since the
+         * thread list lock was acquired.  Try again.
+         */
+        if (thread == NULL) {
+            dvmUnlockThreadList();
+            goto retry;
+        }
+        /*
+         * Wait for the owning thread to suspend.
+         */
+        dvmSuspendThread(thread);
+        if (dvmHoldsLock(thread, obj)) {
+            /*
+             * The owning thread has been suspended.  We can safely
+             * change the hash state to hashed.
+             */
+            *lw |= (LW_HASH_STATE_HASHED << LW_HASH_STATE_SHIFT);
+            dvmResumeThread(thread);
+            dvmUnlockThreadList();
+            return (u4)obj >> 3;
+        }
+        /*
+         * The wrong thread has been suspended.  Try again.
+         */
+        dvmResumeThread(thread);
+        dvmUnlockThreadList();
+        goto retry;
     }
-
-    unlockMonitor(self, mon);
+    LOGE("object %p has an unknown hash state %#x", obj, hashState);
+    dvmDumpThread(dvmThreadSelf(), false);
+    dvmAbort();
+    return 0;  /* Quiet the compiler. */
 }
-
+#endif  /* WITH_COPYING_GC */
 
 #ifdef WITH_DEADLOCK_PREDICTION
 /*
@@ -1462,13 +1705,13 @@ static int expandObjCheckForDuplicates(const ExpandingObjectList* pList)
  */
 static bool objectInChildList(const Object* parent, Object* child)
 {
-    Lock lock = parent->lock;
+    u4 lock = parent->lock;
     if (!IS_LOCK_FAT(&lock)) {
         //LOGI("on thin\n");
         return false;
     }
 
-    return expandObjHas(&lock.mon->historyChildren, child);
+    return expandObjHas(&LW_MONITOR(lock)->historyChildren, child);
 }
 
 /*
@@ -1476,7 +1719,7 @@ static bool objectInChildList(const Object* parent, Object* child)
  */
 static void dumpKids(Object* parent)
 {
-    Monitor* mon = parent->lock.mon;
+    Monitor* mon = LW_MONITOR(parent->lock);
 
     printf("Children of %p:", parent);
     expandObjDump(&mon->historyChildren);
@@ -1489,17 +1732,17 @@ static void dumpKids(Object* parent)
  */
 static void linkParentToChild(Object* parent, Object* child)
 {
-    //assert(parent->lock.mon->owner == dvmThreadSelf());   // !owned for merge
+    //assert(LW_MONITOR(parent->lock)->owner == dvmThreadSelf());   // !owned for merge
     assert(IS_LOCK_FAT(&parent->lock));
     assert(IS_LOCK_FAT(&child->lock));
     assert(parent != child);
     Monitor* mon;
 
-    mon = parent->lock.mon;
+    mon = LW_MONITOR(parent->lock);
     assert(!expandObjHas(&mon->historyChildren, child));
     expandObjAddEntry(&mon->historyChildren, child);
 
-    mon = child->lock.mon;
+    mon = LW_MONITOR(child->lock);
     assert(!expandObjHas(&mon->historyParents, parent));
     expandObjAddEntry(&mon->historyParents, parent);
 }
@@ -1510,20 +1753,20 @@ static void linkParentToChild(Object* parent, Object* child)
  */
 static void unlinkParentFromChild(Object* parent, Object* child)
 {
-    //assert(parent->lock.mon->owner == dvmThreadSelf());   // !owned for GC
+    //assert(LW_MONITOR(parent->lock)->owner == dvmThreadSelf());   // !owned for GC
     assert(IS_LOCK_FAT(&parent->lock));
     assert(IS_LOCK_FAT(&child->lock));
     assert(parent != child);
     Monitor* mon;
 
-    mon = parent->lock.mon;
+    mon = LW_MONITOR(parent->lock);
     if (!expandObjRemoveEntry(&mon->historyChildren, child)) {
         LOGW("WARNING: child %p not found in parent %p\n", child, parent);
     }
     assert(!expandObjHas(&mon->historyChildren, child));
     assert(expandObjCheckForDuplicates(&mon->historyChildren) < 0);
 
-    mon = child->lock.mon;
+    mon = LW_MONITOR(child->lock);
     if (!expandObjRemoveEntry(&mon->historyParents, parent)) {
         LOGW("WARNING: parent %p not found in child %p\n", parent, child);
     }
@@ -1566,7 +1809,7 @@ static void logHeldMonitors(Thread* self)
 static bool traverseTree(Thread* self, const Object* obj)
 {
     assert(IS_LOCK_FAT(&obj->lock));
-    Monitor* mon = obj->lock.mon;
+    Monitor* mon = LW_MONITOR(obj->lock);
 
     /*
      * Have we been here before?
@@ -1610,7 +1853,7 @@ static bool traverseTree(Thread* self, const Object* obj)
     int i;
     for (i = expandBufGetCount(pList)-1; i >= 0; i--) {
         const Object* child = expandBufGetEntry(pList, i);
-        Lock lock = child->lock;
+        u4 lock = child->lock;
         if (!IS_LOCK_FAT(&lock))
             continue;
         if (!traverseTree(self, child)) {
@@ -1667,16 +1910,17 @@ static void updateDeadlockPrediction(Thread* self, Object* acqObj)
      */
     if (!IS_LOCK_FAT(&acqObj->lock)) {
         LOGVV("fattening lockee %p (recur=%d)\n",
-            acqObj, acqObj->lock.thin >> 16);
+            acqObj, LW_LOCK_COUNT(acqObj->lock.thin));
         Monitor* newMon = dvmCreateMonitor(acqObj);
         lockMonitor(self, newMon);      // can't stall, don't need VMWAIT
-        newMon->lockCount += acqObj->lock.thin >> 16;
-        acqObj->lock.mon = newMon;
+        newMon->lockCount += LW_LOCK_COUNT(acqObj->lock);
+        u4 hashState = LW_HASH_STATE(acqObj->lock) << LW_HASH_STATE_SHIFT;
+        acqObj->lock = (u4)newMon | hashState | LW_SHAPE_FAT;
     }
 
     /* if we don't have a stack trace for this monitor, establish one */
-    if (acqObj->lock.mon->historyRawStackTrace == NULL) {
-        Monitor* mon = acqObj->lock.mon;
+    if (LW_MONITOR(acqObj->lock)->historyRawStackTrace == NULL) {
+        Monitor* mon = LW_MONITOR(acqObj->lock);
         mon->historyRawStackTrace = dvmFillInStackTraceRaw(self,
             &mon->historyStackDepth);
     }
@@ -1714,11 +1958,12 @@ static void updateDeadlockPrediction(Thread* self, Object* acqObj)
      */
     if (!IS_LOCK_FAT(&mrl->obj->lock)) {
         LOGVV("fattening parent %p f/b/o child %p (recur=%d)\n",
-            mrl->obj, acqObj, mrl->obj->lock.thin >> 16);
+            mrl->obj, acqObj, LW_LOCK_COUNT(mrl->obj->lock));
         Monitor* newMon = dvmCreateMonitor(mrl->obj);
         lockMonitor(self, newMon);      // can't stall, don't need VMWAIT
-        newMon->lockCount += mrl->obj->lock.thin >> 16;
-        mrl->obj->lock.mon = newMon;
+        newMon->lockCount += LW_LOCK_COUNT(mrl->obj->lock);
+        u4 hashState = LW_HASH_STATE(mrl->obj->lock) << LW_HASH_STATE_SHIFT;
+        mrl->obj->lock = (u4)newMon | hashState | LW_SHAPE_FAT;
     }
 
     /*
@@ -1781,7 +2026,7 @@ static void mergeChildren(Object* parent, const Object* child)
     int i;
 
     assert(IS_LOCK_FAT(&child->lock));
-    mon = child->lock.mon;
+    mon = LW_MONITOR(child->lock);
     ExpandingObjectList* pList = &mon->historyChildren;
 
     for (i = expandBufGetCount(pList)-1; i >= 0; i--) {
@@ -1840,11 +2085,11 @@ static void removeCollectedObject(Object* obj)
     int i;
 
     assert(IS_LOCK_FAT(&obj->lock));
-    mon = obj->lock.mon;
+    mon = LW_MONITOR(obj->lock);
     pList = &mon->historyParents;
     for (i = expandBufGetCount(pList)-1; i >= 0; i--) {
         Object* parent = expandBufGetEntry(pList, i);
-        Monitor* parentMon = parent->lock.mon;
+        Monitor* parentMon = LW_MONITOR(parent->lock);
 
         if (!expandObjRemoveEntry(&parentMon->historyChildren, obj)) {
             LOGW("WARNING: child %p not found in parent %p\n", obj, parent);
@@ -1861,7 +2106,7 @@ static void removeCollectedObject(Object* obj)
     pList = &mon->historyChildren;
     for (i = expandBufGetCount(pList)-1; i >= 0; i--) {
         Object* child = expandBufGetEntry(pList, i);
-        Monitor* childMon = child->lock.mon;
+        Monitor* childMon = LW_MONITOR(child->lock);
 
         if (!expandObjRemoveEntry(&childMon->historyParents, obj)) {
             LOGW("WARNING: parent %p not found in child %p\n", obj, child);
@@ -1871,4 +2116,3 @@ static void removeCollectedObject(Object* obj)
 }
 
 #endif /*WITH_DEADLOCK_PREDICTION*/
-
diff --git a/vm/Sync.h b/vm/Sync.h
index 6baa46d..1a168b9 100644
--- a/vm/Sync.h
+++ b/vm/Sync.h
@@ -19,6 +19,50 @@
 #ifndef _DALVIK_SYNC
 #define _DALVIK_SYNC
 
+/*
+ * Monitor shape field.  Used to distinguish immediate thin locks from
+ * indirecting fat locks.
+ */
+#define LW_SHAPE_THIN 0
+#define LW_SHAPE_FAT 1
+#define LW_SHAPE_MASK 0x1
+#define LW_SHAPE(x) ((x) & LW_SHAPE_MASK)
+
+/*
+ * Hash state field.  Used to signify that an object has had its
+ * identity hash code exposed or relocated.
+ */
+#define LW_HASH_STATE_UNHASHED 0
+#define LW_HASH_STATE_HASHED 1
+#define LW_HASH_STATE_HASHED_AND_MOVED 3
+#define LW_HASH_STATE_MASK 0x3
+#define LW_HASH_STATE_SHIFT 1
+#define LW_HASH_STATE(x) (((x) >> LW_HASH_STATE_SHIFT) & LW_HASH_STATE_MASK)
+
+/*
+ * Monitor accessor.  Extracts a monitor structure pointer from a fat
+ * lock.  Performs no error checking.
+ */
+#define LW_MONITOR(x) \
+  ((Monitor*)((x) & ~((LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT) | \
+                      LW_SHAPE_MASK)))
+
+/*
+ * Lock owner field.  Contains the thread id of the thread currently
+ * holding the lock.
+ */
+#define LW_LOCK_OWNER_MASK 0xffff
+#define LW_LOCK_OWNER_SHIFT 3
+#define LW_LOCK_OWNER(x) (((x) >> LW_LOCK_OWNER_SHIFT) & LW_LOCK_OWNER_MASK)
+
+/*
+ * Lock recursion count field.  Contains a count of the numer of times
+ * a lock has been recursively acquired.
+ */
+#define LW_LOCK_COUNT_MASK 0x1fff
+#define LW_LOCK_COUNT_SHIFT 19
+#define LW_LOCK_COUNT(x) (((x) >> LW_LOCK_COUNT_SHIFT) & LW_LOCK_COUNT_MASK)
+
 struct Object;
 struct Monitor;
 struct Thread;
@@ -27,34 +71,18 @@ typedef struct Monitor Monitor;
 #define QUIET_ZYGOTE_MONITOR 1
 
 /*
- * Synchronization lock, included in every object.
- *
- * We want this to be a 32-bit "thin lock", holding the lock level and
- * the owner's threadId, that inflates to a Monitor pointer when there
- * is contention or somebody waits on it.
- */
-typedef union Lock {
-    u4          thin;
-    Monitor*    mon;
-} Lock;
-
-/*
  * Initialize a Lock to the proper starting value.
  * This is necessary for thin locking.
  */
-#define THIN_LOCKING 1
-#if THIN_LOCKING
-#define DVM_LOCK_INITIAL_THIN_VALUE (0x1)
-#else
 #define DVM_LOCK_INITIAL_THIN_VALUE (0)
-#endif
+
 #define DVM_LOCK_INIT(lock) \
-    do { (lock)->thin = DVM_LOCK_INITIAL_THIN_VALUE; } while (0)
+    do { *(lock) = DVM_LOCK_INITIAL_THIN_VALUE; } while (0)
 
 /*
  * Returns true if the lock has been fattened.
  */
-#define IS_LOCK_FAT(lock)   (((lock)->thin & 1) == 0 && (lock)->mon != NULL)
+#define IS_LOCK_FAT(lock)   (LW_SHAPE(*(lock)) == LW_SHAPE_FAT)
 
 /*
  * Acquire the object's monitor.
@@ -75,6 +103,11 @@ void dvmObjectNotify(struct Thread* self, struct Object* obj);
 void dvmObjectNotifyAll(struct Thread* self, struct Object* obj);
 
 /*
+ * Implementation of System.identityHashCode().
+ */
+u4 dvmIdentityHashCode(struct Object* obj);
+
+/*
  * Implementation of Thread.sleep().
  */
 void dvmThreadSleep(u8 msec, u4 nsec);
@@ -84,20 +117,17 @@ void dvmThreadSleep(u8 msec, u4 nsec);
  *
  * Interrupt a thread.  If it's waiting on a monitor, wake it up.
  */
-void dvmThreadInterrupt(volatile struct Thread* thread);
+void dvmThreadInterrupt(struct Thread* thread);
 
 /* create a new Monitor struct */
 Monitor* dvmCreateMonitor(struct Object* obj);
 
-/* free an object's monitor during GC */
-void dvmFreeObjectMonitor_internal(Lock* lock);
-#define dvmFreeObjectMonitor(obj) \
-    do { \
-        Object *DFM_obj_ = (obj); \
-        if (IS_LOCK_FAT(&DFM_obj_->lock)) { \
-            dvmFreeObjectMonitor_internal(&DFM_obj_->lock); \
-        } \
-    } while (0)
+/*
+ * Frees unmarked monitors from the monitor list.  The given callback
+ * routine should return a non-zero value when passed a pointer to an
+ * unmarked object.
+ */
+void dvmSweepMonitorList(Monitor** mon, int (*isUnmarkedObject)(void*));
 
 /* free monitor list */
 void dvmFreeMonitorList(void);
@@ -111,11 +141,25 @@ void dvmFreeMonitorList(void);
 struct Object* dvmGetMonitorObject(Monitor* mon);
 
 /*
+ * Get the thread that holds the lock on the specified object.  The
+ * object may be unlocked, thin-locked, or fat-locked.
+ *
+ * The caller must lock the thread list before calling here.
+ */
+struct Thread* dvmGetObjectLockHolder(struct Object* obj);
+
+/*
  * Checks whether the object is held by the specified thread.
  */
 bool dvmHoldsLock(struct Thread* thread, struct Object* obj);
 
 /*
+ * Relative timed wait on condition
+ */
+int dvmRelativeCondWait(pthread_cond_t* cond, pthread_mutex_t* mutex,
+                         s8 msec, s4 nsec);
+
+/*
  * Debug.
  */
 void dvmDumpMonitorInfo(const char* msg);
diff --git a/vm/Thread.c b/vm/Thread.c
index be3e952..50f5416 100644
--- a/vm/Thread.c
+++ b/vm/Thread.c
@@ -25,17 +25,22 @@
 #include <stdlib.h>
 #include <unistd.h>
 #include <sys/time.h>
+#include <sys/types.h>
 #include <sys/resource.h>
 #include <sys/mman.h>
+#include <signal.h>
 #include <errno.h>
 #include <fcntl.h>
 
-#include <cutils/sched_policy.h>
-
 #if defined(HAVE_PRCTL)
 #include <sys/prctl.h>
 #endif
 
+#if defined(WITH_SELF_VERIFICATION)
+#include "interp/Jit.h"         // need for self verification
+#endif
+
+
 /* desktop Linux needs a little help with gettid() */
 #if defined(HAVE_GETTID) && !defined(HAVE_ANDROID_OS)
 #define __KERNEL__
@@ -219,8 +224,8 @@ Some other concerns with flinging signals around:
    use printf on stdout to print GC debug messages)
 */
 
-#define kMaxThreadId        ((1<<15) - 1)
-#define kMainThreadId       ((1<<1) | 1)
+#define kMaxThreadId        ((1 << 16) - 1)
+#define kMainThreadId       1
 
 
 static Thread* allocThread(int interpStackSize);
@@ -516,6 +521,12 @@ static const char* getSuspendCauseStr(SuspendCause why)
     case SUSPEND_FOR_DEBUG:         return "debug";
     case SUSPEND_FOR_DEBUG_EVENT:   return "debug-event";
     case SUSPEND_FOR_STACK_DUMP:    return "stack-dump";
+#if defined(WITH_JIT)
+    case SUSPEND_FOR_TBL_RESIZE:    return "table-resize";
+    case SUSPEND_FOR_IC_PATCH:      return "inline-cache-patch";
+    case SUSPEND_FOR_CC_RESET:      return "reset-code-cache";
+    case SUSPEND_FOR_REFRESH:       return "refresh jit status";
+#endif
     default:                        return "UNKNOWN";
     }
 }
@@ -554,6 +565,9 @@ static void lockThreadSuspend(const char* who, SuspendCause why)
                  *
                  * Could be the debugger telling us to resume at roughly
                  * the same time we're posting an event.
+                 *
+                 * Could be two app threads both want to patch predicted
+                 * chaining cells around the same time.
                  */
                 LOGI("threadid=%d ODD: want thread-suspend lock (%s:%s),"
                      " it's held, no suspend pending\n",
@@ -667,6 +681,8 @@ void dvmSlayDaemons(void)
     dvmUnlockThreadList();
 
     if (doWait) {
+        bool complained = false;
+
         usleep(200 * 1000);
 
         dvmLockThreadList(self);
@@ -687,9 +703,10 @@ void dvmSlayDaemons(void)
                 }
 
                 if (target->status == THREAD_RUNNING && !target->isSuspended) {
-                    LOGD("threadid=%d not ready yet\n", target->threadId);
+                    if (!complained)
+                        LOGD("threadid=%d not ready yet\n", target->threadId);
                     allSuspended = false;
-                    break;
+                    /* keep going so we log each running daemon once */
                 }
 
                 target = target->next;
@@ -699,7 +716,11 @@ void dvmSlayDaemons(void)
                 LOGD("threadid=%d: all daemons have suspended\n", threadId);
                 break;
             } else {
-                LOGD("threadid=%d: waiting for daemons to suspend\n", threadId);
+                if (!complained) {
+                    complained = true;
+                    LOGD("threadid=%d: waiting briefly for daemon suspension\n",
+                        threadId);
+                }
             }
 
             usleep(200 * 1000);
@@ -895,6 +916,11 @@ static Thread* allocThread(int interpStackSize)
     if (thread == NULL)
         return NULL;
 
+#if defined(WITH_SELF_VERIFICATION)
+    if (dvmSelfVerificationShadowSpaceAlloc(thread) == NULL)
+        return NULL;
+#endif
+
     assert(interpStackSize >= kMinStackSize && interpStackSize <=kMaxStackSize);
 
     thread->status = THREAD_INITIALIZING;
@@ -914,6 +940,9 @@ static Thread* allocThread(int interpStackSize)
 #ifdef MALLOC_INTERP_STACK
     stackBottom = (u1*) malloc(interpStackSize);
     if (stackBottom == NULL) {
+#if defined(WITH_SELF_VERIFICATION)
+        dvmSelfVerificationShadowSpaceFree(thread);
+#endif
         free(thread);
         return NULL;
     }
@@ -922,6 +951,9 @@ static Thread* allocThread(int interpStackSize)
     stackBottom = mmap(NULL, interpStackSize, PROT_READ | PROT_WRITE,
         MAP_PRIVATE | MAP_ANON, -1, 0);
     if (stackBottom == MAP_FAILED) {
+#if defined(WITH_SELF_VERIFICATION)
+        dvmSelfVerificationShadowSpaceFree(thread);
+#endif
         free(thread);
         return NULL;
     }
@@ -977,7 +1009,7 @@ static bool prepareThread(Thread* thread)
     /*
      * Initialize invokeReq.
      */
-    pthread_mutex_init(&thread->invokeReq.lock, NULL);
+    dvmInitMutex(&thread->invokeReq.lock);
     pthread_cond_init(&thread->invokeReq.cv, NULL);
 
     /*
@@ -1005,6 +1037,9 @@ static bool prepareThread(Thread* thread)
 
     memset(&thread->jniMonitorRefTable, 0, sizeof(thread->jniMonitorRefTable));
 
+    pthread_cond_init(&thread->waitCond, NULL);
+    dvmInitMutex(&thread->waitMutex);
+
     return true;
 }
 
@@ -1061,6 +1096,9 @@ static void freeThread(Thread* thread)
     if (&thread->jniMonitorRefTable.table != NULL)
         dvmClearReferenceTable(&thread->jniMonitorRefTable);
 
+#if defined(WITH_SELF_VERIFICATION)
+    dvmSelfVerificationShadowSpaceFree(thread);
+#endif
     free(thread);
 }
 
@@ -1098,7 +1136,7 @@ static void setThreadSelf(Thread* thread)
  * This is associated with the pthreadKeySelf key.  It's called by the
  * pthread library when a thread is exiting and the "self" pointer in TLS
  * is non-NULL, meaning the VM hasn't had a chance to clean up.  In normal
- * operation this should never be called.
+ * operation this will not be called.
  *
  * This is mainly of use to ensure that we don't leak resources if, for
  * example, a thread attaches itself to us with AttachCurrentThread and
@@ -1109,16 +1147,43 @@ static void setThreadSelf(Thread* thread)
  * will simply be unaware that the thread has exited, leading to resource
  * leaks (and, if this is a non-daemon thread, an infinite hang when the
  * VM tries to shut down).
+ *
+ * Because some implementations may want to use the pthread destructor
+ * to initiate the detach, and the ordering of destructors is not defined,
+ * we want to iterate a couple of times to give those a chance to run.
  */
 static void threadExitCheck(void* arg)
 {
-    Thread* thread = (Thread*) arg;
+    const int kMaxCount = 2;
 
-    LOGI("In threadExitCheck %p\n", arg);
-    assert(thread != NULL);
+    Thread* self = (Thread*) arg;
+    assert(self != NULL);
+
+    LOGV("threadid=%d: threadExitCheck(%p) count=%d\n",
+        self->threadId, arg, self->threadExitCheckCount);
+
+    if (self->status == THREAD_ZOMBIE) {
+        LOGW("threadid=%d: Weird -- shouldn't be in threadExitCheck\n",
+            self->threadId);
+        return;
+    }
 
-    if (thread->status != THREAD_ZOMBIE) {
-        LOGE("Native thread exited without telling us\n");
+    if (self->threadExitCheckCount < kMaxCount) {
+        /*
+         * Spin a couple of times to let other destructors fire.
+         */
+        LOGD("threadid=%d: thread exiting, not yet detached (count=%d)\n",
+            self->threadId, self->threadExitCheckCount);
+        self->threadExitCheckCount++;
+        int cc = pthread_setspecific(gDvm.pthreadKeySelf, self);
+        if (cc != 0) {
+            LOGE("threadid=%d: unable to re-add thread to TLS\n",
+                self->threadId);
+            dvmAbort();
+        }
+    } else {
+        LOGE("threadid=%d: native thread exited without detaching\n",
+            self->threadId);
         dvmAbort();
     }
 }
@@ -1135,12 +1200,10 @@ static void threadExitCheck(void* arg)
  */
 static void assignThreadId(Thread* thread)
 {
-    /* Find a small unique integer.  threadIdMap is a vector of
+    /*
+     * Find a small unique integer.  threadIdMap is a vector of
      * kMaxThreadId bits;  dvmAllocBit() returns the index of a
      * bit, meaning that it will always be < kMaxThreadId.
-     *
-     * The thin locking magic requires that the low bit is always
-     * set, so we do it once, here.
      */
     int num = dvmAllocBit(gDvm.threadIdMap);
     if (num < 0) {
@@ -1148,7 +1211,7 @@ static void assignThreadId(Thread* thread)
         dvmAbort();     // TODO: make this a non-fatal error result
     }
 
-    thread->threadId = ((num + 1) << 1) | 1;
+    thread->threadId = num + 1;
 
     assert(thread->threadId != 0);
     assert(thread->threadId != DVM_LOCK_INITIAL_THIN_VALUE);
@@ -1160,7 +1223,7 @@ static void assignThreadId(Thread* thread)
 static void releaseThreadId(Thread* thread)
 {
     assert(thread->threadId > 0);
-    dvmClearBit(gDvm.threadIdMap, (thread->threadId >> 1) - 1);
+    dvmClearBit(gDvm.threadIdMap, thread->threadId - 1);
     thread->threadId = 0;
 }
 
@@ -1229,8 +1292,6 @@ static bool createFakeRunFrame(Thread* thread)
     ClassObject* nativeStart;
     Method* runMeth;
 
-    assert(thread->threadId != 1);      // not for main thread
-
     nativeStart =
         dvmFindSystemClassNoInit("Ldalvik/system/NativeStart;");
     if (nativeStart == NULL) {
@@ -1252,7 +1313,6 @@ static bool createFakeRunFrame(Thread* thread)
  */
 static void setThreadName(const char *threadName)
 {
-#if defined(HAVE_PRCTL)
     int hasAt = 0;
     int hasDot = 0;
     const char *s = threadName;
@@ -1267,7 +1327,13 @@ static void setThreadName(const char *threadName)
     } else {
         s = threadName + len - 15;
     }
+#if defined(HAVE_PTHREAD_SETNAME_NP)
+    if (pthread_setname_np(pthread_self(), s) != 0)
+        LOGW("Unable to set the name of the current thread\n");
+#elif defined(HAVE_PRCTL)
     prctl(PR_SET_NAME, (unsigned long) s, 0, 0, 0);
+#else
+    LOGD("Unable to set current thread's name: %s\n", s);
 #endif
 }
 
@@ -2419,35 +2485,101 @@ static void printBackTrace(void) {}
  */
 static void dumpWedgedThread(Thread* thread)
 {
-    char exePath[1024];
-
-    /*
-     * The "executablepath" function in libutils is host-side only.
-     */
-    strcpy(exePath, "-");
-#ifdef HAVE_GLIBC
-    {
-        char proc[100];
-        sprintf(proc, "/proc/%d/exe", getpid());
-        int len;
-
-        len = readlink(proc, exePath, sizeof(exePath)-1);
-        exePath[len] = '\0';
-    }
-#endif
-
-    LOGW("dumping state: process %s %d\n", exePath, getpid());
     dvmDumpThread(dvmThreadSelf(), false);
     printBackTrace();
 
     // dumping a running thread is risky, but could be useful
     dvmDumpThread(thread, true);
 
-
     // stop now and get a core dump
     //abort();
 }
 
+/*
+ * If the thread is running at below-normal priority, temporarily elevate
+ * it to "normal".
+ *
+ * Returns zero if no changes were made.  Otherwise, returns bit flags
+ * indicating what was changed, storing the previous values in the
+ * provided locations.
+ */
+int dvmRaiseThreadPriorityIfNeeded(Thread* thread, int* pSavedThreadPrio,
+    SchedPolicy* pSavedThreadPolicy)
+{
+    errno = 0;
+    *pSavedThreadPrio = getpriority(PRIO_PROCESS, thread->systemTid);
+    if (errno != 0) {
+        LOGW("Unable to get priority for threadid=%d sysTid=%d\n",
+            thread->threadId, thread->systemTid);
+        return 0;
+    }
+    if (get_sched_policy(thread->systemTid, pSavedThreadPolicy) != 0) {
+        LOGW("Unable to get policy for threadid=%d sysTid=%d\n",
+            thread->threadId, thread->systemTid);
+        return 0;
+    }
+
+    int changeFlags = 0;
+
+    /*
+     * Change the priority if we're in the background group.
+     */
+    if (*pSavedThreadPolicy == SP_BACKGROUND) {
+        if (set_sched_policy(thread->systemTid, SP_FOREGROUND) != 0) {
+            LOGW("Couldn't set fg policy on tid %d\n", thread->systemTid);
+        } else {
+            changeFlags |= kChangedPolicy;
+            LOGD("Temporarily moving tid %d to fg (was %d)\n",
+                thread->systemTid, *pSavedThreadPolicy);
+        }
+    }
+
+    /*
+     * getpriority() returns the "nice" value, so larger numbers indicate
+     * lower priority, with 0 being normal.
+     */
+    if (*pSavedThreadPrio > 0) {
+        const int kHigher = 0;
+        if (setpriority(PRIO_PROCESS, thread->systemTid, kHigher) != 0) {
+            LOGW("Couldn't raise priority on tid %d to %d\n",
+                thread->systemTid, kHigher);
+        } else {
+            changeFlags |= kChangedPriority;
+            LOGD("Temporarily raised priority on tid %d (%d -> %d)\n",
+                thread->systemTid, *pSavedThreadPrio, kHigher);
+        }
+    }
+
+    return changeFlags;
+}
+
+/*
+ * Reset the priority values for the thread in question.
+ */
+void dvmResetThreadPriority(Thread* thread, int changeFlags,
+    int savedThreadPrio, SchedPolicy savedThreadPolicy)
+{
+    if ((changeFlags & kChangedPolicy) != 0) {
+        if (set_sched_policy(thread->systemTid, savedThreadPolicy) != 0) {
+            LOGW("NOTE: couldn't reset tid %d to (%d)\n",
+                thread->systemTid, savedThreadPolicy);
+        } else {
+            LOGD("Restored policy of %d to %d\n",
+                thread->systemTid, savedThreadPolicy);
+        }
+    }
+
+    if ((changeFlags & kChangedPriority) != 0) {
+        if (setpriority(PRIO_PROCESS, thread->systemTid, savedThreadPrio) != 0)
+        {
+            LOGW("NOTE: couldn't reset priority on thread %d to %d\n",
+                thread->systemTid, savedThreadPrio);
+        } else {
+            LOGD("Restored priority on %d to %d\n",
+                thread->systemTid, savedThreadPrio);
+        }
+    }
+}
 
 /*
  * Wait for another thread to see the pending suspension and stop running.
@@ -2480,8 +2612,9 @@ static void waitForThreadSuspend(Thread* self, Thread* thread)
     const int kMaxRetries = 10;
     int spinSleepTime = FIRST_SLEEP;
     bool complained = false;
-    bool needPriorityReset = false;
+    int priChangeFlags = 0;
     int savedThreadPrio = -500;
+    SchedPolicy savedThreadPolicy = SP_FOREGROUND;
 
     int sleepIter = 0;
     int retryCount = 0;
@@ -2498,63 +2631,59 @@ static void waitForThreadSuspend(Thread* self, Thread* thread)
              * After waiting for a bit, check to see if the target thread is
              * running at a reduced priority.  If so, bump it up temporarily
              * to give it more CPU time.
-             *
-             * getpriority() returns the "nice" value, so larger numbers
-             * indicate lower priority.
-             *
-             * (Not currently changing the cgroup.  Wasn't necessary in some
-             * simple experiments.)
              */
             if (retryCount == 2) {
                 assert(thread->systemTid != 0);
-                errno = 0;
-                int threadPrio = getpriority(PRIO_PROCESS, thread->systemTid);
-                if (errno == 0 && threadPrio > 0) {
-                    const int kHigher = 0;
-                    if (setpriority(PRIO_PROCESS, thread->systemTid, kHigher) < 0)
-                    {
-                        LOGW("Couldn't raise priority on tid %d to %d\n",
-                            thread->systemTid, kHigher);
-                    } else {
-                        savedThreadPrio = threadPrio;
-                        needPriorityReset = true;
-                        LOGD("Temporarily raising priority on tid %d (%d -> %d)\n",
-                            thread->systemTid, threadPrio, kHigher);
-                    }
-                }
+                priChangeFlags = dvmRaiseThreadPriorityIfNeeded(thread,
+                    &savedThreadPrio, &savedThreadPolicy);
             }
         }
 
 #if defined (WITH_JIT)
         /*
-         * If we're still waiting after the first timeout,
-         * unchain all translations.
+         * If we're still waiting after the first timeout, unchain all
+         * translations iff:
+         *   1) There are new chains formed since the last unchain
+         *   2) The top VM frame of the running thread is running JIT'ed code
          */
-        if (gDvmJit.pJitEntryTable && retryCount > 0) {
-            LOGD("JIT unchain all attempt #%d",retryCount);
+        if (gDvmJit.pJitEntryTable && retryCount > 0 &&
+            gDvmJit.hasNewChain && thread->inJitCodeCache) {
+            LOGD("JIT unchain all for threadid=%d", thread->threadId);
             dvmJitUnchainAll();
         }
 #endif
 
         /*
-         * Sleep briefly.  This returns false if we've exceeded the total
-         * time limit for this round of sleeping.
+         * Sleep briefly.  The iterative sleep call returns false if we've
+         * exceeded the total time limit for this round of sleeping.
          */
         if (!dvmIterativeSleep(sleepIter++, spinSleepTime, startWhen)) {
-            LOGW("threadid=%d: spin on suspend #%d threadid=%d (h=%d)\n",
-                self->threadId, retryCount,
-                thread->threadId, (int)thread->handle);
-            dumpWedgedThread(thread);
-            complained = true;
+            if (spinSleepTime != FIRST_SLEEP) {
+                LOGW("threadid=%d: spin on suspend #%d threadid=%d (pcf=%d)\n",
+                    self->threadId, retryCount,
+                    thread->threadId, priChangeFlags);
+                if (retryCount > 1) {
+                    /* stack trace logging is slow; skip on first iter */
+                    dumpWedgedThread(thread);
+                }
+                complained = true;
+            }
 
             // keep going; could be slow due to valgrind
             sleepIter = 0;
             spinSleepTime = MORE_SLEEP;
 
             if (retryCount++ == kMaxRetries) {
+                LOGE("Fatal spin-on-suspend, dumping threads\n");
+                dvmDumpAllThreads(false);
+
+                /* log this after -- long traces will scroll off log */
                 LOGE("threadid=%d: stuck on threadid=%d, giving up\n",
                     self->threadId, thread->threadId);
-                dvmDumpAllThreads(false);
+
+                /* try to get a debuggerd dump from the spinning thread */
+                dvmNukeThread(thread);
+                /* abort the VM */
                 dvmAbort();
             }
         }
@@ -2566,14 +2695,9 @@ static void waitForThreadSuspend(Thread* self, Thread* thread)
             (dvmGetRelativeTimeUsec() - firstStartWhen) / 1000);
         //dvmDumpThread(thread, false);   /* suspended, so dump is safe */
     }
-    if (needPriorityReset) {
-        if (setpriority(PRIO_PROCESS, thread->systemTid, savedThreadPrio) < 0) {
-            LOGW("NOTE: couldn't reset priority on thread %d to %d\n",
-                thread->systemTid, savedThreadPrio);
-        } else {
-            LOGV("Restored priority on %d to %d\n",
-                thread->systemTid, savedThreadPrio);
-        }
+    if (priChangeFlags != 0) {
+        dvmResetThreadPriority(thread, priChangeFlags, savedThreadPrio,
+            savedThreadPolicy);
     }
 }
 
@@ -3043,6 +3167,38 @@ Thread* dvmGetThreadFromThreadObject(Object* vmThreadObj)
     return (Thread*) vmData;
 }
 
+/*
+ * Given a pthread handle, return the associated Thread*.
+ * Caller must hold the thread list lock.
+ *
+ * Returns NULL if the thread was not found.
+ */
+Thread* dvmGetThreadByHandle(pthread_t handle)
+{
+    Thread* thread;
+    for (thread = gDvm.threadList; thread != NULL; thread = thread->next) {
+        if (thread->handle == handle)
+            break;
+    }
+    return thread;
+}
+
+/*
+ * Given a threadId, return the associated Thread*.
+ * Caller must hold the thread list lock.
+ *
+ * Returns NULL if the thread was not found.
+ */
+Thread* dvmGetThreadByThreadId(u4 threadId)
+{
+    Thread* thread;
+    for (thread = gDvm.threadList; thread != NULL; thread = thread->next) {
+        if (thread->threadId == threadId)
+            break;
+    }
+    return thread;
+}
+
 
 /*
  * Conversion map for "nice" values.
@@ -3161,54 +3317,97 @@ void dvmDumpThread(Thread* thread, bool isRunning)
 /*
  * Try to get the scheduler group.
  *
- * The data from /proc/<pid>/cgroup looks like:
+ * The data from /proc/<pid>/cgroup looks (something) like:
  *  2:cpu:/bg_non_interactive
+ *  1:cpuacct:/
  *
  * We return the part after the "/", which will be an empty string for
  * the default cgroup.  If the string is longer than "bufLen", the string
  * will be truncated.
+ *
+ * TODO: this is cloned from a static function in libcutils; expose that?
  */
-static bool getSchedulerGroup(Thread* thread, char* buf, size_t bufLen)
+static int getSchedulerGroup(int tid, char* buf, size_t bufLen)
 {
 #ifdef HAVE_ANDROID_OS
     char pathBuf[32];
-    char readBuf[256];
-    ssize_t count;
-    int fd;
+    char lineBuf[256];
+    FILE *fp;
 
-    snprintf(pathBuf, sizeof(pathBuf), "/proc/%d/cgroup", thread->systemTid);
-    if ((fd = open(pathBuf, O_RDONLY)) < 0) {
-        LOGV("open(%s) failed: %s\n", pathBuf, strerror(errno));
-        return false;
+    snprintf(pathBuf, sizeof(pathBuf), "/proc/%d/cgroup", tid);
+    if (!(fp = fopen(pathBuf, "r"))) {
+        return -1;
     }
 
-    count = read(fd, readBuf, sizeof(readBuf));
-    if (count <= 0) {
-        LOGV("read(%s) failed (%d): %s\n",
-            pathBuf, (int) count, strerror(errno));
-        close(fd);
-        return false;
-    }
-    close(fd);
+    while(fgets(lineBuf, sizeof(lineBuf) -1, fp)) {
+        char *next = lineBuf;
+        char *subsys;
+        char *grp;
+        size_t len;
 
-    readBuf[--count] = '\0';    /* remove the '\n', now count==strlen */
+        /* Junk the first field */
+        if (!strsep(&next, ":")) {
+            goto out_bad_data;
+        }
 
-    char* cp = strchr(readBuf, '/');
-    if (cp == NULL) {
-        readBuf[sizeof(readBuf)-1] = '\0';
-        LOGV("no '/' in '%s' (file=%s count=%d)\n",
-            readBuf, pathBuf, (int) count);
-        return false;
+        if (!(subsys = strsep(&next, ":"))) {
+            goto out_bad_data;
+        }
+
+        if (strcmp(subsys, "cpu")) {
+            /* Not the subsys we're looking for */
+            continue;
+        }
+
+        if (!(grp = strsep(&next, ":"))) {
+            goto out_bad_data;
+        }
+        grp++; /* Drop the leading '/' */
+        len = strlen(grp);
+        grp[len-1] = '\0'; /* Drop the trailing '\n' */
+
+        if (bufLen <= len) {
+            len = bufLen - 1;
+        }
+        strncpy(buf, grp, len);
+        buf[len] = '\0';
+        fclose(fp);
+        return 0;
     }
 
-    memcpy(buf, cp+1, count);   /* count-1 for cp+1, count+1 for NUL */
-    return true;
+    LOGE("Failed to find cpu subsys");
+    fclose(fp);
+    return -1;
+ out_bad_data:
+    LOGE("Bad cgroup data {%s}", lineBuf);
+    fclose(fp);
+    return -1;
 #else
-    return false;
+    errno = ENOSYS;
+    return -1;
 #endif
 }
 
 /*
+ * Convert ThreadStatus to a string.
+ */
+const char* dvmGetThreadStatusStr(ThreadStatus status)
+{
+    switch (status) {
+    case THREAD_ZOMBIE:         return "ZOMBIE";
+    case THREAD_RUNNING:        return "RUNNABLE";
+    case THREAD_TIMED_WAIT:     return "TIMED_WAIT";
+    case THREAD_MONITOR:        return "MONITOR";
+    case THREAD_WAIT:           return "WAIT";
+    case THREAD_INITIALIZING:   return "INITIALIZING";
+    case THREAD_STARTING:       return "STARTING";
+    case THREAD_NATIVE:         return "NATIVE";
+    case THREAD_VMWAIT:         return "VMWAIT";
+    default:                    return "UNKNOWN";
+    }
+}
+
+/*
  * Print information about the specified thread.
  *
  * Works best when the thread in question is "self" or has been suspended.
@@ -3218,11 +3417,6 @@ static bool getSchedulerGroup(Thread* thread, char* buf, size_t bufLen)
 void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
     bool isRunning)
 {
-    /* tied to ThreadStatus enum */
-    static const char* kStatusNames[] = {
-        "ZOMBIE", "RUNNABLE", "TIMED_WAIT", "MONITOR", "WAIT",
-        "INITIALIZING", "STARTING", "NATIVE", "VMWAIT"
-    };
     Object* threadObj;
     Object* groupObj;
     StringObject* nameStr;
@@ -3233,6 +3427,8 @@ void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
     int priority;               // java.lang.Thread priority
     int policy;                 // pthread policy
     struct sched_param sp;      // pthread scheduling parameters
+    char schedstatBuf[64];      // contents of /proc/[pid]/task/[tid]/schedstat
+    int schedstatFd;
 
     threadObj = thread->threadObj;
     if (threadObj == NULL) {
@@ -3251,7 +3447,8 @@ void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
         policy = -1;
         sp.sched_priority = -1;
     }
-    if (!getSchedulerGroup(thread, schedulerGroupBuf,sizeof(schedulerGroupBuf)))
+    if (getSchedulerGroup(thread->systemTid, schedulerGroupBuf,
+            sizeof(schedulerGroupBuf)) != 0)
     {
         strcpy(schedulerGroupBuf, "unknown");
     } else if (schedulerGroupBuf[0] == '\0') {
@@ -3274,11 +3471,16 @@ void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
     if (groupName == NULL)
         groupName = strdup("(BOGUS GROUP)");
 
-    assert(thread->status < NELEM(kStatusNames));
     dvmPrintDebugMessage(target,
-        "\"%s\"%s prio=%d tid=%d %s\n",
+        "\"%s\"%s prio=%d tid=%d %s%s\n",
         threadName, isDaemon ? " daemon" : "",
-        priority, thread->threadId, kStatusNames[thread->status]);
+        priority, thread->threadId, dvmGetThreadStatusStr(thread->status),
+#if defined(WITH_JIT)
+        thread->inJitCodeCache ? " JIT" : ""
+#else
+        ""
+#endif
+        );
     dvmPrintDebugMessage(target,
         "  | group=\"%s\" sCount=%d dsCount=%d s=%c obj=%p self=%p\n",
         groupName, thread->suspendCount, thread->dbgSuspendCount,
@@ -3288,6 +3490,19 @@ void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
         thread->systemTid, getpriority(PRIO_PROCESS, thread->systemTid),
         policy, sp.sched_priority, schedulerGroupBuf, (int)thread->handle);
 
+    snprintf(schedstatBuf, sizeof(schedstatBuf), "/proc/%d/task/%d/schedstat",
+             getpid(), thread->systemTid);
+    schedstatFd = open(schedstatBuf, O_RDONLY);
+    if (schedstatFd >= 0) {
+        int bytes;
+        bytes = read(schedstatFd, schedstatBuf, sizeof(schedstatBuf) - 1);
+        close(schedstatFd);
+        if (bytes > 1) {
+            schedstatBuf[bytes-1] = 0;  // trailing newline
+            dvmPrintDebugMessage(target, "  | schedstat=( %s )\n", schedstatBuf);
+        }
+    }
+
 #ifdef WITH_MONITOR_TRACKING
     if (!isRunning) {
         LockedObjectData* lod = thread->pLockedObjects;
@@ -3296,8 +3511,16 @@ void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
         else
             dvmPrintDebugMessage(target, "  | monitors held: <none>\n");
         while (lod != NULL) {
-            dvmPrintDebugMessage(target, "  >  %p[%d] (%s)\n",
-                lod->obj, lod->recursionCount, lod->obj->clazz->descriptor);
+            Object* obj = lod->obj;
+            if (obj->clazz == gDvm.classJavaLangClass) {
+                ClassObject* clazz = (ClassObject*) obj;
+                dvmPrintDebugMessage(target, "  >  %p[%d] (%s object for class %s)\n",
+                    obj, lod->recursionCount, obj->clazz->descriptor,
+                    clazz->descriptor);
+            } else {
+                dvmPrintDebugMessage(target, "  >  %p[%d] (%s)\n",
+                    obj, lod->recursionCount, obj->clazz->descriptor);
+            }
             lod = lod->next;
         }
     }
@@ -3377,6 +3600,55 @@ void dvmDumpAllThreadsEx(const DebugOutputTarget* target, bool grabLock)
         dvmUnlockThreadList();
 }
 
+/*
+ * Nuke the target thread from orbit.
+ *
+ * The idea is to send a "crash" signal to the target thread so that
+ * debuggerd will take notice and dump an appropriate stack trace.
+ * Because of the way debuggerd works, we have to throw the same signal
+ * at it twice.
+ *
+ * This does not necessarily cause the entire process to stop, but once a
+ * thread has been nuked the rest of the system is likely to be unstable.
+ * This returns so that some limited set of additional operations may be
+ * performed, but it's advisable (and expected) to call dvmAbort soon.
+ * (This is NOT a way to simply cancel a thread.)
+ */
+void dvmNukeThread(Thread* thread)
+{
+    /* suppress the heapworker watchdog to assist anyone using a debugger */
+    gDvm.nativeDebuggerActive = true;
+
+    /*
+     * Send the signals, separated by a brief interval to allow debuggerd
+     * to work its magic.  An uncommon signal like SIGFPE or SIGSTKFLT
+     * can be used instead of SIGSEGV to avoid making it look like the
+     * code actually crashed at the current point of execution.
+     *
+     * (Observed behavior: with SIGFPE, debuggerd will dump the target
+     * thread and then the thread that calls dvmAbort.  With SIGSEGV,
+     * you don't get the second stack trace; possibly something in the
+     * kernel decides that a signal has already been sent and it's time
+     * to just kill the process.  The position in the current thread is
+     * generally known, so the second dump is not useful.)
+     *
+     * The target thread can continue to execute between the two signals.
+     * (The first just causes debuggerd to attach to it.)
+     */
+    LOGD("threadid=%d: sending two SIGSTKFLTs to threadid=%d (tid=%d) to"
+         " cause debuggerd dump\n",
+        dvmThreadSelf()->threadId, thread->threadId, thread->systemTid);
+    pthread_kill(thread->handle, SIGSTKFLT);
+    usleep(2 * 1000 * 1000);    // TODO: timed-wait until debuggerd attaches
+    pthread_kill(thread->handle, SIGSTKFLT);
+    LOGD("Sent, pausing to let debuggerd run\n");
+    usleep(8 * 1000 * 1000);    // TODO: timed-wait until debuggerd finishes
+
+    /* ignore SIGSEGV so the eventual dmvAbort() doesn't notify debuggerd */
+    signal(SIGSEGV, SIG_IGN);
+    LOGD("Continuing\n");
+}
+
 #ifdef WITH_MONITOR_TRACKING
 /*
  * Count up the #of locked objects in the current thread.
@@ -3757,8 +4029,19 @@ static void gcScanThread(Thread *thread)
      * RUNNING without a suspend-pending check, so this shouldn't cause
      * a false-positive.)
      */
-    assert(thread->status != THREAD_RUNNING || thread->isSuspended ||
-            thread == dvmThreadSelf());
+    if (thread->status == THREAD_RUNNING && !thread->isSuspended &&
+        thread != dvmThreadSelf())
+    {
+        Thread* self = dvmThreadSelf();
+        LOGW("threadid=%d: BUG: GC scanning a running thread (%d)\n",
+            self->threadId, thread->threadId);
+        dvmDumpThread(thread, true);
+        LOGW("Found by:\n");
+        dvmDumpThread(self, false);
+
+        /* continue anyway? */
+        dvmAbort();
+    }
 
     HPROF_SET_GC_SCAN_STATE(HPROF_ROOT_THREAD_OBJECT, thread->threadId);
 
diff --git a/vm/Thread.h b/vm/Thread.h
index 1bb5314..5ca73c4 100644
--- a/vm/Thread.h
+++ b/vm/Thread.h
@@ -22,6 +22,9 @@
 
 #include "jni.h"
 
+#include <cutils/sched_policy.h>
+
+
 #if defined(CHECK_MUTEX) && !defined(__USE_UNIX98)
 /* glibc lacks this unless you #define __USE_UNIX98 */
 int pthread_mutexattr_settype(pthread_mutexattr_t *attr, int type);
@@ -157,6 +160,16 @@ typedef struct Thread {
     /* internal reference tracking */
     ReferenceTable  internalLocalRefTable;
 
+#if defined(WITH_JIT)
+    /*
+     * Whether the current top VM frame is in the interpreter or JIT cache:
+     *   NULL    : in the interpreter
+     *   non-NULL: entry address of the JIT'ed code (the actual value doesn't
+     *             matter)
+     */
+    void*       inJitCodeCache;
+#endif
+
     /* JNI local reference tracking */
 #ifdef USE_INDIRECT_REF
     IndirectRefTable jniLocalRefTable;
@@ -170,15 +183,24 @@ typedef struct Thread {
     /* hack to make JNI_OnLoad work right */
     Object*     classLoaderOverride;
 
+    /* mutex to guard the interrupted and the waitMonitor members */
+    pthread_mutex_t    waitMutex;
+
     /* pointer to the monitor lock we're currently waiting on */
-    /* (do not set or clear unless the Monitor itself is held) */
+    /* guarded by waitMutex */
     /* TODO: consider changing this to Object* for better JDWP interaction */
     Monitor*    waitMonitor;
-    /* set when we confirm that the thread must be interrupted from a wait */
-    bool        interruptingWait;
+
     /* thread "interrupted" status; stays raised until queried or thrown */
+    /* guarded by waitMutex */
     bool        interrupted;
 
+    /* links to the next thread in the wait set this thread is part of */
+    struct Thread*     waitNext;
+
+    /* object to sleep on while we are waiting for a monitor */
+    pthread_cond_t     waitCond;
+
     /*
      * Set to true when the thread is in the process of throwing an
      * OutOfMemoryError.
@@ -189,6 +211,9 @@ typedef struct Thread {
     struct Thread* prev;
     struct Thread* next;
 
+    /* used by threadExitCheck when a thread exits without detaching */
+    int         threadExitCheckCount;
+
     /* JDWP invoke-during-breakpoint support */
     DebugInvokeReq  invokeReq;
 
@@ -220,6 +245,11 @@ typedef struct Thread {
     const u2*   currentPc2;
 #endif
 
+#if defined(WITH_SELF_VERIFICATION)
+    /* Buffer for register state during self verification */
+    struct ShadowSpace* shadowSpace;
+#endif
+
     /* system thread state */
     SystemThread* systemThread;
 } Thread;
@@ -270,7 +300,10 @@ typedef enum SuspendCause {
     SUSPEND_FOR_STACK_DUMP,
     SUSPEND_FOR_DEX_OPT,
 #if defined(WITH_JIT)
-    SUSPEND_FOR_JIT,
+    SUSPEND_FOR_TBL_RESIZE,  // jit-table resize
+    SUSPEND_FOR_IC_PATCH,    // polymorphic callsite inline-cache patch
+    SUSPEND_FOR_CC_RESET,    // code-cache reset
+    SUSPEND_FOR_REFRESH,     // Reload data cached in interpState
 #endif
 } SuspendCause;
 void dvmSuspendThread(Thread* thread);
@@ -349,6 +382,14 @@ INLINE void dvmLockMutex(pthread_mutex_t* pMutex)
 }
 
 /*
+ * Try grabbing a plain mutex.  Returns 0 if successful.
+ */
+INLINE int dvmTryLockMutex(pthread_mutex_t* pMutex)
+{
+    return pthread_mutex_trylock(pMutex);
+}
+
+/*
  * Unlock pthread mutex.
  */
 INLINE void dvmUnlockMutex(pthread_mutex_t* pMutex)
@@ -396,6 +437,22 @@ Object* dvmGetSystemThreadGroup(void);
 Thread* dvmGetThreadFromThreadObject(Object* vmThreadObj);
 
 /*
+ * Given a pthread handle, return the associated Thread*.
+ * Caller must hold the thread list lock.
+ *
+ * Returns NULL if the thread was not found.
+ */
+Thread* dvmGetThreadByHandle(pthread_t handle);
+
+/*
+ * Given a thread ID, return the associated Thread*.
+ * Caller must hold the thread list lock.
+ *
+ * Returns NULL if the thread was not found.
+ */
+Thread* dvmGetThreadByThreadId(u4 threadId);
+
+/*
  * Sleep in a thread.  Returns when the sleep timer returns or the thread
  * is interrupted.
  */
@@ -407,6 +464,11 @@ void dvmThreadSleep(u8 msec, u4 nsec);
 char* dvmGetThreadName(Thread* thread);
 
 /*
+ * Convert ThreadStatus to a string.
+ */
+const char* dvmGetThreadStatusStr(ThreadStatus status);
+
+/*
  * Return true if a thread is on the internal list.  If it is, the
  * thread is part of the GC's root set.
  */
@@ -423,6 +485,25 @@ INLINE void dvmSetThreadJNIEnv(Thread* self, JNIEnv* env) { self->jniEnv = env;}
  */
 void dvmChangeThreadPriority(Thread* thread, int newPriority);
 
+/* "change flags" values for raise/reset thread priority calls */
+#define kChangedPriority    0x01
+#define kChangedPolicy      0x02
+
+/*
+ * If necessary, raise the thread's priority to nice=0 cgroup=fg.
+ *
+ * Returns bit flags indicating changes made (zero if nothing was done).
+ */
+int dvmRaiseThreadPriorityIfNeeded(Thread* thread, int* pSavedThreadPrio,
+    SchedPolicy* pSavedThreadPolicy);
+
+/*
+ * Drop the thread priority to what it was before an earlier call to
+ * dvmRaiseThreadPriorityIfNeeded().
+ */
+void dvmResetThreadPriority(Thread* thread, int changeFlags,
+    int savedThreadPrio, SchedPolicy savedThreadPolicy);
+
 /*
  * Debug: dump information about a single thread.
  */
@@ -436,6 +517,12 @@ void dvmDumpThreadEx(const DebugOutputTarget* target, Thread* thread,
 void dvmDumpAllThreads(bool grabLock);
 void dvmDumpAllThreadsEx(const DebugOutputTarget* target, bool grabLock);
 
+/*
+ * Debug: kill a thread to get a debuggerd stack trace.  Leaves the VM
+ * in an uncertain state.
+ */
+void dvmNukeThread(Thread* thread);
+
 #ifdef WITH_MONITOR_TRACKING
 /*
  * Track locks held by the current thread, along with the stack trace at
diff --git a/vm/UtfString.c b/vm/UtfString.c
index dfb76bc..8e20a0f 100644
--- a/vm/UtfString.c
+++ b/vm/UtfString.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * UTF-8 and Unicode string manipulation, plus java/lang/String convenience
  * functions.
@@ -69,6 +70,30 @@ static bool stringStartup()
         return false;
     }
 
+    bool badValue = false;
+    if (gDvm.offJavaLangString_value != STRING_FIELDOFF_VALUE) {
+        LOGE("InlineNative: String.value offset = %d, expected %d\n",
+            gDvm.offJavaLangString_value, STRING_FIELDOFF_VALUE);
+        badValue = true;
+    }
+    if (gDvm.offJavaLangString_count != STRING_FIELDOFF_COUNT) {
+        LOGE("InlineNative: String.count offset = %d, expected %d\n",
+            gDvm.offJavaLangString_count, STRING_FIELDOFF_COUNT);
+        badValue = true;
+    }
+    if (gDvm.offJavaLangString_offset != STRING_FIELDOFF_OFFSET) {
+        LOGE("InlineNative: String.offset offset = %d, expected %d\n",
+            gDvm.offJavaLangString_offset, STRING_FIELDOFF_OFFSET);
+        badValue = true;
+    }
+    if (gDvm.offJavaLangString_hashCode != STRING_FIELDOFF_HASHCODE) {
+        LOGE("InlineNative: String.hashCode offset = %d, expected %d\n",
+            gDvm.offJavaLangString_hashCode, STRING_FIELDOFF_HASHCODE);
+        badValue = true;
+    }
+    if (badValue)
+        return false;
+
     gDvm.javaLangStringReady = 1;
 
     return true;
@@ -213,11 +238,11 @@ static inline u4 dvmComputeUtf16Hash(const u2* utf16Str, int len)
 }
 u4 dvmComputeStringHash(StringObject* strObj) {
     ArrayObject* chars = (ArrayObject*) dvmGetFieldObject((Object*) strObj,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
     int offset, len;
 
-    len = dvmGetFieldInt((Object*) strObj, gDvm.offJavaLangString_count);
-    offset = dvmGetFieldInt((Object*) strObj, gDvm.offJavaLangString_offset);
+    len = dvmGetFieldInt((Object*) strObj, STRING_FIELDOFF_COUNT);
+    offset = dvmGetFieldInt((Object*) strObj, STRING_FIELDOFF_OFFSET);
 
     return dvmComputeUtf16Hash((u2*) chars->contents + offset, len);
 }
@@ -285,11 +310,11 @@ StringObject* dvmCreateStringFromCstrAndLength(const char* utf8Str,
     dvmConvertUtf8ToUtf16((u2*)chars->contents, utf8Str);
     hashCode = dvmComputeUtf16Hash((u2*) chars->contents, utf16Length);
 
-    dvmSetFieldObject((Object*)newObj, gDvm.offJavaLangString_value,
+    dvmSetFieldObject((Object*)newObj, STRING_FIELDOFF_VALUE,
         (Object*)chars);
     dvmReleaseTrackedAllocIFN((Object*) chars, NULL, allocFlags);
-    dvmSetFieldInt((Object*)newObj, gDvm.offJavaLangString_count, utf16Length);
-    dvmSetFieldInt((Object*)newObj, gDvm.offJavaLangString_hashCode, hashCode);
+    dvmSetFieldInt((Object*)newObj, STRING_FIELDOFF_COUNT, utf16Length);
+    dvmSetFieldInt((Object*)newObj, STRING_FIELDOFF_HASHCODE, hashCode);
     /* leave offset set to zero */
 
     /* debugging stuff */
@@ -339,11 +364,11 @@ StringObject* dvmCreateStringFromUnicode(const u2* unichars, int len)
         memcpy(chars->contents, unichars, len * sizeof(u2));
     hashCode = dvmComputeUtf16Hash((u2*) chars->contents, len);
 
-    dvmSetFieldObject((Object*)newObj, gDvm.offJavaLangString_value,
+    dvmSetFieldObject((Object*)newObj, STRING_FIELDOFF_VALUE,
         (Object*)chars);
     dvmReleaseTrackedAlloc((Object*) chars, NULL);
-    dvmSetFieldInt((Object*)newObj, gDvm.offJavaLangString_count, len);
-    dvmSetFieldInt((Object*)newObj, gDvm.offJavaLangString_hashCode, hashCode);
+    dvmSetFieldInt((Object*)newObj, STRING_FIELDOFF_COUNT, len);
+    dvmSetFieldInt((Object*)newObj, STRING_FIELDOFF_HASHCODE, hashCode);
     /* leave offset set to zero */
 
     /* debugging stuff */
@@ -371,10 +396,10 @@ char* dvmCreateCstrFromString(StringObject* jstr)
     if (jstr == NULL)
         return NULL;
 
-    len = dvmGetFieldInt((Object*) jstr, gDvm.offJavaLangString_count);
-    offset = dvmGetFieldInt((Object*) jstr, gDvm.offJavaLangString_offset);
+    len = dvmGetFieldInt((Object*) jstr, STRING_FIELDOFF_COUNT);
+    offset = dvmGetFieldInt((Object*) jstr, STRING_FIELDOFF_OFFSET);
     chars = (ArrayObject*) dvmGetFieldObject((Object*) jstr,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
     data = (const u2*) chars->contents + offset;
     assert(offset + len <= (int) chars->length);
 
@@ -416,10 +441,10 @@ int dvmStringUtf8ByteLen(StringObject* jstr)
     if (jstr == NULL)
         return 0;       // should we throw something?  assert?
 
-    len = dvmGetFieldInt((Object*) jstr, gDvm.offJavaLangString_count);
-    offset = dvmGetFieldInt((Object*) jstr, gDvm.offJavaLangString_offset);
+    len = dvmGetFieldInt((Object*) jstr, STRING_FIELDOFF_COUNT);
+    offset = dvmGetFieldInt((Object*) jstr, STRING_FIELDOFF_OFFSET);
     chars = (ArrayObject*) dvmGetFieldObject((Object*) jstr,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
     data = (const u2*) chars->contents + offset;
     assert(offset + len <= (int) chars->length);
 
@@ -431,7 +456,7 @@ int dvmStringUtf8ByteLen(StringObject* jstr)
  */
 int dvmStringLen(StringObject* jstr)
 {
-    return dvmGetFieldInt((Object*) jstr, gDvm.offJavaLangString_count);
+    return dvmGetFieldInt((Object*) jstr, STRING_FIELDOFF_COUNT);
 }
 
 /*
@@ -440,7 +465,7 @@ int dvmStringLen(StringObject* jstr)
 ArrayObject* dvmStringCharArray(StringObject* jstr)
 {
     return (ArrayObject*) dvmGetFieldObject((Object*) jstr,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
 }
 
 /*
@@ -451,9 +476,9 @@ const u2* dvmStringChars(StringObject* jstr)
     ArrayObject* chars;
     int offset;
 
-    offset = dvmGetFieldInt((Object*) jstr, gDvm.offJavaLangString_offset);
+    offset = dvmGetFieldInt((Object*) jstr, STRING_FIELDOFF_OFFSET);
     chars = (ArrayObject*) dvmGetFieldObject((Object*) jstr,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
     return (const u2*) chars->contents + offset;
 }
 
@@ -476,17 +501,17 @@ int dvmHashcmpStrings(const void* vstrObj1, const void* vstrObj2)
     assert(gDvm.javaLangStringReady > 0);
 
     /* get offset and length into char array; all values are in 16-bit units */
-    len1 = dvmGetFieldInt((Object*) strObj1, gDvm.offJavaLangString_count);
-    offset1 = dvmGetFieldInt((Object*) strObj1, gDvm.offJavaLangString_offset);
-    len2 = dvmGetFieldInt((Object*) strObj2, gDvm.offJavaLangString_count);
-    offset2 = dvmGetFieldInt((Object*) strObj2, gDvm.offJavaLangString_offset);
+    len1 = dvmGetFieldInt((Object*) strObj1, STRING_FIELDOFF_COUNT);
+    offset1 = dvmGetFieldInt((Object*) strObj1, STRING_FIELDOFF_OFFSET);
+    len2 = dvmGetFieldInt((Object*) strObj2, STRING_FIELDOFF_COUNT);
+    offset2 = dvmGetFieldInt((Object*) strObj2, STRING_FIELDOFF_OFFSET);
     if (len1 != len2)
         return len1 - len2;
 
     chars1 = (ArrayObject*) dvmGetFieldObject((Object*) strObj1,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
     chars2 = (ArrayObject*) dvmGetFieldObject((Object*) strObj2,
-                                gDvm.offJavaLangString_value);
+                                STRING_FIELDOFF_VALUE);
 
     /* damage here actually indicates a broken java/lang/String */
     assert(offset1 + len1 <= (int) chars1->length);
diff --git a/vm/UtfString.h b/vm/UtfString.h
index ca500a7..8f0f972 100644
--- a/vm/UtfString.h
+++ b/vm/UtfString.h
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * UTF-8 and Unicode string manipulation functions, plus convenience
  * functions for working with java/lang/String.
@@ -21,6 +22,30 @@
 #define _DALVIK_STRING
 
 /*
+ * (This is private to UtfString.c, but we cheat a bit and also use it
+ * for InlineNative.c.  Not really worth creating a separate header.)
+ *
+ * We can avoid poking around in gDvm by hard-coding the expected values of
+ * the String field offsets.  This will be annoying if String is in flux
+ * or the VM field layout is changing, so we use defines here to make it
+ * easy to switch back to the gDvm version.
+ *
+ * The values are checked for correctness during startup.
+ */
+//#define USE_GLOBAL_STRING_DEFS
+#ifdef USE_GLOBAL_STRING_DEFS
+# define STRING_FIELDOFF_VALUE      gDvm.offJavaLangString_value
+# define STRING_FIELDOFF_OFFSET     gDvm.offJavaLangString_offset
+# define STRING_FIELDOFF_COUNT      gDvm.offJavaLangString_count
+# define STRING_FIELDOFF_HASHCODE   gDvm.offJavaLangString_hashCode
+#else
+# define STRING_FIELDOFF_VALUE      8
+# define STRING_FIELDOFF_HASHCODE   12
+# define STRING_FIELDOFF_OFFSET     16
+# define STRING_FIELDOFF_COUNT      20
+#endif
+
+/*
  * Hash function for modified UTF-8 strings.
  */
 u4 dvmComputeUtf8Hash(const char* str);
diff --git a/vm/alloc/Alloc.c b/vm/alloc/Alloc.c
index 0961520..d209e4a 100644
--- a/vm/alloc/Alloc.c
+++ b/vm/alloc/Alloc.c
@@ -140,7 +140,8 @@ bool dvmCreateStockExceptions(void)
         "[pre-allocated]");
     dvmReleaseTrackedAlloc(gDvm.internalErrorObj, NULL);
     gDvm.noClassDefFoundErrorObj =
-        createStockException("Ljava/lang/NoClassDefFoundError;", NULL);
+        createStockException("Ljava/lang/NoClassDefFoundError;",
+            "[generic]");
     dvmReleaseTrackedAlloc(gDvm.noClassDefFoundErrorObj, NULL);
 
     if (gDvm.outOfMemoryObj == NULL || gDvm.internalErrorObj == NULL ||
@@ -295,7 +296,7 @@ void dvmCollectGarbage(bool collectSoftReferences)
     dvmLockHeap();
 
     LOGVV("Explicit GC\n");
-    dvmCollectGarbageInternal(collectSoftReferences);
+    dvmCollectGarbageInternal(collectSoftReferences, GC_EXPLICIT);
 
     dvmUnlockHeap();
 }
diff --git a/vm/alloc/Heap.c b/vm/alloc/Heap.c
index 17d6a5c..dce5a82 100644
--- a/vm/alloc/Heap.c
+++ b/vm/alloc/Heap.c
@@ -37,6 +37,13 @@
 #define kNonCollectableRefDefault   16
 #define kFinalizableRefDefault      128
 
+static const char* GcReasonStr[] = {
+    [GC_FOR_MALLOC] = "GC_FOR_MALLOC",
+    [GC_EXPLICIT] = "GC_EXPLICIT",
+    [GC_EXTERNAL_ALLOC] = "GC_EXTERNAL_ALLOC",
+    [GC_HPROF_DUMP_HEAP] = "GC_HPROF_DUMP_HEAP"
+};
+
 /*
  * Initialize the GC heap.
  *
@@ -236,9 +243,9 @@ Object *dvmGetNextHeapWorkerObject(HeapWorkerOperation *op)
     if (obj != NULL) {
         uintptr_t workBits;
 
-        workBits = (uintptr_t)obj & (WORKER_CLEAR | WORKER_ENQUEUE);
+        workBits = (uintptr_t)obj & WORKER_ENQUEUE;
         assert(workBits != 0);
-        obj = (Object *)((uintptr_t)obj & ~(WORKER_CLEAR | WORKER_ENQUEUE));
+        obj = (Object *)((uintptr_t)obj & ~WORKER_ENQUEUE);
 
         *op = workBits;
     } else {
@@ -326,7 +333,7 @@ static void gcForMalloc(bool collectSoftReferences)
      */
     LOGD_HEAP("dvmMalloc initiating GC%s\n",
             collectSoftReferences ? "(collect SoftReferences)" : "");
-    dvmCollectGarbageInternal(collectSoftReferences);
+    dvmCollectGarbageInternal(collectSoftReferences, GC_FOR_MALLOC);
 }
 
 /* Try as hard as possible to allocate some memory.
@@ -573,10 +580,6 @@ alloc_succeeded:
             }
         }
 
-#if WITH_OBJECT_HEADERS
-        hc->header = OBJECT_HEADER;
-        hc->birthGeneration = gGeneration;
-#endif
         ptr = hc->data;
 
         /* The caller may not want us to collect this object.
@@ -641,7 +644,7 @@ alloc_succeeded:
             dvmAddTrackedAlloc(ptr, NULL);
         }
     } else {
-        /* 
+        /*
          * The allocation failed; throw an OutOfMemoryError.
          */
         throwOOME();
@@ -724,7 +727,7 @@ size_t dvmObjectSizeInHeap(const Object *obj)
  * way to enforce this is to refuse to GC on an allocation made by the
  * JDWP thread -- we have to expand the heap or fail.
  */
-void dvmCollectGarbageInternal(bool collectSoftReferences)
+void dvmCollectGarbageInternal(bool collectSoftReferences, enum GcReason reason)
 {
     GcHeap *gcHeap = gDvm.gcHeap;
     Object *softReferences;
@@ -766,7 +769,7 @@ void dvmCollectGarbageInternal(bool collectSoftReferences)
     }
     gcHeap->gcStartTime = now;
 
-    LOGV_HEAP("GC starting -- suspending threads\n");
+    LOGV_HEAP("%s starting -- suspending threads\n", GcReasonStr[reason]);
 
     dvmSuspendAllThreads(SUSPEND_FOR_GC);
 
@@ -853,7 +856,8 @@ void dvmCollectGarbageInternal(bool collectSoftReferences)
                 (int) time(NULL), (int) getpid());
             gcHeap->hprofFileName = nameBuf;
         }
-        gcHeap->hprofContext = hprofStartup(gcHeap->hprofFileName);
+        gcHeap->hprofContext = hprofStartup(gcHeap->hprofFileName,
+                gcHeap->hprofDirectToDdms);
         if (gcHeap->hprofContext != NULL) {
             hprofStartHeapDump(gcHeap->hprofContext);
         }
@@ -1028,6 +1032,20 @@ void dvmCollectGarbageInternal(bool collectSoftReferences)
     dvmUnlockMutex(&gDvm.heapWorkerListLock);
     dvmUnlockMutex(&gDvm.heapWorkerLock);
 
+#if defined(WITH_JIT)
+    extern void dvmCompilerPerformSafePointChecks(void);
+
+    /*
+     * Patching a chaining cell is very cheap as it only updates 4 words. It's
+     * the overhead of stopping all threads and synchronizing the I/D cache
+     * that makes it expensive.
+     *
+     * Therefore we batch those work orders in a queue and go through them
+     * when threads are suspended for GC.
+     */
+    dvmCompilerPerformSafePointChecks();
+#endif
+
     dvmResumeAllThreads(SUSPEND_FOR_GC);
     if (oldThreadPriority != kInvalidPriority) {
         if (setpriority(PRIO_PROCESS, 0, oldThreadPriority) != 0) {
@@ -1042,13 +1060,8 @@ void dvmCollectGarbageInternal(bool collectSoftReferences)
         }
     }
     gcElapsedTime = (dvmGetRelativeTimeUsec() - gcHeap->gcStartTime) / 1000;
-    if (gcElapsedTime < 10000) {
-        LOGD("GC freed %d objects / %zd bytes in %dms\n",
-                numFreed, sizeFreed, (int)gcElapsedTime);
-    } else {
-        LOGD("GC freed %d objects / %zd bytes in %d sec\n",
-                numFreed, sizeFreed, (int)(gcElapsedTime / 1000));
-    }
+    LOGD("%s freed %d objects / %zd bytes in %dms\n",
+         GcReasonStr[reason], numFreed, sizeFreed, (int)gcElapsedTime);
     dvmLogGcStats(numFreed, sizeFreed, gcElapsedTime);
 
     if (gcHeap->ddmHpifWhen != 0) {
@@ -1073,7 +1086,7 @@ void dvmCollectGarbageInternal(bool collectSoftReferences)
  *
  * Returns 0 on success, or an error code on failure.
  */
-int hprofDumpHeap(const char* fileName)
+int hprofDumpHeap(const char* fileName, bool directToDdms)
 {
     int result;
 
@@ -1081,7 +1094,8 @@ int hprofDumpHeap(const char* fileName)
 
     gDvm.gcHeap->hprofDumpOnGc = true;
     gDvm.gcHeap->hprofFileName = fileName;
-    dvmCollectGarbageInternal(false);
+    gDvm.gcHeap->hprofDirectToDdms = directToDdms;
+    dvmCollectGarbageInternal(false, GC_HPROF_DUMP_HEAP);
     result = gDvm.gcHeap->hprofResult;
 
     dvmUnlockMutex(&gDvm.gcHeapLock);
diff --git a/vm/alloc/Heap.h b/vm/alloc/Heap.h
index cc29c40..1451739 100644
--- a/vm/alloc/Heap.h
+++ b/vm/alloc/Heap.h
@@ -52,9 +52,21 @@ void dvmHeapShutdown(void);
 size_t dvmObjectSizeInHeap(const Object *obj);
 #endif
 
+enum GcReason {
+    /* Not enough space for an "ordinary" Object to be allocated. */
+    GC_FOR_MALLOC,
+    /* Explicit GC via Runtime.gc(), VMRuntime.gc(), or SIGUSR1. */
+    GC_EXPLICIT,
+    /* GC to try to reduce heap footprint to allow more non-GC'ed memory. */
+    GC_EXTERNAL_ALLOC,
+    /* GC to dump heap contents to a file, only used under WITH_HPROF */
+    GC_HPROF_DUMP_HEAP
+};
+
 /*
  * Run the garbage collector without doing any locking.
  */
-void dvmCollectGarbageInternal(bool collectSoftReferences);
+void dvmCollectGarbageInternal(bool collectSoftReferences,
+                               enum GcReason reason);
 
 #endif  // _DALVIK_ALLOC_HEAP
diff --git a/vm/alloc/HeapBitmap.c b/vm/alloc/HeapBitmap.c
index 2c75678..778fd87 100644
--- a/vm/alloc/HeapBitmap.c
+++ b/vm/alloc/HeapBitmap.c
@@ -23,11 +23,8 @@
 
 #define HB_ASHMEM_NAME "dalvik-heap-bitmap"
 
-#ifndef PAGE_SIZE
-#define PAGE_SIZE 4096
-#endif
 #define ALIGN_UP_TO_PAGE_SIZE(p) \
-    (((size_t)(p) + (PAGE_SIZE - 1)) & ~(PAGE_SIZE - 1))
+    (((size_t)(p) + (SYSTEM_PAGE_SIZE - 1)) & ~(SYSTEM_PAGE_SIZE - 1))
 
 #define LIKELY(exp)     (__builtin_expect((exp) != 0, true))
 #define UNLIKELY(exp)   (__builtin_expect((exp) != 0, false))
diff --git a/vm/alloc/HeapInternal.h b/vm/alloc/HeapInternal.h
index fafb87a..9a5071f 100644
--- a/vm/alloc/HeapInternal.h
+++ b/vm/alloc/HeapInternal.h
@@ -29,27 +29,7 @@
 #define ptr2chunk(p)    (((DvmHeapChunk *)(p)) - 1)
 #define chunk2ptr(p)    ((void *)(((DvmHeapChunk *)(p)) + 1))
 
-#define WITH_OBJECT_HEADERS 0
-#if WITH_OBJECT_HEADERS
-#define OBJECT_HEADER   0x11335577
-extern u2 gGeneration;
-#endif
-
 typedef struct DvmHeapChunk {
-#if WITH_OBJECT_HEADERS
-    u4 header;
-    const Object *parent;
-    const Object *parentOld;
-    const Object *markFinger;
-    const Object *markFingerOld;
-    u2 birthGeneration;
-    u2 markCount;
-    u2 scanCount;
-    u2 oldMarkGeneration;
-    u2 markGeneration;
-    u2 oldScanGeneration;
-    u2 scanGeneration;
-#endif
 #if WITH_HPROF && WITH_HPROF_STACK
     u4 stackTraceSerialNumber;
 #endif
@@ -190,6 +170,7 @@ struct GcHeap {
     const char*     hprofFileName;
     hprof_context_t *hprofContext;
     int             hprofResult;
+    bool            hprofDirectToDdms;
 #endif
 };
 
diff --git a/vm/alloc/HeapSource.c b/vm/alloc/HeapSource.c
index 830e5d7..384ec78 100644
--- a/vm/alloc/HeapSource.c
+++ b/vm/alloc/HeapSource.c
@@ -32,13 +32,10 @@ extern void dlmalloc_walk_free_pages(void(*)(void*, void*, void*), void*);
 static void snapIdealFootprint(void);
 static void setIdealFootprint(size_t max);
 
-#ifndef PAGE_SIZE
-#define PAGE_SIZE 4096
-#endif
 #define ALIGN_UP_TO_PAGE_SIZE(p) \
-    (((size_t)(p) + (PAGE_SIZE - 1)) & ~(PAGE_SIZE - 1))
+    (((size_t)(p) + (SYSTEM_PAGE_SIZE - 1)) & ~(SYSTEM_PAGE_SIZE - 1))
 #define ALIGN_DOWN_TO_PAGE_SIZE(p) \
-    ((size_t)(p) & ~(PAGE_SIZE - 1))
+    ((size_t)(p) & ~(SYSTEM_PAGE_SIZE - 1))
 
 #define HEAP_UTILIZATION_MAX        1024
 #define DEFAULT_HEAP_UTILIZATION    512     // Range 1..HEAP_UTILIZATION_MAX
@@ -100,7 +97,7 @@ live ratio of small heap after a gc; scale it based on that.
 typedef struct {
     /* The mspace to allocate from.
      */
-    mspace *msp;
+    mspace msp;
 
     /* The bitmap that keeps track of where objects are in the heap.
      */
@@ -277,10 +274,10 @@ countFree(Heap *heap, const void *ptr, bool isObj)
 
 static HeapSource *gHs = NULL;
 
-static mspace *
+static mspace
 createMspace(size_t startSize, size_t absoluteMaxSize, size_t id)
 {
-    mspace *msp;
+    mspace msp;
     char name[PATH_MAX];
 
     /* If two ashmem regions have the same name, only one gets
@@ -319,7 +316,7 @@ createMspace(size_t startSize, size_t absoluteMaxSize, size_t id)
 }
 
 static bool
-addNewHeap(HeapSource *hs, mspace *msp, size_t mspAbsoluteMaxSize)
+addNewHeap(HeapSource *hs, mspace msp, size_t mspAbsoluteMaxSize)
 {
     Heap heap;
 
@@ -364,7 +361,7 @@ addNewHeap(HeapSource *hs, mspace *msp, size_t mspAbsoluteMaxSize)
     /* Don't let the soon-to-be-old heap grow any further.
      */
     if (hs->numHeaps > 0) {
-        mspace *msp = hs->heaps[0].msp;
+        mspace msp = hs->heaps[0].msp;
         mspace_set_max_allowed_footprint(msp, mspace_footprint(msp));
     }
 
@@ -498,7 +495,7 @@ dvmHeapSourceStartupBeforeFork()
         /* Create a new heap for post-fork zygote allocations.  We only
          * try once, even if it fails.
          */
-        LOGI("Splitting out new zygote heap\n");
+        LOGV("Splitting out new zygote heap\n");
         gDvm.newZygoteHeapAllocated = true;
         return addNewHeap(hs, NULL, 0);
     }
@@ -594,16 +591,17 @@ dvmHeapSourceGetValue(enum HeapSourceValueSpec spec, size_t perHeapStats[],
 
 /*
  * Writes shallow copies of the currently-used bitmaps into outBitmaps,
- * returning the number of bitmaps written.  Returns <0 if the array
- * was not long enough.
+ * returning the number of bitmaps written.  Returns 0 if the array was
+ * not long enough or if there are no heaps, either of which is an error.
  */
-ssize_t
+size_t
 dvmHeapSourceGetObjectBitmaps(HeapBitmap outBitmaps[], size_t maxBitmaps)
 {
     HeapSource *hs = gHs;
 
     HS_BOILERPLATE();
 
+    assert(hs->numHeaps != 0);
     if (maxBitmaps >= hs->numHeaps) {
         size_t i;
 
@@ -612,7 +610,7 @@ dvmHeapSourceGetObjectBitmaps(HeapBitmap outBitmaps[], size_t maxBitmaps)
         }
         return i;
     }
-    return -1;
+    return 0;
 }
 
 /*
@@ -996,7 +994,7 @@ setSoftLimit(HeapSource *hs, size_t softLimit)
      * max_allowed, because the heap may not have grown all the
      * way to the allowed size yet.
      */
-    mspace *msp = hs->heaps[0].msp;
+    mspace msp = hs->heaps[0].msp;
     size_t currentHeapSize = mspace_footprint(msp);
     if (softLimit < currentHeapSize) {
         /* Don't let the heap grow any more, and impose a soft limit.
@@ -1023,7 +1021,7 @@ setIdealFootprint(size_t max)
     HeapSource *hs = gHs;
 #if DEBUG_HEAP_SOURCE
     HeapSource oldHs = *hs;
-    mspace *msp = hs->heaps[0].msp;
+    mspace msp = hs->heaps[0].msp;
     size_t oldAllowedFootprint =
             mspace_max_allowed_footprint(msp);
 #endif
@@ -1286,7 +1284,7 @@ static void releasePagesInRange(void *start, void *end, void *nbytes)
     * We also align the end address.
     */
     start = (void *)ALIGN_UP_TO_PAGE_SIZE(start);
-    end = (void *)((size_t)end & ~(PAGE_SIZE - 1));
+    end = (void *)((size_t)end & ~(SYSTEM_PAGE_SIZE - 1));
     if (start < end) {
         size_t length = (char *)end - (char *)start;
         madvise(start, length, MADV_DONTNEED);
@@ -1479,7 +1477,7 @@ gcForExternalAlloc(bool collectSoftReferences)
         }
     }
 #endif
-    dvmCollectGarbageInternal(collectSoftReferences);
+    dvmCollectGarbageInternal(collectSoftReferences, GC_EXTERNAL_ALLOC);
 }
 
 /*
diff --git a/vm/alloc/HeapSource.h b/vm/alloc/HeapSource.h
index fdaf119..3909123 100644
--- a/vm/alloc/HeapSource.h
+++ b/vm/alloc/HeapSource.h
@@ -56,10 +56,10 @@ void dvmHeapSourceShutdown(GcHeap *gcHeap);
 
 /*
  * Writes shallow copies of the currently-used bitmaps into outBitmaps,
- * returning the number of bitmaps written.  Returns <0 if the array
- * was not long enough.
+ * returning the number of bitmaps written.  Returns 0 if the array was
+ * not long enough or if there are no heaps, either of which is an error.
  */
-ssize_t dvmHeapSourceGetObjectBitmaps(HeapBitmap outBitmaps[],
+size_t dvmHeapSourceGetObjectBitmaps(HeapBitmap outBitmaps[],
         size_t maxBitmaps);
 
 /*
diff --git a/vm/alloc/HeapWorker.c b/vm/alloc/HeapWorker.c
index e7a4d03..d743ce5 100644
--- a/vm/alloc/HeapWorker.c
+++ b/vm/alloc/HeapWorker.c
@@ -15,10 +15,10 @@
  */
 
 /*
- * An async worker thread to handle certain heap operations that
- * need to be done in a separate thread to avoid synchronization
- * problems.  HeapWorkers and reference clearing/enqueuing are
- * handled by this thread.
+ * An async worker thread to handle certain heap operations that need
+ * to be done in a separate thread to avoid synchronization problems.
+ * HeapWorkers and reference enqueuing are handled by this thread.
+ * The VM does all clearing.
  */
 #include "Dalvik.h"
 #include "HeapInternal.h"
@@ -104,7 +104,7 @@ void dvmHeapWorkerShutdown(void)
          */
         if (pthread_join(gDvm.heapWorkerHandle, &threadReturn) != 0)
             LOGW("HeapWorker thread join failed\n");
-        else
+        else if (gDvm.verboseShutdown)
             LOGD("HeapWorker thread has shut down\n");
 
         gDvm.heapWorkerReady = false;
@@ -145,8 +145,32 @@ void dvmAssertHeapWorkerThreadRunning()
              * watchdog and just reset the timer.
              */
             LOGI("Debugger is attached -- suppressing HeapWorker watchdog\n");
-            heapWorkerInterpStartTime = now;        /* reset timer */
+            gDvm.gcHeap->heapWorkerInterpStartTime = now;   /* reset timer */
         } else if (delta > HEAP_WORKER_WATCHDOG_TIMEOUT) {
+            /*
+             * Before we give up entirely, see if maybe we're just not
+             * getting any CPU time because we're stuck in a background
+             * process group.  If we successfully move the thread into the
+             * foreground we'll just leave it there (it doesn't do anything
+             * if the process isn't GCing).
+             */
+            dvmLockThreadList(NULL);
+            Thread* thread = dvmGetThreadByHandle(gDvm.heapWorkerHandle);
+            dvmUnlockThreadList();
+
+            if (thread != NULL) {
+                int priChangeFlags, threadPrio;
+                SchedPolicy threadPolicy;
+                priChangeFlags = dvmRaiseThreadPriorityIfNeeded(thread,
+                        &threadPrio, &threadPolicy);
+                if (priChangeFlags != 0) {
+                    LOGI("HeapWorker watchdog expired, raising priority"
+                         " and retrying\n");
+                    gDvm.gcHeap->heapWorkerInterpStartTime = now;
+                    return;
+                }
+            }
+
             char* desc = dexProtoCopyMethodDescriptor(
                     &gDvm.gcHeap->heapWorkerCurrentMethod->prototype);
             LOGE("HeapWorker is wedged: %lldms spent inside %s.%s%s\n",
@@ -156,6 +180,9 @@ void dvmAssertHeapWorkerThreadRunning()
             free(desc);
             dvmDumpAllThreads(true);
 
+            /* try to get a debuggerd dump from the target thread */
+            dvmNukeThread(thread);
+
             /* abort the VM */
             dvmAbort();
         } else if (delta > HEAP_WORKER_WATCHDOG_TIMEOUT / 2) {
@@ -221,7 +248,7 @@ static void callMethod(Thread *self, Object *obj, Method *method)
 }
 
 /* Process all enqueued heap work, including finalizers and reference
- * clearing/enqueueing.
+ * enqueueing. Clearing has already been done by the VM.
  *
  * Caller must hold gDvm.heapWorkerLock.
  */
@@ -230,17 +257,9 @@ static void doHeapWork(Thread *self)
     Object *obj;
     HeapWorkerOperation op;
     int numFinalizersCalled, numReferencesEnqueued;
-#if FANCY_REFERENCE_SUBCLASS
-    int numReferencesCleared = 0;
-#endif
 
     assert(gDvm.voffJavaLangObject_finalize >= 0);
-#if FANCY_REFERENCE_SUBCLASS
-    assert(gDvm.voffJavaLangRefReference_clear >= 0);
-    assert(gDvm.voffJavaLangRefReference_enqueue >= 0);
-#else
     assert(gDvm.methJavaLangRefReference_enqueueInternal != NULL);
-#endif
 
     numFinalizersCalled = 0;
     numReferencesEnqueued = 0;
@@ -262,39 +281,11 @@ static void doHeapWork(Thread *self)
             assert(method->clazz != gDvm.classJavaLangObject);
             callMethod(self, obj, method);
         } else {
-#if FANCY_REFERENCE_SUBCLASS
-            /* clear() *must* happen before enqueue(), otherwise
-             * a non-clear reference could appear on a reference
-             * queue.
-             */
-            if (op & WORKER_CLEAR) {
-                numReferencesCleared++;
-                method = obj->clazz->vtable[
-                        gDvm.voffJavaLangRefReference_clear];
-                assert(dvmCompareNameDescriptorAndMethod("clear", "()V",
-                                method) == 0);
-                assert(method->clazz != gDvm.classJavaLangRefReference);
-                callMethod(self, obj, method);
-            }
-            if (op & WORKER_ENQUEUE) {
-                numReferencesEnqueued++;
-                method = obj->clazz->vtable[
-                        gDvm.voffJavaLangRefReference_enqueue];
-                assert(dvmCompareNameDescriptorAndMethod("enqueue", "()Z",
-                                method) == 0);
-                /* We call enqueue() even when it isn't overridden,
-                 * so don't assert(!classJavaLangRefReference) here.
-                 */
-                callMethod(self, obj, method);
-            }
-#else
-            assert((op & WORKER_CLEAR) == 0);
             if (op & WORKER_ENQUEUE) {
                 numReferencesEnqueued++;
                 callMethod(self, obj,
                         gDvm.methJavaLangRefReference_enqueueInternal);
             }
-#endif
         }
 
         /* Let the GC collect the object.
@@ -303,9 +294,6 @@ static void doHeapWork(Thread *self)
     }
     LOGV("Called %d finalizers\n", numFinalizersCalled);
     LOGV("Enqueued %d references\n", numReferencesEnqueued);
-#if FANCY_REFERENCE_SUBCLASS
-    LOGV("Cleared %d overridden references\n", numReferencesCleared);
-#endif
 }
 
 /*
@@ -412,7 +400,8 @@ static void* heapWorkerThreadStart(void* arg)
     }
     dvmUnlockMutex(&gDvm.heapWorkerLock);
 
-    LOGD("HeapWorker thread shutting down\n");
+    if (gDvm.verboseShutdown)
+        LOGD("HeapWorker thread shutting down\n");
     return NULL;
 }
 
diff --git a/vm/alloc/HeapWorker.h b/vm/alloc/HeapWorker.h
index c079570..2734aef 100644
--- a/vm/alloc/HeapWorker.h
+++ b/vm/alloc/HeapWorker.h
@@ -77,11 +77,10 @@ void dvmAssertHeapWorkerThreadRunning();
 typedef enum HeapWorkerOperation {
     WORKER_FINALIZE = 0,
 
-    /* Required: (WORKER_CLEAR | WORKER_ENQUEUE) <= (4-1)
-     * These values will be stuffed in the low bits of a pointer.
+    /* Required: WORKER_ENQUEUE <= (4-1)
+     * This value will be stuffed in the low bits of a pointer.
      */
-    WORKER_CLEAR = (1<<0),
-    WORKER_ENQUEUE = (1<<1),
+    WORKER_ENQUEUE = (1<<0),
 } HeapWorkerOperation;
 
 /*
diff --git a/vm/alloc/MarkSweep.c b/vm/alloc/MarkSweep.c
index 09cc25f..78286cf 100644
--- a/vm/alloc/MarkSweep.c
+++ b/vm/alloc/MarkSweep.c
@@ -68,16 +68,8 @@
 #define LOGV_SWEEP(...) LOGVV_GC("SWEEP: " __VA_ARGS__)
 #define LOGV_REF(...)   LOGVV_GC("REF: " __VA_ARGS__)
 
-#if WITH_OBJECT_HEADERS
-u2 gGeneration = 0;
-static const Object *gMarkParent = NULL;
-#endif
-
-#ifndef PAGE_SIZE
-#define PAGE_SIZE 4096
-#endif
 #define ALIGN_UP_TO_PAGE_SIZE(p) \
-    (((size_t)(p) + (PAGE_SIZE - 1)) & ~(PAGE_SIZE - 1))
+    (((size_t)(p) + (SYSTEM_PAGE_SIZE - 1)) & ~(SYSTEM_PAGE_SIZE - 1))
 
 /* Do not cast the result of this to a boolean; the only set bit
  * may be > 1<<8.
@@ -154,7 +146,7 @@ dvmHeapBeginMarkStep()
 
     numBitmaps = dvmHeapSourceGetObjectBitmaps(objectBitmaps,
             HEAP_SOURCE_MAX_HEAP_COUNT);
-    if (numBitmaps <= 0) {
+    if (numBitmaps == 0) {
         return false;
     }
 
@@ -170,10 +162,6 @@ dvmHeapBeginMarkStep()
     mc->numBitmaps = numBitmaps;
     mc->finger = NULL;
 
-#if WITH_OBJECT_HEADERS
-    gGeneration++;
-#endif
-
     return true;
 }
 
@@ -213,28 +201,6 @@ _markObjectNonNullCommon(const Object *obj, GcMarkContext *ctx,
             MARK_STACK_PUSH(ctx->stack, obj);
         }
 
-#if WITH_OBJECT_HEADERS
-        if (hc->scanGeneration != hc->markGeneration) {
-            LOGE("markObject(0x%08x): wasn't scanned last time\n", (uint)obj);
-            dvmAbort();
-        }
-        if (hc->markGeneration == gGeneration) {
-            LOGE("markObject(0x%08x): already marked this generation\n",
-                    (uint)obj);
-            dvmAbort();
-        }
-        hc->oldMarkGeneration = hc->markGeneration;
-        hc->markGeneration = gGeneration;
-        hc->markFingerOld = hc->markFinger;
-        hc->markFinger = ctx->finger;
-        if (gMarkParent != NULL) {
-            hc->parentOld = hc->parent;
-            hc->parent = gMarkParent;
-        } else {
-            hc->parent = (const Object *)((uintptr_t)hc->parent | 1);
-        }
-        hc->markCount++;
-#endif
 #if WITH_HPROF
         if (gDvm.gcHeap->hprofContext != NULL) {
             hprofMarkRootObject(gDvm.gcHeap->hprofContext, obj, 0);
@@ -380,6 +346,7 @@ void dvmHeapMarkRootSet()
     dvmMarkObjectNonNull(gDvm.outOfMemoryObj);
     dvmMarkObjectNonNull(gDvm.internalErrorObj);
     dvmMarkObjectNonNull(gDvm.noClassDefFoundErrorObj);
+    dvmMarkObject(gDvm.jniWeakGlobalRefQueue);
 //TODO: scan object references sitting in gDvm;  use pointer begin & end
 
     HPROF_CLEAR_GC_SCAN_STATE();
@@ -436,7 +403,7 @@ static void scanStaticFields(const ClassObject *clazz, GcMarkContext *ctx)
 static void scanInstanceFields(const DataObject *obj, ClassObject *clazz,
         GcMarkContext *ctx)
 {
-    if (false && clazz->refOffsets != CLASS_WALK_SUPER) {
+    if (clazz->refOffsets != CLASS_WALK_SUPER) {
         unsigned int refOffsets = clazz->refOffsets;
         while (refOffsets != 0) {
             const int rshift = CLZ(refOffsets);
@@ -535,17 +502,6 @@ static void scanObject(const Object *obj, GcMarkContext *ctx)
     }
 #endif
 
-#if WITH_OBJECT_HEADERS
-    if (ptr2chunk(obj)->scanGeneration == gGeneration) {
-        LOGE("object 0x%08x was already scanned this generation\n",
-                (uintptr_t)obj);
-        dvmAbort();
-    }
-    ptr2chunk(obj)->oldScanGeneration = ptr2chunk(obj)->scanGeneration;
-    ptr2chunk(obj)->scanGeneration = gGeneration;
-    ptr2chunk(obj)->scanCount++;
-#endif
-
     /* Get and mark the class object for this particular instance.
      */
     clazz = obj->clazz;
@@ -567,10 +523,6 @@ static void scanObject(const Object *obj, GcMarkContext *ctx)
         return;
     }
 
-#if WITH_OBJECT_HEADERS
-    gMarkParent = obj;
-#endif
-
     assert(dvmIsValidObject((Object *)clazz));
     markObjectNonNull((Object *)clazz, ctx);
 
@@ -720,10 +672,6 @@ static void scanObject(const Object *obj, GcMarkContext *ctx)
             scanClassObject((ClassObject *)obj, ctx);
         }
     }
-
-#if WITH_OBJECT_HEADERS
-    gMarkParent = NULL;
-#endif
 }
 
 static void
@@ -794,65 +742,36 @@ void dvmHeapScanMarkedObjects()
     LOG_SCAN("done with marked objects\n");
 }
 
-/** @return true if we need to schedule a call to clear().
+/** Clear the referent field.
  */
-static bool clearReference(Object *reference)
+static void clearReference(Object *reference)
 {
     /* This is what the default implementation of Reference.clear()
      * does.  We're required to clear all references to a given
      * referent atomically, so we can't pop in and out of interp
      * code each time.
      *
-     * Also, someone may have subclassed one of the basic Reference
-     * types, overriding clear().  We can't trust the clear()
-     * implementation to call super.clear();  we cannot let clear()
-     * resurrect the referent.  If we clear it here, we can safely
-     * call any overriding implementations.
+     * We don't ever actaully call overriding implementations of
+     * Reference.clear().
      */
     dvmSetFieldObject(reference,
             gDvm.offJavaLangRefReference_referent, NULL);
-
-#if FANCY_REFERENCE_SUBCLASS
-    /* See if clear() has actually been overridden.  If so,
-     * we need to schedule a call to it before calling enqueue().
-     */
-    if (reference->clazz->vtable[gDvm.voffJavaLangRefReference_clear]->clazz !=
-            gDvm.classJavaLangRefReference)
-    {
-        /* clear() has been overridden;  return true to indicate
-         * that we need to schedule a call to the real clear()
-         * implementation.
-         */
-        return true;
-    }
-#endif
-
-    return false;
 }
 
 /** @return true if we need to schedule a call to enqueue().
  */
 static bool enqueueReference(Object *reference)
 {
-#if FANCY_REFERENCE_SUBCLASS
-    /* See if this reference class has overridden enqueue();
-     * if not, we can take a shortcut.
-     */
-    if (reference->clazz->vtable[gDvm.voffJavaLangRefReference_enqueue]->clazz
-            == gDvm.classJavaLangRefReference)
-#endif
-    {
-        Object *queue = dvmGetFieldObject(reference,
-                gDvm.offJavaLangRefReference_queue);
-        Object *queueNext = dvmGetFieldObject(reference,
-                gDvm.offJavaLangRefReference_queueNext);
-        if (queue == NULL || queueNext != NULL) {
-            /* There is no queue, or the reference has already
-             * been enqueued.  The Reference.enqueue() method
-             * will do nothing even if we call it.
-             */
-            return false;
-        }
+    Object *queue = dvmGetFieldObject(reference,
+            gDvm.offJavaLangRefReference_queue);
+    Object *queueNext = dvmGetFieldObject(reference,
+            gDvm.offJavaLangRefReference_queueNext);
+    if (queue == NULL || queueNext != NULL) {
+        /* There is no queue, or the reference has already
+         * been enqueued.  The Reference.enqueue() method
+         * will do nothing even if we call it.
+         */
+        return false;
     }
 
     /* We need to call enqueue(), but if we called it from
@@ -872,8 +791,6 @@ void dvmHeapHandleReferences(Object *refListHead, enum RefType refType)
     const int offReferent = gDvm.offJavaLangRefReference_referent;
     bool workRequired = false;
 
-size_t numCleared = 0;
-size_t numEnqueued = 0;
     reference = refListHead;
     while (reference != NULL) {
         Object *next;
@@ -889,7 +806,7 @@ size_t numEnqueued = 0;
         //      the list, and it would be nice to avoid the extra
         //      work.
         if (referent != NULL && !isMarked(ptr2chunk(referent), markContext)) {
-            bool schedClear, schedEnqueue;
+            bool schedEnqueue;
 
             /* This is the strongest reference that refers to referent.
              * Do the right thing.
@@ -897,7 +814,7 @@ size_t numEnqueued = 0;
             switch (refType) {
             case REF_SOFT:
             case REF_WEAK:
-                schedClear = clearReference(reference);
+                clearReference(reference);
                 schedEnqueue = enqueueReference(reference);
                 break;
             case REF_PHANTOM:
@@ -911,8 +828,21 @@ size_t numEnqueued = 0;
                  * (The referent will be marked outside of this loop,
                  * after handing all references of this strength, in
                  * case multiple references point to the same object.)
+                 *
+                 * One exception: JNI "weak global" references are handled
+                 * as a special case.  They're identified by the queue.
                  */
-                schedClear = false;
+                if (gDvm.jniWeakGlobalRefQueue != NULL) {
+                    Object* queue = dvmGetFieldObject(reference,
+                            gDvm.offJavaLangRefReference_queue);
+                    if (queue == gDvm.jniWeakGlobalRefQueue) {
+                        LOGV("+++ WGR: clearing + not queueing %p:%p\n",
+                            reference, referent);
+                        clearReference(reference);
+                        schedEnqueue = false;
+                        break;
+                    }
+                }
 
                 /* A PhantomReference is only useful with a
                  * queue, but since it's possible to create one
@@ -922,19 +852,15 @@ size_t numEnqueued = 0;
                 break;
             default:
                 assert(!"Bad reference type");
-                schedClear = false;
                 schedEnqueue = false;
                 break;
             }
-numCleared += schedClear ? 1 : 0;
-numEnqueued += schedEnqueue ? 1 : 0;
 
-            if (schedClear || schedEnqueue) {
+            if (schedEnqueue) {
                 uintptr_t workBits;
 
-                /* Stuff the clear/enqueue bits in the bottom of
-                 * the pointer.  Assumes that objects are 8-byte
-                 * aligned.
+                /* Stuff the enqueue bit in the bottom of the pointer.
+                 * Assumes that objects are 8-byte aligned.
                  *
                  * Note that we are adding the *Reference* (which
                  * is by definition already marked at this point) to
@@ -942,12 +868,10 @@ numEnqueued += schedEnqueue ? 1 : 0;
                  * has already been cleared).
                  */
                 assert(((intptr_t)reference & 3) == 0);
-                assert(((WORKER_CLEAR | WORKER_ENQUEUE) & ~3) == 0);
-                workBits = (schedClear ? WORKER_CLEAR : 0) |
-                           (schedEnqueue ? WORKER_ENQUEUE : 0);
+                assert((WORKER_ENQUEUE & ~3) == 0);
                 if (!dvmHeapAddRefToLargeTable(
                         &gDvm.gcHeap->referenceOperations,
-                        (Object *)((uintptr_t)reference | workBits)))
+                        (Object *)((uintptr_t)reference | WORKER_ENQUEUE)))
                 {
                     LOGE_HEAP("dvmMalloc(): no room for any more "
                             "reference operations\n");
@@ -967,15 +891,13 @@ numEnqueued += schedEnqueue ? 1 : 0;
 
         reference = next;
     }
-#define refType2str(r) \
-    ((r) == REF_SOFT ? "soft" : ( \
-     (r) == REF_WEAK ? "weak" : ( \
-     (r) == REF_PHANTOM ? "phantom" : "UNKNOWN" )))
-LOGD_HEAP("dvmHeapHandleReferences(): cleared %zd, enqueued %zd %s references\n", numCleared, numEnqueued, refType2str(refType));
 
     /* Walk though the reference list again, and mark any non-clear/marked
      * referents.  Only PhantomReferences can have non-clear referents
      * at this point.
+     *
+     * (Could skip this for JNI weak globals, since we know they've been
+     * cleared.)
      */
     if (refType == REF_PHANTOM) {
         bool scanRequired = false;
@@ -1225,17 +1147,6 @@ sweepBitmapCallback(size_t numPtrs, void **ptrs, const void *finger, void *arg)
         hc = (DvmHeapChunk *)*ptrs++;
         obj = (Object *)chunk2ptr(hc);
 
-#if WITH_OBJECT_HEADERS
-        if (hc->markGeneration == gGeneration) {
-            LOGE("sweeping marked object: 0x%08x\n", (uint)obj);
-            dvmAbort();
-        }
-#endif
-
-        /* Free the monitor associated with the object.
-         */
-        dvmFreeObjectMonitor(obj);
-
         /* NOTE: Dereferencing clazz is dangerous.  If obj was the last
          * one to reference its class object, the class object could be
          * on the sweep list, and could already have been swept, leaving
@@ -1264,16 +1175,9 @@ sweepBitmapCallback(size_t numPtrs, void **ptrs, const void *finger, void *arg)
         {
             int chunklen;
             ClassObject *clazz = obj->clazz;
-#if WITH_OBJECT_HEADERS
-            DvmHeapChunk chunk = *hc;
-            chunk.header = ~OBJECT_HEADER | 1;
-#endif
             chunklen = dvmHeapSourceChunkSize(hc);
             memset(hc, 0xa5, chunklen);
             obj->clazz = (ClassObject *)((uintptr_t)clazz ^ 0xffffffff);
-#if WITH_OBJECT_HEADERS
-            *hc = chunk;
-#endif
         }
 #endif
     }
@@ -1284,8 +1188,7 @@ sweepBitmapCallback(size_t numPtrs, void **ptrs, const void *finger, void *arg)
     return true;
 }
 
-/* A function suitable for passing to dvmHashForeachRemove()
- * to clear out any unmarked objects.  Clears the low bits
+/* Returns true if the given object is unmarked.  Ignores the low bits
  * of the pointer because the intern table may set them.
  */
 static int isUnmarkedObject(void *object)
@@ -1334,6 +1237,8 @@ dvmHeapSweepUnmarkedObjects(int *numFreed, size_t *sizeFreed)
     hprofDumpUnmarkedObjects(markBitmaps, objectBitmaps, numBitmaps);
 #endif
 
+    dvmSweepMonitorList(&gDvm.monitorList, isUnmarkedObject);
+
     dvmHeapBitmapXorWalkLists(markBitmaps, objectBitmaps, numBitmaps,
             sweepBitmapCallback, NULL);
 
diff --git a/vm/analysis/CodeVerify.c b/vm/analysis/CodeVerify.c
index f945f23..942ac60 100644
--- a/vm/analysis/CodeVerify.c
+++ b/vm/analysis/CodeVerify.c
@@ -2926,6 +2926,9 @@ static void verifyFilledNewArrayRegs(const Method* meth,
  * receive a "nop".  The instruction's length will be left unchanged
  * in "insnFlags".
  *
+ * The verifier explicitly locks out breakpoint activity, so there should
+ * be no clashes with the debugger.
+ *
  * IMPORTANT: this may replace meth->insns with a pointer to a new copy of
  * the instructions.
  *
@@ -2939,7 +2942,7 @@ static bool replaceFailingInstruction(Method* meth, InsnFlags* insnFlags,
     u2 oldInsn = *oldInsns;
     bool result = false;
 
-    dvmMakeCodeReadWrite(meth);
+    //dvmMakeCodeReadWrite(meth);
 
     //LOGD("  was 0x%04x\n", oldInsn);
     u2* newInsns = (u2*) meth->insns + insnIdx;
@@ -3018,7 +3021,8 @@ static bool replaceFailingInstruction(Method* meth, InsnFlags* insnFlags,
         /* nothing to do */
         break;
     case 3:
-        newInsns[2] = OP_NOP;
+        dvmDexChangeDex2(meth->clazz->pDvmDex, newInsns+2, OP_NOP);
+        //newInsns[2] = OP_NOP;
         break;
     default:
         /* whoops */
@@ -3028,13 +3032,15 @@ static bool replaceFailingInstruction(Method* meth, InsnFlags* insnFlags,
     }
 
     /* encode the opcode, with the failure code in the high byte */
-    newInsns[0] = OP_THROW_VERIFICATION_ERROR |
+    u2 newVal = OP_THROW_VERIFICATION_ERROR |
         (failure << 8) | (refType << (8 + kVerifyErrorRefTypeShift));
+    //newInsns[0] = newVal;
+    dvmDexChangeDex2(meth->clazz->pDvmDex, newInsns, newVal);
 
     result = true;
 
 bail:
-    dvmMakeCodeReadOnly(meth);
+    //dvmMakeCodeReadOnly(meth);
     return result;
 }
 
@@ -5406,6 +5412,7 @@ sput_1nr_common:
      * type, which is acceptable for any operation.
      */
     case OP_EXECUTE_INLINE:
+    case OP_EXECUTE_INLINE_RANGE:
     case OP_INVOKE_DIRECT_EMPTY:
     case OP_IGET_QUICK:
     case OP_IGET_WIDE_QUICK:
@@ -5420,7 +5427,7 @@ sput_1nr_common:
         failure = VERIFY_ERROR_GENERIC;
         break;
 
-    /* these should never appear */
+    /* these should never appear during verification */
     case OP_UNUSED_3E:
     case OP_UNUSED_3F:
     case OP_UNUSED_40:
@@ -5439,8 +5446,7 @@ sput_1nr_common:
     case OP_UNUSED_E9:
     case OP_UNUSED_EA:
     case OP_UNUSED_EB:
-    case OP_UNUSED_EC:
-    case OP_UNUSED_EF:
+    case OP_BREAKPOINT:
     case OP_UNUSED_F1:
     case OP_UNUSED_FC:
     case OP_UNUSED_FD:
diff --git a/vm/analysis/DexOptimize.c b/vm/analysis/DexOptimize.c
index b3e2d40..a5b8b6f 100644
--- a/vm/analysis/DexOptimize.c
+++ b/vm/analysis/DexOptimize.c
@@ -54,6 +54,7 @@ static bool writeAuxData(int fd, const DexClassLookup* pClassLookup,\
     const IndexMapSet* pIndexMapSet, const RegisterMapBuilder* pRegMapBuilder);
 static void logFailedWrite(size_t expected, ssize_t actual, const char* msg,
     int err);
+static bool computeFileChecksum(int fd, off_t start, size_t length, u4* pSum);
 
 static bool rewriteDex(u1* addr, int len, bool doVerify, bool doOpt,\
     u4* pHeaderFlags, DexClassLookup** ppClassLookup);
@@ -64,9 +65,11 @@ static void optimizeClass(ClassObject* clazz, const InlineSub* inlineSubs);
 static bool optimizeMethod(Method* method, const InlineSub* inlineSubs);
 static void rewriteInstField(Method* method, u2* insns, OpCode newOpc);
 static bool rewriteVirtualInvoke(Method* method, u2* insns, OpCode newOpc);
-static bool rewriteDirectInvoke(Method* method, u2* insns);
+static bool rewriteEmptyDirectInvoke(Method* method, u2* insns);
 static bool rewriteExecuteInline(Method* method, u2* insns,
     MethodType methodType, const InlineSub* inlineSubs);
+static bool rewriteExecuteInlineRange(Method* method, u2* insns,
+    MethodType methodType, const InlineSub* inlineSubs);
 
 
 /*
@@ -643,6 +646,7 @@ bool dvmContinueOptimization(int fd, off_t dexOffset, long dexLength,
     /* get start offset, and adjust deps start for 64-bit alignment */
     off_t depsOffset, auxOffset, endOffset, adjOffset;
     int depsLength, auxLength;
+    u4 optChecksum;
 
     depsOffset = lseek(fd, 0, SEEK_END);
     if (depsOffset < 0) {
@@ -688,6 +692,13 @@ bool dvmContinueOptimization(int fd, off_t dexOffset, long dexLength,
     endOffset = lseek(fd, 0, SEEK_END);
     auxLength = endOffset - auxOffset;
 
+    /* compute checksum from start of deps to end of aux area */
+    if (!computeFileChecksum(fd, depsOffset,
+            (auxOffset+auxLength) - depsOffset, &optChecksum))
+    {
+        goto bail;
+    }
+
     /*
      * Output the "opt" header with all values filled in and a correct
      * magic number.
@@ -704,6 +715,7 @@ bool dvmContinueOptimization(int fd, off_t dexOffset, long dexLength,
     optHdr.auxLength = (u4) auxLength;
 
     optHdr.flags = headerFlags;
+    optHdr.checksum = optChecksum;
 
     ssize_t actual;
     lseek(fd, 0, SEEK_SET);
@@ -1158,6 +1170,45 @@ static void logFailedWrite(size_t expected, ssize_t actual, const char* msg,
         msg, (int)actual, (int)expected, strerror(err));
 }
 
+/*
+ * Compute a checksum on a piece of an open file.
+ *
+ * File will be positioned at end of checksummed area.
+ *
+ * Returns "true" on success.
+ */
+static bool computeFileChecksum(int fd, off_t start, size_t length, u4* pSum)
+{
+    unsigned char readBuf[8192];
+    ssize_t actual;
+    uLong adler;
+
+    if (lseek(fd, start, SEEK_SET) != start) {
+        LOGE("Unable to seek to start of checksum area (%ld): %s\n",
+            (long) start, strerror(errno));
+        return false;
+    }
+
+    adler = adler32(0L, Z_NULL, 0);
+
+    while (length != 0) {
+        size_t wanted = (length < sizeof(readBuf)) ? length : sizeof(readBuf);
+        actual = read(fd, readBuf, wanted);
+        if (actual <= 0) {
+            LOGE("Read failed (%d) while computing checksum (len=%zu): %s\n",
+                (int) actual, length, strerror(errno));
+            return false;
+        }
+
+        adler = adler32(adler, readBuf, actual);
+
+        length -= actual;
+    }
+
+    *pSum = adler;
+    return true;
+}
+
 
 /*
  * ===========================================================================
@@ -1565,8 +1616,15 @@ static bool optimizeMethod(Method* method, const InlineSub* inlineSubs)
             }
             break;
         case OP_INVOKE_VIRTUAL_RANGE:
-            if (!rewriteVirtualInvoke(method, insns, OP_INVOKE_VIRTUAL_QUICK_RANGE))
-                return false;
+            if (!rewriteExecuteInlineRange(method, insns, METHOD_VIRTUAL,
+                    inlineSubs))
+            {
+                if (!rewriteVirtualInvoke(method, insns,
+                        OP_INVOKE_VIRTUAL_QUICK_RANGE))
+                {
+                    return false;
+                }
+            }
             break;
         case OP_INVOKE_SUPER:
             if (!rewriteVirtualInvoke(method, insns, OP_INVOKE_SUPER_QUICK))
@@ -1580,13 +1638,20 @@ static bool optimizeMethod(Method* method, const InlineSub* inlineSubs)
         case OP_INVOKE_DIRECT:
             if (!rewriteExecuteInline(method, insns, METHOD_DIRECT, inlineSubs))
             {
-                if (!rewriteDirectInvoke(method, insns))
+                if (!rewriteEmptyDirectInvoke(method, insns))
                     return false;
             }
             break;
+        case OP_INVOKE_DIRECT_RANGE:
+            rewriteExecuteInlineRange(method, insns, METHOD_DIRECT, inlineSubs);
+            break;
+
         case OP_INVOKE_STATIC:
             rewriteExecuteInline(method, insns, METHOD_STATIC, inlineSubs);
             break;
+        case OP_INVOKE_STATIC_RANGE:
+            rewriteExecuteInlineRange(method, insns, METHOD_STATIC, inlineSubs);
+            break;
 
         default:
             // ignore this instruction
@@ -2107,7 +2172,7 @@ static bool rewriteVirtualInvoke(Method* method, u2* insns, OpCode newOpc)
  * This must only be used when the invoked method does nothing and has
  * no return value (the latter being very important for verification).
  */
-static bool rewriteDirectInvoke(Method* method, u2* insns)
+static bool rewriteEmptyDirectInvoke(Method* method, u2* insns)
 {
     ClassObject* clazz = method->clazz;
     Method* calledMethod;
@@ -2226,6 +2291,7 @@ Method* dvmOptResolveInterfaceMethod(ClassObject* referrer, u4 methodIdx)
 
     return resMethod;
 }
+
 /*
  * See if the method being called can be rewritten as an inline operation.
  * Works for invoke-virtual, invoke-direct, and invoke-static.
@@ -2276,3 +2342,42 @@ static bool rewriteExecuteInline(Method* method, u2* insns,
     return false;
 }
 
+/*
+ * See if the method being called can be rewritten as an inline operation.
+ * Works for invoke-virtual/range, invoke-direct/range, and invoke-static/range.
+ *
+ * Returns "true" if we replace it.
+ */
+static bool rewriteExecuteInlineRange(Method* method, u2* insns,
+    MethodType methodType, const InlineSub* inlineSubs)
+{
+    ClassObject* clazz = method->clazz;
+    Method* calledMethod;
+    u2 methodIdx = insns[1];
+
+    calledMethod = dvmOptResolveMethod(clazz, methodIdx, methodType, NULL);
+    if (calledMethod == NULL) {
+        LOGV("+++ DexOpt inline/range: can't find %d\n", methodIdx);
+        return false;
+    }
+
+    while (inlineSubs->method != NULL) {
+        if (inlineSubs->method == calledMethod) {
+            assert((insns[0] & 0xff) == OP_INVOKE_DIRECT_RANGE ||
+                   (insns[0] & 0xff) == OP_INVOKE_STATIC_RANGE ||
+                   (insns[0] & 0xff) == OP_INVOKE_VIRTUAL_RANGE);
+            insns[0] = (insns[0] & 0xff00) | (u2) OP_EXECUTE_INLINE_RANGE;
+            insns[1] = (u2) inlineSubs->inlineIdx;
+
+            //LOGI("DexOpt: execute-inline/range %s.%s --> %s.%s\n",
+            //    method->clazz->descriptor, method->name,
+            //    calledMethod->clazz->descriptor, calledMethod->name);
+            return true;
+        }
+
+        inlineSubs++;
+    }
+
+    return false;
+}
+
diff --git a/vm/analysis/DexVerify.c b/vm/analysis/DexVerify.c
index 10251db..088309c 100644
--- a/vm/analysis/DexVerify.c
+++ b/vm/analysis/DexVerify.c
@@ -150,6 +150,7 @@ bool dvmVerifyClass(ClassObject* clazz, int verifyFlags)
     return true;
 }
 
+
 /*
  * Perform verification on a single method.
  *
diff --git a/vm/analysis/RegisterMap.c b/vm/analysis/RegisterMap.c
index 0e3445a..744832f 100644
--- a/vm/analysis/RegisterMap.c
+++ b/vm/analysis/RegisterMap.c
@@ -936,6 +936,7 @@ const u1* dvmRegisterMapGetLine(const RegisterMap* pMap, int addr)
 
             data += lineWidth;
         }
+        assert(data == pMap->data + lineWidth * numEntries);
     } else {
         int hi, lo, mid;
 
@@ -960,7 +961,6 @@ const u1* dvmRegisterMapGetLine(const RegisterMap* pMap, int addr)
         }
     }
 
-    assert(data == pMap->data + lineWidth * numEntries);
     return NULL;
 }
 
@@ -1036,6 +1036,7 @@ const RegisterMap* dvmGetExpandedRegisterMap0(Method* method)
     default:
         LOGE("Unknown format %d in dvmGetExpandedRegisterMap\n", format);
         dvmAbort();
+        newMap = NULL;      // make gcc happy
     }
 
     if (newMap == NULL) {
@@ -2999,6 +3000,7 @@ sget_1nr_common:
      * quickened.  This is feasible but not currently supported.
      */
     case OP_EXECUTE_INLINE:
+    case OP_EXECUTE_INLINE_RANGE:
     case OP_INVOKE_DIRECT_EMPTY:
     case OP_IGET_QUICK:
     case OP_IGET_WIDE_QUICK:
@@ -3033,9 +3035,8 @@ sget_1nr_common:
     case OP_UNUSED_E9:
     case OP_UNUSED_EA:
     case OP_UNUSED_EB:
-    case OP_UNUSED_EC:
+    case OP_BREAKPOINT:
     case OP_UNUSED_ED:
-    case OP_UNUSED_EF:
     case OP_UNUSED_F1:
     case OP_UNUSED_FC:
     case OP_UNUSED_FD:
diff --git a/vm/compiler/Compiler.c b/vm/compiler/Compiler.c
index 203a080..6a58d8b 100644
--- a/vm/compiler/Compiler.c
+++ b/vm/compiler/Compiler.c
@@ -16,12 +16,12 @@
 
 #include <sys/mman.h>
 #include <errno.h>
+#include <cutils/ashmem.h>
 
 #include "Dalvik.h"
 #include "interp/Jit.h"
 #include "CompilerInternals.h"
 
-
 static inline bool workQueueLength(void)
 {
     return gDvmJit.compilerQueueLength;
@@ -39,6 +39,9 @@ static CompilerWorkOrder workDequeue(void)
         gDvmJit.compilerWorkDequeueIndex = 0;
     }
     gDvmJit.compilerQueueLength--;
+    if (gDvmJit.compilerQueueLength == 0) {
+        int cc = pthread_cond_signal(&gDvmJit.compilerQueueEmpty);
+    }
 
     /* Remember the high water mark of the queue length */
     if (gDvmJit.compilerQueueLength > gDvmJit.compilerMaxQueued)
@@ -47,19 +50,32 @@ static CompilerWorkOrder workDequeue(void)
     return work;
 }
 
+/*
+ * Attempt to enqueue a work order, returning true if successful.
+ * This routine will not block, but simply return if it couldn't
+ * aquire the lock or if the queue is full.
+ *
+ * NOTE: Make sure that the caller frees the info pointer if the return value
+ * is false.
+ */
 bool dvmCompilerWorkEnqueue(const u2 *pc, WorkOrderKind kind, void* info)
 {
     int cc;
     int i;
     int numWork;
+    bool result = true;
 
-    dvmLockMutex(&gDvmJit.compilerLock);
+    if (dvmTryLockMutex(&gDvmJit.compilerLock)) {
+        return false;  // Couldn't acquire the lock
+    }
 
-    /* Queue full */
+    /*
+     * Return if queue or code cache is full.
+     */
     if (gDvmJit.compilerQueueLength == COMPILER_WORK_QUEUE_SIZE ||
         gDvmJit.codeCacheFull == true) {
-        dvmUnlockMutex(&gDvmJit.compilerLock);
-        return false;
+        result = false;
+        goto unlockAndExit;
     }
 
     for (numWork = gDvmJit.compilerQueueLength,
@@ -68,15 +84,22 @@ bool dvmCompilerWorkEnqueue(const u2 *pc, WorkOrderKind kind, void* info)
          numWork--) {
         /* Already enqueued */
         if (gDvmJit.compilerWorkQueue[i++].pc == pc)
-            goto done;
+            goto unlockAndExit;
         /* Wrap around */
         if (i == COMPILER_WORK_QUEUE_SIZE)
             i = 0;
     }
 
-    gDvmJit.compilerWorkQueue[gDvmJit.compilerWorkEnqueueIndex].pc = pc;
-    gDvmJit.compilerWorkQueue[gDvmJit.compilerWorkEnqueueIndex].kind = kind;
-    gDvmJit.compilerWorkQueue[gDvmJit.compilerWorkEnqueueIndex].info = info;
+    CompilerWorkOrder *newOrder =
+        &gDvmJit.compilerWorkQueue[gDvmJit.compilerWorkEnqueueIndex];
+    newOrder->pc = pc;
+    newOrder->kind = kind;
+    newOrder->info = info;
+    newOrder->result.codeAddress = NULL;
+    newOrder->result.discardResult =
+        (kind == kWorkOrderTraceDebug) ? true : false;
+    newOrder->result.requestingThread = dvmThreadSelf();
+
     gDvmJit.compilerWorkEnqueueIndex++;
     if (gDvmJit.compilerWorkEnqueueIndex == COMPILER_WORK_QUEUE_SIZE)
         gDvmJit.compilerWorkEnqueueIndex = 0;
@@ -84,84 +107,55 @@ bool dvmCompilerWorkEnqueue(const u2 *pc, WorkOrderKind kind, void* info)
     cc = pthread_cond_signal(&gDvmJit.compilerQueueActivity);
     assert(cc == 0);
 
-done:
+unlockAndExit:
     dvmUnlockMutex(&gDvmJit.compilerLock);
-    return true;
+    return result;
 }
 
-/* Block until queue length is 0 */
+/* Block until the queue length is 0, or there is a pending suspend request */
 void dvmCompilerDrainQueue(void)
 {
-    dvmLockMutex(&gDvmJit.compilerLock);
-    while (workQueueLength() != 0 && !gDvmJit.haltCompilerThread) {
-        pthread_cond_wait(&gDvmJit.compilerQueueEmpty, &gDvmJit.compilerLock);
-    }
-    dvmUnlockMutex(&gDvmJit.compilerLock);
-}
+    Thread *self = dvmThreadSelf();
 
-static void *compilerThreadStart(void *arg)
-{
     dvmLockMutex(&gDvmJit.compilerLock);
-    /*
-     * Since the compiler thread will not touch any objects on the heap once
-     * being created, we just fake its state as VMWAIT so that it can be a
-     * bit late when there is suspend request pending.
-     */
-    dvmChangeStatus(NULL, THREAD_VMWAIT);
-    while (!gDvmJit.haltCompilerThread) {
-        if (workQueueLength() == 0) {
-            int cc;
-            cc = pthread_cond_signal(&gDvmJit.compilerQueueEmpty);
-            assert(cc == 0);
-            pthread_cond_wait(&gDvmJit.compilerQueueActivity,
-                              &gDvmJit.compilerLock);
-            continue;
-        } else {
-            do {
-                CompilerWorkOrder work = workDequeue();
-                dvmUnlockMutex(&gDvmJit.compilerLock);
-                /* Check whether there is a suspend request on me */
-                dvmCheckSuspendPending(NULL);
-                /* Is JitTable filling up? */
-                if (gDvmJit.jitTableEntriesUsed >
-                    (gDvmJit.jitTableSize - gDvmJit.jitTableSize/4)) {
-                    dvmJitResizeJitTable(gDvmJit.jitTableSize * 2);
-                }
-                if (gDvmJit.haltCompilerThread) {
-                    LOGD("Compiler shutdown in progress - discarding request");
-                } else {
-                    /* Compilation is successful */
-                    if (dvmCompilerDoWork(&work)) {
-                        dvmJitSetCodeAddr(work.pc, work.result.codeAddress,
-                                          work.result.instructionSet);
-                    }
-                }
-                free(work.info);
-                dvmLockMutex(&gDvmJit.compilerLock);
-            } while (workQueueLength() != 0);
-        }
+    while (workQueueLength() != 0 && !gDvmJit.haltCompilerThread &&
+           self->suspendCount == 0) {
+        /*
+         * Use timed wait here - more than one mutator threads may be blocked
+         * but the compiler thread will only signal once when the queue is
+         * emptied. Furthermore, the compiler thread may have been shutdown
+         * so the blocked thread may never get the wakeup signal.
+         */
+        dvmRelativeCondWait(&gDvmJit.compilerQueueEmpty, &gDvmJit.compilerLock,                             1000, 0);
     }
-    pthread_cond_signal(&gDvmJit.compilerQueueEmpty);
     dvmUnlockMutex(&gDvmJit.compilerLock);
-
-    LOGD("Compiler thread shutting down\n");
-    return NULL;
 }
 
 bool dvmCompilerSetupCodeCache(void)
 {
     extern void dvmCompilerTemplateStart(void);
     extern void dmvCompilerTemplateEnd(void);
+    int fd;
 
     /* Allocate the code cache */
-    gDvmJit.codeCache = mmap(0, CODE_CACHE_SIZE,
-                          PROT_READ | PROT_WRITE | PROT_EXEC,
-                          MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+    fd = ashmem_create_region("dalvik-jit-code-cache", gDvmJit.codeCacheSize);
+    if (fd < 0) {
+        LOGE("Could not create %u-byte ashmem region for the JIT code cache",
+             gDvmJit.codeCacheSize);
+        return false;
+    }
+    gDvmJit.codeCache = mmap(NULL, gDvmJit.codeCacheSize,
+                             PROT_READ | PROT_WRITE | PROT_EXEC,
+                             MAP_PRIVATE , fd, 0);
+    close(fd);
     if (gDvmJit.codeCache == MAP_FAILED) {
-        LOGE("Failed to create the code cache: %s\n", strerror(errno));
+        LOGE("Failed to mmap the JIT code cache: %s\n", strerror(errno));
         return false;
     }
 
+    /* This can be found through "dalvik-jit-code-cache" in /proc/<pid>/maps */
+    // LOGD("Code cache starts at %p", gDvmJit.codeCache);
+
     /* Copy the template code into the beginning of the code cache */
     int templateSize = (intptr_t) dmvCompilerTemplateEnd -
                        (intptr_t) dvmCompilerTemplateStart;
@@ -169,28 +163,167 @@ bool dvmCompilerSetupCodeCache(void)
            (void *) dvmCompilerTemplateStart,
            templateSize);
 
+    /*
+     * Work around a CPU bug by keeping the 32-bit ARM handler code in its own
+     * page.
+     */
+    if (dvmCompilerInstructionSet() == DALVIK_JIT_THUMB2) {
+        templateSize = (templateSize + 4095) & ~4095;
+    }
+
     gDvmJit.templateSize = templateSize;
     gDvmJit.codeCacheByteUsed = templateSize;
 
-    /* Flush dcache and invalidate the icache to maintain coherence */
+    /* Only flush the part in the code cache that is being used now */
     cacheflush((intptr_t) gDvmJit.codeCache,
-               (intptr_t) gDvmJit.codeCache + CODE_CACHE_SIZE, 0);
+               (intptr_t) gDvmJit.codeCache + templateSize, 0);
     return true;
 }
 
-bool dvmCompilerStartup(void)
+static void crawlDalvikStack(Thread *thread, bool print)
+{
+    void *fp = thread->curFrame;
+    StackSaveArea* saveArea = NULL;
+    int stackLevel = 0;
+
+    if (print) {
+        LOGD("Crawling tid %d (%s / %p %s)", thread->systemTid,
+             dvmGetThreadStatusStr(thread->status),
+             thread->inJitCodeCache,
+             thread->inJitCodeCache ? "jit" : "interp");
+    }
+    /* Crawl the Dalvik stack frames to clear the returnAddr field */
+    while (fp != NULL) {
+        saveArea = SAVEAREA_FROM_FP(fp);
+
+        if (print) {
+            if (dvmIsBreakFrame(fp)) {
+                LOGD("  #%d: break frame (%p)",
+                     stackLevel, saveArea->returnAddr);
+            }
+            else {
+                LOGD("  #%d: %s.%s%s (%p)",
+                     stackLevel,
+                     saveArea->method->clazz->descriptor,
+                     saveArea->method->name,
+                     dvmIsNativeMethod(saveArea->method) ?
+                         " (native)" : "",
+                     saveArea->returnAddr);
+            }
+        }
+        stackLevel++;
+        saveArea->returnAddr = NULL;
+        assert(fp != saveArea->prevFrame);
+        fp = saveArea->prevFrame;
+    }
+    /* Make sure the stack is fully unwound to the bottom */
+    assert(saveArea == NULL ||
+           (u1 *) (saveArea+1) == thread->interpStackStart);
+}
+
+static void resetCodeCache(void)
+{
+    Thread* thread;
+    u8 startTime = dvmGetRelativeTimeUsec();
+    int inJit = 0;
+    int byteUsed = gDvmJit.codeCacheByteUsed;
+
+    /* If any thread is found stuck in the JIT state, don't reset the cache */
+    for (thread = gDvm.threadList; thread != NULL; thread = thread->next) {
+        /*
+         * Crawl the stack to wipe out the returnAddr field so that
+         * 1) the soon-to-be-deleted code in the JIT cache won't be used
+         * 2) or the thread stuck in the JIT land will soon return
+         *    to the interpreter land
+         */
+        crawlDalvikStack(thread, false);
+        if (thread->inJitCodeCache) {
+            inJit++;
+        }
+    }
+
+    if (inJit) {
+        LOGD("JIT code cache reset delayed (%d bytes %d/%d)",
+             gDvmJit.codeCacheByteUsed, gDvmJit.numCodeCacheReset,
+             ++gDvmJit.numCodeCacheResetDelayed);
+        return;
+    }
+
+    /* Lock the mutex to clean up the work queue */
+    dvmLockMutex(&gDvmJit.compilerLock);
+
+    /* Drain the work queue to free the work orders */
+    while (workQueueLength()) {
+        CompilerWorkOrder work = workDequeue();
+        free(work.info);
+    }
+
+    /* Reset the JitEntry table contents to the initial unpopulated state */
+    dvmJitResetTable();
+
+    /*
+     * Wipe out the code cache content to force immediate crashes if
+     * stale JIT'ed code is invoked.
+     */
+    memset((char *) gDvmJit.codeCache + gDvmJit.templateSize,
+           0,
+           gDvmJit.codeCacheByteUsed - gDvmJit.templateSize);
+    cacheflush((intptr_t) gDvmJit.codeCache,
+               (intptr_t) gDvmJit.codeCache + gDvmJit.codeCacheByteUsed, 0);
+
+    /* Reset the current mark of used bytes to the end of template code */
+    gDvmJit.codeCacheByteUsed = gDvmJit.templateSize;
+    gDvmJit.numCompilations = 0;
+
+    /* Reset the work queue */
+    memset(gDvmJit.compilerWorkQueue, 0,
+           sizeof(CompilerWorkOrder) * COMPILER_WORK_QUEUE_SIZE);
+    gDvmJit.compilerWorkEnqueueIndex = gDvmJit.compilerWorkDequeueIndex = 0;
+    gDvmJit.compilerQueueLength = 0;
+
+    /* Reset the IC patch work queue */
+    dvmLockMutex(&gDvmJit.compilerICPatchLock);
+    gDvmJit.compilerICPatchIndex = 0;
+    dvmUnlockMutex(&gDvmJit.compilerICPatchLock);
+
+    /* All clear now */
+    gDvmJit.codeCacheFull = false;
+
+    dvmUnlockMutex(&gDvmJit.compilerLock);
+
+    LOGD("JIT code cache reset in %lld ms (%d bytes %d/%d)",
+         (dvmGetRelativeTimeUsec() - startTime) / 1000,
+         byteUsed, ++gDvmJit.numCodeCacheReset,
+         gDvmJit.numCodeCacheResetDelayed);
+}
+
+/*
+ * Perform actions that are only safe when all threads are suspended. Currently
+ * we do:
+ * 1) Check if the code cache is full. If so reset it and restart populating it
+ *    from scratch.
+ * 2) Patch predicted chaining cells by consuming recorded work orders.
+ */
+void dvmCompilerPerformSafePointChecks(void)
+{
+    if (gDvmJit.codeCacheFull) {
+        resetCodeCache();
+    }
+    dvmCompilerPatchInlineCache();
+}
+
+bool compilerThreadStartup(void)
 {
-    /* Make sure the BBType enum is in sane state */
-    assert(CHAINING_CELL_NORMAL == 0);
+    JitEntry *pJitTable = NULL;
+    unsigned char *pJitProfTable = NULL;
+    unsigned int i;
 
-    /* Architecture-specific chores to initialize */
     if (!dvmCompilerArchInit())
         goto fail;
 
     /*
-     * Setup the code cache if it is not done so already. For apps it should be
-     * done by the Zygote already, but for command-line dalvikvm invocation we
-     * need to do it here.
+     * Setup the code cache if we have not inherited a valid code cache
+     * from the zygote.
      */
     if (gDvmJit.codeCache == NULL) {
         if (!dvmCompilerSetupCodeCache())
@@ -202,44 +335,352 @@ bool dvmCompilerStartup(void)
         goto fail;
     }
 
-    dvmInitMutex(&gDvmJit.compilerLock);
-    pthread_cond_init(&gDvmJit.compilerQueueActivity, NULL);
-    pthread_cond_init(&gDvmJit.compilerQueueEmpty, NULL);
-
     dvmLockMutex(&gDvmJit.compilerLock);
 
-    gDvmJit.haltCompilerThread = false;
+#if defined(WITH_JIT_TUNING)
+    /* Track method-level compilation statistics */
+    gDvmJit.methodStatsTable =  dvmHashTableCreate(32, NULL);
+    gDvm.verboseShutdown = true;
+#endif
 
-    /* Reset the work queue */
-    memset(gDvmJit.compilerWorkQueue, 0,
-           sizeof(CompilerWorkOrder) * COMPILER_WORK_QUEUE_SIZE);
-    gDvmJit.compilerWorkEnqueueIndex = gDvmJit.compilerWorkDequeueIndex = 0;
-    gDvmJit.compilerQueueLength = 0;
-    gDvmJit.compilerHighWater =
-        COMPILER_WORK_QUEUE_SIZE - (COMPILER_WORK_QUEUE_SIZE/4);
+    dvmUnlockMutex(&gDvmJit.compilerLock);
 
-    assert(gDvmJit.compilerHighWater < COMPILER_WORK_QUEUE_SIZE);
-    if (!dvmCreateInternalThread(&gDvmJit.compilerHandle, "Compiler",
-                                 compilerThreadStart, NULL)) {
-        dvmUnlockMutex(&gDvmJit.compilerLock);
+    /* Set up the JitTable */
+
+    /* Power of 2? */
+    assert(gDvmJit.jitTableSize &&
+           !(gDvmJit.jitTableSize & (gDvmJit.jitTableSize - 1)));
+
+    dvmInitMutex(&gDvmJit.tableLock);
+    dvmLockMutex(&gDvmJit.tableLock);
+    pJitTable = (JitEntry*)
+                calloc(gDvmJit.jitTableSize, sizeof(*pJitTable));
+    if (!pJitTable) {
+        LOGE("jit table allocation failed\n");
+        dvmUnlockMutex(&gDvmJit.tableLock);
+        goto fail;
+    }
+    /*
+     * NOTE: the profile table must only be allocated once, globally.
+     * Profiling is turned on and off by nulling out gDvm.pJitProfTable
+     * and then restoring its original value.  However, this action
+     * is not syncronized for speed so threads may continue to hold
+     * and update the profile table after profiling has been turned
+     * off by null'ng the global pointer.  Be aware.
+     */
+    pJitProfTable = (unsigned char *)malloc(JIT_PROF_SIZE);
+    if (!pJitProfTable) {
+        LOGE("jit prof table allocation failed\n");
+        dvmUnlockMutex(&gDvmJit.tableLock);
         goto fail;
     }
+    memset(pJitProfTable, gDvmJit.threshold, JIT_PROF_SIZE);
+    for (i=0; i < gDvmJit.jitTableSize; i++) {
+       pJitTable[i].u.info.chain = gDvmJit.jitTableSize;
+    }
+    /* Is chain field wide enough for termination pattern? */
+    assert(pJitTable[0].u.info.chain == gDvmJit.jitTableSize);
 
-    /* Track method-level compilation statistics */
-    gDvmJit.methodStatsTable =  dvmHashTableCreate(32, NULL);
+    gDvmJit.pJitEntryTable = pJitTable;
+    gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
+    gDvmJit.jitTableEntriesUsed = 0;
+    gDvmJit.compilerHighWater =
+        COMPILER_WORK_QUEUE_SIZE - (COMPILER_WORK_QUEUE_SIZE/4);
+    /*
+     * If the VM is launched with wait-on-the-debugger, we will need to hide
+     * the profile table here
+     */
+    gDvmJit.pProfTable = dvmDebuggerOrProfilerActive() ? NULL : pJitProfTable;
+    gDvmJit.pProfTableCopy = pJitProfTable;
+    dvmUnlockMutex(&gDvmJit.tableLock);
 
-    dvmUnlockMutex(&gDvmJit.compilerLock);
+    /* Signal running threads to refresh their cached pJitTable pointers */
+    dvmSuspendAllThreads(SUSPEND_FOR_REFRESH);
+    dvmResumeAllThreads(SUSPEND_FOR_REFRESH);
+
+    /* Enable signature breakpoints by customizing the following code */
+#if defined(SIGNATURE_BREAKPOINT)
+    /*
+     * Suppose one sees the following native crash in the bugreport:
+     * I/DEBUG   ( 1638): Build fingerprint: 'unknown'
+     * I/DEBUG   ( 1638): pid: 2468, tid: 2507  >>> com.google.android.gallery3d
+     * I/DEBUG   ( 1638): signal 11 (SIGSEGV), fault addr 00001400
+     * I/DEBUG   ( 1638):  r0 44ea7190  r1 44e4f7b8  r2 44ebc710  r3 00000000
+     * I/DEBUG   ( 1638):  r4 00000a00  r5 41862dec  r6 4710dc10  r7 00000280
+     * I/DEBUG   ( 1638):  r8 ad010f40  r9 46a37a12  10 001116b0  fp 42a78208
+     * I/DEBUG   ( 1638):  ip 00000090  sp 4710dbc8  lr ad060e67  pc 46b90682
+     * cpsr 00000030
+     * I/DEBUG   ( 1638):  #00  pc 46b90682 /dev/ashmem/dalvik-jit-code-cache
+     * I/DEBUG   ( 1638):  #01  pc 00060e62  /system/lib/libdvm.so
+     *
+     * I/DEBUG   ( 1638): code around pc:
+     * I/DEBUG   ( 1638): 46b90660 6888d01c 34091dcc d2174287 4a186b68
+     * I/DEBUG   ( 1638): 46b90670 d0052800 68006809 28004790 6b68d00e
+     * I/DEBUG   ( 1638): 46b90680 512000bc 37016eaf 6ea866af 6f696028
+     * I/DEBUG   ( 1638): 46b90690 682a6069 429a686b e003da08 6df1480b
+     * I/DEBUG   ( 1638): 46b906a0 1c2d4788 47806d70 46a378fa 47806d70
+     *
+     * Clearly it is a JIT bug. To find out which translation contains the
+     * offending code, the content of the memory dump around the faulting PC
+     * can be pasted into the gDvmJit.signatureBreakpoint[] array and next time
+     * when a similar compilation is being created, the JIT compiler replay the
+     * trace in the verbose mode and one can investigate the instruction
+     * sequence in details.
+     *
+     * The length of the signature may need additional experiments to determine.
+     * The rule of thumb is don't include PC-relative instructions in the
+     * signature since it may be affected by the alignment of the compiled code.
+     * However, a signature that's too short might increase the chance of false
+     * positive matches. Using gdbjithelper to disassembly the memory content
+     * first might be a good companion approach.
+     *
+     * For example, if the next 4 words starting from 46b90680 is pasted into
+     * the data structure:
+     */
+
+    gDvmJit.signatureBreakpointSize = 4;
+    gDvmJit.signatureBreakpoint =
+        malloc(sizeof(u4) * gDvmJit.signatureBreakpointSize);
+    gDvmJit.signatureBreakpoint[0] = 0x512000bc;
+    gDvmJit.signatureBreakpoint[1] = 0x37016eaf;
+    gDvmJit.signatureBreakpoint[2] = 0x6ea866af;
+    gDvmJit.signatureBreakpoint[3] = 0x6f696028;
+
+    /*
+     * The following log will be printed when a match is found in subsequent
+     * testings:
+     *
+     * D/dalvikvm( 2468): Signature match starting from offset 0x34 (4 words)
+     * D/dalvikvm( 2468): --------
+     * D/dalvikvm( 2468): Compiler: Building trace for computeVisibleItems,
+     * offset 0x1f7
+     * D/dalvikvm( 2468): 0x46a37a12: 0x0090 add-int v42, v5, v26
+     * D/dalvikvm( 2468): 0x46a37a16: 0x004d aput-object v13, v14, v42
+     * D/dalvikvm( 2468): 0x46a37a1a: 0x0028 goto, (#0), (#0)
+     * D/dalvikvm( 2468): 0x46a3794e: 0x00d8 add-int/lit8 v26, v26, (#1)
+     * D/dalvikvm( 2468): 0x46a37952: 0x0028 goto, (#0), (#0)
+     * D/dalvikvm( 2468): 0x46a378ee: 0x0002 move/from16 v0, v26, (#0)
+     * D/dalvikvm( 2468): 0x46a378f2: 0x0002 move/from16 v1, v29, (#0)
+     * D/dalvikvm( 2468): 0x46a378f6: 0x0035 if-ge v0, v1, (#10)
+     * D/dalvikvm( 2468): TRACEINFO (554): 0x46a37624
+     * Lcom/cooliris/media/GridLayer;computeVisibleItems 0x1f7 14 of 934, 8
+     * blocks
+     *     :
+     *     :
+     * D/dalvikvm( 2468): 0x20 (0020): ldr     r0, [r5, #52]
+     * D/dalvikvm( 2468): 0x22 (0022): ldr     r2, [pc, #96]
+     * D/dalvikvm( 2468): 0x24 (0024): cmp     r0, #0
+     * D/dalvikvm( 2468): 0x26 (0026): beq     0x00000034
+     * D/dalvikvm( 2468): 0x28 (0028): ldr     r1, [r1, #0]
+     * D/dalvikvm( 2468): 0x2a (002a): ldr     r0, [r0, #0]
+     * D/dalvikvm( 2468): 0x2c (002c): blx     r2
+     * D/dalvikvm( 2468): 0x2e (002e): cmp     r0, #0
+     * D/dalvikvm( 2468): 0x30 (0030): beq     0x00000050
+     * D/dalvikvm( 2468): 0x32 (0032): ldr     r0, [r5, #52]
+     * D/dalvikvm( 2468): 0x34 (0034): lsls    r4, r7, #2
+     * D/dalvikvm( 2468): 0x36 (0036): str     r0, [r4, r4]
+     * D/dalvikvm( 2468): -------- dalvik offset: 0x01fb @ goto, (#0), (#0)
+     * D/dalvikvm( 2468): L0x0195:
+     * D/dalvikvm( 2468): -------- dalvik offset: 0x0195 @ add-int/lit8 v26,
+     * v26, (#1)
+     * D/dalvikvm( 2468): 0x38 (0038): ldr     r7, [r5, #104]
+     * D/dalvikvm( 2468): 0x3a (003a): adds    r7, r7, #1
+     * D/dalvikvm( 2468): 0x3c (003c): str     r7, [r5, #104]
+     * D/dalvikvm( 2468): -------- dalvik offset: 0x0197 @ goto, (#0), (#0)
+     * D/dalvikvm( 2468): L0x0165:
+     * D/dalvikvm( 2468): -------- dalvik offset: 0x0165 @ move/from16 v0, v26,
+     * (#0)
+     * D/dalvikvm( 2468): 0x3e (003e): ldr     r0, [r5, #104]
+     * D/dalvikvm( 2468): 0x40 (0040): str     r0, [r5, #0]
+     *
+     * The "str r0, [r4, r4]" is indeed the culprit of the native crash.
+     */
+#endif
 
     return true;
 
 fail:
     return false;
+
+}
+
+static void *compilerThreadStart(void *arg)
+{
+    int ret;
+    struct timespec ts;
+
+    dvmChangeStatus(NULL, THREAD_VMWAIT);
+
+    /*
+     * If we're not running stand-alone, wait a little before
+     * recieving translation requests on the assumption that process start
+     * up code isn't worth compiling.  We'll resume when the framework
+     * signals us that the first screen draw has happened, or the timer
+     * below expires (to catch daemons).
+     *
+     * There is a theoretical race between the callback to
+     * VMRuntime.startJitCompiation and when the compiler thread reaches this
+     * point. In case the callback happens earlier, in order not to permanently
+     * hold the system_server (which is not using the timed wait) in
+     * interpreter-only mode we bypass the delay here.
+     */
+    if (gDvmJit.runningInAndroidFramework &&
+        !gDvmJit.alreadyEnabledViaFramework) {
+        /*
+         * If the current VM instance is the system server (detected by having
+         * 0 in gDvm.systemServerPid), we will use the indefinite wait on the
+         * conditional variable to determine whether to start the JIT or not.
+         * If the system server detects that the whole system is booted in
+         * safe mode, the conditional variable will never be signaled and the
+         * system server will remain in the interpreter-only mode. All
+         * subsequent apps will be started with the --enable-safemode flag
+         * explicitly appended.
+         */
+        if (gDvm.systemServerPid == 0) {
+            dvmLockMutex(&gDvmJit.compilerLock);
+            pthread_cond_wait(&gDvmJit.compilerQueueActivity,
+                              &gDvmJit.compilerLock);
+            dvmUnlockMutex(&gDvmJit.compilerLock);
+            LOGD("JIT started for system_server");
+        } else {
+            dvmLockMutex(&gDvmJit.compilerLock);
+            /*
+             * TUNING: experiment with the delay & perhaps make it
+             * target-specific
+             */
+            dvmRelativeCondWait(&gDvmJit.compilerQueueActivity,
+                                 &gDvmJit.compilerLock, 3000, 0);
+            dvmUnlockMutex(&gDvmJit.compilerLock);
+        }
+        if (gDvmJit.haltCompilerThread) {
+             return NULL;
+        }
+    }
+
+    compilerThreadStartup();
+
+    dvmLockMutex(&gDvmJit.compilerLock);
+    /*
+     * Since the compiler thread will not touch any objects on the heap once
+     * being created, we just fake its state as VMWAIT so that it can be a
+     * bit late when there is suspend request pending.
+     */
+    while (!gDvmJit.haltCompilerThread) {
+        if (workQueueLength() == 0) {
+            int cc;
+            cc = pthread_cond_signal(&gDvmJit.compilerQueueEmpty);
+            assert(cc == 0);
+            pthread_cond_wait(&gDvmJit.compilerQueueActivity,
+                              &gDvmJit.compilerLock);
+            continue;
+        } else {
+            do {
+                CompilerWorkOrder work = workDequeue();
+                dvmUnlockMutex(&gDvmJit.compilerLock);
+#if defined(JIT_STATS)
+                u8 startTime = dvmGetRelativeTimeUsec();
+#endif
+                /*
+                 * Check whether there is a suspend request on me.  This
+                 * is necessary to allow a clean shutdown.
+                 *
+                 * However, in the blocking stress testing mode, let the
+                 * compiler thread continue doing compilations to unblock
+                 * other requesting threads. This may occasionally cause
+                 * shutdown from proceeding cleanly in the standalone invocation
+                 * of the vm but this should be acceptable.
+                 */
+                if (!gDvmJit.blockingMode)
+                    dvmCheckSuspendPending(NULL);
+                /* Is JitTable filling up? */
+                if (gDvmJit.jitTableEntriesUsed >
+                    (gDvmJit.jitTableSize - gDvmJit.jitTableSize/4)) {
+                    bool resizeFail =
+                        dvmJitResizeJitTable(gDvmJit.jitTableSize * 2);
+                    /*
+                     * If the jit table is full, consider it's time to reset
+                     * the code cache too.
+                     */
+                    gDvmJit.codeCacheFull |= resizeFail;
+                }
+                if (gDvmJit.haltCompilerThread) {
+                    LOGD("Compiler shutdown in progress - discarding request");
+                } else if (!gDvmJit.codeCacheFull) {
+                    bool compileOK = false;
+                    jmp_buf jmpBuf;
+                    work.bailPtr = &jmpBuf;
+                    bool aborted = setjmp(jmpBuf);
+                    if (!aborted) {
+                        compileOK = dvmCompilerDoWork(&work);
+                    }
+                    if (aborted || !compileOK) {
+                        dvmCompilerArenaReset();
+                        work.result.codeAddress = gDvmJit.interpretTemplate;
+                    } else if (!work.result.discardResult) {
+                        dvmJitSetCodeAddr(work.pc, work.result.codeAddress,
+                                          work.result.instructionSet);
+                    }
+                }
+                free(work.info);
+#if defined(JIT_STATS)
+                gDvmJit.jitTime += dvmGetRelativeTimeUsec() - startTime;
+#endif
+                dvmLockMutex(&gDvmJit.compilerLock);
+            } while (workQueueLength() != 0);
+        }
+    }
+    pthread_cond_signal(&gDvmJit.compilerQueueEmpty);
+    dvmUnlockMutex(&gDvmJit.compilerLock);
+
+    /*
+     * As part of detaching the thread we need to call into Java code to update
+     * the ThreadGroup, and we should not be in VMWAIT state while executing
+     * interpreted code.
+     */
+    dvmChangeStatus(NULL, THREAD_RUNNING);
+
+    if (gDvm.verboseShutdown)
+        LOGD("Compiler thread shutting down\n");
+    return NULL;
+}
+
+bool dvmCompilerStartup(void)
+{
+
+    dvmInitMutex(&gDvmJit.compilerLock);
+    dvmInitMutex(&gDvmJit.compilerICPatchLock);
+    dvmLockMutex(&gDvmJit.compilerLock);
+    pthread_cond_init(&gDvmJit.compilerQueueActivity, NULL);
+    pthread_cond_init(&gDvmJit.compilerQueueEmpty, NULL);
+
+    /* Reset the work queue */
+    gDvmJit.compilerWorkEnqueueIndex = gDvmJit.compilerWorkDequeueIndex = 0;
+    gDvmJit.compilerQueueLength = 0;
+    dvmUnlockMutex(&gDvmJit.compilerLock);
+
+    /*
+     * Defer rest of initialization until we're sure JIT'ng makes sense. Launch
+     * the compiler thread, which will do the real initialization if and
+     * when it is signalled to do so.
+     */
+    return dvmCreateInternalThread(&gDvmJit.compilerHandle, "Compiler",
+                                   compilerThreadStart, NULL);
 }
 
 void dvmCompilerShutdown(void)
 {
     void *threadReturn;
 
+    /* Disable new translation requests */
+    gDvmJit.pProfTable = NULL;
+    gDvmJit.pProfTableCopy = NULL;
+
+    if (gDvm.verboseShutdown) {
+        dvmCompilerDumpStats();
+        while (gDvmJit.compilerQueueLength)
+          sleep(5);
+    }
+
     if (gDvmJit.compilerHandle) {
 
         gDvmJit.haltCompilerThread = true;
@@ -250,7 +691,49 @@ void dvmCompilerShutdown(void)
 
         if (pthread_join(gDvmJit.compilerHandle, &threadReturn) != 0)
             LOGW("Compiler thread join failed\n");
-        else
+        else if (gDvm.verboseShutdown)
             LOGD("Compiler thread has shut down\n");
     }
+
+    /* Break loops within the translation cache */
+    dvmJitUnchainAll();
+
+    /*
+     * NOTE: our current implementatation doesn't allow for the compiler
+     * thread to be restarted after it exits here.  We aren't freeing
+     * the JitTable or the ProfTable because threads which still may be
+     * running or in the process of shutting down may hold references to
+     * them.
+     */
+}
+
+void dvmCompilerStateRefresh()
+{
+    bool jitActive;
+    bool jitActivate;
+    bool needUnchain = false;
+
+    /*
+     * The tableLock might not be initialized yet by the compiler thread if
+     * debugger is attached from the very beginning of the VM launch. If
+     * pProfTableCopy is NULL, the lock is not initialized yet and we don't
+     * need to refresh anything either.
+     */
+    if (gDvmJit.pProfTableCopy == NULL) {
+        return;
+    }
+
+    dvmLockMutex(&gDvmJit.tableLock);
+    jitActive = gDvmJit.pProfTable != NULL;
+    jitActivate = !(gDvm.debuggerActive || (gDvm.activeProfilers > 0));
+
+    if (jitActivate && !jitActive) {
+        gDvmJit.pProfTable = gDvmJit.pProfTableCopy;
+    } else if (!jitActivate && jitActive) {
+        gDvmJit.pProfTable = NULL;
+        needUnchain = true;
+    }
+    dvmUnlockMutex(&gDvmJit.tableLock);
+    if (needUnchain)
+        dvmJitUnchainAll();
 }
diff --git a/vm/compiler/Compiler.h b/vm/compiler/Compiler.h
index 3b7ae54..ba23d7d 100644
--- a/vm/compiler/Compiler.h
+++ b/vm/compiler/Compiler.h
@@ -14,12 +14,20 @@
  * limitations under the License.
  */
 
+#include <Thread.h>
+#include <setjmp.h>
+
 #ifndef _DALVIK_VM_COMPILER
 #define _DALVIK_VM_COMPILER
 
-#define CODE_CACHE_SIZE                 1024*1024
+/*
+ * Uncomment the following to enable JIT signature breakpoint
+ * #define SIGNATURE_BREAKPOINT
+ */
+
 #define MAX_JIT_RUN_LEN                 64
 #define COMPILER_WORK_QUEUE_SIZE        100
+#define COMPILER_IC_PATCH_QUEUE_SIZE    64
 
 #define COMPILER_TRACED(X)
 #define COMPILER_TRACEE(X)
@@ -36,14 +44,17 @@ typedef enum JitInstructionSetType {
 
 /* Description of a compiled trace. */
 typedef struct JitTranslationInfo {
-    void     *codeAddress;
+    void *codeAddress;
     JitInstructionSetType instructionSet;
+    bool discardResult;         // Used for debugging divergence and IC patching
+    Thread *requestingThread;   // For debugging purpose
 } JitTranslationInfo;
 
 typedef enum WorkOrderKind {
     kWorkOrderInvalid = 0,      // Should never see by the backend
     kWorkOrderMethod = 1,       // Work is to compile a whole method
     kWorkOrderTrace = 2,        // Work is to compile code fragment(s)
+    kWorkOrderTraceDebug = 3,   // Work is to compile/debug code fragment(s)
 } WorkOrderKind;
 
 typedef struct CompilerWorkOrder {
@@ -51,19 +62,54 @@ typedef struct CompilerWorkOrder {
     WorkOrderKind kind;
     void* info;
     JitTranslationInfo result;
+    jmp_buf *bailPtr;
 } CompilerWorkOrder;
 
+/* Chain cell for predicted method invocation */
+typedef struct PredictedChainingCell {
+    u4 branch;                  /* Branch to chained destination */
+    const ClassObject *clazz;   /* key #1 for prediction */
+    const Method *method;       /* key #2 to lookup native PC from dalvik PC */
+    u4 counter;                 /* counter to patch the chaining cell */
+} PredictedChainingCell;
+
+/* Work order for inline cache patching */
+typedef struct ICPatchWorkOrder {
+    PredictedChainingCell *cellAddr;    /* Address to be patched */
+    PredictedChainingCell cellContent;  /* content of the new cell */
+} ICPatchWorkOrder;
+
+/* States of the dbg interpreter when serving a JIT-related request */
 typedef enum JitState {
-    kJitOff = 0,
-    kJitNormal = 1,            // Profiling in mterp or running native
-    kJitTSelectRequest = 2,    // Transition state - start trace selection
-    kJitTSelect = 3,           // Actively selecting trace in dbg interp
-    kJitTSelectAbort = 4,      // Something threw during selection - abort
+    /* Entering states in the debug interpreter */
+    kJitNot = 0,               // Non-JIT related reasons */
+    kJitTSelectRequest = 1,    // Request a trace (subject to filtering)
+    kJitTSelectRequestHot = 2, // Request a hot trace (bypass the filter)
+    kJitSelfVerification = 3,  // Self Verification Mode
+
+    /* Operational states in the debug interpreter */
+    kJitTSelect = 4,           // Actively selecting a trace
     kJitTSelectEnd = 5,        // Done with the trace - wrap it up
     kJitSingleStep = 6,        // Single step interpretation
-    kJitSingleStepEnd = 7,     // Done with single step, return to mterp
+    kJitSingleStepEnd = 7,     // Done with single step, ready return to mterp
+    kJitDone = 8,              // Ready to leave the debug interpreter
 } JitState;
 
+#if defined(WITH_SELF_VERIFICATION)
+typedef enum SelfVerificationState {
+    kSVSIdle = 0,           // Idle
+    kSVSStart = 1,          // Shadow space set up, running compiled code
+    kSVSPunt = 2,           // Exiting compiled code by punting
+    kSVSSingleStep = 3,     // Exiting compiled code by single stepping
+    kSVSTraceSelectNoChain = 4,// Exiting compiled code by trace select no chain
+    kSVSTraceSelect = 5,    // Exiting compiled code by trace select
+    kSVSNormal = 6,         // Exiting compiled code normally
+    kSVSNoChain = 7,        // Exiting compiled code by no chain
+    kSVSBackwardBranch = 8, // Exiting compiled code with backward branch trace
+    kSVSDebugInterp = 9,    // Normal state restored, running debug interpreter
+} SelfVerificationState;
+#endif
+
 typedef enum JitHint {
    kJitHintNone = 0,
    kJitHintTaken = 1,         // Last inst in run was taken branch
@@ -116,10 +162,36 @@ bool dvmCompilerWorkEnqueue(const u2* pc, WorkOrderKind kind, void* info);
 void *dvmCheckCodeCache(void *method);
 bool dvmCompileMethod(const Method *method, JitTranslationInfo *info);
 bool dvmCompileTrace(JitTraceDescription *trace, int numMaxInsts,
-                     JitTranslationInfo *info);
+                     JitTranslationInfo *info, jmp_buf *bailPtr);
 void dvmCompilerDumpStats(void);
 void dvmCompilerDrainQueue(void);
 void dvmJitUnchainAll(void);
 void dvmCompilerSortAndPrintTraceProfiles(void);
 
+struct CompilationUnit;
+struct BasicBlock;
+struct SSARepresentation;
+struct GrowableList;
+struct JitEntry;
+
+void dvmInitializeSSAConversion(struct CompilationUnit *cUnit);
+int dvmConvertSSARegToDalvik(struct CompilationUnit *cUnit, int ssaReg);
+void dvmCompilerLoopOpt(struct CompilationUnit *cUnit);
+void dvmCompilerNonLoopAnalysis(struct CompilationUnit *cUnit);
+void dvmCompilerFindLiveIn(struct CompilationUnit *cUnit,
+                           struct BasicBlock *bb);
+void dvmCompilerDoSSAConversion(struct CompilationUnit *cUnit,
+                                struct BasicBlock *bb);
+void dvmCompilerDoConstantPropagation(struct CompilationUnit *cUnit,
+                                      struct BasicBlock *bb);
+void dvmCompilerFindInductionVariables(struct CompilationUnit *cUnit,
+                                       struct BasicBlock *bb);
+char *dvmCompilerGetDalvikDisassembly(DecodedInstruction *insn);
+char *dvmCompilerGetSSAString(struct CompilationUnit *cUnit,
+                              struct SSARepresentation *ssaRep);
+void dvmCompilerDataFlowAnalysisDispatcher(struct CompilationUnit *cUnit,
+                void (*func)(struct CompilationUnit *, struct BasicBlock *));
+void dvmCompilerStateRefresh(void);
+JitTraceDescription *dvmCopyTraceDescriptor(const u2 *pc,
+                                            const struct JitEntry *desc);
 #endif /* _DALVIK_VM_COMPILER */
diff --git a/vm/compiler/CompilerIR.h b/vm/compiler/CompilerIR.h
index d61d2ee..2bf243d 100644
--- a/vm/compiler/CompilerIR.h
+++ b/vm/compiler/CompilerIR.h
@@ -14,26 +14,56 @@
  * limitations under the License.
  */
 
-#include "codegen/Optimizer.h"
-
 #ifndef _DALVIK_VM_COMPILER_IR
 #define _DALVIK_VM_COMPILER_IR
 
+#include "codegen/Optimizer.h"
+
+typedef enum RegisterClass {
+    kCoreReg,
+    kFPReg,
+    kAnyReg,
+} RegisterClass;
+
+typedef enum RegLocationType {
+    kLocDalvikFrame = 0,
+    kLocPhysReg,
+    kLocRetval,          // Return region in interpState
+    kLocSpill,
+} RegLocationType;
+
+typedef struct RegLocation {
+    RegLocationType location:2;
+    unsigned wide:1;
+    unsigned fp:1;      // Hint for float/double
+    u1 lowReg:6;        // First physical register
+    u1 highReg:6;       // 2nd physical register (if wide)
+    s2 sRegLow;         // SSA name for low Dalvik word
+} RegLocation;
+
+#define INVALID_SREG (-1)
+#define INVALID_REG (-1)
+
 typedef enum BBType {
     /* For coding convenience reasons chaining cell types should appear first */
-    CHAINING_CELL_NORMAL = 0,
-    CHAINING_CELL_HOT,
-    CHAINING_CELL_INVOKE_SINGLETON,
-    CHAINING_CELL_INVOKE_PREDICTED,
-    CHAINING_CELL_LAST,
-    DALVIK_BYTECODE,
-    PC_RECONSTRUCTION,
-    EXCEPTION_HANDLING,
+    kChainingCellNormal = 0,
+    kChainingCellHot,
+    kChainingCellInvokeSingleton,
+    kChainingCellInvokePredicted,
+    kChainingCellBackwardBranch,
+    kChainingCellGap,
+    /* Don't insert new fields between Gap and Last */
+    kChainingCellLast = kChainingCellGap + 1,
+    kEntryBlock,
+    kDalvikByteCode,
+    kExitBlock,
+    kPCReconstruction,
+    kExceptionHandling,
 } BBType;
 
 typedef struct ChainCellCounts {
     union {
-        u1 count[CHAINING_CELL_LAST];
+        u1 count[kChainingCellLast]; /* include one more space for the gap # */
         u4 dummyForAlignment;
     } u;
 } ChainCellCounts;
@@ -45,14 +75,43 @@ typedef struct LIR {
     struct LIR *target;
 } LIR;
 
+enum ExtendedMIROpcode {
+    kMirOpFirst = 256,
+    kMirOpPhi = kMirOpFirst,
+    kMirOpNullNRangeUpCheck,
+    kMirOpNullNRangeDownCheck,
+    kMirOpLowerBound,
+    kMirOpPunt,
+    kMirOpLast,
+};
+
+struct SSARepresentation;
+
+typedef enum {
+    kMIRIgnoreNullCheck = 0,
+    kMIRNullCheckOnly,
+    kMIRIgnoreRangeCheck,
+    kMIRRangeCheckOnly,
+} MIROptimizationFlagPositons;
+
+#define MIR_IGNORE_NULL_CHECK           (1 << kMIRIgnoreNullCheck)
+#define MIR_NULL_CHECK_ONLY             (1 << kMIRNullCheckOnly)
+#define MIR_IGNORE_RANGE_CHECK          (1 << kMIRIgnoreRangeCheck)
+#define MIR_RANGE_CHECK_ONLY            (1 << kMIRRangeCheckOnly)
+
 typedef struct MIR {
     DecodedInstruction dalvikInsn;
     unsigned int width;
     unsigned int offset;
     struct MIR *prev;
     struct MIR *next;
+    struct SSARepresentation *ssaRep;
+    int OptimizationFlags;
+    int seqNum;
 } MIR;
 
+struct BasicBlockDataFlow;
+
 typedef struct BasicBlock {
     int id;
     int visited;
@@ -65,8 +124,12 @@ typedef struct BasicBlock {
     struct BasicBlock *fallThrough;
     struct BasicBlock *taken;
     struct BasicBlock *next;            // Serial link for book keeping purposes
+    struct BasicBlockDataFlow *dataFlowInfo;
 } BasicBlock;
 
+struct LoopAnalysis;
+struct RegisterPool;
+
 typedef struct CompilationUnit {
     int numInsts;
     int numBlocks;
@@ -87,21 +150,60 @@ typedef struct CompilationUnit {
     bool allSingleStep;
     bool halveInstCount;
     bool executionCount;                // Add code to count trace executions
-    int numChainingCells[CHAINING_CELL_LAST];
-    LIR *firstChainingLIR[CHAINING_CELL_LAST];
-    RegisterScoreboard registerScoreboard;      // Track register dependency
+    bool hasLoop;
+    bool heapMemOp;                     // Mark mem ops for self verification
+    int numChainingCells[kChainingCellGap];
+    LIR *firstChainingLIR[kChainingCellGap];
+    LIR *chainingCellBottom;
+    struct RegisterPool *regPool;
     int optRound;                       // round number to tell an LIR's age
+    jmp_buf *bailPtr;
     JitInstructionSetType instructionSet;
+    /* Number of total regs used in the whole cUnit after SSA transformation */
+    int numSSARegs;
+    /* Map SSA reg i to the Dalvik[15..0]/Sub[31..16] pair. */
+    GrowableList *ssaToDalvikMap;
+
+    /* The following are new data structures to support SSA representations */
+    /* Map original Dalvik reg i to the SSA[15..0]/Sub[31..16] pair */
+    int *dalvikToSSAMap;                // length == method->registersSize
+    BitVector *isConstantV;             // length == numSSAReg
+    int *constantValues;                // length == numSSAReg
+
+    /* Data structure for loop analysis and optimizations */
+    struct LoopAnalysis *loopAnalysis;
+
+    /* Map SSA names to location */
+    RegLocation *regLocation;
+    int sequenceNumber;
+
+    /*
+     * Set to the Dalvik PC of the switch instruction if it has more than
+     * MAX_CHAINED_SWITCH_CASES cases.
+     */
+    const u2 *switchOverflowPad;
 } CompilationUnit;
 
+#if defined(WITH_SELF_VERIFICATION)
+#define HEAP_ACCESS_SHADOW(_state) cUnit->heapMemOp = _state
+#else
+#define HEAP_ACCESS_SHADOW(_state)
+#endif
+
 BasicBlock *dvmCompilerNewBB(BBType blockType);
 
 void dvmCompilerAppendMIR(BasicBlock *bb, MIR *mir);
 
+void dvmCompilerPrependMIR(BasicBlock *bb, MIR *mir);
+
 void dvmCompilerAppendLIR(CompilationUnit *cUnit, LIR *lir);
 
 void dvmCompilerInsertLIRBefore(LIR *currentLIR, LIR *newLIR);
 
+void dvmCompilerInsertLIRAfter(LIR *currentLIR, LIR *newLIR);
+
+void dvmCompilerAbort(CompilationUnit *cUnit);
+
 /* Debug Utilities */
 void dvmCompilerDumpCompilationUnit(CompilationUnit *cUnit);
 
diff --git a/vm/compiler/CompilerInternals.h b/vm/compiler/CompilerInternals.h
index 410213a..9a30b34 100644
--- a/vm/compiler/CompilerInternals.h
+++ b/vm/compiler/CompilerInternals.h
@@ -19,7 +19,6 @@
 
 #include "Dalvik.h"
 #include "CompilerUtility.h"
-#include "CompilerIR.h"
 #include "codegen/CompilerCodegen.h"
 #include "interp/Jit.h"
 
diff --git a/vm/compiler/CompilerUtility.h b/vm/compiler/CompilerUtility.h
index 7b4de11..4ab650d 100644
--- a/vm/compiler/CompilerUtility.h
+++ b/vm/compiler/CompilerUtility.h
@@ -17,7 +17,10 @@
 #ifndef _DALVIK_VM_COMPILER_UTILITY
 #define _DALVIK_VM_COMPILER_UTILITY
 
-#define ARENA_DEFAULT_SIZE 4096
+#include "Dalvik.h"
+
+/* Each arena page has some overhead, so take a few bytes off 8k */
+#define ARENA_DEFAULT_SIZE 8100
 
 /* Allocate the initial memory block for arena-based allocation */
 bool dvmCompilerHeapInit(void);
@@ -38,7 +41,18 @@ typedef struct GrowableList {
     void **elemList;
 } GrowableList;
 
+#define GET_ELEM_N(LIST, TYPE, N) (((TYPE*) LIST->elemList)[N])
+#define MIN(x,y) (((x) < (y)) ? (x) : (y))
+#define MAX(x,y) (((x) > (y)) ? (x) : (y))
+
+struct LIR;
+
 void dvmInitGrowableList(GrowableList *gList, size_t initLength);
 void dvmInsertGrowableList(GrowableList *gList, void *elem);
+BitVector* dvmCompilerAllocBitVector(int startBits, bool expandable);
+bool dvmCompilerSetBit(BitVector* pBits, int num);
+void dvmDebugBitVector(char *msg, const BitVector *bv, int length);
+void dvmDumpLIRInsn(struct LIR *lir, unsigned char *baseAddr);
+void dvmDumpResourceMask(struct LIR *lir, u8 mask, const char *prefix);
 
 #endif /* _DALVIK_COMPILER_UTILITY */
diff --git a/vm/compiler/Frontend.c b/vm/compiler/Frontend.c
index 66060a0..d97001f 100644
--- a/vm/compiler/Frontend.c
+++ b/vm/compiler/Frontend.c
@@ -16,7 +16,6 @@
 
 #include "Dalvik.h"
 #include "libdex/OpCode.h"
-#include "dexdump/OpCodeNames.h"
 #include "interp/Jit.h"
 #include "CompilerInternals.h"
 
@@ -42,11 +41,14 @@ static inline int parseInsn(const u2 *codePtr, DecodedInstruction *decInsn,
 
     dexDecodeInstruction(gDvm.instrFormat, codePtr, decInsn);
     if (printMe) {
-        LOGD("%p: %#06x %s\n", codePtr, opcode, getOpcodeName(opcode));
+        char *decodedString = dvmCompilerGetDalvikDisassembly(decInsn);
+        LOGD("%p: %#06x %s\n", codePtr, opcode, decodedString);
     }
     return insnWidth;
 }
 
+#define UNKNOWN_TARGET 0xffffffff
+
 /*
  * Identify block-ending instructions and collect supplemental information
  * regarding the following instructions.
@@ -63,6 +65,8 @@ static inline bool findBlockBoundary(const Method *caller, MIR *insn,
         case OP_RETURN_WIDE:
         case OP_RETURN_OBJECT:
         case OP_THROW:
+          *target = UNKNOWN_TARGET;
+          break;
         case OP_INVOKE_VIRTUAL:
         case OP_INVOKE_VIRTUAL_RANGE:
         case OP_INVOKE_INTERFACE:
@@ -146,11 +150,24 @@ static inline bool findBlockBoundary(const Method *caller, MIR *insn,
 
         default:
             return false;
-    } return true;
+    }
+    return true;
+}
+
+static inline bool isGoto(MIR *insn)
+{
+    switch (insn->dalvikInsn.opCode) {
+        case OP_GOTO:
+        case OP_GOTO_16:
+        case OP_GOTO_32:
+            return true;
+        default:
+            return false;
+    }
 }
 
 /*
- * Identify conditional branch instructions
+ * Identify unconditional branch instructions
  */
 static inline bool isUnconditionalBranch(MIR *insn)
 {
@@ -159,12 +176,9 @@ static inline bool isUnconditionalBranch(MIR *insn)
         case OP_RETURN:
         case OP_RETURN_WIDE:
         case OP_RETURN_OBJECT:
-        case OP_GOTO:
-        case OP_GOTO_16:
-        case OP_GOTO_32:
             return true;
         default:
-            return false;
+            return isGoto(insn);
     }
 }
 
@@ -177,6 +191,7 @@ static int compareMethod(const CompilerMethodStats *m1,
     return (int) m1->method - (int) m2->method;
 }
 
+#if defined(WITH_JIT_TUNING)
 /*
  * Analyze each method whose traces are ever compiled. Collect a variety of
  * statistics like the ratio of exercised vs overall code and code bloat
@@ -233,6 +248,37 @@ static CompilerMethodStats *analyzeMethodBody(const Method *method)
     realMethodEntry->dalvikSize = insnSize * 2;
     return realMethodEntry;
 }
+#endif
+
+/*
+ * Crawl the stack of the thread that requesed compilation to see if any of the
+ * ancestors are on the blacklist.
+ */
+bool filterMethodByCallGraph(Thread *thread, const char *curMethodName)
+{
+    /* Crawl the Dalvik stack frames and compare the method name*/
+    StackSaveArea *ssaPtr = ((StackSaveArea *) thread->curFrame) - 1;
+    while (ssaPtr != ((StackSaveArea *) NULL) - 1) {
+        const Method *method = ssaPtr->method;
+        if (method) {
+            int hashValue = dvmComputeUtf8Hash(method->name);
+            bool found =
+                dvmHashTableLookup(gDvmJit.methodTable, hashValue,
+                               (char *) method->name,
+                               (HashCompareFunc) strcmp, false) !=
+                NULL;
+            if (found) {
+                LOGD("Method %s (--> %s) found on the JIT %s list",
+                     method->name, curMethodName,
+                     gDvmJit.includeSelectedMethod ? "white" : "black");
+                return true;
+            }
+
+        }
+        ssaPtr = ((StackSaveArea *) ssaPtr->prevFrame) - 1;
+    };
+    return false;
+}
 
 /*
  * Main entry point to start trace compilation. Basic blocks are constructed
@@ -240,7 +286,7 @@ static CompilerMethodStats *analyzeMethodBody(const Method *method)
  * bytecode into machine code.
  */
 bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
-                     JitTranslationInfo *info)
+                     JitTranslationInfo *info, jmp_buf *bailPtr)
 {
     const DexCode *dexCode = dvmGetMethodCode(desc->method);
     const JitTraceRun* currRun = &desc->trace[0];
@@ -253,16 +299,25 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
     int numBlocks = 0;
     static int compilationId;
     CompilationUnit cUnit;
+#if defined(WITH_JIT_TUNING)
     CompilerMethodStats *methodStats;
+#endif
+
+    /* If we've already compiled this trace, just return success */
+    if (dvmJitGetCodeAddr(startCodePtr) && !info->discardResult) {
+        return true;
+    }
 
     compilationId++;
     memset(&cUnit, 0, sizeof(CompilationUnit));
 
+#if defined(WITH_JIT_TUNING)
     /* Locate the entry to store compilation statistics for this method */
     methodStats = analyzeMethodBody(desc->method);
+#endif
 
-    cUnit.registerScoreboard.nullCheckedRegs =
-        dvmAllocBitVector(desc->method->registersSize, false);
+    /* Set the recover buffer pointer */
+    cUnit.bailPtr = bailPtr;
 
     /* Initialize the printMe flag */
     cUnit.printMe = gDvmJit.printMe;
@@ -308,6 +363,16 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
                                    (char *) desc->method->name,
                                    (HashCompareFunc) strcmp, false) !=
                     NULL;
+
+                /*
+                 * Debug by call-graph is enabled. Check if the debug list
+                 * covers any methods on the VM stack.
+                 */
+                if (methodFound == false && gDvmJit.checkCallGraph == true) {
+                    methodFound =
+                        filterMethodByCallGraph(info->requestingThread,
+                                                desc->method->name);
+                }
             }
         }
 
@@ -333,11 +398,20 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
         }
     }
 
-    /* Allocate the first basic block */
-    lastBB = startBB = curBB = dvmCompilerNewBB(DALVIK_BYTECODE);
+    /* Allocate the entry block */
+    lastBB = startBB = curBB = dvmCompilerNewBB(kEntryBlock);
     curBB->startOffset = curOffset;
     curBB->id = numBlocks++;
 
+    curBB = dvmCompilerNewBB(kDalvikByteCode);
+    curBB->startOffset = curOffset;
+    curBB->id = numBlocks++;
+
+    /* Make the first real dalvik block the fallthrough of the entry block */
+    startBB->fallThrough = curBB;
+    lastBB->next = curBB;
+    lastBB = curBB;
+
     if (cUnit.printMe) {
         LOGD("--------\nCompiler: Building trace for %s, offset 0x%x\n",
              desc->method->name, curOffset);
@@ -350,7 +424,7 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
     while (1) {
         MIR *insn;
         int width;
-        insn = dvmCompilerNew(sizeof(MIR),false);
+        insn = dvmCompilerNew(sizeof(MIR), true);
         insn->offset = curOffset;
         width = parseInsn(codePtr, &insn->dalvikInsn, cUnit.printMe);
 
@@ -368,7 +442,7 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
             if (currRun->frag.runEnd) {
                 break;
             } else {
-                curBB = dvmCompilerNewBB(DALVIK_BYTECODE);
+                curBB = dvmCompilerNewBB(kDalvikByteCode);
                 lastBB->next = curBB;
                 lastBB = curBB;
                 curBB->id = numBlocks++;
@@ -384,8 +458,10 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
         }
     }
 
+#if defined(WITH_JIT_TUNING)
     /* Convert # of half-word to bytes */
     methodStats->compiledDalvikSize += traceSize * 2;
+#endif
 
     /*
      * Now scan basic blocks containing real code to connect the
@@ -394,9 +470,9 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
      */
     for (curBB = startBB; curBB; curBB = curBB->next) {
         MIR *lastInsn = curBB->lastMIRInsn;
-        /* Hit a pseudo block - exit the search now */
+        /* Skip empty blocks */
         if (lastInsn == NULL) {
-            break;
+            continue;
         }
         curOffset = lastInsn->offset;
         unsigned int targetOffset = curOffset;
@@ -436,61 +512,172 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
                        kInstrInvoke)) == 0) ||
             (lastInsn->dalvikInsn.opCode == OP_INVOKE_DIRECT_EMPTY);
 
+        if (curBB->taken == NULL &&
+            curBB->fallThrough == NULL &&
+            flags == (kInstrCanBranch | kInstrCanContinue) &&
+            fallThroughOffset == startBB->startOffset) {
+            BasicBlock *loopBranch = curBB;
+            BasicBlock *exitBB;
+            BasicBlock *exitChainingCell;
+
+            if (cUnit.printMe) {
+                LOGD("Natural loop detected!");
+            }
+            exitBB = dvmCompilerNewBB(kExitBlock);
+            lastBB->next = exitBB;
+            lastBB = exitBB;
+
+            exitBB->startOffset = targetOffset;
+            exitBB->id = numBlocks++;
+            exitBB->needFallThroughBranch = true;
+
+            loopBranch->taken = exitBB;
+#if defined(WITH_SELF_VERIFICATION)
+            BasicBlock *backwardCell =
+                dvmCompilerNewBB(kChainingCellBackwardBranch);
+            lastBB->next = backwardCell;
+            lastBB = backwardCell;
+
+            backwardCell->startOffset = startBB->startOffset;
+            backwardCell->id = numBlocks++;
+            loopBranch->fallThrough = backwardCell;
+#elif defined(WITH_JIT_TUNING)
+            if (gDvmJit.profile) {
+                BasicBlock *backwardCell =
+                    dvmCompilerNewBB(kChainingCellBackwardBranch);
+                lastBB->next = backwardCell;
+                lastBB = backwardCell;
+
+                backwardCell->startOffset = startBB->startOffset;
+                backwardCell->id = numBlocks++;
+                loopBranch->fallThrough = backwardCell;
+            } else {
+                loopBranch->fallThrough = startBB->next;
+            }
+#else
+            loopBranch->fallThrough = startBB->next;
+#endif
+
+            /* Create the chaining cell as the fallthrough of the exit block */
+            exitChainingCell = dvmCompilerNewBB(kChainingCellNormal);
+            lastBB->next = exitChainingCell;
+            lastBB = exitChainingCell;
+
+            exitChainingCell->startOffset = targetOffset;
+            exitChainingCell->id = numBlocks++;
+
+            exitBB->fallThrough = exitChainingCell;
 
+            cUnit.hasLoop = true;
+        }
+
+        if (lastInsn->dalvikInsn.opCode == OP_PACKED_SWITCH ||
+            lastInsn->dalvikInsn.opCode == OP_SPARSE_SWITCH) {
+            int i;
+            const u2 *switchData = desc->method->insns + lastInsn->offset +
+                             lastInsn->dalvikInsn.vB;
+            int size = switchData[1];
+            int maxChains = MIN(size, MAX_CHAINED_SWITCH_CASES);
+
+            /*
+             * Generate the landing pad for cases whose ranks are higher than
+             * MAX_CHAINED_SWITCH_CASES. The code will re-enter the interpreter
+             * through the NoChain point.
+             */
+            if (maxChains != size) {
+                cUnit.switchOverflowPad =
+                    desc->method->insns + lastInsn->offset;
+            }
+
+            s4 *targets = (s4 *) (switchData + 2 +
+                    (lastInsn->dalvikInsn.opCode == OP_PACKED_SWITCH ?
+                     2 : size * 2));
+
+            /* One chaining cell for the first MAX_CHAINED_SWITCH_CASES cases */
+            for (i = 0; i < maxChains; i++) {
+                BasicBlock *caseChain = dvmCompilerNewBB(kChainingCellNormal);
+                lastBB->next = caseChain;
+                lastBB = caseChain;
+
+                caseChain->startOffset = lastInsn->offset + targets[i];
+                caseChain->id = numBlocks++;
+            }
+
+            /* One more chaining cell for the default case */
+            BasicBlock *caseChain = dvmCompilerNewBB(kChainingCellNormal);
+            lastBB->next = caseChain;
+            lastBB = caseChain;
+
+            caseChain->startOffset = lastInsn->offset + lastInsn->width;
+            caseChain->id = numBlocks++;
+        /* Fallthrough block not included in the trace */
+        } else if (!isUnconditionalBranch(lastInsn) &&
+                   curBB->fallThrough == NULL) {
+            /*
+             * If the chaining cell is after an invoke or
+             * instruction that cannot change the control flow, request a hot
+             * chaining cell.
+             */
+            if (isInvoke || curBB->needFallThroughBranch) {
+                lastBB->next = dvmCompilerNewBB(kChainingCellHot);
+            } else {
+                lastBB->next = dvmCompilerNewBB(kChainingCellNormal);
+            }
+            lastBB = lastBB->next;
+            lastBB->id = numBlocks++;
+            lastBB->startOffset = fallThroughOffset;
+            curBB->fallThrough = lastBB;
+        }
         /* Target block not included in the trace */
         if (curBB->taken == NULL &&
-            (isInvoke || (targetOffset != curOffset))) {
+            (isGoto(lastInsn) || isInvoke ||
+            (targetOffset != UNKNOWN_TARGET && targetOffset != curOffset))) {
             BasicBlock *newBB;
             if (isInvoke) {
                 /* Monomorphic callee */
                 if (callee) {
-                    newBB = dvmCompilerNewBB(CHAINING_CELL_INVOKE_SINGLETON);
+                    newBB = dvmCompilerNewBB(kChainingCellInvokeSingleton);
                     newBB->startOffset = 0;
                     newBB->containingMethod = callee;
                 /* Will resolve at runtime */
                 } else {
-                    newBB = dvmCompilerNewBB(CHAINING_CELL_INVOKE_PREDICTED);
+                    newBB = dvmCompilerNewBB(kChainingCellInvokePredicted);
                     newBB->startOffset = 0;
                 }
             /* For unconditional branches, request a hot chaining cell */
             } else {
+#if !defined(WITH_SELF_VERIFICATION)
                 newBB = dvmCompilerNewBB(flags & kInstrUnconditional ?
-                                                  CHAINING_CELL_HOT :
-                                                  CHAINING_CELL_NORMAL);
+                                                  kChainingCellHot :
+                                                  kChainingCellNormal);
+                newBB->startOffset = targetOffset;
+#else
+                /* Handle branches that branch back into the block */
+                if (targetOffset >= curBB->firstMIRInsn->offset &&
+                    targetOffset <= curBB->lastMIRInsn->offset) {
+                    newBB = dvmCompilerNewBB(kChainingCellBackwardBranch);
+                } else {
+                    newBB = dvmCompilerNewBB(flags & kInstrUnconditional ?
+                                                      kChainingCellHot :
+                                                      kChainingCellNormal);
+                }
                 newBB->startOffset = targetOffset;
+#endif
             }
             newBB->id = numBlocks++;
             curBB->taken = newBB;
             lastBB->next = newBB;
             lastBB = newBB;
         }
-
-        /* Fallthrough block not included in the trace */
-        if (!isUnconditionalBranch(lastInsn) && curBB->fallThrough == NULL) {
-            /*
-             * If the chaining cell is after an invoke or
-             * instruction that cannot change the control flow, request a hot
-             * chaining cell.
-             */
-            if (isInvoke || curBB->needFallThroughBranch) {
-                lastBB->next = dvmCompilerNewBB(CHAINING_CELL_HOT);
-            } else {
-                lastBB->next = dvmCompilerNewBB(CHAINING_CELL_NORMAL);
-            }
-            lastBB = lastBB->next;
-            lastBB->id = numBlocks++;
-            lastBB->startOffset = fallThroughOffset;
-            curBB->fallThrough = lastBB;
-        }
     }
 
     /* Now create a special block to host PC reconstruction code */
-    lastBB->next = dvmCompilerNewBB(PC_RECONSTRUCTION);
+    lastBB->next = dvmCompilerNewBB(kPCReconstruction);
     lastBB = lastBB->next;
     lastBB->id = numBlocks++;
 
     /* And one final block that publishes the PC and raise the exception */
-    lastBB->next = dvmCompilerNewBB(EXCEPTION_HANDLING);
+    lastBB->next = dvmCompilerNewBB(kExceptionHandling);
     lastBB = lastBB->next;
     lastBB->id = numBlocks++;
 
@@ -524,12 +711,28 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
     /* Make sure all blocks are added to the cUnit */
     assert(curBB == NULL);
 
+    /* Preparation for SSA conversion */
+    dvmInitializeSSAConversion(&cUnit);
+
+
+    if (cUnit.hasLoop) {
+        dvmCompilerLoopOpt(&cUnit);
+    }
+    else {
+        dvmCompilerNonLoopAnalysis(&cUnit);
+    }
+
+    dvmCompilerInitializeRegAlloc(&cUnit);  // Needs to happen after SSA naming
+
     if (cUnit.printMe) {
         dvmCompilerDumpCompilationUnit(&cUnit);
     }
 
     /* Set the instruction set to use (NOTE: later components may change it) */
-    cUnit.instructionSet = dvmCompilerInstructionSet(&cUnit);
+    cUnit.instructionSet = dvmCompilerInstructionSet();
+
+    /* Allocate Registers */
+    dvmCompilerRegAlloc(&cUnit);
 
     /* Convert MIR to LIR, etc. */
     dvmCompilerMIR2LIR(&cUnit);
@@ -551,17 +754,16 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
     /* Reset the compiler resource pool */
     dvmCompilerArenaReset();
 
-    /* Free the bit vector tracking null-checked registers */
-    dvmFreeBitVector(cUnit.registerScoreboard.nullCheckedRegs);
-
-    if (!cUnit.halveInstCount) {
     /* Success */
+    if (!cUnit.halveInstCount) {
+#if defined(WITH_JIT_TUNING)
         methodStats->nativeSize += cUnit.totalSize;
+#endif
         return info->codeAddress != NULL;
 
     /* Halve the instruction count and retry again */
     } else {
-        return dvmCompileTrace(desc, cUnit.numInsts / 2, info);
+        return dvmCompileTrace(desc, cUnit.numInsts / 2, info, bailPtr);
     }
 }
 
@@ -580,19 +782,20 @@ bool dvmCompileMethod(const Method *method, JitTranslationInfo *info)
     int blockID = 0;
     unsigned int curOffset = 0;
 
-    BasicBlock *firstBlock = dvmCompilerNewBB(DALVIK_BYTECODE);
+    BasicBlock *firstBlock = dvmCompilerNewBB(kDalvikByteCode);
     firstBlock->id = blockID++;
 
     /* Allocate the bit-vector to track the beginning of basic blocks */
-    BitVector *bbStartAddr = dvmAllocBitVector(dexCode->insnsSize+1, false);
-    dvmSetBit(bbStartAddr, 0);
+    BitVector *bbStartAddr = dvmCompilerAllocBitVector(dexCode->insnsSize+1,
+                                                       false);
+    dvmCompilerSetBit(bbStartAddr, 0);
 
     /*
      * Sequentially go through every instruction first and put them in a single
      * basic block. Identify block boundaries at the mean time.
      */
     while (codePtr < codeEnd) {
-        MIR *insn = dvmCompilerNew(sizeof(MIR), false);
+        MIR *insn = dvmCompilerNew(sizeof(MIR), true);
         insn->offset = curOffset;
         int width = parseInsn(codePtr, &insn->dalvikInsn, false);
         bool isInvoke = false;
@@ -616,9 +819,9 @@ bool dvmCompileMethod(const Method *method, JitTranslationInfo *info)
          */
         if (findBlockBoundary(method, insn, curOffset, &target, &isInvoke,
                               &callee)) {
-            dvmSetBit(bbStartAddr, curOffset + width);
+            dvmCompilerSetBit(bbStartAddr, curOffset + width);
             if (target != curOffset) {
-                dvmSetBit(bbStartAddr, target);
+                dvmCompilerSetBit(bbStartAddr, target);
             }
         }
 
@@ -669,7 +872,7 @@ bool dvmCompileMethod(const Method *method, JitTranslationInfo *info)
 
                 /* Block not split yet - do it now */
                 if (j == cUnit.numBlocks) {
-                    BasicBlock *newBB = dvmCompilerNewBB(DALVIK_BYTECODE);
+                    BasicBlock *newBB = dvmCompilerNewBB(kDalvikByteCode);
                     newBB->id = blockID++;
                     newBB->firstMIRInsn = insn;
                     newBB->startOffset = insn->offset;
@@ -696,11 +899,9 @@ bool dvmCompileMethod(const Method *method, JitTranslationInfo *info)
 
     if (numBlocks != cUnit.numBlocks) {
         LOGE("Expect %d vs %d basic blocks\n", numBlocks, cUnit.numBlocks);
-        dvmAbort();
+        dvmCompilerAbort(&cUnit);
     }
 
-    dvmFreeBitVector(bbStartAddr);
-
     /* Connect the basic blocks through the taken links */
     for (i = 0; i < numBlocks; i++) {
         BasicBlock *curBB = blockList[i];
@@ -732,13 +933,13 @@ bool dvmCompileMethod(const Method *method, JitTranslationInfo *info)
             if (j == numBlocks && !isInvoke) {
                 LOGE("Target not found for insn %x: expect target %x\n",
                      curBB->lastMIRInsn->offset, target);
-                dvmAbort();
+                dvmCompilerAbort(&cUnit);
             }
         }
     }
 
     /* Set the instruction set to use (NOTE: later components may change it) */
-    cUnit.instructionSet = dvmCompilerInstructionSet(&cUnit);
+    cUnit.instructionSet = dvmCompilerInstructionSet();
 
     dvmCompilerMIR2LIR(&cUnit);
 
diff --git a/vm/compiler/IntermediateRep.c b/vm/compiler/IntermediateRep.c
index 91b7af7..b3e6490 100644
--- a/vm/compiler/IntermediateRep.c
+++ b/vm/compiler/IntermediateRep.c
@@ -29,7 +29,7 @@ BasicBlock *dvmCompilerNewBB(BBType blockType)
 void dvmCompilerAppendMIR(BasicBlock *bb, MIR *mir)
 {
     if (bb->firstMIRInsn == NULL) {
-        assert(bb->firstMIRInsn == NULL);
+        assert(bb->lastMIRInsn == NULL);
         bb->lastMIRInsn = bb->firstMIRInsn = mir;
         mir->prev = mir->next = NULL;
     } else {
@@ -40,6 +40,21 @@ void dvmCompilerAppendMIR(BasicBlock *bb, MIR *mir)
     }
 }
 
+/* Insert an MIR instruction to the head of a basic block */
+void dvmCompilerPrependMIR(BasicBlock *bb, MIR *mir)
+{
+    if (bb->firstMIRInsn == NULL) {
+        assert(bb->lastMIRInsn == NULL);
+        bb->lastMIRInsn = bb->firstMIRInsn = mir;
+        mir->prev = mir->next = NULL;
+    } else {
+        bb->firstMIRInsn->prev = mir;
+        mir->next = bb->firstMIRInsn;
+        mir->prev = NULL;
+        bb->firstMIRInsn = mir;
+    }
+}
+
 /*
  * Append an LIR instruction to the LIR list maintained by a compilation
  * unit
@@ -66,8 +81,7 @@ void dvmCompilerAppendLIR(CompilationUnit *cUnit, LIR *lir)
  */
 void dvmCompilerInsertLIRBefore(LIR *currentLIR, LIR *newLIR)
 {
-    if (currentLIR->prev == NULL)
-        dvmAbort();
+    assert(currentLIR->prev != NULL);
     LIR *prevLIR = currentLIR->prev;
 
     prevLIR->next = newLIR;
@@ -75,3 +89,17 @@ void dvmCompilerInsertLIRBefore(LIR *currentLIR, LIR *newLIR)
     newLIR->next = currentLIR;
     currentLIR->prev = newLIR;
 }
+
+/*
+ * Insert an LIR instruction after the current instruction, which cannot be the
+ * first instruction.
+ *
+ * currentLIR -> newLIR -> oldNext
+ */
+void dvmCompilerInsertLIRAfter(LIR *currentLIR, LIR *newLIR)
+{
+    newLIR->prev = currentLIR;
+    newLIR->next = currentLIR->next;
+    currentLIR->next = newLIR;
+    newLIR->next->prev = newLIR;
+}
diff --git a/vm/compiler/Utility.c b/vm/compiler/Utility.c
index 715f750..cbafb79 100644
--- a/vm/compiler/Utility.c
+++ b/vm/compiler/Utility.c
@@ -66,11 +66,7 @@ retry:
          * could go above the limit we need to enhance the allocation
          * mechanism.
          */
-        if (size > ARENA_DEFAULT_SIZE) {
-            LOGE("Requesting %d bytes which exceed the maximal size allowed\n",
-                 size);
-            return NULL;
-        }
+        assert(size <= ARENA_DEFAULT_SIZE);
         /* Time to allocate a new arena */
         ArenaMemBlock *newArena = (ArenaMemBlock *)
             malloc(sizeof(ArenaMemBlock) + ARENA_DEFAULT_SIZE);
@@ -79,7 +75,8 @@ retry:
         currentArena->next = newArena;
         currentArena = newArena;
         numArenaBlocks++;
-        LOGD("Total arena pages for JIT: %d", numArenaBlocks);
+        if (numArenaBlocks > 10)
+            LOGI("Total arena pages for JIT: %d", numArenaBlocks);
         goto retry;
     }
     return NULL;
@@ -203,10 +200,91 @@ void dvmCompilerDumpStats(void)
          gDvmJit.compilerMaxQueued);
     dvmJitStats();
     dvmCompilerArchDump();
-    dvmHashForeach(gDvmJit.methodStatsTable, dumpMethodStats,
-                   &totalMethodStats);
+    if (gDvmJit.methodStatsTable) {
+        dvmHashForeach(gDvmJit.methodStatsTable, dumpMethodStats,
+                       &totalMethodStats);
+    }
     LOGD("Code size stats: %d/%d (compiled/total Dalvik), %d (native)",
          totalMethodStats.compiledDalvikSize,
          totalMethodStats.dalvikSize,
          totalMethodStats.nativeSize);
 }
+
+/*
+ * Allocate a bit vector with enough space to hold at least the specified
+ * number of bits.
+ *
+ * NOTE: this is the sister implementation of dvmAllocBitVector. In this version
+ * memory is allocated from the compiler arena.
+ */
+BitVector* dvmCompilerAllocBitVector(int startBits, bool expandable)
+{
+    BitVector* bv;
+    int count;
+
+    assert(sizeof(bv->storage[0]) == 4);        /* assuming 32-bit units */
+    assert(startBits >= 0);
+
+    bv = (BitVector*) dvmCompilerNew(sizeof(BitVector), false);
+
+    count = (startBits + 31) >> 5;
+
+    bv->storageSize = count;
+    bv->expandable = expandable;
+    bv->storage = (u4*) dvmCompilerNew(count * sizeof(u4), true);
+    return bv;
+}
+
+/*
+ * Mark the specified bit as "set".
+ *
+ * Returns "false" if the bit is outside the range of the vector and we're
+ * not allowed to expand.
+ *
+ * NOTE: this is the sister implementation of dvmSetBit. In this version
+ * memory is allocated from the compiler arena.
+ */
+bool dvmCompilerSetBit(BitVector *pBits, int num)
+{
+    assert(num >= 0);
+    if (num >= pBits->storageSize * (int)sizeof(u4) * 8) {
+        if (!pBits->expandable)
+            return false;
+
+        int newSize = (num + 31) >> 5;
+        assert(newSize > pBits->storageSize);
+        u4 *newStorage = dvmCompilerNew(newSize * sizeof(u4), false);
+        memcpy(newStorage, pBits->storage, pBits->storageSize * sizeof(u4));
+        memset(&newStorage[pBits->storageSize], 0,
+               (newSize - pBits->storageSize) * sizeof(u4));
+        pBits->storage = newStorage;
+        pBits->storageSize = newSize;
+    }
+
+    pBits->storage[num >> 5] |= 1 << (num & 0x1f);
+    return true;
+}
+
+void dvmDebugBitVector(char *msg, const BitVector *bv, int length)
+{
+    int i;
+
+    LOGE("%s", msg);
+    for (i = 0; i < length; i++) {
+        if (dvmIsBitSet(bv, i)) {
+            LOGE("Bit %d is set", i);
+        }
+    }
+}
+
+void dvmCompilerAbort(CompilationUnit *cUnit)
+{
+    LOGE("Jit: aborting trace compilation, reverting to interpreter");
+    /* Force a traceback in debug builds */
+    assert(0);
+    /*
+     * Abort translation and force to interpret-only for this trace
+     * Matching setjmp in compiler thread work loop in Compiler.c.
+     */
+    longjmp(*cUnit->bailPtr, 1);
+}
diff --git a/vm/compiler/codegen/CompilerCodegen.h b/vm/compiler/codegen/CompilerCodegen.h
index c9e6bd6..06fd410 100644
--- a/vm/compiler/codegen/CompilerCodegen.h
+++ b/vm/compiler/codegen/CompilerCodegen.h
@@ -14,11 +14,14 @@
  * limitations under the License.
  */
 
-#include "../CompilerIR.h"
-
 #ifndef _DALVIK_VM_COMPILERCODEGEN_H_
 #define _DALVIK_VM_COMPILERCODEGEN_H_
 
+#include "compiler/CompilerIR.h"
+
+/* Maximal number of switch cases to have inline chains */
+#define MAX_CHAINED_SWITCH_CASES 64
+
 /* Work unit is architecture dependent */
 bool dvmCompilerDoWork(CompilerWorkOrder *work);
 
@@ -28,6 +31,9 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit);
 /* Assemble LIR into machine code */
 void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info);
 
+/* Patch inline cache content for polymorphic callsites */
+bool dvmJitPatchInlineCache(void *cellPtr, void *contentPtr);
+
 /* Implemented in the codegen/<target>/ArchUtility.c */
 void dvmCompilerCodegenDump(CompilationUnit *cUnit);
 
@@ -35,8 +41,24 @@ void dvmCompilerCodegenDump(CompilationUnit *cUnit);
 void* dvmJitChain(void *tgtAddr, u4* branchAddr);
 u4* dvmJitUnchain(void *codeAddr);
 void dvmJitUnchainAll(void);
+void dvmCompilerPatchInlineCache(void);
+
+/* Implemented in codegen/<target>/Ralloc.c */
+void dvmCompilerRegAlloc(CompilationUnit *cUnit);
+
+/* Implemented in codegen/<target>/Thumb<version>Util.c */
+void dvmCompilerInitializeRegAlloc(CompilationUnit *cUnit);
+
+/* Implemented in codegen/<target>/<target_variant>/ArchVariant.c */
+JitInstructionSetType dvmCompilerInstructionSet(void);
+
+/*
+ * Implemented in codegen/<target>/<target_variant>/ArchVariant.c
+ * Architecture-specific initializations and checks
+ */
+bool dvmCompilerArchVariantInit(void);
 
 /* Implemented in codegen/<target>/<target_variant>/ArchVariant.c */
-JitInstructionSetType dvmCompilerInstructionSet(CompilationUnit *cUnit);
+int dvmCompilerTargetOptHint(int key);
 
 #endif /* _DALVIK_VM_COMPILERCODEGEN_H_ */
diff --git a/vm/compiler/codegen/Optimizer.h b/vm/compiler/codegen/Optimizer.h
index 1a891b1..713aa41 100644
--- a/vm/compiler/codegen/Optimizer.h
+++ b/vm/compiler/codegen/Optimizer.h
@@ -14,29 +14,25 @@
  * limitations under the License.
  */
 
-#include "Dalvik.h"
-#include "compiler/CompilerInternals.h"
-
 #ifndef _DALVIK_VM_COMPILER_OPTIMIZATION_H
 #define _DALVIK_VM_COMPILER_OPTIMIZATION_H
 
-/* Forward declarations */
-struct CompilationUnit;
-struct LIR;
+#include "Dalvik.h"
 
 /*
- * Data structure tracking the mapping between a Dalvik register (pair) and a
- * native register (pair). The idea is to reuse the previously loaded value
- * if possible, otherwise to keep the value in a native register as long as
- * possible.
+ * If the corresponding bit is set in gDvmJit.disableOpt, the selected
+ * optimization will be suppressed.
  */
-typedef struct RegisterScoreboard {
-    BitVector *nullCheckedRegs; // Track which registers have been null-checked
-    int liveDalvikReg;          // Track which Dalvik register is live
-    int nativeReg;              // And the mapped native register
-    int nativeRegHi;            // And the mapped native register
-    bool isWide;                // Whether a pair of registers are alive
-} RegisterScoreboard;
+typedef enum optControlVector {
+    kLoadStoreElimination = 0,
+    kLoadHoisting,
+    kTrackLiveTemps,
+    kSuppressLoads,
+} optControlVector;
+
+/* Forward declarations */
+struct CompilationUnit;
+struct LIR;
 
 void dvmCompilerApplyLocalOptimizations(struct CompilationUnit *cUnit,
                                         struct LIR *head,
diff --git a/vm/compiler/codegen/arm/ArchUtility.c b/vm/compiler/codegen/arm/ArchUtility.c
index 60c5cdb..b0478f4 100644
--- a/vm/compiler/codegen/arm/ArchUtility.c
+++ b/vm/compiler/codegen/arm/ArchUtility.c
@@ -79,9 +79,20 @@ static void buildInsnString(char *fmt, ArmLIR *lir, char* buf,
                 strcpy(tbuf, "!");
             } else {
                assert(fmt < fmtEnd);
-               assert((unsigned)(nc-'0') < 3);
+               assert((unsigned)(nc-'0') < 4);
                operand = lir->operands[nc-'0'];
                switch(*fmt++) {
+                   case 'b':
+                       strcpy(tbuf,"0000");
+                       for (i=3; i>= 0; i--) {
+                           tbuf[i] += operand & 1;
+                           operand >>= 1;
+                       }
+                       break;
+                   case 'n':
+                       operand = ~expandImmediate(operand);
+                       sprintf(tbuf,"%d [0x%x]", operand, operand);
+                       break;
                    case 'm':
                        operand = expandImmediate(operand);
                        sprintf(tbuf,"%d [0x%x]", operand, operand);
@@ -99,9 +110,6 @@ static void buildInsnString(char *fmt, ArmLIR *lir, char* buf,
                    case 'd':
                        sprintf(tbuf,"%d", operand);
                        break;
-                   case 'D':
-                       sprintf(tbuf,"%d", operand+8);
-                       break;
                    case 'E':
                        sprintf(tbuf,"%d", operand*4);
                        break;
@@ -110,29 +118,29 @@ static void buildInsnString(char *fmt, ArmLIR *lir, char* buf,
                        break;
                    case 'c':
                        switch (operand) {
-                           case ARM_COND_EQ:
-                               strcpy(tbuf, "beq");
+                           case kArmCondEq:
+                               strcpy(tbuf, "eq");
                                break;
-                           case ARM_COND_NE:
-                               strcpy(tbuf, "bne");
+                           case kArmCondNe:
+                               strcpy(tbuf, "ne");
                                break;
-                           case ARM_COND_LT:
-                               strcpy(tbuf, "blt");
+                           case kArmCondLt:
+                               strcpy(tbuf, "lt");
                                break;
-                           case ARM_COND_GE:
-                               strcpy(tbuf, "bge");
+                           case kArmCondGe:
+                               strcpy(tbuf, "ge");
                                break;
-                           case ARM_COND_GT:
-                               strcpy(tbuf, "bgt");
+                           case kArmCondGt:
+                               strcpy(tbuf, "gt");
                                break;
-                           case ARM_COND_LE:
-                               strcpy(tbuf, "ble");
+                           case kArmCondLe:
+                               strcpy(tbuf, "le");
                                break;
-                           case ARM_COND_CS:
-                               strcpy(tbuf, "bcs");
+                           case kArmCondCs:
+                               strcpy(tbuf, "cs");
                                break;
-                           case ARM_COND_MI:
-                               strcpy(tbuf, "bmi");
+                           case kArmCondMi:
+                               strcpy(tbuf, "mi");
                                break;
                            default:
                                strcpy(tbuf, "");
@@ -182,8 +190,49 @@ static void buildInsnString(char *fmt, ArmLIR *lir, char* buf,
     *buf = 0;
 }
 
+void dvmDumpResourceMask(LIR *lir, u8 mask, const char *prefix)
+{
+    char buf[256];
+    buf[0] = 0;
+    ArmLIR *armLIR = (ArmLIR *) lir;
+
+    if (mask == ENCODE_ALL) {
+        strcpy(buf, "all");
+    } else {
+        char num[8];
+        int i;
+
+        for (i = 0; i < kRegEnd; i++) {
+            if (mask & (1ULL << i)) {
+                sprintf(num, "%d ", i);
+                strcat(buf, num);
+            }
+        }
+
+        if (mask & ENCODE_CCODE) {
+            strcat(buf, "cc ");
+        }
+        if (mask & ENCODE_FP_STATUS) {
+            strcat(buf, "fpcc ");
+        }
+        if (armLIR && (mask & ENCODE_DALVIK_REG)) {
+            sprintf(buf + strlen(buf), "dr%d%s", armLIR->aliasInfo & 0xffff,
+                    (armLIR->aliasInfo & 0x80000000) ? "(+1)" : "");
+        }
+    }
+    if (buf[0]) {
+        LOGD("%s: %s", prefix, buf);
+    }
+}
+
+/*
+ * Debugging macros
+ */
+#define DUMP_RESOURCE_MASK(X)
+#define DUMP_SSA_REP(X)
+
 /* Pretty-print a LIR instruction */
-static void dumpLIRInsn(LIR *arg, unsigned char *baseAddr)
+void dvmDumpLIRInsn(LIR *arg, unsigned char *baseAddr)
 {
     ArmLIR *lir = (ArmLIR *) arg;
     char buf[256];
@@ -191,56 +240,88 @@ static void dumpLIRInsn(LIR *arg, unsigned char *baseAddr)
     int offset = lir->generic.offset;
     int dest = lir->operands[0];
     u2 *cPtr = (u2*)baseAddr;
+    const bool dumpNop = false;
+
     /* Handle pseudo-ops individually, and all regular insns as a group */
     switch(lir->opCode) {
-        case ARM_PSEUDO_TARGET_LABEL:
+        case kArmChainingCellBottom:
+            LOGD("-------- end of chaining cells (0x%04x)\n", offset);
+            break;
+        case kArmPseudoBarrier:
+            LOGD("-------- BARRIER");
             break;
-        case ARM_PSEUDO_CHAINING_CELL_NORMAL:
+        case kArmPseudoExtended:
+            /* intentional fallthrough */
+        case kArmPseudoSSARep:
+            DUMP_SSA_REP(LOGD("-------- %s\n", (char *) dest));
+            break;
+        case kArmPseudoTargetLabel:
+            break;
+        case kArmPseudoChainingCellBackwardBranch:
+            LOGD("-------- chaining cell (backward branch): 0x%04x\n", dest);
+            break;
+        case kArmPseudoChainingCellNormal:
             LOGD("-------- chaining cell (normal): 0x%04x\n", dest);
             break;
-        case ARM_PSEUDO_CHAINING_CELL_HOT:
+        case kArmPseudoChainingCellHot:
             LOGD("-------- chaining cell (hot): 0x%04x\n", dest);
             break;
-        case ARM_PSEUDO_CHAINING_CELL_INVOKE_PREDICTED:
+        case kArmPseudoChainingCellInvokePredicted:
             LOGD("-------- chaining cell (predicted)\n");
             break;
-        case ARM_PSEUDO_CHAINING_CELL_INVOKE_SINGLETON:
+        case kArmPseudoChainingCellInvokeSingleton:
             LOGD("-------- chaining cell (invoke singleton): %s/%p\n",
                  ((Method *)dest)->name,
                  ((Method *)dest)->insns);
             break;
-        case ARM_PSEUDO_DALVIK_BYTECODE_BOUNDARY:
+        case kArmPseudoEntryBlock:
+            LOGD("-------- entry offset: 0x%04x\n", dest);
+            break;
+        case kArmPseudoDalvikByteCodeBoundary:
             LOGD("-------- dalvik offset: 0x%04x @ %s\n", dest,
-                   getOpcodeName(lir->operands[1]));
+                 (char *) lir->operands[1]);
             break;
-        case ARM_PSEUDO_ALIGN4:
+        case kArmPseudoExitBlock:
+            LOGD("-------- exit offset: 0x%04x\n", dest);
+            break;
+        case kArmPseudoPseudoAlign4:
             LOGD("%p (%04x): .align4\n", baseAddr + offset, offset);
             break;
-        case ARM_PSEUDO_PC_RECONSTRUCTION_CELL:
+        case kArmPseudoPCReconstructionCell:
             LOGD("-------- reconstruct dalvik PC : 0x%04x @ +0x%04x\n", dest,
                  lir->operands[1]);
             break;
-        case ARM_PSEUDO_PC_RECONSTRUCTION_BLOCK_LABEL:
+        case kArmPseudoPCReconstructionBlockLabel:
             /* Do nothing */
             break;
-        case ARM_PSEUDO_EH_BLOCK_LABEL:
+        case kArmPseudoEHBlockLabel:
             LOGD("Exception_Handling:\n");
             break;
-        case ARM_PSEUDO_NORMAL_BLOCK_LABEL:
+        case kArmPseudoNormalBlockLabel:
             LOGD("L%#06x:\n", dest);
             break;
         default:
-            if (lir->isNop) {
+            if (lir->isNop && !dumpNop) {
                 break;
             }
             buildInsnString(EncodingMap[lir->opCode].name, lir, opName,
                             baseAddr, 256);
             buildInsnString(EncodingMap[lir->opCode].fmt, lir, buf, baseAddr,
                             256);
-            LOGD("%p (%04x): %-8s%s\n",
-                 baseAddr + offset, offset, opName, buf);
+            LOGD("%p (%04x): %-8s%s%s\n",
+                 baseAddr + offset, offset, opName, buf,
+                 lir->isNop ? "(nop)" : "");
             break;
     }
+
+    if (lir->useMask && (!lir->isNop || dumpNop)) {
+        DUMP_RESOURCE_MASK(dvmDumpResourceMask((LIR *) lir,
+                                               lir->useMask, "use"));
+    }
+    if (lir->defMask && (!lir->isNop || dumpNop)) {
+        DUMP_RESOURCE_MASK(dvmDumpResourceMask((LIR *) lir,
+                                               lir->defMask, "def"));
+    }
 }
 
 /* Dump instructions and constant pool contents */
@@ -253,12 +334,13 @@ void dvmCompilerCodegenDump(CompilationUnit *cUnit)
     LOGD("installed code is at %p\n", cUnit->baseAddr);
     LOGD("total size is %d bytes\n", cUnit->totalSize);
     for (lirInsn = cUnit->firstLIRInsn; lirInsn; lirInsn = lirInsn->next) {
-        dumpLIRInsn(lirInsn, cUnit->baseAddr);
+        dvmDumpLIRInsn(lirInsn, cUnit->baseAddr);
     }
     for (lirInsn = cUnit->wordList; lirInsn; lirInsn = lirInsn->next) {
         armLIR = (ArmLIR *) lirInsn;
         LOGD("%p (%04x): .word (0x%x)\n",
-             (char*)cUnit->baseAddr + armLIR->generic.offset, armLIR->generic.offset,
+             (char*)cUnit->baseAddr + armLIR->generic.offset,
+             armLIR->generic.offset,
              armLIR->operands[0]);
     }
 }
diff --git a/vm/compiler/codegen/arm/ArmLIR.h b/vm/compiler/codegen/arm/ArmLIR.h
index 59c7529..f7704ad 100644
--- a/vm/compiler/codegen/arm/ArmLIR.h
+++ b/vm/compiler/codegen/arm/ArmLIR.h
@@ -27,7 +27,7 @@
  * r6 (rGLUE) is reserved [holds current &interpState]
  * r7 (rINST) is scratch for Jit
  * r8 (rIBASE) is scratch for Jit, but must be restored when resuming interp
- * r9 is always scratch
+ * r9 is reserved
  * r10 is always scratch
  * r11 (fp) used by gcc unless -fomit-frame-pointer set [available for jit?]
  * r12 is always scratch
@@ -35,21 +35,30 @@
  * r14 (lr) is scratch for Jit
  * r15 (pc) is reserved
  *
+ * Preserved across C calls: r4, r5, r6, r7, r8, r10, r11
+ * Trashed across C calls: r0, r1, r2, r3, r12, r14
+ *
+ * Floating pointer registers
+ * s0-s31
+ * d0-d15, where d0={s0,s1}, d1={s2,s3}, ... , d15={s30,s31}
+ *
+ * s16-s31 (d8-d15) preserved across C calls
+ * s0-s15 (d0-d7) trashed across C calls
+ *
  * For Thumb code use:
- *       r0, r1, r2, r3 to hold operands/results via scoreboard
+ *       r0, r1, r2, r3 to hold operands/results
  *       r4, r7 for temps
  *
  * For Thumb2 code use:
- *       r0, r1, r2, r3, r8, r9, r10, r11 for operands/results via scoreboard
- *       r4, r7, r14 for temps
+ *       r0, r1, r2, r3, r8, r9, r10, r11, r12, r14 for operands/results
+ *       r4, r7 for temps
+ *       s16-s31/d8-d15 for operands/results
+ *       s0-s15/d0-d7 for temps
  *
  * When transitioning from code cache to interp:
  *       restore rIBASE
  *       restore rPC
- *       restore r11 (fp)?
- *
- * Double precision values are stored in consecutive single precision registers
- * such that dr0 -> (sr0,sr1), dr1 -> (sr2,sr3) ... dr16 -> (sr30,sr31)
+ *       restore r11?
  */
 
 /* Offset to distingish FP regs */
@@ -57,15 +66,150 @@
 /* Offset to distinguish DP FP regs */
 #define FP_DOUBLE 64
 /* Reg types */
+#define REGTYPE(x) (x & (FP_REG_OFFSET | FP_DOUBLE))
 #define FPREG(x) ((x & FP_REG_OFFSET) == FP_REG_OFFSET)
 #define LOWREG(x) ((x & 0x7) == x)
 #define DOUBLEREG(x) ((x & FP_DOUBLE) == FP_DOUBLE)
 #define SINGLEREG(x) (FPREG(x) && !DOUBLEREG(x))
+/*
+ * Note: the low register of a floating point pair is sufficient to
+ * create the name of a double, but require both names to be passed to
+ * allow for asserts to verify that the pair is consecutive if significant
+ * rework is done in this area.  Also, it is a good reminder in the calling
+ * code that reg locations always describe doubles as a pair of singles.
+ */
+#define S2D(x,y) ((x) | FP_DOUBLE)
 /* Mask to strip off fp flags */
 #define FP_REG_MASK (FP_REG_OFFSET-1)
-/* Mask to convert high reg to low for Thumb */
-#define THUMB_REG_MASK 0x7
+/* non-existent Dalvik register */
+#define vNone   (-1)
+/* non-existant physical register */
+#define rNone   (-1)
+
+/* RegisterLocation templates return values (r0, or r0/r1) */
+#define LOC_C_RETURN {kLocPhysReg, 0, 0, r0, 0, -1}
+#define LOC_C_RETURN_WIDE {kLocPhysReg, 1, 0, r0, r1, -1}
+/* RegisterLocation templates for interpState->retVal; */
+#define LOC_DALVIK_RETURN_VAL {kLocRetval, 0, 0, 0, 0, -1}
+#define LOC_DALVIK_RETURN_VAL_WIDE {kLocRetval, 1, 0, 0, 0, -1}
+
+ /*
+ * Data structure tracking the mapping between a Dalvik register (pair) and a
+ * native register (pair). The idea is to reuse the previously loaded value
+ * if possible, otherwise to keep the value in a native register as long as
+ * possible.
+ */
+typedef struct RegisterInfo {
+    int reg;                    // Reg number
+    bool inUse;                 // Has it been allocated?
+    bool pair;                  // Part of a register pair?
+    int partner;                // If pair, other reg of pair
+    bool live;                  // Is there an associated SSA name?
+    bool dirty;                 // If live, is it dirty?
+    int sReg;                   // Name of live value
+    struct LIR *defStart;       // Starting inst in last def sequence
+    struct LIR *defEnd;         // Ending inst in last def sequence
+} RegisterInfo;
+
+typedef struct RegisterPool {
+    BitVector *nullCheckedRegs; // Track which registers have been null-checked
+    int numCoreTemps;
+    RegisterInfo *coreTemps;
+    int nextCoreTemp;
+    int numFPTemps;
+    RegisterInfo *FPTemps;
+    int nextFPTemp;
+    int numCoreRegs;
+    RegisterInfo *coreRegs;
+    int numFPRegs;
+    RegisterInfo *FPRegs;
+} RegisterPool;
+
+typedef enum ResourceEncodingPos {
+    kGPReg0     = 0,
+    kRegSP      = 13,
+    kRegLR      = 14,
+    kRegPC      = 15,
+    kFPReg0     = 16,
+    kRegEnd     = 48,
+    kCCode      = kRegEnd,
+    kFPStatus,
+    kDalvikReg,
+    kLiteral,
+    kFrameRef,
+    kHeapRef,
+    kLitPoolRef
+} ResourceEncodingPos;
+
+#define ENCODE_REG_LIST(N)      ((u8) N)
+#define ENCODE_REG_SP           (1ULL << kRegSP)
+#define ENCODE_REG_LR           (1ULL << kRegLR)
+#define ENCODE_REG_PC           (1ULL << kRegPC)
+#define ENCODE_CCODE            (1ULL << kCCode)
+#define ENCODE_FP_STATUS        (1ULL << kFPStatus)
+
+    /* Must alias */
+#define ENCODE_DALVIK_REG       (1ULL << kDalvikReg)
+#define ENCODE_LITERAL          (1ULL << kLiteral)
 
+    /* May alias */
+#define ENCODE_FRAME_REF        (1ULL << kFrameRef)
+#define ENCODE_HEAP_REF         (1ULL << kHeapRef)
+#define ENCODE_LITPOOL_REF      (1ULL << kLitPoolRef)
+
+#define ENCODE_ALL              (~0ULL)
+#define ENCODE_MEM_DEF          (ENCODE_FRAME_REF | ENCODE_HEAP_REF)
+#define ENCODE_MEM_USE          (ENCODE_FRAME_REF | ENCODE_HEAP_REF \
+                                 | ENCODE_LITPOOL_REF)
+
+#define DECODE_ALIAS_INFO_REG(X)        (X & 0xffff)
+#define DECODE_ALIAS_INFO_WIDE(X)       ((X & 0x80000000) ? 1 : 0)
+
+typedef enum OpSize {
+    kWord,
+    kLong,
+    kSingle,
+    kDouble,
+    kUnsignedHalf,
+    kSignedHalf,
+    kUnsignedByte,
+    kSignedByte,
+} OpSize;
+
+typedef enum OpKind {
+    kOpMov,
+    kOpMvn,
+    kOpCmp,
+    kOpLsl,
+    kOpLsr,
+    kOpAsr,
+    kOpRor,
+    kOpNot,
+    kOpAnd,
+    kOpOr,
+    kOpXor,
+    kOpNeg,
+    kOpAdd,
+    kOpAdc,
+    kOpSub,
+    kOpSbc,
+    kOpRsub,
+    kOpMul,
+    kOpDiv,
+    kOpRem,
+    kOpBic,
+    kOpCmn,
+    kOpTst,
+    kOpBkpt,
+    kOpBlx,
+    kOpPush,
+    kOpPop,
+    kOp2Char,
+    kOp2Short,
+    kOp2Byte,
+    kOpCondBr,
+    kOpUncondBr,
+} OpKind;
 
 typedef enum NativeRegisterPool {
     r0 = 0,
@@ -134,16 +278,32 @@ typedef enum NativeRegisterPool {
     dr15 = fr30 + FP_DOUBLE,
 } NativeRegisterPool;
 
+/* Shift encodings */
+typedef enum ArmShiftEncodings {
+    kArmLsl = 0x0,
+    kArmLsr = 0x1,
+    kArmAsr = 0x2,
+    kArmRor = 0x3
+} ArmShiftEncodings;
+
 /* Thumb condition encodings */
 typedef enum ArmConditionCode {
-    ARM_COND_EQ = 0x0,    /* 0000 */
-    ARM_COND_NE = 0x1,    /* 0001 */
-    ARM_COND_LT = 0xb,    /* 1011 */
-    ARM_COND_GE = 0xa,    /* 1010 */
-    ARM_COND_GT = 0xc,    /* 1100 */
-    ARM_COND_LE = 0xd,    /* 1101 */
-    ARM_COND_CS = 0x2,    /* 0010 */
-    ARM_COND_MI = 0x4,    /* 0100 */
+    kArmCondEq = 0x0,    /* 0000 */
+    kArmCondNe = 0x1,    /* 0001 */
+    kArmCondCs = 0x2,    /* 0010 */
+    kArmCondCc = 0x3,    /* 0011 */
+    kArmCondMi = 0x4,    /* 0100 */
+    kArmCondPl = 0x5,    /* 0101 */
+    kArmCondVs = 0x6,    /* 0110 */
+    kArmCondVc = 0x7,    /* 0111 */
+    kArmCondHi = 0x8,    /* 1000 */
+    kArmCondLs = 0x9,    /* 1001 */
+    kArmCondGe = 0xa,    /* 1010 */
+    kArmCondLt = 0xb,    /* 1011 */
+    kArmCondGt = 0xc,    /* 1100 */
+    kArmCondLe = 0xd,    /* 1101 */
+    kArmCondAl = 0xe,    /* 1110 */
+    kArmCondNv = 0xf,    /* 1111 */
 } ArmConditionCode;
 
 #define isPseudoOpCode(opCode) ((int)(opCode) < 0)
@@ -154,179 +314,399 @@ typedef enum ArmConditionCode {
  * Assemble.c.
  */
 typedef enum ArmOpCode {
-    ARM_PSEUDO_TARGET_LABEL = -11,
-    ARM_PSEUDO_CHAINING_CELL_HOT = -10,
-    ARM_PSEUDO_CHAINING_CELL_INVOKE_PREDICTED = -9,
-    ARM_PSEUDO_CHAINING_CELL_INVOKE_SINGLETON = -8,
-    ARM_PSEUDO_CHAINING_CELL_NORMAL = -7,
-    ARM_PSEUDO_DALVIK_BYTECODE_BOUNDARY = -6,
-    ARM_PSEUDO_ALIGN4 = -5,
-    ARM_PSEUDO_PC_RECONSTRUCTION_CELL = -4,
-    ARM_PSEUDO_PC_RECONSTRUCTION_BLOCK_LABEL = -3,
-    ARM_PSEUDO_EH_BLOCK_LABEL = -2,
-    ARM_PSEUDO_NORMAL_BLOCK_LABEL = -1,
+    kArmChainingCellBottom = -18,
+    kArmPseudoBarrier = -17,
+    kArmPseudoExtended = -16,
+    kArmPseudoSSARep = -15,
+    kArmPseudoEntryBlock = -14,
+    kArmPseudoExitBlock = -13,
+    kArmPseudoTargetLabel = -12,
+    kArmPseudoChainingCellBackwardBranch = -11,
+    kArmPseudoChainingCellHot = -10,
+    kArmPseudoChainingCellInvokePredicted = -9,
+    kArmPseudoChainingCellInvokeSingleton = -8,
+    kArmPseudoChainingCellNormal = -7,
+    kArmPseudoDalvikByteCodeBoundary = -6,
+    kArmPseudoPseudoAlign4 = -5,
+    kArmPseudoPCReconstructionCell = -4,
+    kArmPseudoPCReconstructionBlockLabel = -3,
+    kArmPseudoEHBlockLabel = -2,
+    kArmPseudoNormalBlockLabel = -1,
     /************************************************************************/
-    ARM_16BIT_DATA,       /* DATA   [0] rd[15..0] */
-    THUMB_ADC,            /* adc     [0100000101] rm[5..3] rd[2..0] */
-    THUMB_ADD_RRI3,       /* add(1)  [0001110] imm_3[8..6] rn[5..3] rd[2..0]*/
-    THUMB_ADD_RI8,        /* add(2)  [00110] rd[10..8] imm_8[7..0] */
-    THUMB_ADD_RRR,        /* add(3)  [0001100] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_ADD_RR_LH,      /* add(4)  [01000100] H12[01] rm[5..3] rd[2..0] */
-    THUMB_ADD_RR_HL,      /* add(4)  [01001000] H12[10] rm[5..3] rd[2..0] */
-    THUMB_ADD_RR_HH,      /* add(4)  [01001100] H12[11] rm[5..3] rd[2..0] */
-    THUMB_ADD_PC_REL,     /* add(5)  [10100] rd[10..8] imm_8[7..0] */
-    THUMB_ADD_SP_REL,     /* add(6)  [10101] rd[10..8] imm_8[7..0] */
-    THUMB_ADD_SPI7,       /* add(7)  [101100000] imm_7[6..0] */
-    THUMB_AND_RR,         /* and     [0100000000] rm[5..3] rd[2..0] */
-    THUMB_ASR,            /* asr(1)  [00010] imm_5[10..6] rm[5..3] rd[2..0] */
-    THUMB_ASRV,           /* asr(2)  [0100000100] rs[5..3] rd[2..0] */
-    THUMB_B_COND,         /* b(1)    [1101] cond[11..8] offset_8[7..0] */
-    THUMB_B_UNCOND,       /* b(2)    [11100] offset_11[10..0] */
-    THUMB_BIC,            /* bic     [0100001110] rm[5..3] rd[2..0] */
-    THUMB_BKPT,           /* bkpt    [10111110] imm_8[7..0] */
-    THUMB_BLX_1,          /* blx(1)  [111] H[10] offset_11[10..0] */
-    THUMB_BLX_2,          /* blx(1)  [111] H[01] offset_11[10..0] */
-    THUMB_BL_1,           /* blx(1)  [111] H[10] offset_11[10..0] */
-    THUMB_BL_2,           /* blx(1)  [111] H[11] offset_11[10..0] */
-    THUMB_BLX_R,          /* blx(2)  [010001111] H2[6..6] rm[5..3] SBZ[000] */
-    THUMB_BX,             /* bx      [010001110] H2[6..6] rm[5..3] SBZ[000] */
-    THUMB_CMN,            /* cmn     [0100001011] rm[5..3] rd[2..0] */
-    THUMB_CMP_RI8,        /* cmp(1)  [00101] rn[10..8] imm_8[7..0] */
-    THUMB_CMP_RR,         /* cmp(2)  [0100001010] rm[5..3] rd[2..0] */
-    THUMB_CMP_LH,         /* cmp(3)  [01000101] H12[01] rm[5..3] rd[2..0] */
-    THUMB_CMP_HL,         /* cmp(3)  [01000110] H12[10] rm[5..3] rd[2..0] */
-    THUMB_CMP_HH,         /* cmp(3)  [01000111] H12[11] rm[5..3] rd[2..0] */
-    THUMB_EOR,            /* eor     [0100000001] rm[5..3] rd[2..0] */
-    THUMB_LDMIA,          /* ldmia   [11001] rn[10..8] reglist [7..0] */
-    THUMB_LDR_RRI5,       /* ldr(1)  [01101] imm_5[10..6] rn[5..3] rd[2..0] */
-    THUMB_LDR_RRR,        /* ldr(2)  [0101100] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_LDR_PC_REL,     /* ldr(3)  [01001] rd[10..8] imm_8[7..0] */
-    THUMB_LDR_SP_REL,     /* ldr(4)  [10011] rd[10..8] imm_8[7..0] */
-    THUMB_LDRB_RRI5,      /* ldrb(1) [01111] imm_5[10..6] rn[5..3] rd[2..0] */
-    THUMB_LDRB_RRR,       /* ldrb(2) [0101110] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_LDRH_RRI5,      /* ldrh(1) [10001] imm_5[10..6] rn[5..3] rd[2..0] */
-    THUMB_LDRH_RRR,       /* ldrh(2) [0101101] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_LDRSB_RRR,      /* ldrsb   [0101011] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_LDRSH_RRR,      /* ldrsh   [0101111] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_LSL,            /* lsl(1)  [00000] imm_5[10..6] rm[5..3] rd[2..0] */
-    THUMB_LSLV,           /* lsl(2)  [0100000010] rs[5..3] rd[2..0] */
-    THUMB_LSR,            /* lsr(1)  [00001] imm_5[10..6] rm[5..3] rd[2..0] */
-    THUMB_LSRV,           /* lsr(2)  [0100000011] rs[5..3] rd[2..0] */
-    THUMB_MOV_IMM,        /* mov(1)  [00100] rd[10..8] imm_8[7..0] */
-    THUMB_MOV_RR,         /* mov(2)  [0001110000] rn[5..3] rd[2..0] */
-    THUMB_MOV_RR_H2H,     /* mov(3)  [01000111] H12[11] rm[5..3] rd[2..0] */
-    THUMB_MOV_RR_H2L,     /* mov(3)  [01000110] H12[01] rm[5..3] rd[2..0] */
-    THUMB_MOV_RR_L2H,     /* mov(3)  [01000101] H12[10] rm[5..3] rd[2..0] */
-    THUMB_MUL,            /* mul     [0100001101] rm[5..3] rd[2..0] */
-    THUMB_MVN,            /* mvn     [0100001111] rm[5..3] rd[2..0] */
-    THUMB_NEG,            /* neg     [0100001001] rm[5..3] rd[2..0] */
-    THUMB_ORR,            /* orr     [0100001100] rm[5..3] rd[2..0] */
-    THUMB_POP,            /* pop     [1011110] r[8..8] rl[7..0] */
-    THUMB_PUSH,           /* push    [1011010] r[8..8] rl[7..0] */
-    THUMB_ROR,            /* ror     [0100000111] rs[5..3] rd[2..0] */
-    THUMB_SBC,            /* sbc     [0100000110] rm[5..3] rd[2..0] */
-    THUMB_STMIA,          /* stmia   [11000] rn[10..8] reglist [7.. 0] */
-    THUMB_STR_RRI5,       /* str(1)  [01100] imm_5[10..6] rn[5..3] rd[2..0] */
-    THUMB_STR_RRR,        /* str(2)  [0101000] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_STR_SP_REL,     /* str(3)  [10010] rd[10..8] imm_8[7..0] */
-    THUMB_STRB_RRI5,      /* strb(1) [01110] imm_5[10..6] rn[5..3] rd[2..0] */
-    THUMB_STRB_RRR,       /* strb(2) [0101010] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_STRH_RRI5,      /* strh(1) [10000] imm_5[10..6] rn[5..3] rd[2..0] */
-    THUMB_STRH_RRR,       /* strh(2) [0101001] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_SUB_RRI3,       /* sub(1)  [0001111] imm_3[8..6] rn[5..3] rd[2..0]*/
-    THUMB_SUB_RI8,        /* sub(2)  [00111] rd[10..8] imm_8[7..0] */
-    THUMB_SUB_RRR,        /* sub(3)  [0001101] rm[8..6] rn[5..3] rd[2..0] */
-    THUMB_SUB_SPI7,       /* sub(4)  [101100001] imm_7[6..0] */
-    THUMB_SWI,            /* swi     [11011111] imm_8[7..0] */
-    THUMB_TST,            /* tst     [0100001000] rm[5..3] rn[2..0] */
-    THUMB2_VLDRS,         /* vldr low  sx [111011011001] rn[19..16] rd[15-12]
-                                       [1010] imm_8[7..0] */
-    THUMB2_VLDRD,         /* vldr low  dx [111011011001] rn[19..16] rd[15-12]
-                                       [1011] imm_8[7..0] */
-    THUMB2_VMULS,         /* vmul vd, vn, vm [111011100010] rn[19..16]
-                                       rd[15-12] [10100000] rm[3..0] */
-    THUMB2_VMULD,         /* vmul vd, vn, vm [111011100010] rn[19..16]
-                                       rd[15-12] [10110000] rm[3..0] */
-    THUMB2_VSTRS,         /* vstr low  sx [111011011000] rn[19..16] rd[15-12]
-                                       [1010] imm_8[7..0] */
-    THUMB2_VSTRD,         /* vstr low  dx [111011011000] rn[19..16] rd[15-12]
-                                       [1011] imm_8[7..0] */
-    THUMB2_VSUBS,         /* vsub vd, vn, vm [111011100011] rn[19..16]
-                                       rd[15-12] [10100040] rm[3..0] */
-    THUMB2_VSUBD,         /* vsub vd, vn, vm [111011100011] rn[19..16]
-                                       rd[15-12] [10110040] rm[3..0] */
-    THUMB2_VADDS,         /* vadd vd, vn, vm [111011100011] rn[19..16]
-                                       rd[15-12] [10100000] rm[3..0] */
-    THUMB2_VADDD,         /* vadd vd, vn, vm [111011100011] rn[19..16]
-                                       rd[15-12] [10110000] rm[3..0] */
-    THUMB2_VDIVS,         /* vdiv vd, vn, vm [111011101000] rn[19..16]
-                                       rd[15-12] [10100000] rm[3..0] */
-    THUMB2_VDIVD,         /* vdiv vd, vn, vm [111011101000] rn[19..16]
-                                       rd[15-12] [10110000] rm[3..0] */
-    THUMB2_VCVTIF,        /* vcvt.F32 vd, vm [1110111010111000] vd[15..12]
-                                       [10101100] vm[3..0] */
-    THUMB2_VCVTID,        /* vcvt.F64 vd, vm [1110111010111000] vd[15..12]
+    kArm16BitData,       /* DATA   [0] rd[15..0] */
+    kThumbAdcRR,         /* adc     [0100000101] rm[5..3] rd[2..0] */
+    kThumbAddRRI3,       /* add(1)  [0001110] imm_3[8..6] rn[5..3] rd[2..0]*/
+    kThumbAddRI8,        /* add(2)  [00110] rd[10..8] imm_8[7..0] */
+    kThumbAddRRR,        /* add(3)  [0001100] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbAddRRLH,       /* add(4)  [01000100] H12[01] rm[5..3] rd[2..0] */
+    kThumbAddRRHL,       /* add(4)  [01001000] H12[10] rm[5..3] rd[2..0] */
+    kThumbAddRRHH,       /* add(4)  [01001100] H12[11] rm[5..3] rd[2..0] */
+    kThumbAddPcRel,      /* add(5)  [10100] rd[10..8] imm_8[7..0] */
+    kThumbAddSpRel,      /* add(6)  [10101] rd[10..8] imm_8[7..0] */
+    kThumbAddSpI7,       /* add(7)  [101100000] imm_7[6..0] */
+    kThumbAndRR,         /* and     [0100000000] rm[5..3] rd[2..0] */
+    kThumbAsrRRI5,       /* asr(1)  [00010] imm_5[10..6] rm[5..3] rd[2..0] */
+    kThumbAsrRR,         /* asr(2)  [0100000100] rs[5..3] rd[2..0] */
+    kThumbBCond,         /* b(1)    [1101] cond[11..8] offset_8[7..0] */
+    kThumbBUncond,       /* b(2)    [11100] offset_11[10..0] */
+    kThumbBicRR,         /* bic     [0100001110] rm[5..3] rd[2..0] */
+    kThumbBkpt,          /* bkpt    [10111110] imm_8[7..0] */
+    kThumbBlx1,          /* blx(1)  [111] H[10] offset_11[10..0] */
+    kThumbBlx2,          /* blx(1)  [111] H[01] offset_11[10..0] */
+    kThumbBl1,           /* blx(1)  [111] H[10] offset_11[10..0] */
+    kThumbBl2,           /* blx(1)  [111] H[11] offset_11[10..0] */
+    kThumbBlxR,          /* blx(2)  [010001111] rm[6..3] [000] */
+    kThumbBx,            /* bx      [010001110] H2[6..6] rm[5..3] SBZ[000] */
+    kThumbCmnRR,         /* cmn     [0100001011] rm[5..3] rd[2..0] */
+    kThumbCmpRI8,        /* cmp(1)  [00101] rn[10..8] imm_8[7..0] */
+    kThumbCmpRR,         /* cmp(2)  [0100001010] rm[5..3] rd[2..0] */
+    kThumbCmpLH,         /* cmp(3)  [01000101] H12[01] rm[5..3] rd[2..0] */
+    kThumbCmpHL,         /* cmp(3)  [01000110] H12[10] rm[5..3] rd[2..0] */
+    kThumbCmpHH,         /* cmp(3)  [01000111] H12[11] rm[5..3] rd[2..0] */
+    kThumbEorRR,         /* eor     [0100000001] rm[5..3] rd[2..0] */
+    kThumbLdmia,         /* ldmia   [11001] rn[10..8] reglist [7..0] */
+    kThumbLdrRRI5,       /* ldr(1)  [01101] imm_5[10..6] rn[5..3] rd[2..0] */
+    kThumbLdrRRR,        /* ldr(2)  [0101100] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbLdrPcRel,      /* ldr(3)  [01001] rd[10..8] imm_8[7..0] */
+    kThumbLdrSpRel,      /* ldr(4)  [10011] rd[10..8] imm_8[7..0] */
+    kThumbLdrbRRI5,      /* ldrb(1) [01111] imm_5[10..6] rn[5..3] rd[2..0] */
+    kThumbLdrbRRR,       /* ldrb(2) [0101110] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbLdrhRRI5,      /* ldrh(1) [10001] imm_5[10..6] rn[5..3] rd[2..0] */
+    kThumbLdrhRRR,       /* ldrh(2) [0101101] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbLdrsbRRR,      /* ldrsb   [0101011] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbLdrshRRR,      /* ldrsh   [0101111] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbLslRRI5,       /* lsl(1)  [00000] imm_5[10..6] rm[5..3] rd[2..0] */
+    kThumbLslRR,         /* lsl(2)  [0100000010] rs[5..3] rd[2..0] */
+    kThumbLsrRRI5,       /* lsr(1)  [00001] imm_5[10..6] rm[5..3] rd[2..0] */
+    kThumbLsrRR,         /* lsr(2)  [0100000011] rs[5..3] rd[2..0] */
+    kThumbMovImm,        /* mov(1)  [00100] rd[10..8] imm_8[7..0] */
+    kThumbMovRR,         /* mov(2)  [0001110000] rn[5..3] rd[2..0] */
+    kThumbMovRR_H2H,     /* mov(3)  [01000111] H12[11] rm[5..3] rd[2..0] */
+    kThumbMovRR_H2L,     /* mov(3)  [01000110] H12[01] rm[5..3] rd[2..0] */
+    kThumbMovRR_L2H,     /* mov(3)  [01000101] H12[10] rm[5..3] rd[2..0] */
+    kThumbMul,           /* mul     [0100001101] rm[5..3] rd[2..0] */
+    kThumbMvn,           /* mvn     [0100001111] rm[5..3] rd[2..0] */
+    kThumbNeg,           /* neg     [0100001001] rm[5..3] rd[2..0] */
+    kThumbOrr,           /* orr     [0100001100] rm[5..3] rd[2..0] */
+    kThumbPop,           /* pop     [1011110] r[8..8] rl[7..0] */
+    kThumbPush,          /* push    [1011010] r[8..8] rl[7..0] */
+    kThumbRorRR,         /* ror     [0100000111] rs[5..3] rd[2..0] */
+    kThumbSbc,           /* sbc     [0100000110] rm[5..3] rd[2..0] */
+    kThumbStmia,         /* stmia   [11000] rn[10..8] reglist [7.. 0] */
+    kThumbStrRRI5,       /* str(1)  [01100] imm_5[10..6] rn[5..3] rd[2..0] */
+    kThumbStrRRR,        /* str(2)  [0101000] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbStrSpRel,      /* str(3)  [10010] rd[10..8] imm_8[7..0] */
+    kThumbStrbRRI5,      /* strb(1) [01110] imm_5[10..6] rn[5..3] rd[2..0] */
+    kThumbStrbRRR,       /* strb(2) [0101010] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbStrhRRI5,      /* strh(1) [10000] imm_5[10..6] rn[5..3] rd[2..0] */
+    kThumbStrhRRR,       /* strh(2) [0101001] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbSubRRI3,       /* sub(1)  [0001111] imm_3[8..6] rn[5..3] rd[2..0]*/
+    kThumbSubRI8,        /* sub(2)  [00111] rd[10..8] imm_8[7..0] */
+    kThumbSubRRR,        /* sub(3)  [0001101] rm[8..6] rn[5..3] rd[2..0] */
+    kThumbSubSpI7,       /* sub(4)  [101100001] imm_7[6..0] */
+    kThumbSwi,           /* swi     [11011111] imm_8[7..0] */
+    kThumbTst,           /* tst     [0100001000] rm[5..3] rn[2..0] */
+    kThumb2Vldrs,        /* vldr low  sx [111011011001] rn[19..16] rd[15-12]
+                                    [1010] imm_8[7..0] */
+    kThumb2Vldrd,        /* vldr low  dx [111011011001] rn[19..16] rd[15-12]
+                                    [1011] imm_8[7..0] */
+    kThumb2Vmuls,        /* vmul vd, vn, vm [111011100010] rn[19..16]
+                                    rd[15-12] [10100000] rm[3..0] */
+    kThumb2Vmuld,        /* vmul vd, vn, vm [111011100010] rn[19..16]
+                                    rd[15-12] [10110000] rm[3..0] */
+    kThumb2Vstrs,        /* vstr low  sx [111011011000] rn[19..16] rd[15-12]
+                                    [1010] imm_8[7..0] */
+    kThumb2Vstrd,        /* vstr low  dx [111011011000] rn[19..16] rd[15-12]
+                                    [1011] imm_8[7..0] */
+    kThumb2Vsubs,        /* vsub vd, vn, vm [111011100011] rn[19..16]
+                                    rd[15-12] [10100040] rm[3..0] */
+    kThumb2Vsubd,        /* vsub vd, vn, vm [111011100011] rn[19..16]
+                                    rd[15-12] [10110040] rm[3..0] */
+    kThumb2Vadds,        /* vadd vd, vn, vm [111011100011] rn[19..16]
+                                    rd[15-12] [10100000] rm[3..0] */
+    kThumb2Vaddd,        /* vadd vd, vn, vm [111011100011] rn[19..16]
+                                    rd[15-12] [10110000] rm[3..0] */
+    kThumb2Vdivs,        /* vdiv vd, vn, vm [111011101000] rn[19..16]
+                                    rd[15-12] [10100000] rm[3..0] */
+    kThumb2Vdivd,        /* vdiv vd, vn, vm [111011101000] rn[19..16]
+                                    rd[15-12] [10110000] rm[3..0] */
+    kThumb2VcvtIF,       /* vcvt.F32 vd, vm [1110111010111000] vd[15..12]
+                                    [10101100] vm[3..0] */
+    kThumb2VcvtID,       /* vcvt.F64 vd, vm [1110111010111000] vd[15..12]
                                        [10111100] vm[3..0] */
-    THUMB2_VCVTFI,        /* vcvt.S32.F32 vd, vm [1110111010111101] vd[15..12]
+    kThumb2VcvtFI,       /* vcvt.S32.F32 vd, vm [1110111010111101] vd[15..12]
                                        [10101100] vm[3..0] */
-    THUMB2_VCVTDI,        /* vcvt.S32.F32 vd, vm [1110111010111101] vd[15..12]
+    kThumb2VcvtDI,       /* vcvt.S32.F32 vd, vm [1110111010111101] vd[15..12]
                                        [10111100] vm[3..0] */
-    THUMB2_VCVTFD,        /* vcvt.F64.F32 vd, vm [1110111010110111] vd[15..12]
+    kThumb2VcvtFd,       /* vcvt.F64.F32 vd, vm [1110111010110111] vd[15..12]
                                        [10101100] vm[3..0] */
-    THUMB2_VCVTDF,        /* vcvt.F32.F64 vd, vm [1110111010110111] vd[15..12]
+    kThumb2VcvtDF,       /* vcvt.F32.F64 vd, vm [1110111010110111] vd[15..12]
                                        [10111100] vm[3..0] */
-    THUMB2_VSQRTS,        /* vsqrt.f32 vd, vm [1110111010110001] vd[15..12]
+    kThumb2Vsqrts,       /* vsqrt.f32 vd, vm [1110111010110001] vd[15..12]
                                        [10101100] vm[3..0] */
-    THUMB2_VSQRTD,        /* vsqrt.f64 vd, vm [1110111010110001] vd[15..12]
+    kThumb2Vsqrtd,       /* vsqrt.f64 vd, vm [1110111010110001] vd[15..12]
                                        [10111100] vm[3..0] */
-    THUMB2_MOV_IMM_SHIFT, /* mov(T2) rd, #<const> [11110] i [00001001111]
+    kThumb2MovImmShift,  /* mov(T2) rd, #<const> [11110] i [00001001111]
                                        imm3 rd[11..8] imm8 */
-    THUMB2_MOV_IMM16,     /* mov(T3) rd, #<const> [11110] i [0010100] imm4 [0]
+    kThumb2MovImm16,     /* mov(T3) rd, #<const> [11110] i [0010100] imm4 [0]
                                        imm3 rd[11..8] imm8 */
-    THUMB2_STR_RRI12,     /* str(Imm,T3) rd,[rn,#imm12] [111110001100]
+    kThumb2StrRRI12,     /* str(Imm,T3) rd,[rn,#imm12] [111110001100]
                                        rn[19..16] rt[15..12] imm12[11..0] */
-    THUMB2_LDR_RRI12,     /* str(Imm,T3) rd,[rn,#imm12] [111110001100]
+    kThumb2LdrRRI12,     /* str(Imm,T3) rd,[rn,#imm12] [111110001100]
                                        rn[19..16] rt[15..12] imm12[11..0] */
-    THUMB2_STR_RRI8_PREDEC, /* str(Imm,T4) rd,[rn,#-imm8] [111110000100]
+    kThumb2StrRRI8Predec, /* str(Imm,T4) rd,[rn,#-imm8] [111110000100]
                                        rn[19..16] rt[15..12] [1100] imm[7..0]*/
-    THUMB2_LDR_RRI8_PREDEC, /* ldr(Imm,T4) rd,[rn,#-imm8] [111110000101]
+    kThumb2LdrRRI8Predec, /* ldr(Imm,T4) rd,[rn,#-imm8] [111110000101]
                                        rn[19..16] rt[15..12] [1100] imm[7..0]*/
-    THUMB2_CBNZ,            /* cbnz rd,<label> [101110] i [1] imm5[7..3]
+    kThumb2Cbnz,         /* cbnz rd,<label> [101110] i [1] imm5[7..3]
                                        rn[2..0] */
-    THUMB2_CBZ,             /* cbn rd,<label> [101100] i [1] imm5[7..3]
+    kThumb2Cbz,          /* cbn rd,<label> [101100] i [1] imm5[7..3]
                                        rn[2..0] */
-    THUMB2_ADD_RRI12,       /* add rd, rn, #imm12 [11110] i [100000] rn[19..16]
+    kThumb2AddRRI12,     /* add rd, rn, #imm12 [11110] i [100000] rn[19..16]
                                        [0] imm3[14..12] rd[11..8] imm8[7..0] */
-    THUMB2_MOV_RR,          /* mov rd, rm [11101010010011110000] rd[11..8]
+    kThumb2MovRR,        /* mov rd, rm [11101010010011110000] rd[11..8]
                                        [0000] rm[3..0] */
-    THUMB2_VMOVS,           /* vmov.f32 vd, vm [111011101] D [110000]
+    kThumb2Vmovs,        /* vmov.f32 vd, vm [111011101] D [110000]
                                        vd[15..12] 101001] M [0] vm[3..0] */
-    THUMB2_VMOVD,           /* vmov.f64 vd, vm [111011101] D [110000]
+    kThumb2Vmovd,        /* vmov.f64 vd, vm [111011101] D [110000]
                                        vd[15..12] 101101] M [0] vm[3..0] */
-    ARM_LAST,
+    kThumb2Ldmia,        /* ldmia  [111010001001[ rn[19..16] mask[15..0] */
+    kThumb2Stmia,        /* stmia  [111010001000[ rn[19..16] mask[15..0] */
+    kThumb2AddRRR,       /* add [111010110000] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2SubRRR,       /* sub [111010111010] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2SbcRRR,       /* sbc [111010110110] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2CmpRR,        /* cmp [111010111011] rn[19..16] [0000] [1111]
+                                   [0000] rm[3..0] */
+    kThumb2SubRRI12,     /* sub rd, rn, #imm12 [11110] i [01010] rn[19..16]
+                                       [0] imm3[14..12] rd[11..8] imm8[7..0] */
+    kThumb2MvnImmShift,  /* mov(T2) rd, #<const> [11110] i [00011011110]
+                                       imm3 rd[11..8] imm8 */
+    kThumb2Sel,          /* sel rd, rn, rm [111110101010] rn[19-16] rd[11-8]
+                                       rm[3-0] */
+    kThumb2Ubfx,         /* ubfx rd,rn,#lsb,#width [111100111100] rn[19..16]
+                                       [0] imm3[14-12] rd[11-8] w[4-0] */
+    kThumb2Sbfx,         /* ubfx rd,rn,#lsb,#width [111100110100] rn[19..16]
+                                       [0] imm3[14-12] rd[11-8] w[4-0] */
+    kThumb2LdrRRR,       /* ldr rt,[rn,rm,LSL #imm] [111110000101] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2LdrhRRR,      /* ldrh rt,[rn,rm,LSL #imm] [111110000101] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2LdrshRRR,     /* ldrsh rt,[rn,rm,LSL #imm] [111110000101] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2LdrbRRR,      /* ldrb rt,[rn,rm,LSL #imm] [111110000101] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2LdrsbRRR,     /* ldrsb rt,[rn,rm,LSL #imm] [111110000101] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2StrRRR,       /* str rt,[rn,rm,LSL #imm] [111110000100] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2StrhRRR,      /* str rt,[rn,rm,LSL #imm] [111110000010] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2StrbRRR,      /* str rt,[rn,rm,LSL #imm] [111110000000] rn[19-16]
+                                       rt[15-12] [000000] imm[5-4] rm[3-0] */
+    kThumb2LdrhRRI12,    /* ldrh rt,[rn,#imm12] [111110001011]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+    kThumb2LdrshRRI12,   /* ldrsh rt,[rn,#imm12] [111110011011]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+    kThumb2LdrbRRI12,    /* ldrb rt,[rn,#imm12] [111110001001]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+    kThumb2LdrsbRRI12,   /* ldrsb rt,[rn,#imm12] [111110011001]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+    kThumb2StrhRRI12,    /* strh rt,[rn,#imm12] [111110001010]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+    kThumb2StrbRRI12,    /* strb rt,[rn,#imm12] [111110001000]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+    kThumb2Pop,          /* pop     [1110100010111101] list[15-0]*/
+    kThumb2Push,         /* push    [1110100010101101] list[15-0]*/
+    kThumb2CmpRI8,       /* cmp rn, #<const> [11110] i [011011] rn[19-16] [0]
+                                       imm3 [1111] imm8[7..0] */
+    kThumb2AdcRRR,       /* adc [111010110101] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2AndRRR,       /* and [111010100000] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2BicRRR,       /* bic [111010100010] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2CmnRR,        /* cmn [111010110001] rn[19..16] [0000] [1111]
+                                   [0000] rm[3..0] */
+    kThumb2EorRRR,       /* eor [111010101000] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2MulRRR,       /* mul [111110110000] rn[19..16] [1111] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2MnvRR,        /* mvn [11101010011011110] rd[11-8] [0000]
+                                   rm[3..0] */
+    kThumb2RsubRRI8,     /* rsub [111100011100] rn[19..16] [0000] rd[11..8]
+                                   imm8[7..0] */
+    kThumb2NegRR,        /* actually rsub rd, rn, #0 */
+    kThumb2OrrRRR,       /* orr [111010100100] rn[19..16] [0000] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2TstRR,        /* tst [111010100001] rn[19..16] [0000] [1111]
+                                   [0000] rm[3..0] */
+    kThumb2LslRRR,       /* lsl [111110100000] rn[19..16] [1111] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2LsrRRR,       /* lsr [111110100010] rn[19..16] [1111] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2AsrRRR,       /* asr [111110100100] rn[19..16] [1111] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2RorRRR,       /* ror [111110100110] rn[19..16] [1111] rd[11..8]
+                                   [0000] rm[3..0] */
+    kThumb2LslRRI5,      /* lsl [11101010010011110] imm[14.12] rd[11..8]
+                                   [00] rm[3..0] */
+    kThumb2LsrRRI5,      /* lsr [11101010010011110] imm[14.12] rd[11..8]
+                                   [01] rm[3..0] */
+    kThumb2AsrRRI5,      /* asr [11101010010011110] imm[14.12] rd[11..8]
+                                   [10] rm[3..0] */
+    kThumb2RorRRI5,      /* ror [11101010010011110] imm[14.12] rd[11..8]
+                                   [11] rm[3..0] */
+    kThumb2BicRRI8,      /* bic [111100000010] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2AndRRI8,      /* bic [111100000000] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2OrrRRI8,      /* orr [111100000100] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2EorRRI8,      /* eor [111100001000] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2AddRRI8,      /* add [111100001000] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2AdcRRI8,      /* adc [111100010101] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2SubRRI8,      /* sub [111100011011] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2SbcRRI8,      /* sbc [111100010111] rn[19..16] [0] imm3
+                                   rd[11..8] imm8 */
+    kThumb2It,           /* it [10111111] firstcond[7-4] mask[3-0] */
+    kThumb2Fmstat,       /* fmstat [11101110111100011111101000010000] */
+    kThumb2Vcmpd,        /* vcmp [111011101] D [11011] rd[15-12] [1011]
+                                   E [1] M [0] rm[3-0] */
+    kThumb2Vcmps,        /* vcmp [111011101] D [11010] rd[15-12] [1011]
+                                   E [1] M [0] rm[3-0] */
+    kThumb2LdrPcRel12,   /* ldr rd,[pc,#imm12] [1111100011011111] rt[15-12]
+                                  imm12[11-0] */
+    kThumb2BCond,        /* b<c> [1110] S cond[25-22] imm6[21-16] [10]
+                                  J1 [0] J2 imm11[10..0] */
+    kThumb2Vmovd_RR,     /* vmov [111011101] D [110000] vd[15-12 [101101]
+                                  M [0] vm[3-0] */
+    kThumb2Vmovs_RR,     /* vmov [111011101] D [110000] vd[15-12 [101001]
+                                  M [0] vm[3-0] */
+    kThumb2Fmrs,         /* vmov [111011100000] vn[19-16] rt[15-12] [1010]
+                                  N [0010000] */
+    kThumb2Fmsr,         /* vmov [111011100001] vn[19-16] rt[15-12] [1010]
+                                  N [0010000] */
+    kThumb2Fmrrd,        /* vmov [111011000100] rt2[19-16] rt[15-12]
+                                  [101100] M [1] vm[3-0] */
+    kThumb2Fmdrr,        /* vmov [111011000101] rt2[19-16] rt[15-12]
+                                  [101100] M [1] vm[3-0] */
+    kThumb2Vabsd,        /* vabs.f64 [111011101] D [110000] rd[15-12]
+                                  [1011110] M [0] vm[3-0] */
+    kThumb2Vabss,        /* vabs.f32 [111011101] D [110000] rd[15-12]
+                                  [1010110] M [0] vm[3-0] */
+    kThumb2Vnegd,        /* vneg.f64 [111011101] D [110000] rd[15-12]
+                                  [1011110] M [0] vm[3-0] */
+    kThumb2Vnegs,        /* vneg.f32 [111011101] D [110000] rd[15-12]
+                                 [1010110] M [0] vm[3-0] */
+    kThumb2Vmovs_IMM8,   /* vmov.f32 [111011101] D [11] imm4h[19-16] vd[15-12]
+                                  [10100000] imm4l[3-0] */
+    kThumb2Vmovd_IMM8,   /* vmov.f64 [111011101] D [11] imm4h[19-16] vd[15-12]
+                                  [10110000] imm4l[3-0] */
+    kThumb2Mla,          /* mla [111110110000] rn[19-16] ra[15-12] rd[7-4]
+                                  [0000] rm[3-0] */
+    kThumb2Umull,        /* umull [111110111010] rn[19-16], rdlo[15-12]
+                                  rdhi[11-8] [0000] rm[3-0] */
+    kThumb2Ldrex,        /* ldrex [111010000101] rn[19-16] rt[11-8] [1111]
+                                  imm8[7-0] */
+    kThumb2Strex,        /* strex [111010000100] rn[19-16] rt[11-8] rd[11-8]
+                                  imm8[7-0] */
+    kThumb2Clrex,        /* clrex [111100111011111110000111100101111] */
+    kThumb2Bfi,          /* bfi [111100110110] rn[19-16] [0] imm3[14-12]
+                                  rd[11-8] imm2[7-6] [0] msb[4-0] */
+    kThumb2Bfc,          /* bfc [11110011011011110] [0] imm3[14-12]
+                                  rd[11-8] imm2[7-6] [0] msb[4-0] */
+
+    kArmLast,
 } ArmOpCode;
 
 /* Bit flags describing the behavior of each native opcode */
 typedef enum ArmOpFeatureFlags {
-    IS_BRANCH =           1 << 1,
-    CLOBBER_DEST =        1 << 2,
-    CLOBBER_SRC1 =        1 << 3,
-    NO_OPERAND =          1 << 4,
-    IS_UNARY_OP =         1 << 5,
-    IS_BINARY_OP =        1 << 6,
-    IS_TERTIARY_OP =      1 << 7,
+    kIsBranch = 0,
+    kRegDef0,
+    kRegDef1,
+    kRegDefSP,
+    kRegDefLR,
+    kRegDefList0,
+    kRegDefList1,
+    kRegUse0,
+    kRegUse1,
+    kRegUse2,
+    kRegUse3,
+    kRegUseSP,
+    kRegUsePC,
+    kRegUseList0,
+    kRegUseList1,
+    kNoOperand,
+    kIsUnaryOp,
+    kIsBinaryOp,
+    kIsTertiaryOp,
+    kIsQuadOp,
+    kIsIT,
+    kSetsCCodes,
+    kUsesCCodes,
+    kMemLoad,
+    kMemStore,
 } ArmOpFeatureFlags;
 
+#define IS_LOAD         (1 << kMemLoad)
+#define IS_STORE        (1 << kMemStore)
+#define IS_BRANCH       (1 << kIsBranch)
+#define REG_DEF0        (1 << kRegDef0)
+#define REG_DEF1        (1 << kRegDef1)
+#define REG_DEF_SP      (1 << kRegDefSP)
+#define REG_DEF_LR      (1 << kRegDefLR)
+#define REG_DEF_LIST0   (1 << kRegDefList0)
+#define REG_DEF_LIST1   (1 << kRegDefList1)
+#define REG_USE0        (1 << kRegUse0)
+#define REG_USE1        (1 << kRegUse1)
+#define REG_USE2        (1 << kRegUse2)
+#define REG_USE3        (1 << kRegUse3)
+#define REG_USE_SP      (1 << kRegUseSP)
+#define REG_USE_PC      (1 << kRegUsePC)
+#define REG_USE_LIST0   (1 << kRegUseList0)
+#define REG_USE_LIST1   (1 << kRegUseList1)
+#define NO_OPERAND      (1 << kNoOperand)
+#define IS_UNARY_OP     (1 << kIsUnaryOp)
+#define IS_BINARY_OP    (1 << kIsBinaryOp)
+#define IS_TERTIARY_OP  (1 << kIsTertiaryOp)
+#define IS_QUAD_OP      (1 << kIsQuadOp)
+#define IS_IT           (1 << kIsIT)
+#define SETS_CCODES     (1 << kSetsCCodes)
+#define USES_CCODES     (1 << kUsesCCodes)
+
+/* Common combo register usage patterns */
+#define REG_USE01       (REG_USE0 | REG_USE1)
+#define REG_USE012      (REG_USE01 | REG_USE2)
+#define REG_USE12       (REG_USE1 | REG_USE2)
+#define REG_DEF0_USE0   (REG_DEF0 | REG_USE0)
+#define REG_DEF0_USE1   (REG_DEF0 | REG_USE1)
+#define REG_DEF0_USE01  (REG_DEF0 | REG_USE01)
+#define REG_DEF0_USE12  (REG_DEF0 | REG_USE12)
+#define REG_DEF01_USE2  (REG_DEF0 | REG_DEF1 | REG_USE2)
+
 /* Instruction assembly fieldLoc kind */
 typedef enum ArmEncodingKind {
-    UNUSED,
-    BITBLT,        /* Bit string using end/start */
-    DFP,           /* Double FP reg */
-    SFP,           /* Single FP reg */
-    MODIMM,        /* Shifted 8-bit immediate using [26,14..12,7..0] */
-    IMM16,         /* Zero-extended immediate using [26,19..16,14..12,7..0] */
-    IMM6,          /* Encoded branch target using [9,7..3]0 */
-    IMM12,         /* Zero-extended immediate using [26,14..12,7..0] */
+    kFmtUnused,
+    kFmtBitBlt,        /* Bit string using end/start */
+    kFmtDfp,           /* Double FP reg */
+    kFmtSfp,           /* Single FP reg */
+    kFmtModImm,        /* Shifted 8-bit immed using [26,14..12,7..0] */
+    kFmtImm16,         /* Zero-extended immed using [26,19..16,14..12,7..0] */
+    kFmtImm6,          /* Encoded branch target using [9,7..3]0 */
+    kFmtImm12,         /* Zero-extended immediate using [26,14..12,7..0] */
+    kFmtShift,         /* Shift descriptor, [14..12,7..4] */
+    kFmtLsb,           /* least significant bit using [14..12][7..6] */
+    kFmtBWidth,        /* bit-field width, encoded as width-1 */
+    kFmtShift5,        /* Shift count, [14..12,7..6] */
+    kFmtBrOffset,      /* Signed extended [26,11,13,21-16,10-0]:0 */
+    kFmtFPImm,         /* Encoded floating point immediate */
 } ArmEncodingKind;
 
 /* Struct used to define the snippet positions for each Thumb opcode */
@@ -334,9 +714,9 @@ typedef struct ArmEncodingMap {
     u4 skeleton;
     struct {
         ArmEncodingKind kind;
-        int end;   /* end for BITBLT, 1-bit slice end for FP regs */
-        int start; /* start for BITBLT, 4-bit slice end for FP regs */
-    } fieldLoc[3];
+        int end;   /* end for kFmtBitBlt, 1-bit slice end for FP regs */
+        int start; /* start for kFmtBitBlt, 4-bit slice end for FP regs */
+    } fieldLoc[4];
     ArmOpCode opCode;
     int flags;
     char *name;
@@ -344,38 +724,50 @@ typedef struct ArmEncodingMap {
     int size;
 } ArmEncodingMap;
 
-extern ArmEncodingMap EncodingMap[ARM_LAST];
+/* Keys for target-specific scheduling and other optimization hints */
+typedef enum ArmTargetOptHints {
+    kMaxHoistDistance,
+} ArmTargetOptHints;
+
+extern ArmEncodingMap EncodingMap[kArmLast];
 
 /*
  * Each instance of this struct holds a pseudo or real LIR instruction:
- * - pesudo ones (eg labels and marks) and will be discarded by the assembler.
- * - real ones will e assembled into Thumb instructions.
+ * - pseudo ones (eg labels and marks) and will be discarded by the assembler.
+ * - real ones will be assembled into Thumb instructions.
+ *
+ * Machine resources are encoded into a 64-bit vector, where the encodings are
+ * as following:
+ * - [ 0..15]: general purpose registers including PC, SP, and LR
+ * - [16..47]: floating-point registers where d0 is expanded to s[01] and s0
+ *   starts at bit 16
+ * - [48]: IT block
+ * - [49]: integer condition code
+ * - [50]: floatint-point status word
  */
 typedef struct ArmLIR {
     LIR generic;
     ArmOpCode opCode;
-    int operands[3];    // [0..2] = [dest, src1, src2]
+    int operands[4];    // [0..3] = [dest, src1, src2, extra]
     bool isNop;         // LIR is optimized away
+    bool branchInsertSV;// mark for insertion of branch before this instruction,
+                        // used to identify mem ops for self verification mode
     int age;            // default is 0, set lazily by the optimizer
     int size;           // 16-bit unit size (1 for thumb, 1 or 2 for thumb2)
+    int aliasInfo;      // For Dalvik register access & litpool disambiguation
+    u8 useMask;         // Resource mask for use
+    u8 defMask;         // Resource mask for def
 } ArmLIR;
 
-/* Chain cell for predicted method invocation */
-typedef struct PredictedChainingCell {
-    u4 branch;                  /* Branch to chained destination */
-    const ClassObject *clazz;   /* key #1 for prediction */
-    const Method *method;       /* key #2 to lookup native PC from dalvik PC */
-    u4 counter;                 /* counter to patch the chaining cell */
-} PredictedChainingCell;
-
 /* Init values when a predicted chain is initially assembled */
-#define PREDICTED_CHAIN_BX_PAIR_INIT     0
+/* E7FE is branch to self */
+#define PREDICTED_CHAIN_BX_PAIR_INIT     0xe7fe
 #define PREDICTED_CHAIN_CLAZZ_INIT       0
 #define PREDICTED_CHAIN_METHOD_INIT      0
 #define PREDICTED_CHAIN_COUNTER_INIT     0
 
 /* Used when the callee is not compiled yet */
-#define PREDICTED_CHAIN_COUNTER_DELAY    16
+#define PREDICTED_CHAIN_COUNTER_DELAY    512
 
 /* Rechain after this many mis-predictions have happened */
 #define PREDICTED_CHAIN_COUNTER_RECHAIN  1024
diff --git a/vm/compiler/codegen/arm/Assemble.c b/vm/compiler/codegen/arm/Assemble.c
index ea133e7..1951e07 100644
--- a/vm/compiler/codegen/arm/Assemble.c
+++ b/vm/compiler/codegen/arm/Assemble.c
@@ -36,12 +36,12 @@
  * s2e: src2 end bit position
  * operands: number of operands (for sanity check purposes)
  * name: mnemonic name
- * fmt: for pretty-prining
+ * fmt: for pretty-printing
  */
 #define ENCODING_MAP(opcode, skeleton, k0, ds, de, k1, s1s, s1e, k2, s2s, s2e, \
-                     operands, name, fmt, size) \
-        {skeleton, {{k0, ds, de}, {k1, s1s, s1e}, {k2, s2s, s2e}}, \
-         opcode, operands, name, fmt, size}
+                     k3, k3s, k3e, flags, name, fmt, size) \
+        {skeleton, {{k0, ds, de}, {k1, s1s, s1e}, {k2, s2s, s2e}, \
+                    {k3, k3s, k3e}}, opcode, flags, name, fmt, size}
 
 /* Instruction dump string format keys: !pf, where "!" is the start
  * of the key, "p" is which numeric operand to use and "f" is the
@@ -51,11 +51,11 @@
  *     0 -> operands[0] (dest)
  *     1 -> operands[1] (src1)
  *     2 -> operands[2] (src2)
+ *     3 -> operands[3] (extra)
  *
  * [f]ormats:
  *     h -> 4-digit hex
  *     d -> decimal
- *     D -> decimal+8 (used to convert 3-bit regnum field to high reg)
  *     E -> decimal*4
  *     F -> decimal*2
  *     c -> branch condition (beq, bne, etc.)
@@ -66,436 +66,816 @@
  *     s -> single precision floating point register
  *     S -> double precision floating point register
  *     m -> Thumb2 modified immediate
+ *     n -> complimented Thumb2 modified immediate
  *     M -> Thumb2 16-bit zero-extended immediate
+ *     b -> 4-digit binary
  *
  *  [!] escape.  To insert "!", use "!!"
  */
 /* NOTE: must be kept in sync with enum ArmOpcode from ArmLIR.h */
-ArmEncodingMap EncodingMap[ARM_LAST] = {
-    ENCODING_MAP(ARM_16BIT_DATA,    0x0000,
-                 BITBLT, 15, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP,
-                 "data", "0x!0h(!0d)", 1),
-    ENCODING_MAP(THUMB_ADC,           0x4140,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "adc", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_ADD_RRI3,      0x1c00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "add", "r!0d, r!1d, #!2d", 1),
-    ENCODING_MAP(THUMB_ADD_RI8,       0x3000,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "add", "r!0d, r!0d, #!1d", 1),
-    ENCODING_MAP(THUMB_ADD_RRR,       0x1800,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "add", "r!0d, r!1d, r!2d", 1),
-    ENCODING_MAP(THUMB_ADD_RR_LH,     0x4440,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "add",
-                 "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_ADD_RR_HL,     0x4480,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+ArmEncodingMap EncodingMap[kArmLast] = {
+    ENCODING_MAP(kArm16BitData,    0x0000,
+                 kFmtBitBlt, 15, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP, "data", "0x!0h(!0d)", 1),
+    ENCODING_MAP(kThumbAdcRR,        0x4140,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES | USES_CCODES,
+                 "adcs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbAddRRI3,      0x1c00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "adds", "r!0d, r!1d, #!2d", 1),
+    ENCODING_MAP(kThumbAddRI8,       0x3000,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE0 | SETS_CCODES,
+                 "adds", "r!0d, r!0d, #!1d", 1),
+    ENCODING_MAP(kThumbAddRRR,       0x1800,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE12 | SETS_CCODES,
+                 "adds", "r!0d, r!1d, r!2d", 1),
+    ENCODING_MAP(kThumbAddRRLH,     0x4440,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE01,
                  "add", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_ADD_RR_HH,     0x44c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbAddRRHL,     0x4480,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE01,
                  "add", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_ADD_PC_REL,    0xa000,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbAddRRHH,     0x44c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE01,
+                 "add", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbAddPcRel,    0xa000,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | IS_BRANCH,
                  "add", "r!0d, pc, #!1E", 1),
-    ENCODING_MAP(THUMB_ADD_SP_REL,    0xa800,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "add", "r!0d, sp, #!1E", 1),
-    ENCODING_MAP(THUMB_ADD_SPI7,      0xb000,
-                 BITBLT, 6, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbAddSpRel,    0xa800,
+                 kFmtBitBlt, 10, 8, kFmtUnused, -1, -1, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF_SP | REG_USE_SP,
+                 "add", "r!0d, sp, #!2E", 1),
+    ENCODING_MAP(kThumbAddSpI7,      0xb000,
+                 kFmtBitBlt, 6, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP | REG_DEF_SP | REG_USE_SP,
                  "add", "sp, #!0d*4", 1),
-    ENCODING_MAP(THUMB_AND_RR,        0x4000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "and", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_ASR,           0x1000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "asr", "r!0d, r!1d, #!2d", 1),
-    ENCODING_MAP(THUMB_ASRV,          0x4100,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "asr", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_B_COND,        0xd000,
-                 BITBLT, 7, 0, BITBLT, 11, 8, UNUSED, -1, -1,
-                 IS_BINARY_OP | IS_BRANCH,
-                 "!1c", "!0t", 1),
-    ENCODING_MAP(THUMB_B_UNCOND,      0xe000,
-                 BITBLT, 10, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 NO_OPERAND | IS_BRANCH,
+    ENCODING_MAP(kThumbAndRR,        0x4000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "ands", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbAsrRRI5,      0x1000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "asrs", "r!0d, r!1d, #!2d", 1),
+    ENCODING_MAP(kThumbAsrRR,        0x4100,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "asrs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbBCond,        0xd000,
+                 kFmtBitBlt, 7, 0, kFmtBitBlt, 11, 8, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | IS_BRANCH | USES_CCODES,
+                 "b!1c", "!0t", 1),
+    ENCODING_MAP(kThumbBUncond,      0xe000,
+                 kFmtBitBlt, 10, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, NO_OPERAND | IS_BRANCH,
                  "b", "!0t", 1),
-    ENCODING_MAP(THUMB_BIC,           0x4380,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "bic", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_BKPT,          0xbe00,
-                 BITBLT, 7, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBicRR,        0x4380,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "bics", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbBkpt,          0xbe00,
+                 kFmtBitBlt, 7, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP | IS_BRANCH,
                  "bkpt", "!0d", 1),
-    ENCODING_MAP(THUMB_BLX_1,         0xf000,
-                 BITBLT, 10, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_BINARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBlx1,         0xf000,
+                 kFmtBitBlt, 10, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | IS_BRANCH | REG_DEF_LR,
                  "blx_1", "!0u", 1),
-    ENCODING_MAP(THUMB_BLX_2,         0xe800,
-                 BITBLT, 10, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_BINARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBlx2,         0xe800,
+                 kFmtBitBlt, 10, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | IS_BRANCH | REG_DEF_LR,
                  "blx_2", "!0v", 1),
-    ENCODING_MAP(THUMB_BL_1,          0xf000,
-                 BITBLT, 10, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBl1,          0xf000,
+                 kFmtBitBlt, 10, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP | IS_BRANCH | REG_DEF_LR,
                  "bl_1", "!0u", 1),
-    ENCODING_MAP(THUMB_BL_2,          0xf800,
-                 BITBLT, 10, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBl2,          0xf800,
+                 kFmtBitBlt, 10, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP | IS_BRANCH | REG_DEF_LR,
                  "bl_2", "!0v", 1),
-    ENCODING_MAP(THUMB_BLX_R,         0x4780,
-                 BITBLT, 6, 3, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBlxR,         0x4780,
+                 kFmtBitBlt, 6, 3, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_UNARY_OP | REG_USE0 | IS_BRANCH | REG_DEF_LR,
                  "blx", "r!0d", 1),
-    ENCODING_MAP(THUMB_BX,            0x4700,
-                 BITBLT, 6, 3, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbBx,            0x4700,
+                 kFmtBitBlt, 6, 3, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP | IS_BRANCH,
                  "bx", "r!0d", 1),
-    ENCODING_MAP(THUMB_CMN,           0x42c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP,
+    ENCODING_MAP(kThumbCmnRR,        0x42c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01 | SETS_CCODES,
                  "cmn", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_CMP_RI8,       0x2800,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP,
+    ENCODING_MAP(kThumbCmpRI8,       0x2800,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE0 | SETS_CCODES,
                  "cmp", "r!0d, #!1d", 1),
-    ENCODING_MAP(THUMB_CMP_RR,        0x4280,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP,
+    ENCODING_MAP(kThumbCmpRR,        0x4280,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01 | SETS_CCODES,
+                 "cmp", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbCmpLH,        0x4540,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01 | SETS_CCODES,
+                 "cmp", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbCmpHL,        0x4580,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01 | SETS_CCODES,
+                 "cmp", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbCmpHH,        0x45c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01 | SETS_CCODES,
                  "cmp", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_CMP_LH,        0x4540,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP,
-                 "cmp", "r!0d, r!1D", 1),
-    ENCODING_MAP(THUMB_CMP_HL,        0x4580,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP,
-                 "cmp", "r!0D, r!1d", 1),
-    ENCODING_MAP(THUMB_CMP_HH,        0x45c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP,
-                 "cmp", "r!0D, r!1D", 1),
-    ENCODING_MAP(THUMB_EOR,           0x4040,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "eor", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_LDMIA,         0xc800,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST | CLOBBER_SRC1,
+    ENCODING_MAP(kThumbEorRR,        0x4040,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "eors", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbLdmia,         0xc800,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE0 | REG_DEF_LIST1 | IS_LOAD,
                  "ldmia", "r!0d!!, <!1R>", 1),
-    ENCODING_MAP(THUMB_LDR_RRI5,      0x6800,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrRRI5,      0x6800,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "ldr", "r!0d, [r!1d, #!2E]", 1),
-    ENCODING_MAP(THUMB_LDR_RRR,       0x5800,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrRRR,       0x5800,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12 | IS_LOAD,
                  "ldr", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_LDR_PC_REL,    0x4800,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "ldr", "r!0d, [pc, #!1E]", 1),
-    ENCODING_MAP(THUMB_LDR_SP_REL,    0x9800,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "ldr", "r!0d, [sp, #!1E]", 1),
-    ENCODING_MAP(THUMB_LDRB_RRI5,     0x7800,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrPcRel,    0x4800,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0 | REG_USE_PC
+                 | IS_LOAD, "ldr", "r!0d, [pc, #!1E]", 1),
+    ENCODING_MAP(kThumbLdrSpRel,    0x9800,
+                 kFmtBitBlt, 10, 8, kFmtUnused, -1, -1, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0 | REG_USE_SP
+                 | IS_LOAD, "ldr", "r!0d, [sp, #!2E]", 1),
+    ENCODING_MAP(kThumbLdrbRRI5,     0x7800,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "ldrb", "r!0d, [r!1d, #2d]", 1),
-    ENCODING_MAP(THUMB_LDRB_RRR,      0x5c00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrbRRR,      0x5c00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12 | IS_LOAD,
                  "ldrb", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_LDRH_RRI5,     0x8800,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrhRRI5,     0x8800,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "ldrh", "r!0d, [r!1d, #!2F]", 1),
-    ENCODING_MAP(THUMB_LDRH_RRR,      0x5a00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrhRRR,      0x5a00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12 | IS_LOAD,
                  "ldrh", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_LDRSB_RRR,     0x5600,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrsbRRR,     0x5600,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12 | IS_LOAD,
                  "ldrsb", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_LDRSH_RRR,     0x5e00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLdrshRRR,     0x5e00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12 | IS_LOAD,
                  "ldrsh", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_LSL,           0x0000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "lsl", "r!0d, r!1d, #!2d", 1),
-    ENCODING_MAP(THUMB_LSLV,          0x4080,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "lsl", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_LSR,           0x0800,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "lsr", "r!0d, r!1d, #!2d", 1),
-    ENCODING_MAP(THUMB_LSRV,          0x40c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "lsr", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_MOV_IMM,       0x2000,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "mov", "r!0d, #!1d", 1),
-    ENCODING_MAP(THUMB_MOV_RR,        0x1c00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbLslRRI5,      0x0000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "lsls", "r!0d, r!1d, #!2d", 1),
+    ENCODING_MAP(kThumbLslRR,        0x4080,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "lsls", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbLsrRRI5,      0x0800,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "lsrs", "r!0d, r!1d, #!2d", 1),
+    ENCODING_MAP(kThumbLsrRR,        0x40c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "lsrs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbMovImm,       0x2000,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0 | SETS_CCODES,
+                 "movs", "r!0d, #!1d", 1),
+    ENCODING_MAP(kThumbMovRR,        0x1c00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "movs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbMovRR_H2H,    0x46c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "mov", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_MOV_RR_H2H,    0x46c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "mov", "r!0D, r!1D", 1),
-    ENCODING_MAP(THUMB_MOV_RR_H2L,    0x4640,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "mov", "r!0d, r!1D", 1),
-    ENCODING_MAP(THUMB_MOV_RR_L2H,    0x4680,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "mov", "r!0D, r!1d", 1),
-    ENCODING_MAP(THUMB_MUL,           0x4340,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "mul", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_MVN,           0x43c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "mvn", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_NEG,           0x4240,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "neg", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_ORR,           0x4300,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "orr", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_POP,           0xbc00,
-                 BITBLT, 8, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP,
-                 "pop", "<!0R>", 1),
-    ENCODING_MAP(THUMB_PUSH,          0xb400,
-                 BITBLT, 8, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP,
-                 "push", "<!0R>", 1),
-    ENCODING_MAP(THUMB_ROR,           0x41c0,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "ror", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_SBC,           0x4180,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "sbc", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB_STMIA,         0xc000,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_SRC1,
+    ENCODING_MAP(kThumbMovRR_H2L,    0x4640,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "mov", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbMovRR_L2H,    0x4680,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "mov", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbMul,           0x4340,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "muls", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbMvn,           0x43c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "mvns", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbNeg,           0x4240,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "negs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbOrr,           0x4300,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "orrs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbPop,           0xbc00,
+                 kFmtBitBlt, 8, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_UNARY_OP | REG_DEF_SP | REG_USE_SP | REG_DEF_LIST0
+                 | IS_LOAD, "pop", "<!0R>", 1),
+    ENCODING_MAP(kThumbPush,          0xb400,
+                 kFmtBitBlt, 8, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_UNARY_OP | REG_DEF_SP | REG_USE_SP | REG_USE_LIST0
+                 | IS_STORE, "push", "<!0R>", 1),
+    ENCODING_MAP(kThumbRorRR,        0x41c0,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | SETS_CCODES,
+                 "rors", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbSbc,           0x4180,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE01 | USES_CCODES | SETS_CCODES,
+                 "sbcs", "r!0d, r!1d", 1),
+    ENCODING_MAP(kThumbStmia,         0xc000,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0 | REG_USE0 | REG_USE_LIST1 | IS_STORE,
                  "stmia", "r!0d!!, <!1R>", 1),
-    ENCODING_MAP(THUMB_STR_RRI5,      0x6000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumbStrRRI5,      0x6000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "str", "r!0d, [r!1d, #!2E]", 1),
-    ENCODING_MAP(THUMB_STR_RRR,       0x5000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumbStrRRR,       0x5000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE012 | IS_STORE,
                  "str", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_STR_SP_REL,    0x9000,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP,
-                 "str", "r!0d, [sp, #!1E]", 1),
-    ENCODING_MAP(THUMB_STRB_RRI5,     0x7000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumbStrSpRel,    0x9000,
+                 kFmtBitBlt, 10, 8, kFmtUnused, -1, -1, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE0 | REG_USE_SP
+                 | IS_STORE, "str", "r!0d, [sp, #!2E]", 1),
+    ENCODING_MAP(kThumbStrbRRI5,     0x7000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "strb", "r!0d, [r!1d, #!2d]", 1),
-    ENCODING_MAP(THUMB_STRB_RRR,      0x5400,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumbStrbRRR,      0x5400,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE012 | IS_STORE,
                  "strb", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_STRH_RRI5,     0x8000,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 10, 6,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumbStrhRRI5,     0x8000,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 10, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "strh", "r!0d, [r!1d, #!2F]", 1),
-    ENCODING_MAP(THUMB_STRH_RRR,      0x5200,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumbStrhRRR,      0x5200,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE012 | IS_STORE,
                  "strh", "r!0d, [r!1d, r!2d]", 1),
-    ENCODING_MAP(THUMB_SUB_RRI3,      0x1e00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "sub", "r!0d, r!1d, #!2d]", 1),
-    ENCODING_MAP(THUMB_SUB_RI8,       0x3800,
-                 BITBLT, 10, 8, BITBLT, 7, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "sub", "r!0d, #!1d", 1),
-    ENCODING_MAP(THUMB_SUB_RRR,       0x1a00,
-                 BITBLT, 2, 0, BITBLT, 5, 3, BITBLT, 8, 6,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "sub", "r!0d, r!1d, r!2d", 1),
-    ENCODING_MAP(THUMB_SUB_SPI7,      0xb080,
-                 BITBLT, 6, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumbSubRRI3,      0x1e00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "subs", "r!0d, r!1d, #!2d]", 1),
+    ENCODING_MAP(kThumbSubRI8,       0x3800,
+                 kFmtBitBlt, 10, 8, kFmtBitBlt, 7, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE0 | SETS_CCODES,
+                 "subs", "r!0d, #!1d", 1),
+    ENCODING_MAP(kThumbSubRRR,       0x1a00,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtBitBlt, 8, 6,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE12 | SETS_CCODES,
+                 "subs", "r!0d, r!1d, r!2d", 1),
+    ENCODING_MAP(kThumbSubSpI7,      0xb080,
+                 kFmtBitBlt, 6, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_UNARY_OP | REG_DEF_SP | REG_USE_SP,
                  "sub", "sp, #!0d", 1),
-    ENCODING_MAP(THUMB_SWI,           0xdf00,
-                 BITBLT, 7, 0, UNUSED, -1, -1, UNUSED, -1, -1,
-                 IS_UNARY_OP | IS_BRANCH,
+    ENCODING_MAP(kThumbSwi,           0xdf00,
+                 kFmtBitBlt, 7, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,                       kFmtUnused, -1, -1, IS_UNARY_OP | IS_BRANCH,
                  "swi", "!0d", 1),
-    ENCODING_MAP(THUMB_TST,           0x4200,
-                 BITBLT, 2, 0, BITBLT, 5, 3, UNUSED, -1, -1,
-                 IS_UNARY_OP,
+    ENCODING_MAP(kThumbTst,           0x4200,
+                 kFmtBitBlt, 2, 0, kFmtBitBlt, 5, 3, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_UNARY_OP | REG_USE01 | SETS_CCODES,
                  "tst", "r!0d, r!1d", 1),
-    ENCODING_MAP(THUMB2_VLDRS,       0xed900a00,
-                 SFP, 22, 12, BITBLT, 19, 16, BITBLT, 7, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vldrs,       0xed900a00,
+                 kFmtSfp, 22, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "vldr", "!0s, [r!1d, #!2E]", 2),
-    ENCODING_MAP(THUMB2_VLDRD,       0xed900b00,
-                 DFP, 22, 12, BITBLT, 19, 16, BITBLT, 7, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vldrd,       0xed900b00,
+                 kFmtDfp, 22, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "vldr", "!0S, [r!1d, #!2E]", 2),
-    ENCODING_MAP(THUMB2_VMULS,        0xee200a00,
-                 SFP, 22, 12, SFP, 7, 16, SFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vmuls,        0xee200a00,
+                 kFmtSfp, 22, 12, kFmtSfp, 7, 16, kFmtSfp, 5, 0,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vmuls", "!0s, !1s, !2s", 2),
-    ENCODING_MAP(THUMB2_VMULD,        0xee200b00,
-                 DFP, 22, 12, DFP, 7, 16, DFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vmuld,        0xee200b00,
+                 kFmtDfp, 22, 12, kFmtDfp, 7, 16, kFmtDfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vmuld", "!0S, !1S, !2S", 2),
-    ENCODING_MAP(THUMB2_VSTRS,       0xed800a00,
-                 SFP, 22, 12, BITBLT, 19, 16, BITBLT, 7, 0,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumb2Vstrs,       0xed800a00,
+                 kFmtSfp, 22, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "vstr", "!0s, [r!1d, #!2E]", 2),
-    ENCODING_MAP(THUMB2_VSTRD,       0xed800b00,
-                 DFP, 22, 12, BITBLT, 19, 16, BITBLT, 7, 0,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumb2Vstrd,       0xed800b00,
+                 kFmtDfp, 22, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "vstr", "!0S, [r!1d, #!2E]", 2),
-    ENCODING_MAP(THUMB2_VSUBS,        0xee300a40,
-                 SFP, 22, 12, SFP, 7, 16, SFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vsubs,        0xee300a40,
+                 kFmtSfp, 22, 12, kFmtSfp, 7, 16, kFmtSfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vsub", "!0s, !1s, !2s", 2),
-    ENCODING_MAP(THUMB2_VSUBD,        0xee300b40,
-                 DFP, 22, 12, DFP, 7, 16, DFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vsubd,        0xee300b40,
+                 kFmtDfp, 22, 12, kFmtDfp, 7, 16, kFmtDfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vsub", "!0S, !1S, !2S", 2),
-    ENCODING_MAP(THUMB2_VADDS,        0xee300a00,
-                 SFP, 22, 12, SFP, 7, 16, SFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vadds,        0xee300a00,
+                 kFmtSfp, 22, 12, kFmtSfp, 7, 16, kFmtSfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vadd", "!0s, !1s, !2s", 2),
-    ENCODING_MAP(THUMB2_VADDD,        0xee300b00,
-                 DFP, 22, 12, DFP, 7, 16, DFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vaddd,        0xee300b00,
+                 kFmtDfp, 22, 12, kFmtDfp, 7, 16, kFmtDfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vadd", "!0S, !1S, !2S", 2),
-    ENCODING_MAP(THUMB2_VDIVS,        0xee800a00,
-                 SFP, 22, 12, SFP, 7, 16, SFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vdivs,        0xee800a00,
+                 kFmtSfp, 22, 12, kFmtSfp, 7, 16, kFmtSfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
                  "vdivs", "!0s, !1s, !2s", 2),
-    ENCODING_MAP(THUMB2_VDIVD,        0xee800b00,
-                 DFP, 22, 12, DFP, 7, 16, DFP, 5, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
-                 "vdivs", "!0S, !1S, !2S", 2),
-    ENCODING_MAP(THUMB2_VCVTIF,       0xeeb80ac0,
-                 SFP, 22, 12, SFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vdivd,        0xee800b00,
+                 kFmtDfp, 22, 12, kFmtDfp, 7, 16, kFmtDfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "vdivd", "!0S, !1S, !2S", 2),
+    ENCODING_MAP(kThumb2VcvtIF,       0xeeb80ac0,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vcvt.f32", "!0s, !1s", 2),
-    ENCODING_MAP(THUMB2_VCVTID,       0xeeb80bc0,
-                 DFP, 22, 12, SFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2VcvtID,       0xeeb80bc0,
+                 kFmtDfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vcvt.f64", "!0S, !1s", 2),
-    ENCODING_MAP(THUMB2_VCVTFI,       0xeebd0ac0,
-                 SFP, 22, 12, SFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2VcvtFI,       0xeebd0ac0,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vcvt.s32.f32 ", "!0s, !1s", 2),
-    ENCODING_MAP(THUMB2_VCVTDI,       0xeebd0bc0,
-                 SFP, 22, 12, DFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2VcvtDI,       0xeebd0bc0,
+                 kFmtSfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vcvt.s32.f64 ", "!0s, !1S", 2),
-    ENCODING_MAP(THUMB2_VCVTFD,       0xeeb70ac0,
-                 DFP, 22, 12, SFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2VcvtFd,       0xeeb70ac0,
+                 kFmtDfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vcvt.f64.f32 ", "!0S, !1s", 2),
-    ENCODING_MAP(THUMB2_VCVTDF,       0xeeb70bc0,
-                 SFP, 22, 12, DFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2VcvtDF,       0xeeb70bc0,
+                 kFmtSfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vcvt.f32.f64 ", "!0s, !1S", 2),
-    ENCODING_MAP(THUMB2_VSQRTS,       0xeeb10ac0,
-                 SFP, 22, 12, SFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vsqrts,       0xeeb10ac0,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vsqrt.f32 ", "!0s, !1s", 2),
-    ENCODING_MAP(THUMB2_VSQRTD,       0xeeb10bc0,
-                 DFP, 22, 12, DFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2Vsqrtd,       0xeeb10bc0,
+                 kFmtDfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "vsqrt.f64 ", "!0S, !1S", 2),
-    ENCODING_MAP(THUMB2_MOV_IMM_SHIFT,       0xf04f0000,
-                 BITBLT, 11, 8, MODIMM, -1, -1, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2MovImmShift, 0xf04f0000, /* no setflags encoding */
+                 kFmtBitBlt, 11, 8, kFmtModImm, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0,
                  "mov", "r!0d, #!1m", 2),
-    ENCODING_MAP(THUMB2_MOV_IMM16,       0xf2400000,
-                 BITBLT, 11, 8, IMM16, -1, -1, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2MovImm16,       0xf2400000,
+                 kFmtBitBlt, 11, 8, kFmtImm16, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0,
                  "mov", "r!0d, #!1M", 2),
-    ENCODING_MAP(THUMB2_STR_RRI12,       0xf8c00000,
-                 BITBLT, 15, 12, BITBLT, 19, 16, BITBLT, 11, 0,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumb2StrRRI12,       0xf8c00000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "str", "r!0d,[r!1d, #!2d", 2),
-    ENCODING_MAP(THUMB2_LDR_RRI12,       0xf8d00000,
-                 BITBLT, 15, 12, BITBLT, 19, 16, BITBLT, 11, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2LdrRRI12,       0xf8d00000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "ldr", "r!0d,[r!1d, #!2d", 2),
-    ENCODING_MAP(THUMB2_STR_RRI8_PREDEC,       0xf8400c00,
-                 BITBLT, 15, 12, BITBLT, 19, 16, BITBLT, 8, 0,
-                 IS_TERTIARY_OP,
+    ENCODING_MAP(kThumb2StrRRI8Predec,       0xf8400c00,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 8, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
                  "str", "r!0d,[r!1d, #-!2d]", 2),
-    ENCODING_MAP(THUMB2_LDR_RRI8_PREDEC,       0xf8500c00,
-                 BITBLT, 15, 12, BITBLT, 19, 16, BITBLT, 8, 0,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2LdrRRI8Predec,       0xf8500c00,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 8, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
                  "ldr", "r!0d,[r!1d, #-!2d]", 2),
-    ENCODING_MAP(THUMB2_CBNZ,       0xb900,
-                 BITBLT, 2, 0, IMM6, -1, -1, UNUSED, -1, -1,
-                 IS_BINARY_OP,
+    ENCODING_MAP(kThumb2Cbnz,       0xb900, /* Note: does not affect flags */
+                 kFmtBitBlt, 2, 0, kFmtImm6, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE0 | IS_BRANCH,
                  "cbnz", "r!0d,!1t", 1),
-    ENCODING_MAP(THUMB2_CBZ,       0xb100,
-                 BITBLT, 2, 0, IMM6, -1, -1, UNUSED, -1, -1,
-                 IS_BINARY_OP,
+    ENCODING_MAP(kThumb2Cbz,       0xb100, /* Note: does not affect flags */
+                 kFmtBitBlt, 2, 0, kFmtImm6, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE0 | IS_BRANCH,
                  "cbz", "r!0d,!1t", 1),
-    ENCODING_MAP(THUMB2_ADD_RRI12,       0xf1000000,
-                 BITBLT, 11, 8, BITBLT, 19, 16, IMM12, -1, -1,
-                 IS_TERTIARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2AddRRI12,       0xf2000000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtImm12, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1,/* Note: doesn't affect flags */
                  "add", "r!0d,r!1d,#!2d", 2),
-    ENCODING_MAP(THUMB2_MOV_RR,       0xea4f0000,
-                 BITBLT, 11, 8, BITBLT, 3, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
+    ENCODING_MAP(kThumb2MovRR,       0xea4f0000, /* no setflags encoding */
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 3, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
                  "mov", "r!0d, r!1d", 2),
-    ENCODING_MAP(THUMB2_VMOVS,       0xeeb00a40,
-                 SFP, 22, 12, SFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "vmov.f32 ", "!0s, !1s", 2),
-    ENCODING_MAP(THUMB2_VMOVD,       0xeeb00b40,
-                 DFP, 22, 12, DFP, 5, 0, UNUSED, -1, -1,
-                 IS_BINARY_OP | CLOBBER_DEST,
-                 "vmov.f64 ", "!0s, !1s", 2),
+    ENCODING_MAP(kThumb2Vmovs,       0xeeb00a40,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vmov.f32 ", " !0s, !1s", 2),
+    ENCODING_MAP(kThumb2Vmovd,       0xeeb00b40,
+                 kFmtDfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vmov.f64 ", " !0S, !1S", 2),
+    ENCODING_MAP(kThumb2Ldmia,         0xe8900000,
+                 kFmtBitBlt, 19, 16, kFmtBitBlt, 15, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE0 | REG_DEF_LIST1 | IS_LOAD,
+                 "ldmia", "r!0d!!, <!1R>", 2),
+    ENCODING_MAP(kThumb2Stmia,         0xe8800000,
+                 kFmtBitBlt, 19, 16, kFmtBitBlt, 15, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE0 | REG_USE_LIST1 | IS_STORE,
+                 "stmia", "r!0d!!, <!1R>", 2),
+    ENCODING_MAP(kThumb2AddRRR,  0xeb100000, /* setflags encoding */
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1,
+                 IS_QUAD_OP | REG_DEF0_USE12 | SETS_CCODES,
+                 "adds", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2SubRRR,       0xebb00000, /* setflags enconding */
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1,
+                 IS_QUAD_OP | REG_DEF0_USE12 | SETS_CCODES,
+                 "subs", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2SbcRRR,       0xeb700000, /* setflags encoding */
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1,
+                 IS_QUAD_OP | REG_DEF0_USE12 | USES_CCODES | SETS_CCODES,
+                 "sbcs", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2CmpRR,       0xebb00f00,
+                 kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0, kFmtShift, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_USE01 | SETS_CCODES,
+                 "cmp", "r!0d, r!1d", 2),
+    ENCODING_MAP(kThumb2SubRRI12,       0xf2a00000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtImm12, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1,/* Note: doesn't affect flags */
+                 "sub", "r!0d,r!1d,#!2d", 2),
+    ENCODING_MAP(kThumb2MvnImmShift,  0xf06f0000, /* no setflags encoding */
+                 kFmtBitBlt, 11, 8, kFmtModImm, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0,
+                 "mvn", "r!0d, #!1n", 2),
+    ENCODING_MAP(kThumb2Sel,       0xfaa0f080,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE12 | USES_CCODES,
+                 "sel", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2Ubfx,       0xf3c00000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtLsb, -1, -1,
+                 kFmtBWidth, 4, 0, IS_QUAD_OP | REG_DEF0_USE1,
+                 "ubfx", "r!0d, r!1d, #!2d, #!3d", 2),
+    ENCODING_MAP(kThumb2Sbfx,       0xf3400000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtLsb, -1, -1,
+                 kFmtBWidth, 4, 0, IS_QUAD_OP | REG_DEF0_USE1,
+                 "sbfx", "r!0d, r!1d, #!2d, #!3d", 2),
+    ENCODING_MAP(kThumb2LdrRRR,    0xf8500000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_DEF0_USE12 | IS_LOAD,
+                 "ldr", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2LdrhRRR,    0xf8300000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_DEF0_USE12 | IS_LOAD,
+                 "ldrh", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2LdrshRRR,    0xf9300000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_DEF0_USE12 | IS_LOAD,
+                 "ldrsh", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2LdrbRRR,    0xf8100000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_DEF0_USE12 | IS_LOAD,
+                 "ldrb", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2LdrsbRRR,    0xf9100000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_DEF0_USE12 | IS_LOAD,
+                 "ldrsb", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2StrRRR,    0xf8400000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_USE012 | IS_STORE,
+                 "str", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2StrhRRR,    0xf8200000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_USE012 | IS_STORE,
+                 "strh", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2StrbRRR,    0xf8000000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 5, 4, IS_QUAD_OP | REG_USE012 | IS_STORE,
+                 "strb", "r!0d,[r!1d, r!2d, LSL #!3d]", 2),
+    ENCODING_MAP(kThumb2LdrhRRI12,       0xf8b00000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
+                 "ldrh", "r!0d,[r!1d, #!2d]", 2),
+    ENCODING_MAP(kThumb2LdrshRRI12,       0xf9b00000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
+                 "ldrsh", "r!0d,[r!1d, #!2d]", 2),
+    ENCODING_MAP(kThumb2LdrbRRI12,       0xf8900000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
+                 "ldrb", "r!0d,[r!1d, #!2d]", 2),
+    ENCODING_MAP(kThumb2LdrsbRRI12,       0xf9900000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
+                 "ldrsb", "r!0d,[r!1d, #!2d]", 2),
+    ENCODING_MAP(kThumb2StrhRRI12,       0xf8a00000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
+                 "strh", "r!0d,[r!1d, #!2d]", 2),
+    ENCODING_MAP(kThumb2StrbRRI12,       0xf8800000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 11, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_USE01 | IS_STORE,
+                 "strb", "r!0d,[r!1d, #!2d]", 2),
+    ENCODING_MAP(kThumb2Pop,           0xe8bd0000,
+                 kFmtBitBlt, 15, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_UNARY_OP | REG_DEF_SP | REG_USE_SP | REG_DEF_LIST0
+                 | IS_LOAD, "pop", "<!0R>", 2),
+    ENCODING_MAP(kThumb2Push,          0xe8ad0000,
+                 kFmtBitBlt, 15, 0, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_UNARY_OP | REG_DEF_SP | REG_USE_SP | REG_USE_LIST0
+                 | IS_STORE, "push", "<!0R>", 2),
+    ENCODING_MAP(kThumb2CmpRI8, 0xf1b00f00,
+                 kFmtBitBlt, 19, 16, kFmtModImm, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_USE0 | SETS_CCODES,
+                 "cmp", "r!0d, #!1m", 2),
+    ENCODING_MAP(kThumb2AdcRRR,  0xeb500000, /* setflags encoding */
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1,
+                 IS_QUAD_OP | REG_DEF0_USE12 | SETS_CCODES,
+                 "acds", "r!0d, r!1d, r!2d, shift !3d", 2),
+    ENCODING_MAP(kThumb2AndRRR,  0xea000000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1, IS_QUAD_OP | REG_DEF0_USE12,
+                 "and", "r!0d, r!1d, r!2d, shift !3d", 2),
+    ENCODING_MAP(kThumb2BicRRR,  0xea200000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1, IS_QUAD_OP | REG_DEF0_USE12,
+                 "bic", "r!0d, r!1d, r!2d, shift !3d", 2),
+    ENCODING_MAP(kThumb2CmnRR,  0xeb000000,
+                 kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0, kFmtShift, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "cmn", "r!0d, r!1d, shift !2d", 2),
+    ENCODING_MAP(kThumb2EorRRR,  0xea800000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1, IS_QUAD_OP | REG_DEF0_USE12,
+                 "eor", "r!0d, r!1d, r!2d, shift !3d", 2),
+    ENCODING_MAP(kThumb2MulRRR,  0xfb00f000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "mul", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2MnvRR,  0xea6f0000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 3, 0, kFmtShift, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "mvn", "r!0d, r!1d, shift !2d", 2),
+    ENCODING_MAP(kThumb2RsubRRI8,       0xf1d00000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "rsb", "r!0d,r!1d,#!2m", 2),
+    ENCODING_MAP(kThumb2NegRR,       0xf1d00000, /* instance of rsub */
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "neg", "r!0d,r!1d", 2),
+    ENCODING_MAP(kThumb2OrrRRR,  0xea400000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtShift, -1, -1, IS_QUAD_OP | REG_DEF0_USE12,
+                 "orr", "r!0d, r!1d, r!2d, shift !3d", 2),
+    ENCODING_MAP(kThumb2TstRR,       0xea100f00,
+                 kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0, kFmtShift, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_USE01 | SETS_CCODES,
+                 "tst", "r!0d, r!1d, shift !2d", 2),
+    ENCODING_MAP(kThumb2LslRRR,  0xfa00f000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "lsl", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2LsrRRR,  0xfa20f000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "lsr", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2AsrRRR,  0xfa40f000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "asr", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2RorRRR,  0xfa60f000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "ror", "r!0d, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2LslRRI5,  0xea4f0000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 3, 0, kFmtShift5, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "lsl", "r!0d, r!1d, #!2d", 2),
+    ENCODING_MAP(kThumb2LsrRRI5,  0xea4f0010,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 3, 0, kFmtShift5, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "lsr", "r!0d, r!1d, #!2d", 2),
+    ENCODING_MAP(kThumb2AsrRRI5,  0xea4f0020,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 3, 0, kFmtShift5, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "asr", "r!0d, r!1d, #!2d", 2),
+    ENCODING_MAP(kThumb2RorRRI5,  0xea4f0030,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 3, 0, kFmtShift5, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "ror", "r!0d, r!1d, #!2d", 2),
+    ENCODING_MAP(kThumb2BicRRI8,  0xf0200000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "bic", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2AndRRI8,  0xf0000000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "and", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2OrrRRI8,  0xf0400000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "orr", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2EorRRI8,  0xf0800000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1,
+                 "eor", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2AddRRI8,  0xf1100000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "adds", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2AdcRRI8,  0xf1500000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES | USES_CCODES,
+                 "adcs", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2SubRRI8,  0xf1b00000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES,
+                 "subs", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2SbcRRI8,  0xf1700000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0_USE1 | SETS_CCODES | USES_CCODES,
+                 "sbcs", "r!0d, r!1d, #!2m", 2),
+    ENCODING_MAP(kThumb2It,  0xbf00,
+                 kFmtBitBlt, 7, 4, kFmtBitBlt, 3, 0, kFmtModImm, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | IS_IT | USES_CCODES,
+                 "it:!1b", "!0c", 1),
+    ENCODING_MAP(kThumb2Fmstat,  0xeef1fa10,
+                 kFmtUnused, -1, -1, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, NO_OPERAND | SETS_CCODES,
+                 "fmstat", "", 2),
+    ENCODING_MAP(kThumb2Vcmpd,        0xeeb40b40,
+                 kFmtDfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01,
+                 "vcmp.f64", "!0S, !1S", 2),
+    ENCODING_MAP(kThumb2Vcmps,        0xeeb40a40,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_USE01,
+                 "vcmp.f32", "!0s, !1s", 2),
+    ENCODING_MAP(kThumb2LdrPcRel12,       0xf8df0000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 11, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_TERTIARY_OP | REG_DEF0 | REG_USE_PC | IS_LOAD,
+                 "ldr", "r!0d,[rpc, #!1d]", 2),
+    ENCODING_MAP(kThumb2BCond,        0xf0008000,
+                 kFmtBrOffset, -1, -1, kFmtBitBlt, 25, 22, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1,
+                 IS_BINARY_OP | IS_BRANCH | USES_CCODES,
+                 "b!1c", "!0t", 2),
+    ENCODING_MAP(kThumb2Vmovd_RR,       0xeeb00b40,
+                 kFmtDfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vmov.f64", "!0S, !1S", 2),
+    ENCODING_MAP(kThumb2Vmovs_RR,       0xeeb00a40,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vmov.f32", "!0s, !1s", 2),
+    ENCODING_MAP(kThumb2Fmrs,       0xee100a10,
+                 kFmtBitBlt, 15, 12, kFmtSfp, 7, 16, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "fmrs", "r!0d, !1s", 2),
+    ENCODING_MAP(kThumb2Fmsr,       0xee000a10,
+                 kFmtSfp, 7, 16, kFmtBitBlt, 15, 12, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "fmsr", "!0s, r!1d", 2),
+    ENCODING_MAP(kThumb2Fmrrd,       0xec500b10,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtDfp, 5, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF01_USE2,
+                 "fmrrd", "r!0d, r!1d, !2S", 2),
+    ENCODING_MAP(kThumb2Fmdrr,       0xec400b10,
+                 kFmtDfp, 5, 0, kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE12,
+                 "fmdrr", "!0S, r!1d, r!2d", 2),
+    ENCODING_MAP(kThumb2Vabsd,       0xeeb00bc0,
+                 kFmtDfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vabs.f64", "!0S, !1S", 2),
+    ENCODING_MAP(kThumb2Vabss,       0xeeb00ac0,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vabs.f32", "!0s, !1s", 2),
+    ENCODING_MAP(kThumb2Vnegd,       0xeeb10b40,
+                 kFmtDfp, 22, 12, kFmtDfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vneg.f64", "!0S, !1S", 2),
+    ENCODING_MAP(kThumb2Vnegs,       0xeeb10a40,
+                 kFmtSfp, 22, 12, kFmtSfp, 5, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0_USE1,
+                 "vneg.f32", "!0s, !1s", 2),
+    ENCODING_MAP(kThumb2Vmovs_IMM8,       0xeeb00a00,
+                 kFmtSfp, 22, 12, kFmtFPImm, 16, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0,
+                 "vmov.f32", "!0s, #0x!1h", 2),
+    ENCODING_MAP(kThumb2Vmovd_IMM8,       0xeeb00b00,
+                 kFmtDfp, 22, 12, kFmtFPImm, 16, 0, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, IS_BINARY_OP | REG_DEF0,
+                 "vmov.f64", "!0S, #0x!1h", 2),
+    ENCODING_MAP(kThumb2Mla,  0xfb000000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtBitBlt, 3, 0,
+                 kFmtBitBlt, 15, 12,
+                 IS_QUAD_OP | REG_DEF0 | REG_USE1 | REG_USE2 | REG_USE3,
+                 "mla", "r!0d, r!1d, r!2d, r!3d", 2),
+    ENCODING_MAP(kThumb2Umull,  0xfba00000,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16,
+                 kFmtBitBlt, 3, 0,
+                 IS_QUAD_OP | REG_DEF0 | REG_DEF1 | REG_USE2 | REG_USE3,
+                 "umull", "r!0d, r!1d, r!2d, r!3d", 2),
+    ENCODING_MAP(kThumb2Ldrex,       0xe8500f00,
+                 kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16, kFmtBitBlt, 7, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0_USE1 | IS_LOAD,
+                 "ldrex", "r!0d,[r!1d, #!2E]", 2),
+    ENCODING_MAP(kThumb2Strex,       0xe8400000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 15, 12, kFmtBitBlt, 19, 16,
+                 kFmtBitBlt, 7, 0, IS_QUAD_OP | REG_DEF0_USE12 | IS_STORE,
+                 "strex", "r!0d,r!1d, [r!2d, #!2E]", 2),
+    ENCODING_MAP(kThumb2Clrex,       0xf3bf8f2f,
+                 kFmtUnused, -1, -1, kFmtUnused, -1, -1, kFmtUnused, -1, -1,
+                 kFmtUnused, -1, -1, NO_OPERAND,
+                 "clrex", "", 2),
+    ENCODING_MAP(kThumb2Bfi,         0xf3600000,
+                 kFmtBitBlt, 11, 8, kFmtBitBlt, 19, 16, kFmtShift5, -1, -1,
+                 kFmtBitBlt, 4, 0, IS_QUAD_OP | REG_DEF0_USE1,
+                 "bfi", "r!0d,r!1d,#!2d,#!3d", 2),
+    ENCODING_MAP(kThumb2Bfc,         0xf36f0000,
+                 kFmtBitBlt, 11, 8, kFmtShift5, -1, -1, kFmtBitBlt, 4, 0,
+                 kFmtUnused, -1, -1, IS_TERTIARY_OP | REG_DEF0,
+                 "bfc", "r!0d,#!1d,#!2d", 2),
 };
 
-#define PADDING_MOV_R0_R0               0x1C00
+/*
+ * The fake NOP of moving r0 to r0 actually will incur data stalls if r0 is
+ * not ready. Since r5 (rFP) is not updated often, it is less likely to
+ * generate unnecessary stall cycles.
+ */
+#define PADDING_MOV_R5_R5               0x1C2D
 
 /* Write the numbers in the literal pool to the codegen stream */
 static void installDataContent(CompilationUnit *cUnit)
@@ -527,10 +907,10 @@ static bool assembleInstructions(CompilationUnit *cUnit, intptr_t startAddr)
 
     for (lir = (ArmLIR *) cUnit->firstLIRInsn; lir; lir = NEXT_LIR(lir)) {
         if (lir->opCode < 0) {
-            if ((lir->opCode == ARM_PSEUDO_ALIGN4) &&
+            if ((lir->opCode == kArmPseudoPseudoAlign4) &&
                 /* 1 means padding is needed */
                 (lir->operands[0] == 1)) {
-                *bufferAddr++ = PADDING_MOV_R0_R0;
+                *bufferAddr++ = PADDING_MOV_R5_R5;
             }
             continue;
         }
@@ -539,8 +919,10 @@ static bool assembleInstructions(CompilationUnit *cUnit, intptr_t startAddr)
             continue;
         }
 
-        if (lir->opCode == THUMB_LDR_PC_REL ||
-            lir->opCode == THUMB_ADD_PC_REL) {
+        if (lir->opCode == kThumbLdrPcRel ||
+            lir->opCode == kThumb2LdrPcRel12 ||
+            lir->opCode == kThumbAddPcRel ||
+            ((lir->opCode == kThumb2Vldrs) && (lir->operands[1] == rpc))) {
             ArmLIR *lirTarget = (ArmLIR *) lir->generic.target;
             intptr_t pc = (lir->generic.offset + 4) & ~3;
             /*
@@ -552,42 +934,55 @@ static bool assembleInstructions(CompilationUnit *cUnit, intptr_t startAddr)
             int delta = target - pc;
             if (delta & 0x3) {
                 LOGE("PC-rel distance is not multiples of 4: %d\n", delta);
-                dvmAbort();
+                dvmCompilerAbort(cUnit);
             }
-            if (delta > 1023) {
+            if ((lir->opCode == kThumb2LdrPcRel12) && (delta > 4091)) {
+                return true;
+            } else if (delta > 1020) {
                 return true;
             }
-            lir->operands[1] = delta >> 2;
-        } else if (lir->opCode == THUMB2_CBNZ || lir->opCode == THUMB2_CBZ) {
+            if (lir->opCode == kThumb2Vldrs) {
+                lir->operands[2] = delta >> 2;
+            } else {
+                lir->operands[1] = (lir->opCode == kThumb2LdrPcRel12) ?
+                                    delta : delta >> 2;
+            }
+        } else if (lir->opCode == kThumb2Cbnz || lir->opCode == kThumb2Cbz) {
             ArmLIR *targetLIR = (ArmLIR *) lir->generic.target;
             intptr_t pc = lir->generic.offset + 4;
             intptr_t target = targetLIR->generic.offset;
             int delta = target - pc;
             if (delta > 126 || delta < 0) {
+                /*
+                 * TODO: allow multiple kinds of assembler failure to allow
+                 * change of code patterns when things don't fit.
+                 */
                 return true;
+            } else {
+                lir->operands[1] = delta >> 1;
             }
-            lir->operands[1] = delta >> 1;
-        } else if (lir->opCode == THUMB_B_COND) {
+        } else if (lir->opCode == kThumbBCond ||
+                   lir->opCode == kThumb2BCond) {
             ArmLIR *targetLIR = (ArmLIR *) lir->generic.target;
             intptr_t pc = lir->generic.offset + 4;
             intptr_t target = targetLIR->generic.offset;
             int delta = target - pc;
-            if (delta > 254 || delta < -256) {
+            if ((lir->opCode == kThumbBCond) && (delta > 254 || delta < -256)) {
                 return true;
             }
             lir->operands[0] = delta >> 1;
-        } else if (lir->opCode == THUMB_B_UNCOND) {
+        } else if (lir->opCode == kThumbBUncond) {
             ArmLIR *targetLIR = (ArmLIR *) lir->generic.target;
             intptr_t pc = lir->generic.offset + 4;
             intptr_t target = targetLIR->generic.offset;
             int delta = target - pc;
             if (delta > 2046 || delta < -2048) {
                 LOGE("Unconditional branch distance out of range: %d\n", delta);
-                dvmAbort();
+                dvmCompilerAbort(cUnit);
             }
             lir->operands[0] = delta >> 1;
-        } else if (lir->opCode == THUMB_BLX_1) {
-            assert(NEXT_LIR(lir)->opCode == THUMB_BLX_2);
+        } else if (lir->opCode == kThumbBlx1) {
+            assert(NEXT_LIR(lir)->opCode == kThumbBlx2);
             /* curPC is Thumb */
             intptr_t curPC = (startAddr + lir->generic.offset + 4) & ~3;
             intptr_t target = lir->operands[1];
@@ -606,51 +1001,96 @@ static bool assembleInstructions(CompilationUnit *cUnit, intptr_t startAddr)
         ArmEncodingMap *encoder = &EncodingMap[lir->opCode];
         u4 bits = encoder->skeleton;
         int i;
-        for (i = 0; i < 3; i++) {
+        for (i = 0; i < 4; i++) {
+            u4 operand;
             u4 value;
+            operand = lir->operands[i];
             switch(encoder->fieldLoc[i].kind) {
-                case UNUSED:
+                case kFmtUnused:
                     break;
-                case IMM6:
-                    value = ((lir->operands[i] & 0x20) >> 5) << 9;
-                    value |= (lir->operands[i] & 0x1f) << 3;
+                case kFmtFPImm:
+                    value = ((operand & 0xF0) >> 4) << encoder->fieldLoc[i].end;
+                    value |= (operand & 0x0F) << encoder->fieldLoc[i].start;
                     bits |= value;
                     break;
-                case BITBLT:
-                    value = (lir->operands[i] << encoder->fieldLoc[i].start) &
+                case kFmtBrOffset:
+                    /*
+                     * NOTE: branch offsets are not handled here, but
+                     * in the main assembly loop (where label values
+                     * are known).  For reference, here is what the
+                     * encoder handing would be:
+                         value = ((operand  & 0x80000) >> 19) << 26;
+                         value |= ((operand & 0x40000) >> 18) << 11;
+                         value |= ((operand & 0x20000) >> 17) << 13;
+                         value |= ((operand & 0x1f800) >> 11) << 16;
+                         value |= (operand  & 0x007ff);
+                         bits |= value;
+                     */
+                    break;
+                case kFmtShift5:
+                    value = ((operand & 0x1c) >> 2) << 12;
+                    value |= (operand & 0x03) << 6;
+                    bits |= value;
+                    break;
+                case kFmtShift:
+                    value = ((operand & 0x70) >> 4) << 12;
+                    value |= (operand & 0x0f) << 4;
+                    bits |= value;
+                    break;
+                case kFmtBWidth:
+                    value = operand - 1;
+                    bits |= value;
+                    break;
+                case kFmtLsb:
+                    value = ((operand & 0x1c) >> 2) << 12;
+                    value |= (operand & 0x03) << 6;
+                    bits |= value;
+                    break;
+                case kFmtImm6:
+                    value = ((operand & 0x20) >> 5) << 9;
+                    value |= (operand & 0x1f) << 3;
+                    bits |= value;
+                    break;
+                case kFmtBitBlt:
+                    value = (operand << encoder->fieldLoc[i].start) &
                             ((1 << (encoder->fieldLoc[i].end + 1)) - 1);
                     bits |= value;
                     break;
-                case DFP:
+                case kFmtDfp: {
+                    assert(DOUBLEREG(operand));
+                    assert((operand & 0x1) == 0);
+                    int regName = (operand & FP_REG_MASK) >> 1;
                     /* Snag the 1-bit slice and position it */
-                    value = ((lir->operands[i] & 0x10) >> 4) <<
+                    value = ((regName & 0x10) >> 4) <<
                             encoder->fieldLoc[i].end;
                     /* Extract and position the 4-bit slice */
-                    value |= (lir->operands[i] & 0x0f) <<
+                    value |= (regName & 0x0f) <<
                             encoder->fieldLoc[i].start;
                     bits |= value;
                     break;
-                case SFP:
+                }
+                case kFmtSfp:
+                    assert(SINGLEREG(operand));
                     /* Snag the 1-bit slice and position it */
-                    value = (lir->operands[i] & 0x1) <<
+                    value = (operand & 0x1) <<
                             encoder->fieldLoc[i].end;
                     /* Extract and position the 4-bit slice */
-                    value |= ((lir->operands[i] & 0x1e) >> 1) <<
+                    value |= ((operand & 0x1e) >> 1) <<
                             encoder->fieldLoc[i].start;
                     bits |= value;
                     break;
-                case IMM12:
-                case MODIMM:
-                    value = ((lir->operands[i] & 0x800) >> 11) << 26;
-                    value |= ((lir->operands[i] & 0x700) >> 8) << 12;
-                    value |= lir->operands[i] & 0x0ff;
+                case kFmtImm12:
+                case kFmtModImm:
+                    value = ((operand & 0x800) >> 11) << 26;
+                    value |= ((operand & 0x700) >> 8) << 12;
+                    value |= operand & 0x0ff;
                     bits |= value;
                     break;
-                case IMM16:
-                    value = ((lir->operands[i] & 0x0800) >> 11) << 26;
-                    value |= ((lir->operands[i] & 0xf000) >> 12) << 16;
-                    value |= ((lir->operands[i] & 0x0700) >> 8) << 12;
-                    value |= lir->operands[i] & 0x0ff;
+                case kFmtImm16:
+                    value = ((operand & 0x0800) >> 11) << 26;
+                    value |= ((operand & 0xf000) >> 12) << 16;
+                    value |= ((operand & 0x0700) >> 8) << 12;
+                    value |= operand & 0x0ff;
                     bits |= value;
                     break;
                 default:
@@ -665,6 +1105,36 @@ static bool assembleInstructions(CompilationUnit *cUnit, intptr_t startAddr)
     return false;
 }
 
+#if defined(SIGNATURE_BREAKPOINT)
+/* Inspect the assembled instruction stream to find potential matches */
+static void matchSignatureBreakpoint(const CompilationUnit *cUnit,
+                                     unsigned int size)
+{
+    unsigned int i, j;
+    u4 *ptr = (u4 *) cUnit->codeBuffer;
+
+    for (i = 0; i < size - gDvmJit.signatureBreakpointSize + 1; i++) {
+        if (ptr[i] == gDvmJit.signatureBreakpoint[0]) {
+            for (j = 1; j < gDvmJit.signatureBreakpointSize; j++) {
+                if (ptr[i+j] != gDvmJit.signatureBreakpoint[j]) {
+                    break;
+                }
+            }
+            if (j == gDvmJit.signatureBreakpointSize) {
+                LOGD("Signature match starting from offset %#x (%d words)",
+                     i*4, gDvmJit.signatureBreakpointSize);
+                int descSize = jitTraceDescriptionSize(cUnit->traceDesc);
+                JitTraceDescription *newCopy =
+                    (JitTraceDescription *) malloc(descSize);
+                memcpy(newCopy, cUnit->traceDesc, descSize);
+                dvmCompilerWorkEnqueue(NULL, kWorkOrderTraceDebug, newCopy);
+                break;
+            }
+        }
+    }
+}
+#endif
+
 /*
  * Translation layout in the code cache.  Note that the codeAddress pointer
  * in JitTable will point directly to the code body (field codeAddress).  The
@@ -687,7 +1157,9 @@ static bool assembleInstructions(CompilationUnit *cUnit, intptr_t startAddr)
  *   |  .                            .
  *   |  |                            |
  *   |  +----------------------------+
- *   +->| Chaining cell counts       |  -> 4 bytes, chain cell counts by type
+ *   |  | Gap for large switch stmt  |  -> # cases >= MAX_CHAINED_SWITCH_CASES
+ *   |  +----------------------------+
+ *   +->| Chaining cell counts       |  -> 8 bytes, chain cell counts by type
  *      +----------------------------+
  *      | Trace description          |  -> variable sized
  *      .                            .
@@ -711,8 +1183,8 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
     int i;
     ChainCellCounts chainCellCounts;
     int descSize = jitTraceDescriptionSize(cUnit->traceDesc);
+    int chainingCellGap;
 
-    info->codeAddress = NULL;
     info->instructionSet = cUnit->instructionSet;
 
     /* Beginning offset needs to allow space for chain cell offset */
@@ -723,7 +1195,7 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
         if (armLIR->opCode >= 0 && !armLIR->isNop) {
             armLIR->size = EncodingMap[armLIR->opCode].size * 2;
             offset += armLIR->size;
-        } else if (armLIR->opCode == ARM_PSEUDO_ALIGN4) {
+        } else if (armLIR->opCode == kArmPseudoPseudoAlign4) {
             if (offset & 0x2) {
                 offset += 2;
                 armLIR->operands[0] = 1;
@@ -737,12 +1209,19 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
     /* Const values have to be word aligned */
     offset = (offset + 3) & ~3;
 
+    /*
+     * Get the gap (# of u4) between the offset of chaining cell count and
+     * the bottom of real chaining cells. If the translation has chaining
+     * cells, the gap is guaranteed to be multiples of 4.
+     */
+    chainingCellGap = (offset - cUnit->chainingCellBottom->offset) >> 2;
+
     /* Add space for chain cell counts & trace description */
     u4 chainCellOffset = offset;
     ArmLIR *chainCellOffsetLIR = (ArmLIR *) cUnit->chainCellOffsetLIR;
     assert(chainCellOffsetLIR);
     assert(chainCellOffset < 0x10000);
-    assert(chainCellOffsetLIR->opCode == ARM_16BIT_DATA &&
+    assert(chainCellOffsetLIR->opCode == kArm16BitData &&
            chainCellOffsetLIR->operands[0] == CHAIN_CELL_OFFSET_TAG);
 
     /*
@@ -768,7 +1247,7 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
 
     cUnit->totalSize = offset;
 
-    if (gDvmJit.codeCacheByteUsed + cUnit->totalSize > CODE_CACHE_SIZE) {
+    if (gDvmJit.codeCacheByteUsed + cUnit->totalSize > gDvmJit.codeCacheSize) {
         gDvmJit.codeCacheFull = true;
         cUnit->baseAddr = NULL;
         return;
@@ -794,6 +1273,15 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
         return;
     }
 
+#if defined(SIGNATURE_BREAKPOINT)
+    if (info->discardResult == false && gDvmJit.signatureBreakpoint != NULL &&
+        chainCellOffset/4 >= gDvmJit.signatureBreakpointSize) {
+        matchSignatureBreakpoint(cUnit, chainCellOffset/4);
+    }
+#endif
+
+    /* Don't go all the way if the goal is just to get the verbose output */
+    if (info->discardResult) return;
 
     cUnit->baseAddr = (char *) gDvmJit.codeCache + gDvmJit.codeCacheByteUsed;
     gDvmJit.codeCacheByteUsed += offset;
@@ -803,9 +1291,13 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
     gDvmJit.numCompilations++;
 
     /* Install the chaining cell counts */
-    for (i=0; i< CHAINING_CELL_LAST; i++) {
+    for (i=0; i< kChainingCellGap; i++) {
         chainCellCounts.u.count[i] = cUnit->numChainingCells[i];
     }
+
+    /* Set the gap number in the chaining cell count structure */
+    chainCellCounts.u.count[kChainingCellGap] = chainingCellGap;
+
     memcpy((char*)cUnit->baseAddr + chainCellOffset, &chainCellCounts,
            sizeof(chainCellCounts));
 
@@ -822,22 +1314,33 @@ void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo *info)
 
     /* Record code entry point and instruction set */
     info->codeAddress = (char*)cUnit->baseAddr + cUnit->headerSize;
-    info->instructionSet = cUnit->instructionSet;
     /* If applicable, mark low bit to denote thumb */
     if (info->instructionSet != DALVIK_JIT_ARM)
         info->codeAddress = (char*)info->codeAddress + 1;
 }
 
-static u4 assembleBXPair(int branchOffset)
+/*
+ * Returns the skeleton bit pattern associated with an opcode.  All
+ * variable fields are zeroed.
+ */
+static u4 getSkeleton(ArmOpCode op)
+{
+    return EncodingMap[op].skeleton;
+}
+
+static u4 assembleChainingBranch(int branchOffset, bool thumbTarget)
 {
     u4 thumb1, thumb2;
 
-    if ((branchOffset < -2048) | (branchOffset > 2046)) {
-        thumb1 =  (0xf000 | ((branchOffset>>12) & 0x7ff));
-        thumb2 =  (0xf800 | ((branchOffset>> 1) & 0x7ff));
+    if (!thumbTarget) {
+        thumb1 =  (getSkeleton(kThumbBlx1) | ((branchOffset>>12) & 0x7ff));
+        thumb2 =  (getSkeleton(kThumbBlx2) | ((branchOffset>> 1) & 0x7ff));
+    } else if ((branchOffset < -2048) | (branchOffset > 2046)) {
+        thumb1 =  (getSkeleton(kThumbBl1) | ((branchOffset>>12) & 0x7ff));
+        thumb2 =  (getSkeleton(kThumbBl2) | ((branchOffset>> 1) & 0x7ff));
     } else {
-        thumb1 =  (0xe000 | ((branchOffset>> 1) & 0x7ff));
-        thumb2 =  0x4300;  /* nop -> or r0, r0 */
+        thumb1 =  (getSkeleton(kThumbBUncond) | ((branchOffset>> 1) & 0x7ff));
+        thumb2 =  getSkeleton(kThumbOrr);  /* nop -> or r0, r0 */
     }
 
     return thumb2<<16 | thumb1;
@@ -847,7 +1350,8 @@ static u4 assembleBXPair(int branchOffset)
  * Perform translation chain operation.
  * For ARM, we'll use a pair of thumb instructions to generate
  * an unconditional chaining branch of up to 4MB in distance.
- * Use a BL, though we don't really need the link.  The format is
+ * Use a BL, because the generic "interpret" translation needs
+ * the link register to find the dalvik pc of teh target.
  *     111HHooooooooooo
  * Where HH is 10 for the 1st inst, and 11 for the second and
  * the "o" field is each instruction's 11-bit contribution to the
@@ -860,8 +1364,14 @@ void* dvmJitChain(void* tgtAddr, u4* branchAddr)
     int baseAddr = (u4) branchAddr + 4;
     int branchOffset = (int) tgtAddr - baseAddr;
     u4 newInst;
+    bool thumbTarget;
 
-    if (gDvm.sumThreadSuspendCount == 0) {
+    /*
+     * Only chain translations when there is no urge to ask all threads to
+     * suspend themselves via the interpreter.
+     */
+    if ((gDvmJit.pProfTable != NULL) && (gDvm.sumThreadSuspendCount == 0) &&
+        (gDvmJit.codeCacheFull == false)) {
         assert((branchOffset >= -(1<<22)) && (branchOffset <= ((1<<22)-2)));
 
         gDvmJit.translationChains++;
@@ -870,16 +1380,84 @@ void* dvmJitChain(void* tgtAddr, u4* branchAddr)
             LOGD("Jit Runtime: chaining 0x%x to 0x%x\n",
                  (int) branchAddr, (int) tgtAddr & -2));
 
-        newInst = assembleBXPair(branchOffset);
+        /*
+         * NOTE: normally, all translations are Thumb[2] mode, with
+         * a single exception: the default TEMPLATE_INTERPRET
+         * pseudo-translation.  If the need ever arises to
+         * mix Arm & Thumb[2] translations, the following code should be
+         * generalized.
+         */
+        thumbTarget = (tgtAddr != gDvmJit.interpretTemplate);
+
+        newInst = assembleChainingBranch(branchOffset, thumbTarget);
 
         *branchAddr = newInst;
         cacheflush((long)branchAddr, (long)branchAddr + 4, 0);
+        gDvmJit.hasNewChain = true;
     }
 
     return tgtAddr;
 }
 
 /*
+ * Attempt to enqueue a work order to patch an inline cache for a predicted
+ * chaining cell for virtual/interface calls.
+ */
+static bool inlineCachePatchEnqueue(PredictedChainingCell *cellAddr,
+                                    PredictedChainingCell *newContent)
+{
+    bool result = true;
+
+    /*
+     * Make sure only one thread gets here since updating the cell (ie fast
+     * path and queueing the request (ie the queued path) have to be done
+     * in an atomic fashion.
+     */
+    dvmLockMutex(&gDvmJit.compilerICPatchLock);
+
+    /* Fast path for uninitialized chaining cell */
+    if (cellAddr->clazz == NULL &&
+        cellAddr->branch == PREDICTED_CHAIN_BX_PAIR_INIT) {
+        cellAddr->method = newContent->method;
+        cellAddr->branch = newContent->branch;
+        cellAddr->counter = newContent->counter;
+        /*
+         * The update order matters - make sure clazz is updated last since it
+         * will bring the uninitialized chaining cell to life.
+         */
+        MEM_BARRIER();
+        cellAddr->clazz = newContent->clazz;
+        cacheflush((intptr_t) cellAddr, (intptr_t) (cellAddr+1), 0);
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchFast++;
+#endif
+    }
+    /*
+     * Otherwise the patch request will be queued and handled in the next
+     * GC cycle. At that time all other mutator threads are suspended so
+     * there will be no partial update in the inline cache state.
+     */
+    else if (gDvmJit.compilerICPatchIndex < COMPILER_IC_PATCH_QUEUE_SIZE)  {
+        int index = gDvmJit.compilerICPatchIndex++;
+        gDvmJit.compilerICPatchQueue[index].cellAddr = cellAddr;
+        gDvmJit.compilerICPatchQueue[index].cellContent = *newContent;
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchQueued++;
+#endif
+    }
+    /* Queue is full - just drop this patch request */
+    else {
+        result = false;
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchDropped++;
+#endif
+    }
+
+    dvmUnlockMutex(&gDvmJit.compilerICPatchLock);
+    return result;
+}
+
+/*
  * This method is called from the invoke templates for virtual and interface
  * methods to speculatively setup a chain to the callee. The templates are
  * written in assembly and have setup method, cell, and clazz at r0, r2, and
@@ -902,6 +1480,12 @@ const Method *dvmJitToPatchPredictedChain(const Method *method,
                                           PredictedChainingCell *cell,
                                           const ClassObject *clazz)
 {
+#if defined(WITH_SELF_VERIFICATION)
+    /* Disable chaining and prevent this from triggering again for a while */
+    cell->counter = PREDICTED_CHAIN_COUNTER_AVOID;
+    cacheflush((long) cell, (long) (cell+1), 0);
+    goto done;
+#else
     /* Don't come back here for a long time if the method is native */
     if (dvmIsNativeMethod(method)) {
         cell->counter = PREDICTED_CHAIN_COUNTER_AVOID;
@@ -917,7 +1501,7 @@ const Method *dvmJitToPatchPredictedChain(const Method *method,
      * Compilation not made yet for the callee. Reset the counter to a small
      * value and come back to check soon.
      */
-    if (tgtAddr == 0) {
+    if ((tgtAddr == 0) || ((void*)tgtAddr == gDvmJit.interpretTemplate)) {
         /*
          * Wait for a few invocations (currently set to be 16) before trying
          * to setup the chain again.
@@ -925,35 +1509,98 @@ const Method *dvmJitToPatchPredictedChain(const Method *method,
         cell->counter = PREDICTED_CHAIN_COUNTER_DELAY;
         cacheflush((long) cell, (long) (cell+1), 0);
         COMPILER_TRACE_CHAINING(
-            LOGD("Jit Runtime: predicted chain %p to method %s delayed",
-                 cell, method->name));
+            LOGD("Jit Runtime: predicted chain %p to method %s%s delayed",
+                 cell, method->clazz->descriptor, method->name));
         goto done;
     }
 
-    /* Stop the world */
-    dvmSuspendAllThreads(SUSPEND_FOR_JIT);
+    PredictedChainingCell newCell;
+
+    /* Avoid back-to-back orders to the same cell */
+    cell->counter = PREDICTED_CHAIN_COUNTER_AVOID;
 
     int baseAddr = (int) cell + 4;   // PC is cur_addr + 4
     int branchOffset = tgtAddr - baseAddr;
 
-    COMPILER_TRACE_CHAINING(
-        LOGD("Jit Runtime: predicted chain %p from %s to %s (%s) patched",
-             cell, cell->clazz ? cell->clazz->descriptor : "NULL",
-             clazz->descriptor,
-             method->name));
+    newCell.branch = assembleChainingBranch(branchOffset, true);
+    newCell.clazz = clazz;
+    newCell.method = method;
+    newCell.counter = PREDICTED_CHAIN_COUNTER_RECHAIN;
 
-    cell->branch = assembleBXPair(branchOffset);
-    cell->clazz = clazz;
-    cell->method = method;
-    cell->counter = PREDICTED_CHAIN_COUNTER_RECHAIN;
+    /*
+     * Enter the work order to the queue and the chaining cell will be patched
+     * the next time a safe point is entered.
+     *
+     * If the enqueuing fails reset the rechain count to a normal value so that
+     * it won't get indefinitely delayed.
+     */
+    if (!inlineCachePatchEnqueue(cell, &newCell)) {
+        cell->counter = PREDICTED_CHAIN_COUNTER_RECHAIN;
+    }
+#endif
+done:
+    return method;
+}
 
-    cacheflush((long) cell, (long) (cell+1), 0);
+/*
+ * Patch the inline cache content based on the content passed from the work
+ * order.
+ */
+void dvmCompilerPatchInlineCache(void)
+{
+    int i;
+    PredictedChainingCell *minAddr, *maxAddr;
 
-    /* All done - resume all other threads */
-    dvmResumeAllThreads(SUSPEND_FOR_JIT);
+    /* Nothing to be done */
+    if (gDvmJit.compilerICPatchIndex == 0) return;
 
-done:
-    return method;
+    /*
+     * Since all threads are already stopped we don't really need to acquire
+     * the lock. But race condition can be easily introduced in the future w/o
+     * paying attention so we still acquire the lock here.
+     */
+    dvmLockMutex(&gDvmJit.compilerICPatchLock);
+
+    //LOGD("Number of IC patch work orders: %d", gDvmJit.compilerICPatchIndex);
+
+    /* Initialize the min/max address range */
+    minAddr = (PredictedChainingCell *)
+        ((char *) gDvmJit.codeCache + gDvmJit.codeCacheSize);
+    maxAddr = (PredictedChainingCell *) gDvmJit.codeCache;
+
+    for (i = 0; i < gDvmJit.compilerICPatchIndex; i++) {
+        PredictedChainingCell *cellAddr =
+            gDvmJit.compilerICPatchQueue[i].cellAddr;
+        PredictedChainingCell *cellContent =
+            &gDvmJit.compilerICPatchQueue[i].cellContent;
+
+        if (cellAddr->clazz == NULL) {
+            COMPILER_TRACE_CHAINING(
+                LOGD("Jit Runtime: predicted chain %p to %s (%s) initialized",
+                     cellAddr,
+                     cellContent->clazz->descriptor,
+                     cellContent->method->name));
+        } else {
+            COMPILER_TRACE_CHAINING(
+                LOGD("Jit Runtime: predicted chain %p from %s to %s (%s) "
+                     "patched",
+                     cellAddr,
+                     cellAddr->clazz->descriptor,
+                     cellContent->clazz->descriptor,
+                     cellContent->method->name));
+        }
+
+        /* Patch the chaining cell */
+        *cellAddr = *cellContent;
+        minAddr = (cellAddr < minAddr) ? cellAddr : minAddr;
+        maxAddr = (cellAddr > maxAddr) ? cellAddr : maxAddr;
+    }
+
+    /* Then synchronize the I/D cache */
+    cacheflush((long) minAddr, (long) (maxAddr+1), 0);
+
+    gDvmJit.compilerICPatchIndex = 0;
+    dvmUnlockMutex(&gDvmJit.compilerICPatchLock);
 }
 
 /*
@@ -979,47 +1626,67 @@ u4* dvmJitUnchain(void* codeAddr)
     PredictedChainingCell *predChainCell;
 
     /* Get total count of chain cells */
-    for (i = 0, cellSize = 0; i < CHAINING_CELL_LAST; i++) {
-        if (i != CHAINING_CELL_INVOKE_PREDICTED) {
+    for (i = 0, cellSize = 0; i < kChainingCellGap; i++) {
+        if (i != kChainingCellInvokePredicted) {
             cellSize += pChainCellCounts->u.count[i] * 2;
         } else {
             cellSize += pChainCellCounts->u.count[i] * 4;
         }
     }
 
+    if (cellSize == 0)
+        return (u4 *) pChainCellCounts;
+
     /* Locate the beginning of the chain cell region */
-    pStart = pChainCells = ((u4 *) pChainCellCounts) - cellSize;
+    pStart = pChainCells = ((u4 *) pChainCellCounts) - cellSize -
+             pChainCellCounts->u.count[kChainingCellGap];
 
     /* The cells are sorted in order - walk through them and reset */
-    for (i = 0; i < CHAINING_CELL_LAST; i++) {
+    for (i = 0; i < kChainingCellGap; i++) {
         int elemSize = 2; /* Most chaining cell has two words */
-        if (i == CHAINING_CELL_INVOKE_PREDICTED) {
+        if (i == kChainingCellInvokePredicted) {
             elemSize = 4;
         }
 
         for (j = 0; j < pChainCellCounts->u.count[i]; j++) {
             int targetOffset;
             switch(i) {
-                case CHAINING_CELL_NORMAL:
+                case kChainingCellNormal:
                     targetOffset = offsetof(InterpState,
                           jitToInterpEntries.dvmJitToInterpNormal);
                     break;
-                case CHAINING_CELL_HOT:
-                case CHAINING_CELL_INVOKE_SINGLETON:
+                case kChainingCellHot:
+                case kChainingCellInvokeSingleton:
                     targetOffset = offsetof(InterpState,
-                          jitToInterpEntries.dvmJitToTraceSelect);
+                          jitToInterpEntries.dvmJitToInterpTraceSelect);
                     break;
-                case CHAINING_CELL_INVOKE_PREDICTED:
+                case kChainingCellInvokePredicted:
                     targetOffset = 0;
                     predChainCell = (PredictedChainingCell *) pChainCells;
-                    /* Reset the cell to the init state */
-                    predChainCell->branch = PREDICTED_CHAIN_BX_PAIR_INIT;
+                    /*
+                     * There could be a race on another mutator thread to use
+                     * this particular predicted cell and the check has passed
+                     * the clazz comparison. So we cannot safely wipe the
+                     * method and branch but it is safe to clear the clazz,
+                     * which serves as the key.
+                     */
                     predChainCell->clazz = PREDICTED_CHAIN_CLAZZ_INIT;
-                    predChainCell->method = PREDICTED_CHAIN_METHOD_INIT;
-                    predChainCell->counter = PREDICTED_CHAIN_COUNTER_INIT;
                     break;
+#if defined(WITH_SELF_VERIFICATION)
+                case kChainingCellBackwardBranch:
+                    targetOffset = offsetof(InterpState,
+                          jitToInterpEntries.dvmJitToInterpBackwardBranch);
+                    break;
+#elif defined(WITH_JIT_TUNING)
+                case kChainingCellBackwardBranch:
+                    targetOffset = offsetof(InterpState,
+                          jitToInterpEntries.dvmJitToInterpNormal);
+                    break;
+#endif
                 default:
-                    dvmAbort();
+                    targetOffset = 0; // make gcc happy
+                    LOGE("Unexpected chaining type: %d", i);
+                    dvmAbort();  // dvmAbort OK here - can't safely recover
             }
             COMPILER_TRACE_CHAINING(
                 LOGD("Jit Runtime: unchaining 0x%x", (int)pChainCells));
@@ -1028,7 +1695,7 @@ u4* dvmJitUnchain(void* codeAddr)
              *     ldr  r0, rGLUE, #<word offset>
              *     blx  r0
              */
-            if (i != CHAINING_CELL_INVOKE_PREDICTED) {
+            if (i != kChainingCellInvokePredicted) {
                 targetOffset = targetOffset >> 2;  /* convert to word offset */
                 thumb1 = 0x6800 | (targetOffset << 6) |
                          (rGLUE << 3) | (r0 << 0);
@@ -1053,7 +1720,9 @@ void dvmJitUnchainAll()
         dvmLockMutex(&gDvmJit.tableLock);
         for (i = 0; i < gDvmJit.jitTableSize; i++) {
             if (gDvmJit.pJitEntryTable[i].dPC &&
-                   gDvmJit.pJitEntryTable[i].codeAddress) {
+                   gDvmJit.pJitEntryTable[i].codeAddress &&
+                   (gDvmJit.pJitEntryTable[i].codeAddress !=
+                    gDvmJit.interpretTemplate)) {
                 u4* lastAddress;
                 lastAddress =
                       dvmJitUnchain(gDvmJit.pJitEntryTable[i].codeAddress);
@@ -1066,7 +1735,9 @@ void dvmJitUnchainAll()
         }
         cacheflush((long)lowAddress, (long)highAddress, 0);
         dvmUnlockMutex(&gDvmJit.tableLock);
+        gDvmJit.translationChains = 0;
     }
+    gDvmJit.hasNewChain = false;
 }
 
 typedef struct jitProfileAddrToLine {
@@ -1094,11 +1765,13 @@ char *getTraceBase(const JitEntry *p)
 }
 
 /* Dumps profile info for a single trace */
-static int dumpTraceProfile(JitEntry *p)
+static int dumpTraceProfile(JitEntry *p, bool silent, bool reset,
+                            unsigned long sum)
 {
     ChainCellCounts* pCellCounts;
     char* traceBase;
     u4* pExecutionCount;
+    u4 executionCount;
     u2* pCellOffset;
     JitTraceDescription *desc;
     const Method* method;
@@ -1106,11 +1779,24 @@ static int dumpTraceProfile(JitEntry *p)
     traceBase = getTraceBase(p);
 
     if (p->codeAddress == NULL) {
-        LOGD("TRACEPROFILE 0x%08x 0 NULL 0 0", (int)traceBase);
+        if (!silent)
+            LOGD("TRACEPROFILE 0x%08x 0 NULL 0 0", (int)traceBase);
+        return 0;
+    }
+    if (p->codeAddress == gDvmJit.interpretTemplate) {
+        if (!silent)
+            LOGD("TRACEPROFILE 0x%08x 0 INTERPRET_ONLY  0 0", (int)traceBase);
         return 0;
     }
 
     pExecutionCount = (u4*) (traceBase);
+    executionCount = *pExecutionCount;
+    if (reset) {
+        *pExecutionCount =0;
+    }
+    if (silent) {
+        return executionCount;
+    }
     pCellOffset = (u2*) (traceBase + 4);
     pCellCounts = (ChainCellCounts*) ((char *)pCellOffset + *pCellOffset);
     desc = (JitTraceDescription*) ((char*)pCellCounts + sizeof(*pCellCounts));
@@ -1132,16 +1818,43 @@ static int dumpTraceProfile(JitEntry *p)
                        method->accessFlags,
                        addrToLineCb, NULL, &addrToLine);
 
-    LOGD("TRACEPROFILE 0x%08x % 10d [%#x(+%d), %d] %s%s;%s",
+    LOGD("TRACEPROFILE 0x%08x % 10d %5.2f%% [%#x(+%d), %d] %s%s;%s",
          (int)traceBase,
-         *pExecutionCount,
+         executionCount,
+         ((float ) executionCount) / sum * 100.0,
          desc->trace[0].frag.startOffset,
          desc->trace[0].frag.numInsts,
          addrToLine.lineNum,
          method->clazz->descriptor, method->name, methodDesc);
     free(methodDesc);
 
-    return *pExecutionCount;
+    return executionCount;
+}
+
+/* Create a copy of the trace descriptor of an existing compilation */
+JitTraceDescription *dvmCopyTraceDescriptor(const u2 *pc,
+                                            const JitEntry *knownEntry)
+{
+    const JitEntry *jitEntry = knownEntry ? knownEntry : dvmFindJitEntry(pc);
+    if (jitEntry == NULL) return NULL;
+
+    /* Find out the startint point */
+    char *traceBase = getTraceBase(jitEntry);
+
+    /* Then find out the starting point of the chaining cell */
+    u2 *pCellOffset = (u2*) (traceBase + 4);
+    ChainCellCounts *pCellCounts =
+        (ChainCellCounts*) ((char *)pCellOffset + *pCellOffset);
+
+    /* From there we can find out the starting point of the trace descriptor */
+    JitTraceDescription *desc =
+        (JitTraceDescription*) ((char*)pCellCounts + sizeof(*pCellCounts));
+
+    /* Now make a copy and return */
+    int descSize = jitTraceDescriptionSize(desc);
+    JitTraceDescription *newCopy = (JitTraceDescription *) malloc(descSize);
+    memcpy(newCopy, desc, descSize);
+    return newCopy;
 }
 
 /* Handy function to retrieve the profile count */
@@ -1171,7 +1884,7 @@ void dvmCompilerSortAndPrintTraceProfiles()
 {
     JitEntry *sortedEntries;
     int numTraces = 0;
-    unsigned long counts = 0;
+    unsigned long sum = 0;
     unsigned int i;
 
     /* Make sure that the table is not changing */
@@ -1186,19 +1899,657 @@ void dvmCompilerSortAndPrintTraceProfiles()
     qsort(sortedEntries, gDvmJit.jitTableSize, sizeof(JitEntry),
           sortTraceProfileCount);
 
-    /* Dump the sorted entries */
+    /* Analyze the sorted entries */
     for (i=0; i < gDvmJit.jitTableSize; i++) {
         if (sortedEntries[i].dPC != 0) {
-            counts += dumpTraceProfile(&sortedEntries[i]);
+            sum += dumpTraceProfile(&sortedEntries[i],
+                                       true /* silent */,
+                                       false /* reset */,
+                                       0);
             numTraces++;
         }
     }
     if (numTraces == 0)
         numTraces = 1;
-    LOGD("JIT: Average execution count -> %d",(int)(counts / numTraces));
+    if (sum == 0) {
+        sum = 1;
+    }
+
+    LOGD("JIT: Average execution count -> %d",(int)(sum / numTraces));
+
+    /* Dump the sorted entries. The count of each trace will be reset to 0. */
+    for (i=0; i < gDvmJit.jitTableSize; i++) {
+        if (sortedEntries[i].dPC != 0) {
+            dumpTraceProfile(&sortedEntries[i],
+                             false /* silent */,
+                             true /* reset */,
+                             sum);
+        }
+    }
+
+    for (i=0; i < gDvmJit.jitTableSize && i < 10; i++) {
+        JitTraceDescription* desc =
+            dvmCopyTraceDescriptor(NULL, &sortedEntries[i]);
+        dvmCompilerWorkEnqueue(sortedEntries[i].dPC,
+                               kWorkOrderTraceDebug, desc);
+    }
 
     free(sortedEntries);
 done:
     dvmUnlockMutex(&gDvmJit.tableLock);
     return;
 }
+
+#if defined(WITH_SELF_VERIFICATION)
+/*
+ * The following are used to keep compiled loads and stores from modifying
+ * memory during self verification mode.
+ *
+ * Stores do not modify memory. Instead, the address and value pair are stored
+ * into heapSpace. Addresses within heapSpace are unique. For accesses smaller
+ * than a word, the word containing the address is loaded first before being
+ * updated.
+ *
+ * Loads check heapSpace first and return data from there if an entry exists.
+ * Otherwise, data is loaded from memory as usual.
+ */
+
+/* Used to specify sizes of memory operations */
+enum {
+    kSVByte,
+    kSVSignedByte,
+    kSVHalfword,
+    kSVSignedHalfword,
+    kSVWord,
+    kSVDoubleword,
+    kSVVariable,
+};
+
+/* Load the value of a decoded register from the stack */
+static int selfVerificationMemRegLoad(int* sp, int reg)
+{
+    return *(sp + reg);
+}
+
+/* Load the value of a decoded doubleword register from the stack */
+static s8 selfVerificationMemRegLoadDouble(int* sp, int reg)
+{
+    return *((s8*)(sp + reg));
+}
+
+/* Store the value of a decoded register out to the stack */
+static void selfVerificationMemRegStore(int* sp, int data, int reg)
+{
+    *(sp + reg) = data;
+}
+
+/* Store the value of a decoded doubleword register out to the stack */
+static void selfVerificationMemRegStoreDouble(int* sp, s8 data, int reg)
+{
+    *((s8*)(sp + reg)) = data;
+}
+
+/*
+ * Load the specified size of data from the specified address, checking
+ * heapSpace first if Self Verification mode wrote to it previously, and
+ * falling back to actual memory otherwise.
+ */
+static int selfVerificationLoad(int addr, int size)
+{
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    ShadowHeap *heapSpacePtr;
+
+    int data;
+    int maskedAddr = addr & 0xFFFFFFFC;
+    int alignment = addr & 0x3;
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == maskedAddr) {
+            addr = ((unsigned int) &(heapSpacePtr->data)) | alignment;
+            break;
+        }
+    }
+
+    switch (size) {
+        case kSVByte:
+            data = *((u1*) addr);
+            break;
+        case kSVSignedByte:
+            data = *((s1*) addr);
+            break;
+        case kSVHalfword:
+            data = *((u2*) addr);
+            break;
+        case kSVSignedHalfword:
+            data = *((s2*) addr);
+            break;
+        case kSVWord:
+            data = *((u4*) addr);
+            break;
+        default:
+            LOGE("*** ERROR: BAD SIZE IN selfVerificationLoad: %d", size);
+            data = 0;
+            dvmAbort();
+    }
+
+    //LOGD("*** HEAP LOAD: Addr: 0x%x Data: 0x%x Size: %d", addr, data, size);
+    return data;
+}
+
+/* Like selfVerificationLoad, but specifically for doublewords */
+static s8 selfVerificationLoadDoubleword(int addr)
+{
+    Thread *self = dvmThreadSelf();
+    ShadowSpace* shadowSpace = self->shadowSpace;
+    ShadowHeap* heapSpacePtr;
+
+    int addr2 = addr+4;
+    unsigned int data = *((unsigned int*) addr);
+    unsigned int data2 = *((unsigned int*) addr2);
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == addr) {
+            data = heapSpacePtr->data;
+        } else if (heapSpacePtr->addr == addr2) {
+            data2 = heapSpacePtr->data;
+        }
+    }
+
+    //LOGD("*** HEAP LOAD DOUBLEWORD: Addr: 0x%x Data: 0x%x Data2: 0x%x",
+    //    addr, data, data2);
+    return (((s8) data2) << 32) | data;
+}
+
+/*
+ * Handles a store of a specified size of data to a specified address.
+ * This gets logged as an addr/data pair in heapSpace instead of modifying
+ * memory.  Addresses in heapSpace are unique, and accesses smaller than a
+ * word pull the entire word from memory first before updating.
+ */
+static void selfVerificationStore(int addr, int data, int size)
+{
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    ShadowHeap *heapSpacePtr;
+
+    int maskedAddr = addr & 0xFFFFFFFC;
+    int alignment = addr & 0x3;
+
+    //LOGD("*** HEAP STORE: Addr: 0x%x Data: 0x%x Size: %d", addr, data, size);
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == maskedAddr) break;
+    }
+
+    if (heapSpacePtr == shadowSpace->heapSpaceTail) {
+        heapSpacePtr->addr = maskedAddr;
+        heapSpacePtr->data = *((unsigned int*) maskedAddr);
+        shadowSpace->heapSpaceTail++;
+    }
+
+    addr = ((unsigned int) &(heapSpacePtr->data)) | alignment;
+    switch (size) {
+        case kSVByte:
+            *((u1*) addr) = data;
+            break;
+        case kSVSignedByte:
+            *((s1*) addr) = data;
+            break;
+        case kSVHalfword:
+            *((u2*) addr) = data;
+            break;
+        case kSVSignedHalfword:
+            *((s2*) addr) = data;
+            break;
+        case kSVWord:
+            *((u4*) addr) = data;
+            break;
+        default:
+            LOGE("*** ERROR: BAD SIZE IN selfVerificationSave: %d", size);
+            dvmAbort();
+    }
+}
+
+/* Like selfVerificationStore, but specifically for doublewords */
+static void selfVerificationStoreDoubleword(int addr, s8 double_data)
+{
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    ShadowHeap *heapSpacePtr;
+
+    int addr2 = addr+4;
+    int data = double_data;
+    int data2 = double_data >> 32;
+    bool store1 = false, store2 = false;
+
+    //LOGD("*** HEAP STORE DOUBLEWORD: Addr: 0x%x Data: 0x%x, Data2: 0x%x",
+    //    addr, data, data2);
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == addr) {
+            heapSpacePtr->data = data;
+            store1 = true;
+        } else if (heapSpacePtr->addr == addr2) {
+            heapSpacePtr->data = data2;
+            store2 = true;
+        }
+    }
+
+    if (!store1) {
+        shadowSpace->heapSpaceTail->addr = addr;
+        shadowSpace->heapSpaceTail->data = data;
+        shadowSpace->heapSpaceTail++;
+    }
+    if (!store2) {
+        shadowSpace->heapSpaceTail->addr = addr2;
+        shadowSpace->heapSpaceTail->data = data2;
+        shadowSpace->heapSpaceTail++;
+    }
+}
+
+/*
+ * Decodes the memory instruction at the address specified in the link
+ * register. All registers (r0-r12,lr) and fp registers (d0-d15) are stored
+ * consecutively on the stack beginning at the specified stack pointer.
+ * Calls the proper Self Verification handler for the memory instruction and
+ * updates the link register to point past the decoded memory instruction.
+ */
+void dvmSelfVerificationMemOpDecode(int lr, int* sp)
+{
+    enum {
+        kMemOpLdrPcRel = 0x09, // ldr(3)  [01001] rd[10..8] imm_8[7..0]
+        kMemOpRRR      = 0x0A, // Full opcode is 7 bits
+        kMemOp2Single  = 0x0A, // Used for Vstrs and Vldrs
+        kMemOpRRR2     = 0x0B, // Full opcode is 7 bits
+        kMemOp2Double  = 0x0B, // Used for Vstrd and Vldrd
+        kMemOpStrRRI5  = 0x0C, // str(1)  [01100] imm_5[10..6] rn[5..3] rd[2..0]
+        kMemOpLdrRRI5  = 0x0D, // ldr(1)  [01101] imm_5[10..6] rn[5..3] rd[2..0]
+        kMemOpStrbRRI5 = 0x0E, // strb(1) [01110] imm_5[10..6] rn[5..3] rd[2..0]
+        kMemOpLdrbRRI5 = 0x0F, // ldrb(1) [01111] imm_5[10..6] rn[5..3] rd[2..0]
+        kMemOpStrhRRI5 = 0x10, // strh(1) [10000] imm_5[10..6] rn[5..3] rd[2..0]
+        kMemOpLdrhRRI5 = 0x11, // ldrh(1) [10001] imm_5[10..6] rn[5..3] rd[2..0]
+        kMemOpLdrSpRel = 0x13, // ldr(4)  [10011] rd[10..8] imm_8[7..0]
+        kMemOpStmia    = 0x18, // stmia   [11000] rn[10..8] reglist [7..0]
+        kMemOpLdmia    = 0x19, // ldmia   [11001] rn[10..8] reglist [7..0]
+        kMemOpStrRRR   = 0x28, // str(2)  [0101000] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpStrhRRR  = 0x29, // strh(2) [0101001] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpStrbRRR  = 0x2A, // strb(2) [0101010] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpLdrsbRRR = 0x2B, // ldrsb   [0101011] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpLdrRRR   = 0x2C, // ldr(2)  [0101100] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpLdrhRRR  = 0x2D, // ldrh(2) [0101101] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpLdrbRRR  = 0x2E, // ldrb(2) [0101110] rm[8..6] rn[5..3] rd[2..0]
+        kMemOpLdrshRRR = 0x2F, // ldrsh   [0101111] rm[8..6] rn[5..3] rd[2..0]
+        kMemOp2Stmia   = 0xE88, // stmia  [111010001000[ rn[19..16] mask[15..0]
+        kMemOp2Ldmia   = 0xE89, // ldmia  [111010001001[ rn[19..16] mask[15..0]
+        kMemOp2Stmia2  = 0xE8A, // stmia  [111010001010[ rn[19..16] mask[15..0]
+        kMemOp2Ldmia2  = 0xE8B, // ldmia  [111010001011[ rn[19..16] mask[15..0]
+        kMemOp2Vstr    = 0xED8, // Used for Vstrs and Vstrd
+        kMemOp2Vldr    = 0xED9, // Used for Vldrs and Vldrd
+        kMemOp2Vstr2   = 0xEDC, // Used for Vstrs and Vstrd
+        kMemOp2Vldr2   = 0xEDD, // Used for Vstrs and Vstrd
+        kMemOp2StrbRRR = 0xF80, /* str rt,[rn,rm,LSL #imm] [111110000000]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2LdrbRRR = 0xF81, /* ldrb rt,[rn,rm,LSL #imm] [111110000001]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2StrhRRR = 0xF82, /* str rt,[rn,rm,LSL #imm] [111110000010]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2LdrhRRR = 0xF83, /* ldrh rt,[rn,rm,LSL #imm] [111110000011]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2StrRRR  = 0xF84, /* str rt,[rn,rm,LSL #imm] [111110000100]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2LdrRRR  = 0xF85, /* ldr rt,[rn,rm,LSL #imm] [111110000101]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2StrbRRI12 = 0xF88, /* strb rt,[rn,#imm12] [111110001000]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+        kMemOp2LdrbRRI12 = 0xF89, /* ldrb rt,[rn,#imm12] [111110001001]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+        kMemOp2StrhRRI12 = 0xF8A, /* strh rt,[rn,#imm12] [111110001010]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+        kMemOp2LdrhRRI12 = 0xF8B, /* ldrh rt,[rn,#imm12] [111110001011]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+        kMemOp2StrRRI12 = 0xF8C, /* str(Imm,T3) rd,[rn,#imm12] [111110001100]
+                                       rn[19..16] rt[15..12] imm12[11..0] */
+        kMemOp2LdrRRI12 = 0xF8D, /* ldr(Imm,T3) rd,[rn,#imm12] [111110001101]
+                                       rn[19..16] rt[15..12] imm12[11..0] */
+        kMemOp2LdrsbRRR = 0xF91, /* ldrsb rt,[rn,rm,LSL #imm] [111110010001]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2LdrshRRR = 0xF93, /* ldrsh rt,[rn,rm,LSL #imm] [111110010011]
+                                rn[19-16] rt[15-12] [000000] imm[5-4] rm[3-0] */
+        kMemOp2LdrsbRRI12 = 0xF99, /* ldrsb rt,[rn,#imm12] [111110011001]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+        kMemOp2LdrshRRI12 = 0xF9B, /* ldrsh rt,[rn,#imm12] [111110011011]
+                                       rt[15..12] rn[19..16] imm12[11..0] */
+        kMemOp2        = 0xE000, // top 3 bits set indicates Thumb2
+    };
+
+    int addr, offset, data;
+    long long double_data;
+    int size = kSVWord;
+    bool store = false;
+    unsigned int *lr_masked = (unsigned int *) (lr & 0xFFFFFFFE);
+    unsigned int insn = *lr_masked;
+
+    int old_lr;
+    old_lr = selfVerificationMemRegLoad(sp, 13);
+
+    if ((insn & kMemOp2) == kMemOp2) {
+        insn = (insn << 16) | (insn >> 16);
+        //LOGD("*** THUMB2 - Addr: 0x%x Insn: 0x%x", lr, insn);
+
+        int opcode12 = (insn >> 20) & 0xFFF;
+        int opcode6 = (insn >> 6) & 0x3F;
+        int opcode4 = (insn >> 8) & 0xF;
+        int imm2 = (insn >> 4) & 0x3;
+        int imm8 = insn & 0xFF;
+        int imm12 = insn & 0xFFF;
+        int rd = (insn >> 12) & 0xF;
+        int rm = insn & 0xF;
+        int rn = (insn >> 16) & 0xF;
+        int rt = (insn >> 12) & 0xF;
+        bool wBack = true;
+
+        // Update the link register
+        selfVerificationMemRegStore(sp, old_lr+4, 13);
+
+        // Determine whether the mem op is a store or load
+        switch (opcode12) {
+            case kMemOp2Stmia:
+            case kMemOp2Stmia2:
+            case kMemOp2Vstr:
+            case kMemOp2Vstr2:
+            case kMemOp2StrbRRR:
+            case kMemOp2StrhRRR:
+            case kMemOp2StrRRR:
+            case kMemOp2StrbRRI12:
+            case kMemOp2StrhRRI12:
+            case kMemOp2StrRRI12:
+                store = true;
+        }
+
+        // Determine the size of the mem access
+        switch (opcode12) {
+            case kMemOp2StrbRRR:
+            case kMemOp2LdrbRRR:
+            case kMemOp2StrbRRI12:
+            case kMemOp2LdrbRRI12:
+                size = kSVByte;
+                break;
+            case kMemOp2LdrsbRRR:
+            case kMemOp2LdrsbRRI12:
+                size = kSVSignedByte;
+                break;
+            case kMemOp2StrhRRR:
+            case kMemOp2LdrhRRR:
+            case kMemOp2StrhRRI12:
+            case kMemOp2LdrhRRI12:
+                size = kSVHalfword;
+                break;
+            case kMemOp2LdrshRRR:
+            case kMemOp2LdrshRRI12:
+                size = kSVSignedHalfword;
+                break;
+            case kMemOp2Vstr:
+            case kMemOp2Vstr2:
+            case kMemOp2Vldr:
+            case kMemOp2Vldr2:
+                if (opcode4 == kMemOp2Double) size = kSVDoubleword;
+                break;
+            case kMemOp2Stmia:
+            case kMemOp2Ldmia:
+            case kMemOp2Stmia2:
+            case kMemOp2Ldmia2:
+                size = kSVVariable;
+                break;
+        }
+
+        // Load the value of the address
+        addr = selfVerificationMemRegLoad(sp, rn);
+
+        // Figure out the offset
+        switch (opcode12) {
+            case kMemOp2Vstr:
+            case kMemOp2Vstr2:
+            case kMemOp2Vldr:
+            case kMemOp2Vldr2:
+                offset = imm8 << 2;
+                if (opcode4 == kMemOp2Single) {
+                    rt = rd << 1;
+                    if (insn & 0x400000) rt |= 0x1;
+                } else if (opcode4 == kMemOp2Double) {
+                    if (insn & 0x400000) rt |= 0x10;
+                    rt = rt << 1;
+                } else {
+                    LOGE("*** ERROR: UNRECOGNIZED VECTOR MEM OP: %x", opcode4);
+                    dvmAbort();
+                }
+                rt += 14;
+                break;
+            case kMemOp2StrbRRR:
+            case kMemOp2LdrbRRR:
+            case kMemOp2StrhRRR:
+            case kMemOp2LdrhRRR:
+            case kMemOp2StrRRR:
+            case kMemOp2LdrRRR:
+            case kMemOp2LdrsbRRR:
+            case kMemOp2LdrshRRR:
+                offset = selfVerificationMemRegLoad(sp, rm) << imm2;
+                break;
+            case kMemOp2StrbRRI12:
+            case kMemOp2LdrbRRI12:
+            case kMemOp2StrhRRI12:
+            case kMemOp2LdrhRRI12:
+            case kMemOp2StrRRI12:
+            case kMemOp2LdrRRI12:
+            case kMemOp2LdrsbRRI12:
+            case kMemOp2LdrshRRI12:
+                offset = imm12;
+                break;
+            case kMemOp2Stmia:
+            case kMemOp2Ldmia:
+                wBack = false;
+            case kMemOp2Stmia2:
+            case kMemOp2Ldmia2:
+                offset = 0;
+                break;
+            default:
+                LOGE("*** ERROR: UNRECOGNIZED THUMB2 MEM OP: %x", opcode12);
+                offset = 0;
+                dvmAbort();
+        }
+
+        // Handle the decoded mem op accordingly
+        if (store) {
+            if (size == kSVVariable) {
+                LOGD("*** THUMB2 STMIA CURRENTLY UNUSED (AND UNTESTED)");
+                int i;
+                int regList = insn & 0xFFFF;
+                for (i = 0; i < 16; i++) {
+                    if (regList & 0x1) {
+                        data = selfVerificationMemRegLoad(sp, i);
+                        selfVerificationStore(addr, data, kSVWord);
+                        addr += 4;
+                    }
+                    regList = regList >> 1;
+                }
+                if (wBack) selfVerificationMemRegStore(sp, addr, rn);
+            } else if (size == kSVDoubleword) {
+                double_data = selfVerificationMemRegLoadDouble(sp, rt);
+                selfVerificationStoreDoubleword(addr+offset, double_data);
+            } else {
+                data = selfVerificationMemRegLoad(sp, rt);
+                selfVerificationStore(addr+offset, data, size);
+            }
+        } else {
+            if (size == kSVVariable) {
+                LOGD("*** THUMB2 LDMIA CURRENTLY UNUSED (AND UNTESTED)");
+                int i;
+                int regList = insn & 0xFFFF;
+                for (i = 0; i < 16; i++) {
+                    if (regList & 0x1) {
+                        data = selfVerificationLoad(addr, kSVWord);
+                        selfVerificationMemRegStore(sp, data, i);
+                        addr += 4;
+                    }
+                    regList = regList >> 1;
+                }
+                if (wBack) selfVerificationMemRegStore(sp, addr, rn);
+            } else if (size == kSVDoubleword) {
+                double_data = selfVerificationLoadDoubleword(addr+offset);
+                selfVerificationMemRegStoreDouble(sp, double_data, rt);
+            } else {
+                data = selfVerificationLoad(addr+offset, size);
+                selfVerificationMemRegStore(sp, data, rt);
+            }
+        }
+    } else {
+        //LOGD("*** THUMB - Addr: 0x%x Insn: 0x%x", lr, insn);
+
+        // Update the link register
+        selfVerificationMemRegStore(sp, old_lr+2, 13);
+
+        int opcode5 = (insn >> 11) & 0x1F;
+        int opcode7 = (insn >> 9) & 0x7F;
+        int imm = (insn >> 6) & 0x1F;
+        int rd = (insn >> 8) & 0x7;
+        int rm = (insn >> 6) & 0x7;
+        int rn = (insn >> 3) & 0x7;
+        int rt = insn & 0x7;
+
+        // Determine whether the mem op is a store or load
+        switch (opcode5) {
+            case kMemOpRRR:
+                switch (opcode7) {
+                    case kMemOpStrRRR:
+                    case kMemOpStrhRRR:
+                    case kMemOpStrbRRR:
+                        store = true;
+                }
+                break;
+            case kMemOpStrRRI5:
+            case kMemOpStrbRRI5:
+            case kMemOpStrhRRI5:
+            case kMemOpStmia:
+                store = true;
+        }
+
+        // Determine the size of the mem access
+        switch (opcode5) {
+            case kMemOpRRR:
+            case kMemOpRRR2:
+                switch (opcode7) {
+                    case kMemOpStrbRRR:
+                    case kMemOpLdrbRRR:
+                        size = kSVByte;
+                        break;
+                    case kMemOpLdrsbRRR:
+                        size = kSVSignedByte;
+                        break;
+                    case kMemOpStrhRRR:
+                    case kMemOpLdrhRRR:
+                        size = kSVHalfword;
+                        break;
+                    case kMemOpLdrshRRR:
+                        size = kSVSignedHalfword;
+                        break;
+                }
+                break;
+            case kMemOpStrbRRI5:
+            case kMemOpLdrbRRI5:
+                size = kSVByte;
+                break;
+            case kMemOpStrhRRI5:
+            case kMemOpLdrhRRI5:
+                size = kSVHalfword;
+                break;
+            case kMemOpStmia:
+            case kMemOpLdmia:
+                size = kSVVariable;
+                break;
+        }
+
+        // Load the value of the address
+        if (opcode5 == kMemOpLdrPcRel)
+            addr = selfVerificationMemRegLoad(sp, 4);
+        else if (opcode5 == kMemOpStmia || opcode5 == kMemOpLdmia)
+            addr = selfVerificationMemRegLoad(sp, rd);
+        else
+            addr = selfVerificationMemRegLoad(sp, rn);
+
+        // Figure out the offset
+        switch (opcode5) {
+            case kMemOpLdrPcRel:
+                offset = (insn & 0xFF) << 2;
+                rt = rd;
+                break;
+            case kMemOpRRR:
+            case kMemOpRRR2:
+                offset = selfVerificationMemRegLoad(sp, rm);
+                break;
+            case kMemOpStrRRI5:
+            case kMemOpLdrRRI5:
+                offset = imm << 2;
+                break;
+            case kMemOpStrhRRI5:
+            case kMemOpLdrhRRI5:
+                offset = imm << 1;
+                break;
+            case kMemOpStrbRRI5:
+            case kMemOpLdrbRRI5:
+                offset = imm;
+                break;
+            case kMemOpStmia:
+            case kMemOpLdmia:
+                offset = 0;
+                break;
+            default:
+                LOGE("*** ERROR: UNRECOGNIZED THUMB MEM OP: %x", opcode5);
+                offset = 0;
+                dvmAbort();
+        }
+
+        // Handle the decoded mem op accordingly
+        if (store) {
+            if (size == kSVVariable) {
+                int i;
+                int regList = insn & 0xFF;
+                for (i = 0; i < 8; i++) {
+                    if (regList & 0x1) {
+                        data = selfVerificationMemRegLoad(sp, i);
+                        selfVerificationStore(addr, data, kSVWord);
+                        addr += 4;
+                    }
+                    regList = regList >> 1;
+                }
+                selfVerificationMemRegStore(sp, addr, rd);
+            } else {
+                data = selfVerificationMemRegLoad(sp, rt);
+                selfVerificationStore(addr+offset, data, size);
+            }
+        } else {
+            if (size == kSVVariable) {
+                bool wBack = true;
+                int i;
+                int regList = insn & 0xFF;
+                for (i = 0; i < 8; i++) {
+                    if (regList & 0x1) {
+                        if (i == rd) wBack = false;
+                        data = selfVerificationLoad(addr, kSVWord);
+                        selfVerificationMemRegStore(sp, data, i);
+                        addr += 4;
+                    }
+                    regList = regList >> 1;
+                }
+                if (wBack) selfVerificationMemRegStore(sp, addr, rd);
+            } else {
+                data = selfVerificationLoad(addr+offset, size);
+                selfVerificationMemRegStore(sp, data, rt);
+            }
+        }
+    }
+}
+#endif
diff --git a/vm/compiler/codegen/arm/Codegen-armv5te-vfp.c b/vm/compiler/codegen/arm/Codegen-armv5te-vfp.c
deleted file mode 100644
index c3a60e4..0000000
--- a/vm/compiler/codegen/arm/Codegen-armv5te-vfp.c
+++ /dev/null
@@ -1,29 +0,0 @@
-/*
- * Copyright (C) 2009 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Dalvik.h"
-#include "interp/InterpDefs.h"
-#include "libdex/OpCode.h"
-#include "dexdump/OpCodeNames.h"
-#include "vm/compiler/CompilerInternals.h"
-#include "ArmLIR.h"
-#include "vm/mterp/common/FindInterface.h"
-
-#include "armv5te-vfp/ArchVariant.h"
-
-#include "ThumbUtil.c"
-#include "Codegen.c"
-#include "armv5te-vfp/ArchVariant.c"
diff --git a/vm/compiler/codegen/arm/Codegen-armv5te.c b/vm/compiler/codegen/arm/Codegen-armv5te.c
deleted file mode 100644
index baf9dc9..0000000
--- a/vm/compiler/codegen/arm/Codegen-armv5te.c
+++ /dev/null
@@ -1,29 +0,0 @@
-/*
- * Copyright (C) 2009 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Dalvik.h"
-#include "interp/InterpDefs.h"
-#include "libdex/OpCode.h"
-#include "dexdump/OpCodeNames.h"
-#include "vm/compiler/CompilerInternals.h"
-#include "ArmLIR.h"
-#include "vm/mterp/common/FindInterface.h"
-
-#include "armv5te/ArchVariant.h"
-
-#include "ThumbUtil.c"
-#include "Codegen.c"
-#include "armv5te/ArchVariant.c"
diff --git a/vm/compiler/codegen/arm/Codegen-armv7-a.c b/vm/compiler/codegen/arm/Codegen-armv7-a.c
deleted file mode 100644
index a691231..0000000
--- a/vm/compiler/codegen/arm/Codegen-armv7-a.c
+++ /dev/null
@@ -1,29 +0,0 @@
-/*
- * Copyright (C) 2009 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Dalvik.h"
-#include "interp/InterpDefs.h"
-#include "libdex/OpCode.h"
-#include "dexdump/OpCodeNames.h"
-#include "vm/compiler/CompilerInternals.h"
-#include "ArmLIR.h"
-#include "vm/mterp/common/FindInterface.h"
-
-#include "armv7-a/ArchVariant.h"
-
-#include "Thumb2Util.c"
-#include "Codegen.c"
-#include "armv7-a/ArchVariant.c"
diff --git a/vm/compiler/codegen/arm/Codegen.c b/vm/compiler/codegen/arm/Codegen.c
deleted file mode 100644
index 7d127f8..0000000
--- a/vm/compiler/codegen/arm/Codegen.c
+++ /dev/null
@@ -1,3337 +0,0 @@
-/*
- * Copyright (C) 2009 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
- * This file contains codegen and support common to all supported
- * ARM variants.  It is included by:
- *
- *        Codegen-$(TARGET_ARCH_VARIANT).c
- *
- * which combines this common code with specific support found in the
- * applicable directory below this one.
- */
-
-
-/* Array holding the entry offset of each template relative to the first one */
-static intptr_t templateEntryOffsets[TEMPLATE_LAST_MARK];
-
-/* Track exercised opcodes */
-static int opcodeCoverage[256];
-
-/*****************************************************************************/
-
-/*
- * The following are building blocks to construct low-level IRs with 0 - 3
- * operands.
- */
-static ArmLIR *newLIR0(CompilationUnit *cUnit, ArmOpCode opCode)
-{
-    ArmLIR *insn = dvmCompilerNew(sizeof(ArmLIR), true);
-    assert(isPseudoOpCode(opCode) || (EncodingMap[opCode].flags & NO_OPERAND));
-    insn->opCode = opCode;
-    dvmCompilerAppendLIR(cUnit, (LIR *) insn);
-    return insn;
-}
-
-static ArmLIR *newLIR1(CompilationUnit *cUnit, ArmOpCode opCode,
-                           int dest)
-{
-    ArmLIR *insn = dvmCompilerNew(sizeof(ArmLIR), true);
-    assert(isPseudoOpCode(opCode) || (EncodingMap[opCode].flags & IS_UNARY_OP));
-    insn->opCode = opCode;
-    insn->operands[0] = dest;
-    dvmCompilerAppendLIR(cUnit, (LIR *) insn);
-    return insn;
-}
-
-static ArmLIR *newLIR2(CompilationUnit *cUnit, ArmOpCode opCode,
-                           int dest, int src1)
-{
-    ArmLIR *insn = dvmCompilerNew(sizeof(ArmLIR), true);
-    assert(isPseudoOpCode(opCode) ||
-           (EncodingMap[opCode].flags & IS_BINARY_OP));
-    insn->opCode = opCode;
-    insn->operands[0] = dest;
-    insn->operands[1] = src1;
-    dvmCompilerAppendLIR(cUnit, (LIR *) insn);
-    return insn;
-}
-
-static ArmLIR *newLIR3(CompilationUnit *cUnit, ArmOpCode opCode,
-                           int dest, int src1, int src2)
-{
-    ArmLIR *insn = dvmCompilerNew(sizeof(ArmLIR), true);
-    assert(isPseudoOpCode(opCode) ||
-           (EncodingMap[opCode].flags & IS_TERTIARY_OP));
-    insn->opCode = opCode;
-    insn->operands[0] = dest;
-    insn->operands[1] = src1;
-    insn->operands[2] = src2;
-    dvmCompilerAppendLIR(cUnit, (LIR *) insn);
-    return insn;
-}
-
-static ArmLIR *newLIR23(CompilationUnit *cUnit, ArmOpCode opCode,
-                            int srcdest, int src2)
-{
-    assert(!isPseudoOpCode(opCode));
-    if (EncodingMap[opCode].flags & IS_BINARY_OP)
-        return newLIR2(cUnit, opCode, srcdest, src2);
-    else
-        return newLIR3(cUnit, opCode, srcdest, srcdest, src2);
-}
-
-/*****************************************************************************/
-
-/*
- * The following are building blocks to insert constants into the pool or
- * instruction streams.
- */
-
-/* Add a 32-bit constant either in the constant pool or mixed with code */
-static ArmLIR *addWordData(CompilationUnit *cUnit, int value, bool inPlace)
-{
-    /* Add the constant to the literal pool */
-    if (!inPlace) {
-        ArmLIR *newValue = dvmCompilerNew(sizeof(ArmLIR), true);
-        newValue->operands[0] = value;
-        newValue->generic.next = cUnit->wordList;
-        cUnit->wordList = (LIR *) newValue;
-        return newValue;
-    } else {
-        /* Add the constant in the middle of code stream */
-        newLIR1(cUnit, ARM_16BIT_DATA, (value & 0xffff));
-        newLIR1(cUnit, ARM_16BIT_DATA, (value >> 16));
-    }
-    return NULL;
-}
-
-/*
- * Search the existing constants in the literal pool for an exact or close match
- * within specified delta (greater or equal to 0).
- */
-static ArmLIR *scanLiteralPool(CompilationUnit *cUnit, int value,
-                                   unsigned int delta)
-{
-    LIR *dataTarget = cUnit->wordList;
-    while (dataTarget) {
-        if (((unsigned) (value - ((ArmLIR *) dataTarget)->operands[0])) <=
-            delta)
-            return (ArmLIR *) dataTarget;
-        dataTarget = dataTarget->next;
-    }
-    return NULL;
-}
-
-/* Perform the actual operation for OP_RETURN_* */
-static void genReturnCommon(CompilationUnit *cUnit, MIR *mir)
-{
-    genDispatchToHandler(cUnit, TEMPLATE_RETURN);
-#if defined(INVOKE_STATS)
-    gDvmJit.returnOp++;
-#endif
-    int dPC = (int) (cUnit->method->insns + mir->offset);
-    /* Insert branch, but defer setting of target */
-    ArmLIR *branch = genUnconditionalBranch(cUnit, NULL);
-    /* Set up the place holder to reconstruct this Dalvik PC */
-    ArmLIR *pcrLabel = dvmCompilerNew(sizeof(ArmLIR), true);
-    pcrLabel->opCode = ARM_PSEUDO_PC_RECONSTRUCTION_CELL;
-    pcrLabel->operands[0] = dPC;
-    pcrLabel->operands[1] = mir->offset;
-    /* Insert the place holder to the growable list */
-    dvmInsertGrowableList(&cUnit->pcReconstructionList, pcrLabel);
-    /* Branch to the PC reconstruction code */
-    branch->generic.target = (LIR *) pcrLabel;
-}
-
-/*
- * Perform a binary operation on 64-bit operands and leave the results in the
- * r0/r1 pair.
- */
-static void genBinaryOpWide(CompilationUnit *cUnit, int vDest,
-                            ArmOpCode preinst, ArmOpCode inst,
-                            int reg0, int reg2)
-{
-    int reg1 = NEXT_REG(reg0);
-    int reg3 = NEXT_REG(reg2);
-    newLIR23(cUnit, preinst, reg0, reg2);
-    newLIR23(cUnit, inst, reg1, reg3);
-    storeValuePair(cUnit, reg0, reg1, vDest, reg2);
-}
-
-/* Perform a binary operation on 32-bit operands and leave the results in r0. */
-static void genBinaryOp(CompilationUnit *cUnit, int vDest, ArmOpCode inst,
-                        int reg0, int reg1, int regDest)
-{
-    if (EncodingMap[inst].flags & IS_BINARY_OP) {
-        newLIR2(cUnit, inst, reg0, reg1);
-        storeValue(cUnit, reg0, vDest, reg1);
-    } else {
-        newLIR3(cUnit, inst, regDest, reg0, reg1);
-        storeValue(cUnit, regDest, vDest, reg1);
-    }
-}
-
-/* Create the PC reconstruction slot if not already done */
-static inline ArmLIR *genCheckCommon(CompilationUnit *cUnit, int dOffset,
-                                         ArmLIR *branch,
-                                         ArmLIR *pcrLabel)
-{
-    /* Set up the place holder to reconstruct this Dalvik PC */
-    if (pcrLabel == NULL) {
-        int dPC = (int) (cUnit->method->insns + dOffset);
-        pcrLabel = dvmCompilerNew(sizeof(ArmLIR), true);
-        pcrLabel->opCode = ARM_PSEUDO_PC_RECONSTRUCTION_CELL;
-        pcrLabel->operands[0] = dPC;
-        pcrLabel->operands[1] = dOffset;
-        /* Insert the place holder to the growable list */
-        dvmInsertGrowableList(&cUnit->pcReconstructionList, pcrLabel);
-    }
-    /* Branch to the PC reconstruction code */
-    branch->generic.target = (LIR *) pcrLabel;
-    return pcrLabel;
-}
-
-
-/*
- * Perform a "reg cmp reg" operation and jump to the PCR region if condition
- * satisfies.
- */
-static inline ArmLIR *inertRegRegCheck(CompilationUnit *cUnit,
-                                           ArmConditionCode cond,
-                                           int reg1, int reg2, int dOffset,
-                                           ArmLIR *pcrLabel)
-{
-    newLIR2(cUnit, THUMB_CMP_RR, reg1, reg2);
-    ArmLIR *branch = newLIR2(cUnit, THUMB_B_COND, 0, cond);
-    return genCheckCommon(cUnit, dOffset, branch, pcrLabel);
-}
-
-/*
- * Perform null-check on a register. vReg is the Dalvik register being checked,
- * and mReg is the machine register holding the actual value. If internal state
- * indicates that vReg has been checked before the check request is ignored.
- */
-static ArmLIR *genNullCheck(CompilationUnit *cUnit, int vReg, int mReg,
-                                int dOffset, ArmLIR *pcrLabel)
-{
-    /* This particular Dalvik register has been null-checked */
-    if (dvmIsBitSet(cUnit->registerScoreboard.nullCheckedRegs, vReg)) {
-        return pcrLabel;
-    }
-    dvmSetBit(cUnit->registerScoreboard.nullCheckedRegs, vReg);
-    return genRegImmCheck(cUnit, ARM_COND_EQ, mReg, 0, dOffset, pcrLabel);
-}
-
-/*
- * Perform zero-check on a register. Similar to genNullCheck but the value being
- * checked does not have a corresponding Dalvik register.
- */
-static ArmLIR *genZeroCheck(CompilationUnit *cUnit, int mReg,
-                                int dOffset, ArmLIR *pcrLabel)
-{
-    return genRegImmCheck(cUnit, ARM_COND_EQ, mReg, 0, dOffset, pcrLabel);
-}
-
-/* Perform bound check on two registers */
-static ArmLIR *genBoundsCheck(CompilationUnit *cUnit, int rIndex,
-                                  int rBound, int dOffset, ArmLIR *pcrLabel)
-{
-    return inertRegRegCheck(cUnit, ARM_COND_CS, rIndex, rBound, dOffset,
-                            pcrLabel);
-}
-
-/* Generate a unconditional branch to go to the interpreter */
-static inline ArmLIR *genTrap(CompilationUnit *cUnit, int dOffset,
-                                  ArmLIR *pcrLabel)
-{
-    ArmLIR *branch = newLIR0(cUnit, THUMB_B_UNCOND);
-    return genCheckCommon(cUnit, dOffset, branch, pcrLabel);
-}
-
-/* Load a wide field from an object instance */
-static void genIGetWide(CompilationUnit *cUnit, MIR *mir, int fieldOffset)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0, reg1, reg2, reg3;
-
-    /* Allocate reg0..reg3 into physical registers r0..r3 */
-
-    /* See if vB is in a native register. If so, reuse it. */
-    reg2 = selectFirstRegister(cUnit, dInsn->vB, false);
-    /* Ping reg3 to the other register of the same pair containing reg2 */
-    reg3 = reg2 ^ 0x1;
-    /*
-     * Ping reg0 to the first register of the alternate register pair
-     */
-    reg0 = (reg2 + 2) & 0x2;
-    reg1 = NEXT_REG(reg0);
-
-    loadValue(cUnit, dInsn->vB, reg2);
-    loadConstant(cUnit, reg3, fieldOffset);
-    genNullCheck(cUnit, dInsn->vB, reg2, mir->offset, NULL); /* null object? */
-    newLIR3(cUnit, THUMB_ADD_RRR, reg2, reg2, reg3);
-    newLIR2(cUnit, THUMB_LDMIA, reg2, (1<<reg0 | 1<<reg1));
-    storeValuePair(cUnit, reg0, reg1, dInsn->vA, reg3);
-}
-
-/* Store a wide field to an object instance */
-static void genIPutWide(CompilationUnit *cUnit, MIR *mir, int fieldOffset)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0, reg1, reg2, reg3;
-
-    /* Allocate reg0..reg3 into physical registers r0..r3 */
-
-    /* See if vB is in a native register. If so, reuse it. */
-    reg2 = selectFirstRegister(cUnit, dInsn->vB, false);
-    /* Ping reg3 to the other register of the same pair containing reg2 */
-    reg3 = reg2 ^ 0x1;
-    /*
-     * Ping reg0 to the first register of the alternate register pair
-     */
-    reg0 = (reg2 + 2) & 0x2;
-    reg1 = NEXT_REG(reg0);
-
-
-    loadValue(cUnit, dInsn->vB, reg2);
-    loadValuePair(cUnit, dInsn->vA, reg0, reg1);
-    updateLiveRegisterPair(cUnit, dInsn->vA, reg0, reg1);
-    loadConstant(cUnit, reg3, fieldOffset);
-    genNullCheck(cUnit, dInsn->vB, reg2, mir->offset, NULL); /* null object? */
-    newLIR3(cUnit, THUMB_ADD_RRR, reg2, reg2, reg3);
-    newLIR2(cUnit, THUMB_STMIA, reg2, (1<<reg0 | 1<<reg1));
-}
-
-/*
- * Load a field from an object instance
- *
- * Inst should be one of:
- *      THUMB_LDR_RRR
- *      THUMB_LDRB_RRR
- *      THUMB_LDRH_RRR
- *      THUMB_LDRSB_RRR
- *      THUMB_LDRSH_RRR
- */
-static void genIGet(CompilationUnit *cUnit, MIR *mir, ArmOpCode inst,
-                    int fieldOffset)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0, reg1;
-
-    reg0 = selectFirstRegister(cUnit, dInsn->vB, false);
-    reg1 = NEXT_REG(reg0);
-    /* TUNING: write a utility routine to load via base + constant offset */
-    loadValue(cUnit, dInsn->vB, reg0);
-    loadConstant(cUnit, reg1, fieldOffset);
-    genNullCheck(cUnit, dInsn->vB, reg0, mir->offset, NULL); /* null object? */
-    newLIR3(cUnit, inst, reg0, reg0, reg1);
-    storeValue(cUnit, reg0, dInsn->vA, reg1);
-}
-
-/*
- * Store a field to an object instance
- *
- * Inst should be one of:
- *      THUMB_STR_RRR
- *      THUMB_STRB_RRR
- *      THUMB_STRH_RRR
- */
-static void genIPut(CompilationUnit *cUnit, MIR *mir, ArmOpCode inst,
-                    int fieldOffset)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0, reg1, reg2;
-
-    reg0 = selectFirstRegister(cUnit, dInsn->vB, false);
-    reg1 = NEXT_REG(reg0);
-    reg2 = NEXT_REG(reg1);
-
-    /* TUNING: write a utility routine to load via base + constant offset */
-    loadValue(cUnit, dInsn->vB, reg0);
-    loadConstant(cUnit, reg1, fieldOffset);
-    loadValue(cUnit, dInsn->vA, reg2);
-    updateLiveRegister(cUnit, dInsn->vA, reg2);
-    genNullCheck(cUnit, dInsn->vB, reg0, mir->offset, NULL); /* null object? */
-    newLIR3(cUnit, inst, reg2, reg0, reg1);
-}
-
-
-/* TODO: This should probably be done as an out-of-line instruction handler. */
-
-/*
- * Generate array load
- *
- * Inst should be one of:
- *      THUMB_LDR_RRR
- *      THUMB_LDRB_RRR
- *      THUMB_LDRH_RRR
- *      THUMB_LDRSB_RRR
- *      THUMB_LDRSH_RRR
- */
-static void genArrayGet(CompilationUnit *cUnit, MIR *mir, ArmOpCode inst,
-                        int vArray, int vIndex, int vDest, int scale)
-{
-    int lenOffset = offsetof(ArrayObject, length);
-    int dataOffset = offsetof(ArrayObject, contents);
-    int reg0, reg1, reg2, reg3;
-
-    reg0 = selectFirstRegister(cUnit, vArray, false);
-    reg1 = NEXT_REG(reg0);
-    reg2 = NEXT_REG(reg1);
-    reg3 = NEXT_REG(reg2);
-
-    loadValue(cUnit, vArray, reg2);
-    loadValue(cUnit, vIndex, reg3);
-
-    /* null object? */
-    ArmLIR * pcrLabel = genNullCheck(cUnit, vArray, reg2, mir->offset,
-                                         NULL);
-    newLIR3(cUnit, THUMB_LDR_RRI5, reg0, reg2, lenOffset >> 2);  /* Get len */
-    newLIR2(cUnit, THUMB_ADD_RI8, reg2, dataOffset); /* reg2 -> array data */
-    genBoundsCheck(cUnit, reg3, reg0, mir->offset, pcrLabel);
-    if (scale) {
-        newLIR3(cUnit, THUMB_LSL, reg3, reg3, scale);
-    }
-    if (scale==3) {
-        newLIR3(cUnit, inst, reg0, reg2, reg3);
-        newLIR2(cUnit, THUMB_ADD_RI8, reg2, 4);
-        newLIR3(cUnit, inst, reg1, reg2, reg3);
-        storeValuePair(cUnit, reg0, reg1, vDest, reg3);
-    } else {
-        newLIR3(cUnit, inst, reg0, reg2, reg3);
-        storeValue(cUnit, reg0, vDest, reg3);
-    }
-}
-
-/* TODO: This should probably be done as an out-of-line instruction handler. */
-
-/*
- * Generate array store
- *
- * Inst should be one of:
- *      THUMB_STR_RRR
- *      THUMB_STRB_RRR
- *      THUMB_STRH_RRR
- */
-static void genArrayPut(CompilationUnit *cUnit, MIR *mir, ArmOpCode inst,
-                        int vArray, int vIndex, int vSrc, int scale)
-{
-    int lenOffset = offsetof(ArrayObject, length);
-    int dataOffset = offsetof(ArrayObject, contents);
-    int reg0, reg1, reg2, reg3;
-
-    reg0 = selectFirstRegister(cUnit, vArray, false);
-    reg1 = NEXT_REG(reg0);
-    reg2 = NEXT_REG(reg1);
-    reg3 = NEXT_REG(reg2);
-
-    loadValue(cUnit, vArray, reg2);
-    loadValue(cUnit, vIndex, reg3);
-
-    /* null object? */
-    ArmLIR * pcrLabel = genNullCheck(cUnit, vArray, reg2, mir->offset,
-                                         NULL);
-    newLIR3(cUnit, THUMB_LDR_RRI5, reg0, reg2, lenOffset >> 2);  /* Get len */
-    newLIR2(cUnit, THUMB_ADD_RI8, reg2, dataOffset); /* reg2 -> array data */
-    genBoundsCheck(cUnit, reg3, reg0, mir->offset, pcrLabel);
-    /* at this point, reg2 points to array, reg3 is unscaled index */
-    if (scale==3) {
-        loadValuePair(cUnit, vSrc, reg0, reg1);
-        updateLiveRegisterPair(cUnit, vSrc, reg0, reg1);
-    } else {
-        loadValue(cUnit, vSrc, reg0);
-        updateLiveRegister(cUnit, vSrc, reg0);
-    }
-    if (scale) {
-        newLIR3(cUnit, THUMB_LSL, reg3, reg3, scale);
-    }
-    /*
-     * at this point, reg2 points to array, reg3 is scaled index, and
-     * reg0[reg1] is data
-     */
-    if (scale==3) {
-        newLIR3(cUnit, inst, reg0, reg2, reg3);
-        newLIR2(cUnit, THUMB_ADD_RI8, reg2, 4);
-        newLIR3(cUnit, inst, reg1, reg2, reg3);
-    } else {
-        newLIR3(cUnit, inst, reg0, reg2, reg3);
-    }
-}
-
-static bool genShiftOpLong(CompilationUnit *cUnit, MIR *mir, int vDest,
-                           int vSrc1, int vShift)
-{
-    /*
-     * Don't mess with the regsiters here as there is a particular calling
-     * convention to the out-of-line handler.
-     */
-    loadValue(cUnit, vShift, r2);
-    loadValuePair(cUnit, vSrc1, r0, r1);
-    switch( mir->dalvikInsn.opCode) {
-        case OP_SHL_LONG:
-        case OP_SHL_LONG_2ADDR:
-            genDispatchToHandler(cUnit, TEMPLATE_SHL_LONG);
-            break;
-        case OP_SHR_LONG:
-        case OP_SHR_LONG_2ADDR:
-            genDispatchToHandler(cUnit, TEMPLATE_SHR_LONG);
-            break;
-        case OP_USHR_LONG:
-        case OP_USHR_LONG_2ADDR:
-            genDispatchToHandler(cUnit, TEMPLATE_USHR_LONG);
-            break;
-        default:
-            return true;
-    }
-    storeValuePair(cUnit, r0, r1, vDest, r2);
-    return false;
-}
-bool genArithOpFloatPortable(CompilationUnit *cUnit, MIR *mir,
-                             int vDest, int vSrc1, int vSrc2)
-{
-    /*
-     * Don't optimize the regsiter usage here as they are governed by the EABI
-     * calling convention.
-     */
-    void* funct;
-    int reg0, reg1;
-
-    /* TODO: use a proper include file to define these */
-    float __aeabi_fadd(float a, float b);
-    float __aeabi_fsub(float a, float b);
-    float __aeabi_fdiv(float a, float b);
-    float __aeabi_fmul(float a, float b);
-    float fmodf(float a, float b);
-
-    reg0 = selectFirstRegister(cUnit, vSrc2, false);
-    reg1 = NEXT_REG(reg0);
-
-    switch (mir->dalvikInsn.opCode) {
-        case OP_ADD_FLOAT_2ADDR:
-        case OP_ADD_FLOAT:
-            funct = (void*) __aeabi_fadd;
-            break;
-        case OP_SUB_FLOAT_2ADDR:
-        case OP_SUB_FLOAT:
-            funct = (void*) __aeabi_fsub;
-            break;
-        case OP_DIV_FLOAT_2ADDR:
-        case OP_DIV_FLOAT:
-            funct = (void*) __aeabi_fdiv;
-            break;
-        case OP_MUL_FLOAT_2ADDR:
-        case OP_MUL_FLOAT:
-            funct = (void*) __aeabi_fmul;
-            break;
-        case OP_REM_FLOAT_2ADDR:
-        case OP_REM_FLOAT:
-            funct = (void*) fmodf;
-            break;
-        case OP_NEG_FLOAT: {
-            loadValue(cUnit, vSrc2, reg0);
-            loadConstant(cUnit, reg1, 0x80000000);
-            newLIR3(cUnit, THUMB_ADD_RRR, reg0, reg0, reg1);
-            storeValue(cUnit, reg0, vDest, reg1);
-            return false;
-        }
-        default:
-            return true;
-    }
-    loadConstant(cUnit, r2, (int)funct);
-    loadValue(cUnit, vSrc1, r0);
-    loadValue(cUnit, vSrc2, r1);
-    newLIR1(cUnit, THUMB_BLX_R, r2);
-    storeValue(cUnit, r0, vDest, r1);
-    return false;
-}
-
-bool genArithOpDoublePortable(CompilationUnit *cUnit, MIR *mir,
-                              int vDest, int vSrc1, int vSrc2)
-{
-    void* funct;
-    int reg0, reg1, reg2;
-
-    /* TODO: use a proper include file to define these */
-    double __aeabi_dadd(double a, double b);
-    double __aeabi_dsub(double a, double b);
-    double __aeabi_ddiv(double a, double b);
-    double __aeabi_dmul(double a, double b);
-    double fmod(double a, double b);
-
-    reg0 = selectFirstRegister(cUnit, vSrc2, true);
-    reg1 = NEXT_REG(reg0);
-    reg2 = NEXT_REG(reg1);
-
-    switch (mir->dalvikInsn.opCode) {
-        case OP_ADD_DOUBLE_2ADDR:
-        case OP_ADD_DOUBLE:
-            funct = (void*) __aeabi_dadd;
-            break;
-        case OP_SUB_DOUBLE_2ADDR:
-        case OP_SUB_DOUBLE:
-            funct = (void*) __aeabi_dsub;
-            break;
-        case OP_DIV_DOUBLE_2ADDR:
-        case OP_DIV_DOUBLE:
-            funct = (void*) __aeabi_ddiv;
-            break;
-        case OP_MUL_DOUBLE_2ADDR:
-        case OP_MUL_DOUBLE:
-            funct = (void*) __aeabi_dmul;
-            break;
-        case OP_REM_DOUBLE_2ADDR:
-        case OP_REM_DOUBLE:
-            funct = (void*) fmod;
-            break;
-        case OP_NEG_DOUBLE: {
-            loadValuePair(cUnit, vSrc2, reg0, reg1);
-            loadConstant(cUnit, reg2, 0x80000000);
-            newLIR3(cUnit, THUMB_ADD_RRR, reg1, reg1, reg2);
-            storeValuePair(cUnit, reg0, reg1, vDest, reg2);
-            return false;
-        }
-        default:
-            return true;
-    }
-    /*
-     * Don't optimize the regsiter usage here as they are governed by the EABI
-     * calling convention.
-     */
-    loadConstant(cUnit, r4PC, (int)funct);
-    loadValuePair(cUnit, vSrc1, r0, r1);
-    loadValuePair(cUnit, vSrc2, r2, r3);
-    newLIR1(cUnit, THUMB_BLX_R, r4PC);
-    storeValuePair(cUnit, r0, r1, vDest, r2);
-    return false;
-}
-
-static bool genArithOpLong(CompilationUnit *cUnit, MIR *mir, int vDest,
-                           int vSrc1, int vSrc2)
-{
-    int firstOp = THUMB_BKPT;
-    int secondOp = THUMB_BKPT;
-    bool callOut = false;
-    void *callTgt;
-    int retReg = r0;
-    int reg0, reg1, reg2, reg3;
-    /* TODO - find proper .h file to declare these */
-    long long __aeabi_ldivmod(long long op1, long long op2);
-
-    switch (mir->dalvikInsn.opCode) {
-        case OP_NOT_LONG:
-            firstOp = THUMB_MVN;
-            secondOp = THUMB_MVN;
-            break;
-        case OP_ADD_LONG:
-        case OP_ADD_LONG_2ADDR:
-            firstOp = THUMB_ADD_RRR;
-            secondOp = THUMB_ADC;
-            break;
-        case OP_SUB_LONG:
-        case OP_SUB_LONG_2ADDR:
-            firstOp = THUMB_SUB_RRR;
-            secondOp = THUMB_SBC;
-            break;
-        case OP_MUL_LONG:
-        case OP_MUL_LONG_2ADDR:
-            loadValuePair(cUnit, vSrc1, r0, r1);
-            loadValuePair(cUnit, vSrc2, r2, r3);
-            genDispatchToHandler(cUnit, TEMPLATE_MUL_LONG);
-            storeValuePair(cUnit, r0, r1, vDest, r2);
-            return false;
-            break;
-        case OP_DIV_LONG:
-        case OP_DIV_LONG_2ADDR:
-            callOut = true;
-            retReg = r0;
-            callTgt = (void*)__aeabi_ldivmod;
-            break;
-        /* NOTE - result is in r2/r3 instead of r0/r1 */
-        case OP_REM_LONG:
-        case OP_REM_LONG_2ADDR:
-            callOut = true;
-            callTgt = (void*)__aeabi_ldivmod;
-            retReg = r2;
-            break;
-        case OP_AND_LONG:
-        case OP_AND_LONG_2ADDR:
-            firstOp = THUMB_AND_RR;
-            secondOp = THUMB_AND_RR;
-            break;
-        case OP_OR_LONG:
-        case OP_OR_LONG_2ADDR:
-            firstOp = THUMB_ORR;
-            secondOp = THUMB_ORR;
-            break;
-        case OP_XOR_LONG:
-        case OP_XOR_LONG_2ADDR:
-            firstOp = THUMB_EOR;
-            secondOp = THUMB_EOR;
-            break;
-        case OP_NEG_LONG: {
-            reg0 = selectFirstRegister(cUnit, vSrc2, true);
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-            reg3 = NEXT_REG(reg2);
-
-            loadValuePair(cUnit, vSrc2, reg0, reg1);
-            loadConstant(cUnit, reg3, 0);
-            newLIR3(cUnit, THUMB_SUB_RRR, reg2, reg3, reg0);
-            newLIR2(cUnit, THUMB_SBC, reg3, reg1);
-            storeValuePair(cUnit, reg2, reg3, vDest, reg0);
-            return false;
-        }
-        default:
-            LOGE("Invalid long arith op");
-            dvmAbort();
-    }
-    if (!callOut) {
-        reg0 = selectFirstRegister(cUnit, vSrc1, true);
-        reg1 = NEXT_REG(reg0);
-        reg2 = NEXT_REG(reg1);
-        reg3 = NEXT_REG(reg2);
-
-        loadValuePair(cUnit, vSrc1, reg0, reg1);
-        loadValuePair(cUnit, vSrc2, reg2, reg3);
-        genBinaryOpWide(cUnit, vDest, firstOp, secondOp, reg0, reg2);
-    /*
-     * Don't optimize the regsiter usage here as they are governed by the EABI
-     * calling convention.
-     */
-    } else {
-        loadValuePair(cUnit, vSrc2, r2, r3);
-        loadConstant(cUnit, r4PC, (int) callTgt);
-        loadValuePair(cUnit, vSrc1, r0, r1);
-        newLIR1(cUnit, THUMB_BLX_R, r4PC);
-        storeValuePair(cUnit, retReg, retReg+1, vDest, r4PC);
-    }
-    return false;
-}
-
-static bool genArithOpInt(CompilationUnit *cUnit, MIR *mir, int vDest,
-                          int vSrc1, int vSrc2)
-{
-    int armOp = THUMB_BKPT;
-    bool callOut = false;
-    bool checkZero = false;
-    int retReg = r0;
-    void *callTgt;
-    int reg0, reg1, regDest;
-
-    /* TODO - find proper .h file to declare these */
-    int __aeabi_idivmod(int op1, int op2);
-    int __aeabi_idiv(int op1, int op2);
-
-    switch (mir->dalvikInsn.opCode) {
-        case OP_NEG_INT:
-            armOp = THUMB_NEG;
-            break;
-        case OP_NOT_INT:
-            armOp = THUMB_MVN;
-            break;
-        case OP_ADD_INT:
-        case OP_ADD_INT_2ADDR:
-            armOp = THUMB_ADD_RRR;
-            break;
-        case OP_SUB_INT:
-        case OP_SUB_INT_2ADDR:
-            armOp = THUMB_SUB_RRR;
-            break;
-        case OP_MUL_INT:
-        case OP_MUL_INT_2ADDR:
-            armOp = THUMB_MUL;
-            break;
-        case OP_DIV_INT:
-        case OP_DIV_INT_2ADDR:
-            callOut = true;
-            checkZero = true;
-            callTgt = __aeabi_idiv;
-            retReg = r0;
-            break;
-        /* NOTE: returns in r1 */
-        case OP_REM_INT:
-        case OP_REM_INT_2ADDR:
-            callOut = true;
-            checkZero = true;
-            callTgt = __aeabi_idivmod;
-            retReg = r1;
-            break;
-        case OP_AND_INT:
-        case OP_AND_INT_2ADDR:
-            armOp = THUMB_AND_RR;
-            break;
-        case OP_OR_INT:
-        case OP_OR_INT_2ADDR:
-            armOp = THUMB_ORR;
-            break;
-        case OP_XOR_INT:
-        case OP_XOR_INT_2ADDR:
-            armOp = THUMB_EOR;
-            break;
-        case OP_SHL_INT:
-        case OP_SHL_INT_2ADDR:
-            armOp = THUMB_LSLV;
-            break;
-        case OP_SHR_INT:
-        case OP_SHR_INT_2ADDR:
-            armOp = THUMB_ASRV;
-            break;
-        case OP_USHR_INT:
-        case OP_USHR_INT_2ADDR:
-            armOp = THUMB_LSRV;
-            break;
-        default:
-            LOGE("Invalid word arith op: 0x%x(%d)",
-                 mir->dalvikInsn.opCode, mir->dalvikInsn.opCode);
-            dvmAbort();
-    }
-    if (!callOut) {
-         /* Try to allocate reg0 to the currently cached source operand  */
-        if (cUnit->registerScoreboard.liveDalvikReg == vSrc1) {
-            reg0 = selectFirstRegister(cUnit, vSrc1, false);
-            reg1 = NEXT_REG(reg0);
-            regDest = NEXT_REG(reg1);
-
-            loadValue(cUnit, vSrc1, reg0); /* Should be optimized away */
-            loadValue(cUnit, vSrc2, reg1);
-            genBinaryOp(cUnit, vDest, armOp, reg0, reg1, regDest);
-        } else {
-            reg0 = selectFirstRegister(cUnit, vSrc2, false);
-            reg1 = NEXT_REG(reg0);
-            regDest = NEXT_REG(reg1);
-
-            loadValue(cUnit, vSrc1, reg1); /* Load this value first */
-            loadValue(cUnit, vSrc2, reg0); /* May be optimized away */
-            genBinaryOp(cUnit, vDest, armOp, reg1, reg0, regDest);
-        }
-    } else {
-        /*
-         * Load the callout target first since it will never be eliminated
-         * and its value will be used first.
-         */
-        loadConstant(cUnit, r2, (int) callTgt);
-        /*
-         * Load vSrc2 first if it is not cached in a native register or it
-         * is in r0 which will be clobbered if vSrc1 is loaded first.
-         */
-        if (cUnit->registerScoreboard.liveDalvikReg != vSrc2 ||
-            cUnit->registerScoreboard.nativeReg == r0) {
-            /* Cannot be optimized and won't clobber r0 */
-            loadValue(cUnit, vSrc2, r1);
-            /* May be optimized if vSrc1 is cached */
-            loadValue(cUnit, vSrc1, r0);
-        } else {
-            loadValue(cUnit, vSrc1, r0);
-            loadValue(cUnit, vSrc2, r1);
-        }
-        if (checkZero) {
-            genNullCheck(cUnit, vSrc2, r1, mir->offset, NULL);
-        }
-        newLIR1(cUnit, THUMB_BLX_R, r2);
-        storeValue(cUnit, retReg, vDest, r2);
-    }
-    return false;
-}
-
-static bool genArithOp(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    int vC = mir->dalvikInsn.vC;
-
-    if ((opCode >= OP_ADD_LONG_2ADDR) && (opCode <= OP_XOR_LONG_2ADDR)) {
-        return genArithOpLong(cUnit,mir, vA, vA, vB);
-    }
-    if ((opCode >= OP_ADD_LONG) && (opCode <= OP_XOR_LONG)) {
-        return genArithOpLong(cUnit,mir, vA, vB, vC);
-    }
-    if ((opCode >= OP_SHL_LONG_2ADDR) && (opCode <= OP_USHR_LONG_2ADDR)) {
-        return genShiftOpLong(cUnit,mir, vA, vA, vB);
-    }
-    if ((opCode >= OP_SHL_LONG) && (opCode <= OP_USHR_LONG)) {
-        return genShiftOpLong(cUnit,mir, vA, vB, vC);
-    }
-    if ((opCode >= OP_ADD_INT_2ADDR) && (opCode <= OP_USHR_INT_2ADDR)) {
-        return genArithOpInt(cUnit,mir, vA, vA, vB);
-    }
-    if ((opCode >= OP_ADD_INT) && (opCode <= OP_USHR_INT)) {
-        return genArithOpInt(cUnit,mir, vA, vB, vC);
-    }
-    if ((opCode >= OP_ADD_FLOAT_2ADDR) && (opCode <= OP_REM_FLOAT_2ADDR)) {
-        return genArithOpFloat(cUnit,mir, vA, vA, vB);
-    }
-    if ((opCode >= OP_ADD_FLOAT) && (opCode <= OP_REM_FLOAT)) {
-        return genArithOpFloat(cUnit, mir, vA, vB, vC);
-    }
-    if ((opCode >= OP_ADD_DOUBLE_2ADDR) && (opCode <= OP_REM_DOUBLE_2ADDR)) {
-        return genArithOpDouble(cUnit,mir, vA, vA, vB);
-    }
-    if ((opCode >= OP_ADD_DOUBLE) && (opCode <= OP_REM_DOUBLE)) {
-        return genArithOpDouble(cUnit,mir, vA, vB, vC);
-    }
-    return true;
-}
-
-static bool genConversionCall(CompilationUnit *cUnit, MIR *mir, void *funct,
-                                     int srcSize, int tgtSize)
-{
-    /*
-     * Don't optimize the register usage since it calls out to template
-     * functions
-     */
-    loadConstant(cUnit, r2, (int)funct);
-    if (srcSize == 1) {
-        loadValue(cUnit, mir->dalvikInsn.vB, r0);
-    } else {
-        loadValuePair(cUnit, mir->dalvikInsn.vB, r0, r1);
-    }
-    newLIR1(cUnit, THUMB_BLX_R, r2);
-    if (tgtSize == 1) {
-        storeValue(cUnit, r0, mir->dalvikInsn.vA, r1);
-    } else {
-        storeValuePair(cUnit, r0, r1, mir->dalvikInsn.vA, r2);
-    }
-    return false;
-}
-
-static bool genInlinedStringLength(CompilationUnit *cUnit, MIR *mir)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int offset = offsetof(InterpState, retval);
-    int regObj = selectFirstRegister(cUnit, dInsn->arg[0], false);
-    int reg1 = NEXT_REG(regObj);
-    loadValue(cUnit, dInsn->arg[0], regObj);
-    genNullCheck(cUnit, dInsn->arg[0], regObj, mir->offset, NULL);
-    loadWordDisp(cUnit, regObj, gDvm.offJavaLangString_count, reg1);
-    newLIR3(cUnit, THUMB_STR_RRI5, reg1, rGLUE, offset >> 2);
-    return false;
-}
-
-/*
- * NOTE: The amount of code for this body suggests it ought to
- * be handled in a template (and could also be coded quite a bit
- * more efficiently in ARM).  However, the code is dependent on the
- * internal structure layout of string objects which are most safely
- * known at run time.
- * TUNING:  One possibility (which could also be used for StringCompareTo
- * and StringEquals) is to generate string access helper subroutines on
- * Jit startup, and then call them from the translated inline-executes.
- */
-static bool genInlinedStringCharAt(CompilationUnit *cUnit, MIR *mir)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int offset = offsetof(InterpState, retval);
-    int contents = offsetof(ArrayObject, contents);
-    int regObj = selectFirstRegister(cUnit, dInsn->arg[0], false);
-    int regIdx = NEXT_REG(regObj);
-    int regMax = NEXT_REG(regIdx);
-    int regOff = NEXT_REG(regMax);
-    loadValue(cUnit, dInsn->arg[0], regObj);
-    loadValue(cUnit, dInsn->arg[1], regIdx);
-    ArmLIR * pcrLabel = genNullCheck(cUnit, dInsn->arg[0], regObj,
-                                         mir->offset, NULL);
-    loadWordDisp(cUnit, regObj, gDvm.offJavaLangString_count, regMax);
-    loadWordDisp(cUnit, regObj, gDvm.offJavaLangString_offset, regOff);
-    loadWordDisp(cUnit, regObj, gDvm.offJavaLangString_value, regObj);
-    genBoundsCheck(cUnit, regIdx, regMax, mir->offset, pcrLabel);
-
-    newLIR2(cUnit, THUMB_ADD_RI8, regObj, contents);
-    newLIR3(cUnit, THUMB_ADD_RRR, regIdx, regIdx, regOff);
-    newLIR3(cUnit, THUMB_ADD_RRR, regIdx, regIdx, regIdx);
-    newLIR3(cUnit, THUMB_LDRH_RRR, regMax, regObj, regIdx);
-    newLIR3(cUnit, THUMB_STR_RRI5, regMax, rGLUE, offset >> 2);
-    return false;
-}
-
-static bool genInlinedAbsInt(CompilationUnit *cUnit, MIR *mir)
-{
-    int offset = offsetof(InterpState, retval);
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0 = selectFirstRegister(cUnit, dInsn->arg[0], false);
-    int sign = NEXT_REG(reg0);
-    /* abs(x) = y<=x>>31, (x+y)^y.  Shorter in ARM/THUMB2, no skip in THUMB */
-    loadValue(cUnit, dInsn->arg[0], reg0);
-    newLIR3(cUnit, THUMB_ASR, sign, reg0, 31);
-    newLIR3(cUnit, THUMB_ADD_RRR, reg0, reg0, sign);
-    newLIR2(cUnit, THUMB_EOR, reg0, sign);
-    newLIR3(cUnit, THUMB_STR_RRI5, reg0, rGLUE, offset >> 2);
-    return false;
-}
-
-static bool genInlinedAbsFloat(CompilationUnit *cUnit, MIR *mir)
-{
-    int offset = offsetof(InterpState, retval);
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0 = selectFirstRegister(cUnit, dInsn->arg[0], false);
-    int signMask = NEXT_REG(reg0);
-    loadValue(cUnit, dInsn->arg[0], reg0);
-    loadConstant(cUnit, signMask, 0x7fffffff);
-    newLIR2(cUnit, THUMB_AND_RR, reg0, signMask);
-    newLIR3(cUnit, THUMB_STR_RRI5, reg0, rGLUE, offset >> 2);
-    return false;
-}
-
-static bool genInlinedAbsDouble(CompilationUnit *cUnit, MIR *mir)
-{
-    int offset = offsetof(InterpState, retval);
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int oplo = selectFirstRegister(cUnit, dInsn->arg[0], true);
-    int ophi = NEXT_REG(oplo);
-    int signMask = NEXT_REG(ophi);
-    loadValuePair(cUnit, dInsn->arg[0], oplo, ophi);
-    loadConstant(cUnit, signMask, 0x7fffffff);
-    newLIR3(cUnit, THUMB_STR_RRI5, oplo, rGLUE, offset >> 2);
-    newLIR2(cUnit, THUMB_AND_RR, ophi, signMask);
-    newLIR3(cUnit, THUMB_STR_RRI5, ophi, rGLUE, (offset >> 2)+1);
-    return false;
-}
-
- /* No select in thumb, so we need to branch.  Thumb2 will do better */
-static bool genInlinedMinMaxInt(CompilationUnit *cUnit, MIR *mir, bool isMin)
-{
-    int offset = offsetof(InterpState, retval);
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int reg0 = selectFirstRegister(cUnit, dInsn->arg[0], false);
-    int reg1 = NEXT_REG(reg0);
-    loadValue(cUnit, dInsn->arg[0], reg0);
-    loadValue(cUnit, dInsn->arg[1], reg1);
-    newLIR2(cUnit, THUMB_CMP_RR, reg0, reg1);
-    ArmLIR *branch1 = newLIR2(cUnit, THUMB_B_COND, 2,
-           isMin ? ARM_COND_LT : ARM_COND_GT);
-    newLIR2(cUnit, THUMB_MOV_RR, reg0, reg1);
-    ArmLIR *target =
-        newLIR3(cUnit, THUMB_STR_RRI5, reg0, rGLUE, offset >> 2);
-    branch1->generic.target = (LIR *)target;
-    return false;
-}
-
-static bool genInlinedAbsLong(CompilationUnit *cUnit, MIR *mir)
-{
-    int offset = offsetof(InterpState, retval);
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    int oplo = selectFirstRegister(cUnit, dInsn->arg[0], true);
-    int ophi = NEXT_REG(oplo);
-    int sign = NEXT_REG(ophi);
-    /* abs(x) = y<=x>>31, (x+y)^y.  Shorter in ARM/THUMB2, no skip in THUMB */
-    loadValuePair(cUnit, dInsn->arg[0], oplo, ophi);
-    newLIR3(cUnit, THUMB_ASR, sign, ophi, 31);
-    newLIR3(cUnit, THUMB_ADD_RRR, oplo, oplo, sign);
-    newLIR2(cUnit, THUMB_ADC, ophi, sign);
-    newLIR2(cUnit, THUMB_EOR, oplo, sign);
-    newLIR2(cUnit, THUMB_EOR, ophi, sign);
-    newLIR3(cUnit, THUMB_STR_RRI5, oplo, rGLUE, offset >> 2);
-    newLIR3(cUnit, THUMB_STR_RRI5, ophi, rGLUE, (offset >> 2)+1);
-    return false;
-}
-
-static void genProcessArgsNoRange(CompilationUnit *cUnit, MIR *mir,
-                                  DecodedInstruction *dInsn,
-                                  ArmLIR **pcrLabel)
-{
-    unsigned int i;
-    unsigned int regMask = 0;
-
-    /* Load arguments to r0..r4 */
-    for (i = 0; i < dInsn->vA; i++) {
-        regMask |= 1 << i;
-        loadValue(cUnit, dInsn->arg[i], i);
-    }
-    if (regMask) {
-        /* Up to 5 args are pushed on top of FP - sizeofStackSaveArea */
-        newLIR2(cUnit, THUMB_MOV_RR, r7, rFP);
-        newLIR2(cUnit, THUMB_SUB_RI8, r7,
-                sizeof(StackSaveArea) + (dInsn->vA << 2));
-        /* generate null check */
-        if (pcrLabel) {
-            *pcrLabel = genNullCheck(cUnit, dInsn->arg[0], r0, mir->offset,
-                                     NULL);
-        }
-        newLIR2(cUnit, THUMB_STMIA, r7, regMask);
-    }
-}
-
-static void genProcessArgsRange(CompilationUnit *cUnit, MIR *mir,
-                                DecodedInstruction *dInsn,
-                                ArmLIR **pcrLabel)
-{
-    int srcOffset = dInsn->vC << 2;
-    int numArgs = dInsn->vA;
-    int regMask;
-    /*
-     * r4PC     : &rFP[vC]
-     * r7: &newFP[0]
-     */
-    if (srcOffset < 8) {
-        newLIR3(cUnit, THUMB_ADD_RRI3, r4PC, rFP, srcOffset);
-    } else {
-        loadConstant(cUnit, r4PC, srcOffset);
-        newLIR3(cUnit, THUMB_ADD_RRR, r4PC, rFP, r4PC);
-    }
-    /* load [r0 .. min(numArgs,4)] */
-    regMask = (1 << ((numArgs < 4) ? numArgs : 4)) - 1;
-    newLIR2(cUnit, THUMB_LDMIA, r4PC, regMask);
-
-    if (sizeof(StackSaveArea) + (numArgs << 2) < 256) {
-        newLIR2(cUnit, THUMB_MOV_RR, r7, rFP);
-        newLIR2(cUnit, THUMB_SUB_RI8, r7,
-                sizeof(StackSaveArea) + (numArgs << 2));
-    } else {
-        loadConstant(cUnit, r7, sizeof(StackSaveArea) + (numArgs << 2));
-        newLIR3(cUnit, THUMB_SUB_RRR, r7, rFP, r7);
-    }
-
-    /* generate null check */
-    if (pcrLabel) {
-        *pcrLabel = genNullCheck(cUnit, dInsn->vC, r0, mir->offset, NULL);
-    }
-
-    /*
-     * Handle remaining 4n arguments:
-     * store previously loaded 4 values and load the next 4 values
-     */
-    if (numArgs >= 8) {
-        ArmLIR *loopLabel = NULL;
-        /*
-         * r0 contains "this" and it will be used later, so push it to the stack
-         * first. Pushing r5 is just for stack alignment purposes.
-         */
-        newLIR1(cUnit, THUMB_PUSH, 1 << r0 | 1 << 5);
-        /* No need to generate the loop structure if numArgs <= 11 */
-        if (numArgs > 11) {
-            loadConstant(cUnit, 5, ((numArgs - 4) >> 2) << 2);
-            loopLabel = newLIR0(cUnit, ARM_PSEUDO_TARGET_LABEL);
-        }
-        newLIR2(cUnit, THUMB_STMIA, r7, regMask);
-        newLIR2(cUnit, THUMB_LDMIA, r4PC, regMask);
-        /* No need to generate the loop structure if numArgs <= 11 */
-        if (numArgs > 11) {
-            newLIR2(cUnit, THUMB_SUB_RI8, 5, 4);
-            genConditionalBranch(cUnit, ARM_COND_NE, loopLabel);
-        }
-    }
-
-    /* Save the last batch of loaded values */
-    newLIR2(cUnit, THUMB_STMIA, r7, regMask);
-
-    /* Generate the loop epilogue - don't use r0 */
-    if ((numArgs > 4) && (numArgs % 4)) {
-        regMask = ((1 << (numArgs & 0x3)) - 1) << 1;
-        newLIR2(cUnit, THUMB_LDMIA, r4PC, regMask);
-    }
-    if (numArgs >= 8)
-        newLIR1(cUnit, THUMB_POP, 1 << r0 | 1 << 5);
-
-    /* Save the modulo 4 arguments */
-    if ((numArgs > 4) && (numArgs % 4)) {
-        newLIR2(cUnit, THUMB_STMIA, r7, regMask);
-    }
-}
-
-/*
- * Generate code to setup the call stack then jump to the chaining cell if it
- * is not a native method.
- */
-static void genInvokeSingletonCommon(CompilationUnit *cUnit, MIR *mir,
-                                     BasicBlock *bb, ArmLIR *labelList,
-                                     ArmLIR *pcrLabel,
-                                     const Method *calleeMethod)
-{
-    ArmLIR *retChainingCell = &labelList[bb->fallThrough->id];
-
-    /* r1 = &retChainingCell */
-    ArmLIR *addrRetChain = newLIR3(cUnit, THUMB_ADD_PC_REL,
-                                           r1, 0, 0);
-    /* r4PC = dalvikCallsite */
-    loadConstant(cUnit, r4PC,
-                 (int) (cUnit->method->insns + mir->offset));
-    addrRetChain->generic.target = (LIR *) retChainingCell;
-    /*
-     * r0 = calleeMethod (loaded upon calling genInvokeSingletonCommon)
-     * r1 = &ChainingCell
-     * r4PC = callsiteDPC
-     */
-    if (dvmIsNativeMethod(calleeMethod)) {
-        genDispatchToHandler(cUnit, TEMPLATE_INVOKE_METHOD_NATIVE);
-#if defined(INVOKE_STATS)
-        gDvmJit.invokeNative++;
-#endif
-    } else {
-        genDispatchToHandler(cUnit, TEMPLATE_INVOKE_METHOD_CHAIN);
-#if defined(INVOKE_STATS)
-        gDvmJit.invokeChain++;
-#endif
-        /* Branch to the chaining cell */
-        genUnconditionalBranch(cUnit, &labelList[bb->taken->id]);
-    }
-    /* Handle exceptions using the interpreter */
-    genTrap(cUnit, mir->offset, pcrLabel);
-}
-
-/*
- * Generate code to check the validity of a predicted chain and take actions
- * based on the result.
- *
- * 0x426a99aa : ldr     r4, [pc, #72] --> r4 <- dalvikPC of this invoke
- * 0x426a99ac : add     r1, pc, #32   --> r1 <- &retChainingCell
- * 0x426a99ae : add     r2, pc, #40   --> r2 <- &predictedChainingCell
- * 0x426a99b0 : blx_1   0x426a918c    --+ TEMPLATE_INVOKE_METHOD_PREDICTED_CHAIN
- * 0x426a99b2 : blx_2   see above     --+
- * 0x426a99b4 : b       0x426a99d8    --> off to the predicted chain
- * 0x426a99b6 : b       0x426a99c8    --> punt to the interpreter
- * 0x426a99b8 : ldr     r0, [r7, #44] --> r0 <- this->class->vtable[methodIdx]
- * 0x426a99ba : cmp     r1, #0        --> compare r1 (rechain count) against 0
- * 0x426a99bc : bgt     0x426a99c2    --> >=0? don't rechain
- * 0x426a99be : ldr     r7, [r6, #96] --+ dvmJitToPatchPredictedChain
- * 0x426a99c0 : blx     r7            --+
- * 0x426a99c2 : add     r1, pc, #12   --> r1 <- &retChainingCell
- * 0x426a99c4 : blx_1   0x426a9098    --+ TEMPLATE_INVOKE_METHOD_NO_OPT
- * 0x426a99c6 : blx_2   see above     --+
- */
-static void genInvokeVirtualCommon(CompilationUnit *cUnit, MIR *mir,
-                                   int methodIndex,
-                                   ArmLIR *retChainingCell,
-                                   ArmLIR *predChainingCell,
-                                   ArmLIR *pcrLabel)
-{
-    /* "this" is already left in r0 by genProcessArgs* */
-
-    /* r4PC = dalvikCallsite */
-    loadConstant(cUnit, r4PC,
-                 (int) (cUnit->method->insns + mir->offset));
-
-    /* r1 = &retChainingCell */
-    ArmLIR *addrRetChain = newLIR2(cUnit, THUMB_ADD_PC_REL,
-                                       r1, 0);
-    addrRetChain->generic.target = (LIR *) retChainingCell;
-
-    /* r2 = &predictedChainingCell */
-    ArmLIR *predictedChainingCell =
-        newLIR2(cUnit, THUMB_ADD_PC_REL, r2, 0);
-    predictedChainingCell->generic.target = (LIR *) predChainingCell;
-
-    genDispatchToHandler(cUnit, TEMPLATE_INVOKE_METHOD_PREDICTED_CHAIN);
-
-    /* return through lr - jump to the chaining cell */
-    genUnconditionalBranch(cUnit, predChainingCell);
-
-    /*
-     * null-check on "this" may have been eliminated, but we still need a PC-
-     * reconstruction label for stack overflow bailout.
-     */
-    if (pcrLabel == NULL) {
-        int dPC = (int) (cUnit->method->insns + mir->offset);
-        pcrLabel = dvmCompilerNew(sizeof(ArmLIR), true);
-        pcrLabel->opCode = ARM_PSEUDO_PC_RECONSTRUCTION_CELL;
-        pcrLabel->operands[0] = dPC;
-        pcrLabel->operands[1] = mir->offset;
-        /* Insert the place holder to the growable list */
-        dvmInsertGrowableList(&cUnit->pcReconstructionList, pcrLabel);
-    }
-
-    /* return through lr+2 - punt to the interpreter */
-    genUnconditionalBranch(cUnit, pcrLabel);
-
-    /*
-     * return through lr+4 - fully resolve the callee method.
-     * r1 <- count
-     * r2 <- &predictedChainCell
-     * r3 <- this->class
-     * r4 <- dPC
-     * r7 <- this->class->vtable
-     */
-
-    /* r0 <- calleeMethod */
-    if (methodIndex < 32) {
-        newLIR3(cUnit, THUMB_LDR_RRI5, r0, r7, methodIndex);
-    } else {
-        loadConstant(cUnit, r0, methodIndex<<2);
-        newLIR3(cUnit, THUMB_LDR_RRR, r0, r7, r0);
-    }
-
-    /* Check if rechain limit is reached */
-    newLIR2(cUnit, THUMB_CMP_RI8, r1, 0);
-
-    ArmLIR *bypassRechaining =
-        newLIR2(cUnit, THUMB_B_COND, 0, ARM_COND_GT);
-
-    newLIR3(cUnit, THUMB_LDR_RRI5, r7, rGLUE,
-            offsetof(InterpState,
-                     jitToInterpEntries.dvmJitToPatchPredictedChain)
-            >> 2);
-
-    /*
-     * r0 = calleeMethod
-     * r2 = &predictedChainingCell
-     * r3 = class
-     *
-     * &returnChainingCell has been loaded into r1 but is not needed
-     * when patching the chaining cell and will be clobbered upon
-     * returning so it will be reconstructed again.
-     */
-    newLIR1(cUnit, THUMB_BLX_R, r7);
-
-    /* r1 = &retChainingCell */
-    addrRetChain = newLIR3(cUnit, THUMB_ADD_PC_REL, r1, 0, 0);
-    addrRetChain->generic.target = (LIR *) retChainingCell;
-
-    bypassRechaining->generic.target = (LIR *) addrRetChain;
-    /*
-     * r0 = calleeMethod,
-     * r1 = &ChainingCell,
-     * r4PC = callsiteDPC,
-     */
-    genDispatchToHandler(cUnit, TEMPLATE_INVOKE_METHOD_NO_OPT);
-#if defined(INVOKE_STATS)
-    gDvmJit.invokePredictedChain++;
-#endif
-    /* Handle exceptions using the interpreter */
-    genTrap(cUnit, mir->offset, pcrLabel);
-}
-
-/*
- * Up calling this function, "this" is stored in r0. The actual class will be
- * chased down off r0 and the predicted one will be retrieved through
- * predictedChainingCell then a comparison is performed to see whether the
- * previously established chaining is still valid.
- *
- * The return LIR is a branch based on the comparison result. The actual branch
- * target will be setup in the caller.
- */
-static ArmLIR *genCheckPredictedChain(CompilationUnit *cUnit,
-                                          ArmLIR *predChainingCell,
-                                          ArmLIR *retChainingCell,
-                                          MIR *mir)
-{
-    /* r3 now contains this->clazz */
-    newLIR3(cUnit, THUMB_LDR_RRI5, r3, r0,
-            offsetof(Object, clazz) >> 2);
-
-    /*
-     * r2 now contains predicted class. The starting offset of the
-     * cached value is 4 bytes into the chaining cell.
-     */
-    ArmLIR *getPredictedClass =
-        newLIR3(cUnit, THUMB_LDR_PC_REL, r2, 0,
-                offsetof(PredictedChainingCell, clazz));
-    getPredictedClass->generic.target = (LIR *) predChainingCell;
-
-    /*
-     * r0 now contains predicted method. The starting offset of the
-     * cached value is 8 bytes into the chaining cell.
-     */
-    ArmLIR *getPredictedMethod =
-        newLIR3(cUnit, THUMB_LDR_PC_REL, r0, 0,
-                offsetof(PredictedChainingCell, method));
-    getPredictedMethod->generic.target = (LIR *) predChainingCell;
-
-    /* Load the stats counter to see if it is time to unchain and refresh */
-    ArmLIR *getRechainingRequestCount =
-        newLIR3(cUnit, THUMB_LDR_PC_REL, r7, 0,
-                offsetof(PredictedChainingCell, counter));
-    getRechainingRequestCount->generic.target =
-        (LIR *) predChainingCell;
-
-    /* r4PC = dalvikCallsite */
-    loadConstant(cUnit, r4PC,
-                 (int) (cUnit->method->insns + mir->offset));
-
-    /* r1 = &retChainingCell */
-    ArmLIR *addrRetChain = newLIR3(cUnit, THUMB_ADD_PC_REL,
-                                       r1, 0, 0);
-    addrRetChain->generic.target = (LIR *) retChainingCell;
-
-    /* Check if r2 (predicted class) == r3 (actual class) */
-    newLIR2(cUnit, THUMB_CMP_RR, r2, r3);
-
-    return newLIR2(cUnit, THUMB_B_COND, 0, ARM_COND_EQ);
-}
-
-/* Geneate a branch to go back to the interpreter */
-static void genPuntToInterp(CompilationUnit *cUnit, unsigned int offset)
-{
-    /* r0 = dalvik pc */
-    loadConstant(cUnit, r0, (int) (cUnit->method->insns + offset));
-    newLIR3(cUnit, THUMB_LDR_RRI5, r1, rGLUE,
-            offsetof(InterpState, jitToInterpEntries.dvmJitToInterpPunt) >> 2);
-    newLIR1(cUnit, THUMB_BLX_R, r1);
-}
-
-/*
- * Attempt to single step one instruction using the interpreter and return
- * to the compiled code for the next Dalvik instruction
- */
-static void genInterpSingleStep(CompilationUnit *cUnit, MIR *mir)
-{
-    int flags = dexGetInstrFlags(gDvm.instrFlags, mir->dalvikInsn.opCode);
-    int flagsToCheck = kInstrCanBranch | kInstrCanSwitch | kInstrCanReturn |
-                       kInstrCanThrow;
-    if ((mir->next == NULL) || (flags & flagsToCheck)) {
-       genPuntToInterp(cUnit, mir->offset);
-       return;
-    }
-    int entryAddr = offsetof(InterpState,
-                             jitToInterpEntries.dvmJitToInterpSingleStep);
-    newLIR3(cUnit, THUMB_LDR_RRI5, r2, rGLUE, entryAddr >> 2);
-    /* r0 = dalvik pc */
-    loadConstant(cUnit, r0, (int) (cUnit->method->insns + mir->offset));
-    /* r1 = dalvik pc of following instruction */
-    loadConstant(cUnit, r1, (int) (cUnit->method->insns + mir->next->offset));
-    newLIR1(cUnit, THUMB_BLX_R, r2);
-}
-
-
-/*****************************************************************************/
-/*
- * The following are the first-level codegen routines that analyze the format
- * of each bytecode then either dispatch special purpose codegen routines
- * or produce corresponding Thumb instructions directly.
- */
-
-static bool handleFmt10t_Fmt20t_Fmt30t(CompilationUnit *cUnit, MIR *mir,
-                                       BasicBlock *bb, ArmLIR *labelList)
-{
-    /* For OP_GOTO, OP_GOTO_16, and OP_GOTO_32 */
-    genUnconditionalBranch(cUnit, &labelList[bb->taken->id]);
-    return false;
-}
-
-static bool handleFmt10x(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    if (((dalvikOpCode >= OP_UNUSED_3E) && (dalvikOpCode <= OP_UNUSED_43)) ||
-        ((dalvikOpCode >= OP_UNUSED_E3) && (dalvikOpCode <= OP_UNUSED_EC))) {
-        LOGE("Codegen: got unused opcode 0x%x\n",dalvikOpCode);
-        return true;
-    }
-    switch (dalvikOpCode) {
-        case OP_RETURN_VOID:
-            genReturnCommon(cUnit,mir);
-            break;
-        case OP_UNUSED_73:
-        case OP_UNUSED_79:
-        case OP_UNUSED_7A:
-            LOGE("Codegen: got unused opcode 0x%x\n",dalvikOpCode);
-            return true;
-        case OP_NOP:
-            break;
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt11n_Fmt31i(CompilationUnit *cUnit, MIR *mir)
-{
-    int reg0, reg1, reg2;
-
-    switch (mir->dalvikInsn.opCode) {
-        case OP_CONST:
-        case OP_CONST_4: {
-            /* Avoid using the previously used register */
-            reg0 = selectFirstRegister(cUnit, vNone, false);
-            reg1 = NEXT_REG(reg0);
-            loadConstant(cUnit, reg0, mir->dalvikInsn.vB);
-            storeValue(cUnit, reg0, mir->dalvikInsn.vA, reg1);
-            break;
-        }
-        case OP_CONST_WIDE_32: {
-            /* Avoid using the previously used register */
-            reg0 = selectFirstRegister(cUnit, vNone, true);
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-            loadConstant(cUnit, reg0, mir->dalvikInsn.vB);
-            newLIR3(cUnit, THUMB_ASR, reg1, reg0, 31);
-            storeValuePair(cUnit, reg0, reg1, mir->dalvikInsn.vA, reg2);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt21h(CompilationUnit *cUnit, MIR *mir)
-{
-    int reg0, reg1, reg2;
-
-    /* Avoid using the previously used register */
-    switch (mir->dalvikInsn.opCode) {
-        case OP_CONST_HIGH16: {
-            reg0 = selectFirstRegister(cUnit, vNone, false);
-            reg1 = NEXT_REG(reg0);
-            loadConstant(cUnit, reg0, mir->dalvikInsn.vB << 16);
-            storeValue(cUnit, reg0, mir->dalvikInsn.vA, reg1);
-            break;
-        }
-        case OP_CONST_WIDE_HIGH16: {
-            reg0 = selectFirstRegister(cUnit, vNone, true);
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-            loadConstant(cUnit, reg1, mir->dalvikInsn.vB << 16);
-            loadConstant(cUnit, reg0, 0);
-            storeValuePair(cUnit, reg0, reg1, mir->dalvikInsn.vA, reg2);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt20bc(CompilationUnit *cUnit, MIR *mir)
-{
-    /* For OP_THROW_VERIFICATION_ERROR */
-    genInterpSingleStep(cUnit, mir);
-    return false;
-}
-
-static bool handleFmt21c_Fmt31c(CompilationUnit *cUnit, MIR *mir)
-{
-    /* Native register to use if the interested value is vA */
-    int regvA = selectFirstRegister(cUnit, mir->dalvikInsn.vA, false);
-    /* Native register to use if source is not from Dalvik registers */
-    int regvNone = selectFirstRegister(cUnit, vNone, false);
-    /* Similar to regvA but for 64-bit values */
-    int regvAWide = selectFirstRegister(cUnit, mir->dalvikInsn.vA, true);
-    /* Similar to regvNone but for 64-bit values */
-    int regvNoneWide = selectFirstRegister(cUnit, vNone, true);
-
-    switch (mir->dalvikInsn.opCode) {
-        /*
-         * TODO: Verify that we can ignore the resolution check here because
-         * it will have already successfully been interpreted once
-         */
-        case OP_CONST_STRING_JUMBO:
-        case OP_CONST_STRING: {
-            void *strPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResStrings[mir->dalvikInsn.vB]);
-            assert(strPtr != NULL);
-            loadConstant(cUnit, regvNone, (int) strPtr );
-            storeValue(cUnit, regvNone, mir->dalvikInsn.vA, NEXT_REG(regvNone));
-            break;
-        }
-        /*
-         * TODO: Verify that we can ignore the resolution check here because
-         * it will have already successfully been interpreted once
-         */
-        case OP_CONST_CLASS: {
-            void *classPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResClasses[mir->dalvikInsn.vB]);
-            assert(classPtr != NULL);
-            loadConstant(cUnit, regvNone, (int) classPtr );
-            storeValue(cUnit, regvNone, mir->dalvikInsn.vA, NEXT_REG(regvNone));
-            break;
-        }
-        case OP_SGET_OBJECT:
-        case OP_SGET_BOOLEAN:
-        case OP_SGET_CHAR:
-        case OP_SGET_BYTE:
-        case OP_SGET_SHORT:
-        case OP_SGET: {
-            int valOffset = offsetof(StaticField, value);
-            void *fieldPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResFields[mir->dalvikInsn.vB]);
-            assert(fieldPtr != NULL);
-            loadConstant(cUnit, regvNone,  (int) fieldPtr + valOffset);
-            newLIR3(cUnit, THUMB_LDR_RRI5, regvNone, regvNone, 0);
-            storeValue(cUnit, regvNone, mir->dalvikInsn.vA, NEXT_REG(regvNone));
-            break;
-        }
-        case OP_SGET_WIDE: {
-            int valOffset = offsetof(StaticField, value);
-            void *fieldPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResFields[mir->dalvikInsn.vB]);
-            int reg0, reg1, reg2;
-
-            assert(fieldPtr != NULL);
-            reg0 = regvNoneWide;
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-            loadConstant(cUnit, reg2,  (int) fieldPtr + valOffset);
-            newLIR2(cUnit, THUMB_LDMIA, reg2, (1<<reg0 | 1<<reg1));
-            storeValuePair(cUnit, reg0, reg1, mir->dalvikInsn.vA, reg2);
-            break;
-        }
-        case OP_SPUT_OBJECT:
-        case OP_SPUT_BOOLEAN:
-        case OP_SPUT_CHAR:
-        case OP_SPUT_BYTE:
-        case OP_SPUT_SHORT:
-        case OP_SPUT: {
-            int valOffset = offsetof(StaticField, value);
-            void *fieldPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResFields[mir->dalvikInsn.vB]);
-
-            assert(fieldPtr != NULL);
-            loadValue(cUnit, mir->dalvikInsn.vA, regvA);
-            updateLiveRegister(cUnit, mir->dalvikInsn.vA, regvA);
-            loadConstant(cUnit, NEXT_REG(regvA),  (int) fieldPtr + valOffset);
-            newLIR3(cUnit, THUMB_STR_RRI5, regvA, NEXT_REG(regvA), 0);
-            break;
-        }
-        case OP_SPUT_WIDE: {
-            int reg0, reg1, reg2;
-            int valOffset = offsetof(StaticField, value);
-            void *fieldPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResFields[mir->dalvikInsn.vB]);
-
-            assert(fieldPtr != NULL);
-            reg0 = regvAWide;
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-            loadValuePair(cUnit, mir->dalvikInsn.vA, reg0, reg1);
-            updateLiveRegisterPair(cUnit, mir->dalvikInsn.vA, reg0, reg1);
-            loadConstant(cUnit, reg2,  (int) fieldPtr + valOffset);
-            newLIR2(cUnit, THUMB_STMIA, reg2, (1<<reg0 | 1<<reg1));
-            break;
-        }
-        case OP_NEW_INSTANCE: {
-            /*
-             * Obey the calling convention and don't mess with the register
-             * usage.
-             */
-            ClassObject *classPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResClasses[mir->dalvikInsn.vB]);
-            assert(classPtr != NULL);
-            assert(classPtr->status & CLASS_INITIALIZED);
-            if ((classPtr->accessFlags & (ACC_INTERFACE|ACC_ABSTRACT)) != 0) {
-                /* It's going to throw, just let the interp. deal with it. */
-                genInterpSingleStep(cUnit, mir);
-                return false;
-            }
-            loadConstant(cUnit, r4PC, (int)dvmAllocObject);
-            loadConstant(cUnit, r0, (int) classPtr);
-            genExportPC(cUnit, mir, r2, r3 );
-            loadConstant(cUnit, r1, ALLOC_DONT_TRACK);
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-            /*
-             * TODO: As coded, we'll bail and reinterpret on alloc failure.
-             * Need a general mechanism to bail to thrown exception code.
-             */
-            genZeroCheck(cUnit, r0, mir->offset, NULL);
-            storeValue(cUnit, r0, mir->dalvikInsn.vA, r1);
-            break;
-        }
-        case OP_CHECK_CAST: {
-            /*
-             * Obey the calling convention and don't mess with the register
-             * usage.
-             */
-            ClassObject *classPtr =
-              (cUnit->method->clazz->pDvmDex->pResClasses[mir->dalvikInsn.vB]);
-            loadConstant(cUnit, r1, (int) classPtr );
-            loadValue(cUnit, mir->dalvikInsn.vA, r0);  /* Ref */
-            /*
-             * TODO - in theory classPtr should be resoved by the time this
-             * instruction made into a trace, but we are seeing NULL at runtime
-             * so this check is temporarily used as a workaround.
-             */
-            ArmLIR * pcrLabel = genZeroCheck(cUnit, r1, mir->offset, NULL);
-            newLIR2(cUnit, THUMB_CMP_RI8, r0, 0);    /* Null? */
-            ArmLIR *branch1 =
-                newLIR2(cUnit, THUMB_B_COND, 4, ARM_COND_EQ);
-            /* r0 now contains object->clazz */
-            newLIR3(cUnit, THUMB_LDR_RRI5, r0, r0,
-                    offsetof(Object, clazz) >> 2);
-            loadConstant(cUnit, r4PC, (int)dvmInstanceofNonTrivial);
-            newLIR2(cUnit, THUMB_CMP_RR, r0, r1);
-            ArmLIR *branch2 =
-                newLIR2(cUnit, THUMB_B_COND, 2, ARM_COND_EQ);
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-            /* check cast failed - punt to the interpreter */
-            genZeroCheck(cUnit, r0, mir->offset, pcrLabel);
-            /* check cast passed - branch target here */
-            ArmLIR *target = newLIR0(cUnit, ARM_PSEUDO_TARGET_LABEL);
-            branch1->generic.target = (LIR *)target;
-            branch2->generic.target = (LIR *)target;
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt11x(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    switch (dalvikOpCode) {
-        case OP_MOVE_EXCEPTION: {
-            int offset = offsetof(InterpState, self);
-            int exOffset = offsetof(Thread, exception);
-            newLIR3(cUnit, THUMB_LDR_RRI5, r1, rGLUE, offset >> 2);
-            newLIR3(cUnit, THUMB_LDR_RRI5, r0, r1, exOffset >> 2);
-            storeValue(cUnit, r0, mir->dalvikInsn.vA, r1);
-           break;
-        }
-        case OP_MOVE_RESULT:
-        case OP_MOVE_RESULT_OBJECT: {
-            int offset = offsetof(InterpState, retval);
-            newLIR3(cUnit, THUMB_LDR_RRI5, r0, rGLUE, offset >> 2);
-            storeValue(cUnit, r0, mir->dalvikInsn.vA, r1);
-            break;
-        }
-        case OP_MOVE_RESULT_WIDE: {
-            int offset = offsetof(InterpState, retval);
-            newLIR3(cUnit, THUMB_LDR_RRI5, r0, rGLUE, offset >> 2);
-            newLIR3(cUnit, THUMB_LDR_RRI5, r1, rGLUE, (offset >> 2)+1);
-            storeValuePair(cUnit, r0, r1, mir->dalvikInsn.vA, r2);
-            break;
-        }
-        case OP_RETURN_WIDE: {
-            loadValuePair(cUnit, mir->dalvikInsn.vA, r0, r1);
-            int offset = offsetof(InterpState, retval);
-            newLIR3(cUnit, THUMB_STR_RRI5, r0, rGLUE, offset >> 2);
-            newLIR3(cUnit, THUMB_STR_RRI5, r1, rGLUE, (offset >> 2)+1);
-            genReturnCommon(cUnit,mir);
-            break;
-        }
-        case OP_RETURN:
-        case OP_RETURN_OBJECT: {
-            loadValue(cUnit, mir->dalvikInsn.vA, r0);
-            int offset = offsetof(InterpState, retval);
-            newLIR3(cUnit, THUMB_STR_RRI5, r0, rGLUE, offset >> 2);
-            genReturnCommon(cUnit,mir);
-            break;
-        }
-        /*
-         * TODO-VERIFY: May be playing a bit fast and loose here.  As coded,
-         * a failure on lock/unlock will cause us to revert to the interpeter
-         * to try again. This means we essentially ignore the first failure on
-         * the assumption that the interpreter will correctly handle the 2nd.
-         */
-        case OP_MONITOR_ENTER:
-        case OP_MONITOR_EXIT: {
-            int offset = offsetof(InterpState, self);
-            loadValue(cUnit, mir->dalvikInsn.vA, r1);
-            newLIR3(cUnit, THUMB_LDR_RRI5, r0, rGLUE, offset >> 2);
-            if (dalvikOpCode == OP_MONITOR_ENTER) {
-                loadConstant(cUnit, r2, (int)dvmLockObject);
-            } else {
-                loadConstant(cUnit, r2, (int)dvmUnlockObject);
-            }
-          /*
-           * TODO-VERIFY: Note that we're not doing an EXPORT_PC, as
-           * Lock/unlock won't throw, and this code does not support
-           * DEADLOCK_PREDICTION or MONITOR_TRACKING.  Should it?
-           */
-            genNullCheck(cUnit, mir->dalvikInsn.vA, r1, mir->offset, NULL);
-            /* Do the call */
-            newLIR1(cUnit, THUMB_BLX_R, r2);
-            break;
-        }
-        case OP_THROW: {
-            genInterpSingleStep(cUnit, mir);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool genConversionPortable(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode opCode = mir->dalvikInsn.opCode;
-
-    float  __aeabi_i2f(  int op1 );
-    int    __aeabi_f2iz( float op1 );
-    float  __aeabi_d2f(  double op1 );
-    double __aeabi_f2d(  float op1 );
-    double __aeabi_i2d(  int op1 );
-    int    __aeabi_d2iz( double op1 );
-    float  __aeabi_l2f(  long op1 );
-    double __aeabi_l2d(  long op1 );
-
-    switch (opCode) {
-        case OP_INT_TO_FLOAT:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_i2f, 1, 1);
-        case OP_FLOAT_TO_INT:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_f2iz, 1, 1);
-        case OP_DOUBLE_TO_FLOAT:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_d2f, 2, 1);
-        case OP_FLOAT_TO_DOUBLE:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_f2d, 1, 2);
-        case OP_INT_TO_DOUBLE:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_i2d, 1, 2);
-        case OP_DOUBLE_TO_INT:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_d2iz, 2, 1);
-        case OP_FLOAT_TO_LONG:
-            return genConversionCall(cUnit, mir, (void*)dvmJitf2l, 1, 2);
-        case OP_LONG_TO_FLOAT:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_l2f, 2, 1);
-        case OP_DOUBLE_TO_LONG:
-            return genConversionCall(cUnit, mir, (void*)dvmJitd2l, 2, 2);
-        case OP_LONG_TO_DOUBLE:
-            return genConversionCall(cUnit, mir, (void*)__aeabi_l2d, 2, 2);
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt12x(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vSrc1Dest = mir->dalvikInsn.vA;
-    int vSrc2 = mir->dalvikInsn.vB;
-    int reg0, reg1, reg2;
-
-    /* TODO - find the proper include file to declare these */
-
-    if ( (opCode >= OP_ADD_INT_2ADDR) && (opCode <= OP_REM_DOUBLE_2ADDR)) {
-        return genArithOp( cUnit, mir );
-    }
-
-    /*
-     * If data type is 64-bit, re-calculate the register numbers in the
-     * corresponding cases.
-     */
-    reg0 = selectFirstRegister(cUnit, vSrc2, false);
-    reg1 = NEXT_REG(reg0);
-    reg2 = NEXT_REG(reg1);
-
-    switch (opCode) {
-        case OP_INT_TO_FLOAT:
-        case OP_FLOAT_TO_INT:
-        case OP_DOUBLE_TO_FLOAT:
-        case OP_FLOAT_TO_DOUBLE:
-        case OP_INT_TO_DOUBLE:
-        case OP_DOUBLE_TO_INT:
-        case OP_FLOAT_TO_LONG:
-        case OP_LONG_TO_FLOAT:
-        case OP_DOUBLE_TO_LONG:
-        case OP_LONG_TO_DOUBLE:
-            return genConversion(cUnit, mir);
-        case OP_NEG_INT:
-        case OP_NOT_INT:
-            return genArithOpInt(cUnit, mir, vSrc1Dest, vSrc1Dest, vSrc2);
-        case OP_NEG_LONG:
-        case OP_NOT_LONG:
-            return genArithOpLong(cUnit,mir, vSrc1Dest, vSrc1Dest, vSrc2);
-        case OP_NEG_FLOAT:
-            return genArithOpFloat(cUnit, mir, vSrc1Dest, vSrc1Dest, vSrc2);
-        case OP_NEG_DOUBLE:
-            return genArithOpDouble(cUnit, mir, vSrc1Dest, vSrc1Dest, vSrc2);
-        case OP_MOVE_WIDE: {
-            reg0 = selectFirstRegister(cUnit, vSrc2, true);
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-
-            loadValuePair(cUnit, vSrc2, reg0, reg1);
-            storeValuePair(cUnit, reg0, reg1, vSrc1Dest, reg2);
-            break;
-        }
-        case OP_INT_TO_LONG: {
-            reg0 = selectFirstRegister(cUnit, vSrc2, true);
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-
-            loadValue(cUnit, vSrc2, reg0);
-            newLIR3(cUnit, THUMB_ASR, reg1, reg0, 31);
-            storeValuePair(cUnit, reg0, reg1, vSrc1Dest, reg2);
-            break;
-        }
-        case OP_MOVE:
-        case OP_MOVE_OBJECT:
-        case OP_LONG_TO_INT:
-            loadValue(cUnit, vSrc2, reg0);
-            storeValue(cUnit, reg0, vSrc1Dest, reg1);
-            break;
-        case OP_INT_TO_BYTE:
-            loadValue(cUnit, vSrc2, reg0);
-            newLIR3(cUnit, THUMB_LSL, reg0, reg0, 24);
-            newLIR3(cUnit, THUMB_ASR, reg0, reg0, 24);
-            storeValue(cUnit, reg0, vSrc1Dest, reg1);
-            break;
-        case OP_INT_TO_SHORT:
-            loadValue(cUnit, vSrc2, reg0);
-            newLIR3(cUnit, THUMB_LSL, reg0, reg0, 16);
-            newLIR3(cUnit, THUMB_ASR, reg0, reg0, 16);
-            storeValue(cUnit, reg0, vSrc1Dest, reg1);
-            break;
-        case OP_INT_TO_CHAR:
-            loadValue(cUnit, vSrc2, reg0);
-            newLIR3(cUnit, THUMB_LSL, reg0, reg0, 16);
-            newLIR3(cUnit, THUMB_LSR, reg0, reg0, 16);
-            storeValue(cUnit, reg0, vSrc1Dest, reg1);
-            break;
-        case OP_ARRAY_LENGTH: {
-            int lenOffset = offsetof(ArrayObject, length);
-            loadValue(cUnit, vSrc2, reg0);
-            genNullCheck(cUnit, vSrc2, reg0, mir->offset, NULL);
-            newLIR3(cUnit, THUMB_LDR_RRI5, reg0, reg0, lenOffset >> 2);
-            storeValue(cUnit, reg0, vSrc1Dest, reg1);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt21s(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    int reg0, reg1, reg2;
-
-    /* It takes few instructions to handle OP_CONST_WIDE_16 inline */
-    if (dalvikOpCode == OP_CONST_WIDE_16) {
-        int vDest = mir->dalvikInsn.vA;
-        int BBBB = mir->dalvikInsn.vB;
-
-        reg0 = selectFirstRegister(cUnit, vNone, true);
-        reg1 = NEXT_REG(reg0);
-        reg2 = NEXT_REG(reg1);
-
-        loadConstant(cUnit, reg0, BBBB);
-        newLIR3(cUnit, THUMB_ASR, reg1, reg0, 31);
-
-        /* Save the long values to the specified Dalvik register pair */
-        storeValuePair(cUnit, reg0, reg1, vDest, reg2);
-    } else if (dalvikOpCode == OP_CONST_16) {
-        int vDest = mir->dalvikInsn.vA;
-        int BBBB = mir->dalvikInsn.vB;
-
-        reg0 = selectFirstRegister(cUnit, vNone, false);
-        reg1 = NEXT_REG(reg0);
-
-        loadConstant(cUnit, reg0, BBBB);
-        storeValue(cUnit, reg0, vDest, reg1);
-    } else {
-        return true;
-    }
-    return false;
-}
-
-/* Compare agaist zero */
-static bool handleFmt21t(CompilationUnit *cUnit, MIR *mir, BasicBlock *bb,
-                         ArmLIR *labelList)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    ArmConditionCode cond;
-    int reg0 = selectFirstRegister(cUnit, mir->dalvikInsn.vA, false);
-
-    loadValue(cUnit, mir->dalvikInsn.vA, reg0);
-    newLIR2(cUnit, THUMB_CMP_RI8, reg0, 0);
-
-    switch (dalvikOpCode) {
-        case OP_IF_EQZ:
-            cond = ARM_COND_EQ;
-            break;
-        case OP_IF_NEZ:
-            cond = ARM_COND_NE;
-            break;
-        case OP_IF_LTZ:
-            cond = ARM_COND_LT;
-            break;
-        case OP_IF_GEZ:
-            cond = ARM_COND_GE;
-            break;
-        case OP_IF_GTZ:
-            cond = ARM_COND_GT;
-            break;
-        case OP_IF_LEZ:
-            cond = ARM_COND_LE;
-            break;
-        default:
-            cond = 0;
-            LOGE("Unexpected opcode (%d) for Fmt21t\n", dalvikOpCode);
-            dvmAbort();
-    }
-    genConditionalBranch(cUnit, cond, &labelList[bb->taken->id]);
-    /* This mostly likely will be optimized away in a later phase */
-    genUnconditionalBranch(cUnit, &labelList[bb->fallThrough->id]);
-    return false;
-}
-
-static bool handleFmt22b_Fmt22s(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    int vSrc = mir->dalvikInsn.vB;
-    int vDest = mir->dalvikInsn.vA;
-    int lit = mir->dalvikInsn.vC;
-    int armOp;
-    int reg0, reg1, regDest;
-
-    reg0 = selectFirstRegister(cUnit, vSrc, false);
-    reg1 = NEXT_REG(reg0);
-    regDest = NEXT_REG(reg1);
-
-    /* TODO: find the proper .h file to declare these */
-    int __aeabi_idivmod(int op1, int op2);
-    int __aeabi_idiv(int op1, int op2);
-
-    switch (dalvikOpCode) {
-        case OP_ADD_INT_LIT8:
-        case OP_ADD_INT_LIT16:
-            loadValue(cUnit, vSrc, reg0);
-            if (lit <= 7 && lit >= 0) {
-                newLIR3(cUnit, THUMB_ADD_RRI3, regDest, reg0, lit);
-                storeValue(cUnit, regDest, vDest, reg1);
-            } else if (lit <= 255 && lit >= 0) {
-                newLIR2(cUnit, THUMB_ADD_RI8, reg0, lit);
-                storeValue(cUnit, reg0, vDest, reg1);
-            } else if (lit >= -7 && lit <= 0) {
-                /* Convert to a small constant subtraction */
-                newLIR3(cUnit, THUMB_SUB_RRI3, regDest, reg0, -lit);
-                storeValue(cUnit, regDest, vDest, reg1);
-            } else if (lit >= -255 && lit <= 0) {
-                /* Convert to a small constant subtraction */
-                newLIR2(cUnit, THUMB_SUB_RI8, reg0, -lit);
-                storeValue(cUnit, reg0, vDest, reg1);
-            } else {
-                loadConstant(cUnit, reg1, lit);
-                genBinaryOp(cUnit, vDest, THUMB_ADD_RRR, reg0, reg1, regDest);
-            }
-            break;
-
-        case OP_RSUB_INT_LIT8:
-        case OP_RSUB_INT:
-            loadValue(cUnit, vSrc, reg1);
-            loadConstant(cUnit, reg0, lit);
-            genBinaryOp(cUnit, vDest, THUMB_SUB_RRR, reg0, reg1, regDest);
-            break;
-
-        case OP_MUL_INT_LIT8:
-        case OP_MUL_INT_LIT16:
-        case OP_AND_INT_LIT8:
-        case OP_AND_INT_LIT16:
-        case OP_OR_INT_LIT8:
-        case OP_OR_INT_LIT16:
-        case OP_XOR_INT_LIT8:
-        case OP_XOR_INT_LIT16:
-            loadValue(cUnit, vSrc, reg0);
-            loadConstant(cUnit, reg1, lit);
-            switch (dalvikOpCode) {
-                case OP_MUL_INT_LIT8:
-                case OP_MUL_INT_LIT16:
-                    armOp = THUMB_MUL;
-                    break;
-                case OP_AND_INT_LIT8:
-                case OP_AND_INT_LIT16:
-                    armOp = THUMB_AND_RR;
-                    break;
-                case OP_OR_INT_LIT8:
-                case OP_OR_INT_LIT16:
-                    armOp = THUMB_ORR;
-                    break;
-                case OP_XOR_INT_LIT8:
-                case OP_XOR_INT_LIT16:
-                    armOp = THUMB_EOR;
-                    break;
-                default:
-                    dvmAbort();
-            }
-            genBinaryOp(cUnit, vDest, armOp, reg0, reg1, regDest);
-            break;
-
-        case OP_SHL_INT_LIT8:
-        case OP_SHR_INT_LIT8:
-        case OP_USHR_INT_LIT8:
-            loadValue(cUnit, vSrc, reg0);
-            switch (dalvikOpCode) {
-                case OP_SHL_INT_LIT8:
-                    armOp = THUMB_LSL;
-                    break;
-                case OP_SHR_INT_LIT8:
-                    armOp = THUMB_ASR;
-                    break;
-                case OP_USHR_INT_LIT8:
-                    armOp = THUMB_LSR;
-                    break;
-                default: dvmAbort();
-            }
-            newLIR3(cUnit, armOp, reg0, reg0, lit);
-            storeValue(cUnit, reg0, vDest, reg1);
-            break;
-
-        case OP_DIV_INT_LIT8:
-        case OP_DIV_INT_LIT16:
-            /* Register usage based on the calling convention */
-            if (lit == 0) {
-                /* Let the interpreter deal with div by 0 */
-                genInterpSingleStep(cUnit, mir);
-                return false;
-            }
-            loadConstant(cUnit, r2, (int)__aeabi_idiv);
-            loadConstant(cUnit, r1, lit);
-            loadValue(cUnit, vSrc, r0);
-            newLIR1(cUnit, THUMB_BLX_R, r2);
-            storeValue(cUnit, r0, vDest, r2);
-            break;
-
-        case OP_REM_INT_LIT8:
-        case OP_REM_INT_LIT16:
-            /* Register usage based on the calling convention */
-            if (lit == 0) {
-                /* Let the interpreter deal with div by 0 */
-                genInterpSingleStep(cUnit, mir);
-                return false;
-            }
-            loadConstant(cUnit, r2, (int)__aeabi_idivmod);
-            loadConstant(cUnit, r1, lit);
-            loadValue(cUnit, vSrc, r0);
-            newLIR1(cUnit, THUMB_BLX_R, r2);
-            storeValue(cUnit, r1, vDest, r2);
-            break;
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt22c(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    int fieldOffset;
-
-    if (dalvikOpCode >= OP_IGET && dalvikOpCode <= OP_IPUT_SHORT) {
-        InstField *pInstField = (InstField *)
-            cUnit->method->clazz->pDvmDex->pResFields[mir->dalvikInsn.vC];
-        int fieldOffset;
-
-        assert(pInstField != NULL);
-        fieldOffset = pInstField->byteOffset;
-    } else {
-        /* To make the compiler happy */
-        fieldOffset = 0;
-    }
-    switch (dalvikOpCode) {
-        /*
-         * TODO: I may be assuming too much here.
-         * Verify what is known at JIT time.
-         */
-        case OP_NEW_ARRAY: {
-            void *classPtr = (void*)
-              (cUnit->method->clazz->pDvmDex->pResClasses[mir->dalvikInsn.vC]);
-            assert(classPtr != NULL);
-            loadValue(cUnit, mir->dalvikInsn.vB, r1);  /* Len */
-            loadConstant(cUnit, r0, (int) classPtr );
-            loadConstant(cUnit, r4PC, (int)dvmAllocArrayByClass);
-            ArmLIR *pcrLabel =
-                genRegImmCheck(cUnit, ARM_COND_MI, r1, 0, mir->offset, NULL);
-            genExportPC(cUnit, mir, r2, r3 );
-            newLIR2(cUnit, THUMB_MOV_IMM,r2,ALLOC_DONT_TRACK);
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-            /*
-             * TODO: As coded, we'll bail and reinterpret on alloc failure.
-             * Need a general mechanism to bail to thrown exception code.
-             */
-            genZeroCheck(cUnit, r0, mir->offset, pcrLabel);
-            storeValue(cUnit, r0, mir->dalvikInsn.vA, r1);
-            break;
-        }
-        /*
-         * TODO: I may be assuming too much here.
-         * Verify what is known at JIT time.
-         */
-        case OP_INSTANCE_OF: {
-            ClassObject *classPtr =
-              (cUnit->method->clazz->pDvmDex->pResClasses[mir->dalvikInsn.vC]);
-            assert(classPtr != NULL);
-            loadValue(cUnit, mir->dalvikInsn.vB, r0);  /* Ref */
-            loadConstant(cUnit, r2, (int) classPtr );
-            newLIR2(cUnit, THUMB_CMP_RI8, r0, 0);    /* Null? */
-            /* When taken r0 has NULL which can be used for store directly */
-            ArmLIR *branch1 = newLIR2(cUnit, THUMB_B_COND, 4,
-                                          ARM_COND_EQ);
-            /* r1 now contains object->clazz */
-            newLIR3(cUnit, THUMB_LDR_RRI5, r1, r0,
-                    offsetof(Object, clazz) >> 2);
-            loadConstant(cUnit, r4PC, (int)dvmInstanceofNonTrivial);
-            loadConstant(cUnit, r0, 1);                /* Assume true */
-            newLIR2(cUnit, THUMB_CMP_RR, r1, r2);
-            ArmLIR *branch2 = newLIR2(cUnit, THUMB_B_COND, 2,
-                                          ARM_COND_EQ);
-            newLIR2(cUnit, THUMB_MOV_RR, r0, r1);
-            newLIR2(cUnit, THUMB_MOV_RR, r1, r2);
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-            /* branch target here */
-            ArmLIR *target = newLIR0(cUnit, ARM_PSEUDO_TARGET_LABEL);
-            storeValue(cUnit, r0, mir->dalvikInsn.vA, r1);
-            branch1->generic.target = (LIR *)target;
-            branch2->generic.target = (LIR *)target;
-            break;
-        }
-        case OP_IGET_WIDE:
-            genIGetWide(cUnit, mir, fieldOffset);
-            break;
-        case OP_IGET:
-        case OP_IGET_OBJECT:
-            genIGet(cUnit, mir, THUMB_LDR_RRR, fieldOffset);
-            break;
-        case OP_IGET_BOOLEAN:
-            genIGet(cUnit, mir, THUMB_LDRB_RRR, fieldOffset);
-            break;
-        case OP_IGET_BYTE:
-            genIGet(cUnit, mir, THUMB_LDRSB_RRR, fieldOffset);
-            break;
-        case OP_IGET_CHAR:
-            genIGet(cUnit, mir, THUMB_LDRH_RRR, fieldOffset);
-            break;
-        case OP_IGET_SHORT:
-            genIGet(cUnit, mir, THUMB_LDRSH_RRR, fieldOffset);
-            break;
-        case OP_IPUT_WIDE:
-            genIPutWide(cUnit, mir, fieldOffset);
-            break;
-        case OP_IPUT:
-        case OP_IPUT_OBJECT:
-            genIPut(cUnit, mir, THUMB_STR_RRR, fieldOffset);
-            break;
-        case OP_IPUT_SHORT:
-        case OP_IPUT_CHAR:
-            genIPut(cUnit, mir, THUMB_STRH_RRR, fieldOffset);
-            break;
-        case OP_IPUT_BYTE:
-        case OP_IPUT_BOOLEAN:
-            genIPut(cUnit, mir, THUMB_STRB_RRR, fieldOffset);
-            break;
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt22cs(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    int fieldOffset =  mir->dalvikInsn.vC;
-    switch (dalvikOpCode) {
-        case OP_IGET_QUICK:
-        case OP_IGET_OBJECT_QUICK:
-            genIGet(cUnit, mir, THUMB_LDR_RRR, fieldOffset);
-            break;
-        case OP_IPUT_QUICK:
-        case OP_IPUT_OBJECT_QUICK:
-            genIPut(cUnit, mir, THUMB_STR_RRR, fieldOffset);
-            break;
-        case OP_IGET_WIDE_QUICK:
-            genIGetWide(cUnit, mir, fieldOffset);
-            break;
-        case OP_IPUT_WIDE_QUICK:
-            genIPutWide(cUnit, mir, fieldOffset);
-            break;
-        default:
-            return true;
-    }
-    return false;
-
-}
-
-/* Compare agaist zero */
-static bool handleFmt22t(CompilationUnit *cUnit, MIR *mir, BasicBlock *bb,
-                         ArmLIR *labelList)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    ArmConditionCode cond;
-    int reg0, reg1;
-
-    if (cUnit->registerScoreboard.liveDalvikReg == (int) mir->dalvikInsn.vA) {
-        reg0 = selectFirstRegister(cUnit, mir->dalvikInsn.vA, false);
-        reg1 = NEXT_REG(reg0);
-        /* Load vB first since vA can be fetched via a move */
-        loadValue(cUnit, mir->dalvikInsn.vB, reg1);
-        loadValue(cUnit, mir->dalvikInsn.vA, reg0);
-    } else {
-        reg0 = selectFirstRegister(cUnit, mir->dalvikInsn.vB, false);
-        reg1 = NEXT_REG(reg0);
-        /* Load vA first since vB can be fetched via a move */
-        loadValue(cUnit, mir->dalvikInsn.vA, reg0);
-        loadValue(cUnit, mir->dalvikInsn.vB, reg1);
-    }
-    newLIR2(cUnit, THUMB_CMP_RR, reg0, reg1);
-
-    switch (dalvikOpCode) {
-        case OP_IF_EQ:
-            cond = ARM_COND_EQ;
-            break;
-        case OP_IF_NE:
-            cond = ARM_COND_NE;
-            break;
-        case OP_IF_LT:
-            cond = ARM_COND_LT;
-            break;
-        case OP_IF_GE:
-            cond = ARM_COND_GE;
-            break;
-        case OP_IF_GT:
-            cond = ARM_COND_GT;
-            break;
-        case OP_IF_LE:
-            cond = ARM_COND_LE;
-            break;
-        default:
-            cond = 0;
-            LOGE("Unexpected opcode (%d) for Fmt22t\n", dalvikOpCode);
-            dvmAbort();
-    }
-    genConditionalBranch(cUnit, cond, &labelList[bb->taken->id]);
-    /* This mostly likely will be optimized away in a later phase */
-    genUnconditionalBranch(cUnit, &labelList[bb->fallThrough->id]);
-    return false;
-}
-
-static bool handleFmt22x_Fmt32x(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vSrc1Dest = mir->dalvikInsn.vA;
-    int vSrc2 = mir->dalvikInsn.vB;
-    int reg0, reg1, reg2;
-
-    switch (opCode) {
-        case OP_MOVE_16:
-        case OP_MOVE_OBJECT_16:
-        case OP_MOVE_FROM16:
-        case OP_MOVE_OBJECT_FROM16: {
-            reg0 = selectFirstRegister(cUnit, vSrc2, false);
-            reg1 = NEXT_REG(reg0);
-            loadValue(cUnit, vSrc2, reg0);
-            storeValue(cUnit, reg0, vSrc1Dest, reg1);
-            break;
-        }
-        case OP_MOVE_WIDE_16:
-        case OP_MOVE_WIDE_FROM16: {
-            reg0 = selectFirstRegister(cUnit, vSrc2, true);
-            reg1 = NEXT_REG(reg0);
-            reg2 = NEXT_REG(reg1);
-            loadValuePair(cUnit, vSrc2, reg0, reg1);
-            storeValuePair(cUnit, reg0, reg1, vSrc1Dest, reg2);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt23x(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    int vC = mir->dalvikInsn.vC;
-
-    /* Don't optimize for register usage since out-of-line handlers are used */
-    if ( (opCode >= OP_ADD_INT) && (opCode <= OP_REM_DOUBLE)) {
-        return genArithOp( cUnit, mir );
-    }
-
-    switch (opCode) {
-        case OP_CMPL_FLOAT:
-        case OP_CMPG_FLOAT:
-        case OP_CMPL_DOUBLE:
-        case OP_CMPG_DOUBLE:
-            return genCmpX(cUnit, mir, vA, vB, vC);
-        case OP_CMP_LONG:
-            loadValuePair(cUnit,vB, r0, r1);
-            loadValuePair(cUnit, vC, r2, r3);
-            genDispatchToHandler(cUnit, TEMPLATE_CMP_LONG);
-            storeValue(cUnit, r0, vA, r1);
-            break;
-        case OP_AGET_WIDE:
-            genArrayGet(cUnit, mir, THUMB_LDR_RRR, vB, vC, vA, 3);
-            break;
-        case OP_AGET:
-        case OP_AGET_OBJECT:
-            genArrayGet(cUnit, mir, THUMB_LDR_RRR, vB, vC, vA, 2);
-            break;
-        case OP_AGET_BOOLEAN:
-            genArrayGet(cUnit, mir, THUMB_LDRB_RRR, vB, vC, vA, 0);
-            break;
-        case OP_AGET_BYTE:
-            genArrayGet(cUnit, mir, THUMB_LDRSB_RRR, vB, vC, vA, 0);
-            break;
-        case OP_AGET_CHAR:
-            genArrayGet(cUnit, mir, THUMB_LDRH_RRR, vB, vC, vA, 1);
-            break;
-        case OP_AGET_SHORT:
-            genArrayGet(cUnit, mir, THUMB_LDRSH_RRR, vB, vC, vA, 1);
-            break;
-        case OP_APUT_WIDE:
-            genArrayPut(cUnit, mir, THUMB_STR_RRR, vB, vC, vA, 3);
-            break;
-        case OP_APUT:
-        case OP_APUT_OBJECT:
-            genArrayPut(cUnit, mir, THUMB_STR_RRR, vB, vC, vA, 2);
-            break;
-        case OP_APUT_SHORT:
-        case OP_APUT_CHAR:
-            genArrayPut(cUnit, mir, THUMB_STRH_RRR, vB, vC, vA, 1);
-            break;
-        case OP_APUT_BYTE:
-        case OP_APUT_BOOLEAN:
-            genArrayPut(cUnit, mir, THUMB_STRB_RRR, vB, vC, vA, 0);
-            break;
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt31t(CompilationUnit *cUnit, MIR *mir)
-{
-    OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-    switch (dalvikOpCode) {
-        case OP_FILL_ARRAY_DATA: {
-            loadConstant(cUnit, r4PC, (int)dvmInterpHandleFillArrayData);
-            loadValue(cUnit, mir->dalvikInsn.vA, r0);
-            loadConstant(cUnit, r1, (mir->dalvikInsn.vB << 1) +
-                 (int) (cUnit->method->insns + mir->offset));
-            genExportPC(cUnit, mir, r2, r3 );
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-            genZeroCheck(cUnit, r0, mir->offset, NULL);
-            break;
-        }
-        /*
-         * TODO
-         * - Add a 1 to 3-entry per-location cache here to completely
-         *   bypass the dvmInterpHandle[Packed/Sparse]Switch call w/ chaining
-         * - Use out-of-line handlers for both of these
-         */
-        case OP_PACKED_SWITCH:
-        case OP_SPARSE_SWITCH: {
-            if (dalvikOpCode == OP_PACKED_SWITCH) {
-                loadConstant(cUnit, r4PC, (int)dvmInterpHandlePackedSwitch);
-            } else {
-                loadConstant(cUnit, r4PC, (int)dvmInterpHandleSparseSwitch);
-            }
-            loadValue(cUnit, mir->dalvikInsn.vA, r1);
-            loadConstant(cUnit, r0, (mir->dalvikInsn.vB << 1) +
-                 (int) (cUnit->method->insns + mir->offset));
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-            loadConstant(cUnit, r1, (int)(cUnit->method->insns + mir->offset));
-            newLIR3(cUnit, THUMB_LDR_RRI5, r2, rGLUE,
-                offsetof(InterpState, jitToInterpEntries.dvmJitToInterpNoChain)
-                    >> 2);
-            newLIR3(cUnit, THUMB_ADD_RRR, r0, r0, r0);
-            newLIR3(cUnit, THUMB_ADD_RRR, r4PC, r0, r1);
-            newLIR1(cUnit, THUMB_BLX_R, r2);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt35c_3rc(CompilationUnit *cUnit, MIR *mir, BasicBlock *bb,
-                             ArmLIR *labelList)
-{
-    ArmLIR *retChainingCell = NULL;
-    ArmLIR *pcrLabel = NULL;
-
-    if (bb->fallThrough != NULL)
-        retChainingCell = &labelList[bb->fallThrough->id];
-
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    switch (mir->dalvikInsn.opCode) {
-        /*
-         * calleeMethod = this->clazz->vtable[
-         *     method->clazz->pDvmDex->pResMethods[BBBB]->methodIndex
-         * ]
-         */
-        case OP_INVOKE_VIRTUAL:
-        case OP_INVOKE_VIRTUAL_RANGE: {
-            ArmLIR *predChainingCell = &labelList[bb->taken->id];
-            int methodIndex =
-                cUnit->method->clazz->pDvmDex->pResMethods[dInsn->vB]->
-                methodIndex;
-
-            if (mir->dalvikInsn.opCode == OP_INVOKE_VIRTUAL)
-                genProcessArgsNoRange(cUnit, mir, dInsn, &pcrLabel);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn, &pcrLabel);
-
-            genInvokeVirtualCommon(cUnit, mir, methodIndex,
-                                   retChainingCell,
-                                   predChainingCell,
-                                   pcrLabel);
-            break;
-        }
-        /*
-         * calleeMethod = method->clazz->super->vtable[method->clazz->pDvmDex
-         *                ->pResMethods[BBBB]->methodIndex]
-         */
-        /* TODO - not excersized in RunPerf.jar */
-        case OP_INVOKE_SUPER:
-        case OP_INVOKE_SUPER_RANGE: {
-            int mIndex = cUnit->method->clazz->pDvmDex->
-                pResMethods[dInsn->vB]->methodIndex;
-            const Method *calleeMethod =
-                cUnit->method->clazz->super->vtable[mIndex];
-
-            if (mir->dalvikInsn.opCode == OP_INVOKE_SUPER)
-                genProcessArgsNoRange(cUnit, mir, dInsn, &pcrLabel);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn, &pcrLabel);
-
-            /* r0 = calleeMethod */
-            loadConstant(cUnit, r0, (int) calleeMethod);
-
-            genInvokeSingletonCommon(cUnit, mir, bb, labelList, pcrLabel,
-                                     calleeMethod);
-            break;
-        }
-        /* calleeMethod = method->clazz->pDvmDex->pResMethods[BBBB] */
-        case OP_INVOKE_DIRECT:
-        case OP_INVOKE_DIRECT_RANGE: {
-            const Method *calleeMethod =
-                cUnit->method->clazz->pDvmDex->pResMethods[dInsn->vB];
-
-            if (mir->dalvikInsn.opCode == OP_INVOKE_DIRECT)
-                genProcessArgsNoRange(cUnit, mir, dInsn, &pcrLabel);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn, &pcrLabel);
-
-            /* r0 = calleeMethod */
-            loadConstant(cUnit, r0, (int) calleeMethod);
-
-            genInvokeSingletonCommon(cUnit, mir, bb, labelList, pcrLabel,
-                                     calleeMethod);
-            break;
-        }
-        /* calleeMethod = method->clazz->pDvmDex->pResMethods[BBBB] */
-        case OP_INVOKE_STATIC:
-        case OP_INVOKE_STATIC_RANGE: {
-            const Method *calleeMethod =
-                cUnit->method->clazz->pDvmDex->pResMethods[dInsn->vB];
-
-            if (mir->dalvikInsn.opCode == OP_INVOKE_STATIC)
-                genProcessArgsNoRange(cUnit, mir, dInsn,
-                                      NULL /* no null check */);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn,
-                                    NULL /* no null check */);
-
-            /* r0 = calleeMethod */
-            loadConstant(cUnit, r0, (int) calleeMethod);
-
-            genInvokeSingletonCommon(cUnit, mir, bb, labelList, pcrLabel,
-                                     calleeMethod);
-            break;
-        }
-        /*
-         * calleeMethod = dvmFindInterfaceMethodInCache(this->clazz,
-         *                    BBBB, method, method->clazz->pDvmDex)
-         *
-         *  Given "invoke-interface {v0}", the following is the generated code:
-         *
-         * 0x426a9abe : ldr     r0, [r5, #0]   --+
-         * 0x426a9ac0 : mov     r7, r5           |
-         * 0x426a9ac2 : sub     r7, #24          |
-         * 0x426a9ac4 : cmp     r0, #0           | genProcessArgsNoRange
-         * 0x426a9ac6 : beq     0x426a9afe       |
-         * 0x426a9ac8 : stmia   r7, <r0>       --+
-         * 0x426a9aca : ldr     r4, [pc, #104] --> r4 <- dalvikPC of this invoke
-         * 0x426a9acc : add     r1, pc, #52    --> r1 <- &retChainingCell
-         * 0x426a9ace : add     r2, pc, #60    --> r2 <- &predictedChainingCell
-         * 0x426a9ad0 : blx_1   0x426a918c     --+ TEMPLATE_INVOKE_METHOD_
-         * 0x426a9ad2 : blx_2   see above      --+     PREDICTED_CHAIN
-         * 0x426a9ad4 : b       0x426a9b0c     --> off to the predicted chain
-         * 0x426a9ad6 : b       0x426a9afe     --> punt to the interpreter
-         * 0x426a9ad8 : mov     r9, r1         --+
-         * 0x426a9ada : mov     r10, r2          |
-         * 0x426a9adc : mov     r12, r3          |
-         * 0x426a9ade : mov     r0, r3           |
-         * 0x426a9ae0 : mov     r1, #74          | dvmFindInterfaceMethodInCache
-         * 0x426a9ae2 : ldr     r2, [pc, #76]    |
-         * 0x426a9ae4 : ldr     r3, [pc, #68]    |
-         * 0x426a9ae6 : ldr     r7, [pc, #64]    |
-         * 0x426a9ae8 : blx     r7             --+
-         * 0x426a9aea : mov     r1, r9         --> r1 <- rechain count
-         * 0x426a9aec : cmp     r1, #0         --> compare against 0
-         * 0x426a9aee : bgt     0x426a9af8     --> >=0? don't rechain
-         * 0x426a9af0 : ldr     r7, [r6, #96]  --+
-         * 0x426a9af2 : mov     r2, r10          | dvmJitToPatchPredictedChain
-         * 0x426a9af4 : mov     r3, r12          |
-         * 0x426a9af6 : blx     r7             --+
-         * 0x426a9af8 : add     r1, pc, #8     --> r1 <- &retChainingCell
-         * 0x426a9afa : blx_1   0x426a9098     --+ TEMPLATE_INVOKE_METHOD_NO_OPT
-         * 0x426a9afc : blx_2   see above      --+
-         * -------- reconstruct dalvik PC : 0x428b786c @ +0x001e
-         * 0x426a9afe (0042): ldr     r0, [pc, #52]
-         * Exception_Handling:
-         * 0x426a9b00 (0044): ldr     r1, [r6, #84]
-         * 0x426a9b02 (0046): blx     r1
-         * 0x426a9b04 (0048): .align4
-         * -------- chaining cell (hot): 0x0021
-         * 0x426a9b04 (0048): ldr     r0, [r6, #92]
-         * 0x426a9b06 (004a): blx     r0
-         * 0x426a9b08 (004c): data    0x7872(30834)
-         * 0x426a9b0a (004e): data    0x428b(17035)
-         * 0x426a9b0c (0050): .align4
-         * -------- chaining cell (predicted)
-         * 0x426a9b0c (0050): data    0x0000(0) --> will be patched into bx
-         * 0x426a9b0e (0052): data    0x0000(0)
-         * 0x426a9b10 (0054): data    0x0000(0) --> class
-         * 0x426a9b12 (0056): data    0x0000(0)
-         * 0x426a9b14 (0058): data    0x0000(0) --> method
-         * 0x426a9b16 (005a): data    0x0000(0)
-         * 0x426a9b18 (005c): data    0x0000(0) --> reset count
-         * 0x426a9b1a (005e): data    0x0000(0)
-         * 0x426a9b28 (006c): .word (0xad0392a5)
-         * 0x426a9b2c (0070): .word (0x6e750)
-         * 0x426a9b30 (0074): .word (0x4109a618)
-         * 0x426a9b34 (0078): .word (0x428b786c)
-         */
-        case OP_INVOKE_INTERFACE:
-        case OP_INVOKE_INTERFACE_RANGE: {
-            ArmLIR *predChainingCell = &labelList[bb->taken->id];
-            int methodIndex = dInsn->vB;
-
-            if (mir->dalvikInsn.opCode == OP_INVOKE_INTERFACE)
-                genProcessArgsNoRange(cUnit, mir, dInsn, &pcrLabel);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn, &pcrLabel);
-
-            /* "this" is already left in r0 by genProcessArgs* */
-
-            /* r4PC = dalvikCallsite */
-            loadConstant(cUnit, r4PC,
-                         (int) (cUnit->method->insns + mir->offset));
-
-            /* r1 = &retChainingCell */
-            ArmLIR *addrRetChain = newLIR2(cUnit, THUMB_ADD_PC_REL,
-                                               r1, 0);
-            addrRetChain->generic.target = (LIR *) retChainingCell;
-
-            /* r2 = &predictedChainingCell */
-            ArmLIR *predictedChainingCell =
-                newLIR2(cUnit, THUMB_ADD_PC_REL, r2, 0);
-            predictedChainingCell->generic.target = (LIR *) predChainingCell;
-
-            genDispatchToHandler(cUnit, TEMPLATE_INVOKE_METHOD_PREDICTED_CHAIN);
-
-            /* return through lr - jump to the chaining cell */
-            genUnconditionalBranch(cUnit, predChainingCell);
-
-            /*
-             * null-check on "this" may have been eliminated, but we still need
-             * a PC-reconstruction label for stack overflow bailout.
-             */
-            if (pcrLabel == NULL) {
-                int dPC = (int) (cUnit->method->insns + mir->offset);
-                pcrLabel = dvmCompilerNew(sizeof(ArmLIR), true);
-                pcrLabel->opCode = ARM_PSEUDO_PC_RECONSTRUCTION_CELL;
-                pcrLabel->operands[0] = dPC;
-                pcrLabel->operands[1] = mir->offset;
-                /* Insert the place holder to the growable list */
-                dvmInsertGrowableList(&cUnit->pcReconstructionList, pcrLabel);
-            }
-
-            /* return through lr+2 - punt to the interpreter */
-            genUnconditionalBranch(cUnit, pcrLabel);
-
-            /*
-             * return through lr+4 - fully resolve the callee method.
-             * r1 <- count
-             * r2 <- &predictedChainCell
-             * r3 <- this->class
-             * r4 <- dPC
-             * r7 <- this->class->vtable
-             */
-
-            /* Save count, &predictedChainCell, and class to high regs first */
-            newLIR2(cUnit, THUMB_MOV_RR_L2H, r9 & THUMB_REG_MASK, r1);
-            newLIR2(cUnit, THUMB_MOV_RR_L2H, r10 & THUMB_REG_MASK, r2);
-            newLIR2(cUnit, THUMB_MOV_RR_L2H, r12 & THUMB_REG_MASK, r3);
-
-            /* r0 now contains this->clazz */
-            newLIR2(cUnit, THUMB_MOV_RR, r0, r3);
-
-            /* r1 = BBBB */
-            loadConstant(cUnit, r1, dInsn->vB);
-
-            /* r2 = method (caller) */
-            loadConstant(cUnit, r2, (int) cUnit->method);
-
-            /* r3 = pDvmDex */
-            loadConstant(cUnit, r3, (int) cUnit->method->clazz->pDvmDex);
-
-            loadConstant(cUnit, r7,
-                         (intptr_t) dvmFindInterfaceMethodInCache);
-            newLIR1(cUnit, THUMB_BLX_R, r7);
-
-            /* r0 = calleeMethod (returned from dvmFindInterfaceMethodInCache */
-
-            newLIR2(cUnit, THUMB_MOV_RR_H2L, r1, r9 & THUMB_REG_MASK);
-
-            /* Check if rechain limit is reached */
-            newLIR2(cUnit, THUMB_CMP_RI8, r1, 0);
-
-            ArmLIR *bypassRechaining =
-                newLIR2(cUnit, THUMB_B_COND, 0, ARM_COND_GT);
-
-            newLIR3(cUnit, THUMB_LDR_RRI5, r7, rGLUE,
-                    offsetof(InterpState,
-                             jitToInterpEntries.dvmJitToPatchPredictedChain)
-                    >> 2);
-
-            newLIR2(cUnit, THUMB_MOV_RR_H2L, r2, r10 & THUMB_REG_MASK);
-            newLIR2(cUnit, THUMB_MOV_RR_H2L, r3, r12 & THUMB_REG_MASK);
-
-            /*
-             * r0 = calleeMethod
-             * r2 = &predictedChainingCell
-             * r3 = class
-             *
-             * &returnChainingCell has been loaded into r1 but is not needed
-             * when patching the chaining cell and will be clobbered upon
-             * returning so it will be reconstructed again.
-             */
-            newLIR1(cUnit, THUMB_BLX_R, r7);
-
-            /* r1 = &retChainingCell */
-            addrRetChain = newLIR3(cUnit, THUMB_ADD_PC_REL,
-                                               r1, 0, 0);
-            addrRetChain->generic.target = (LIR *) retChainingCell;
-
-            bypassRechaining->generic.target = (LIR *) addrRetChain;
-
-            /*
-             * r0 = this, r1 = calleeMethod,
-             * r1 = &ChainingCell,
-             * r4PC = callsiteDPC,
-             */
-            genDispatchToHandler(cUnit, TEMPLATE_INVOKE_METHOD_NO_OPT);
-#if defined(INVOKE_STATS)
-            gDvmJit.invokePredictedChain++;
-#endif
-            /* Handle exceptions using the interpreter */
-            genTrap(cUnit, mir->offset, pcrLabel);
-            break;
-        }
-        /* NOP */
-        case OP_INVOKE_DIRECT_EMPTY: {
-            return false;
-        }
-        case OP_FILLED_NEW_ARRAY:
-        case OP_FILLED_NEW_ARRAY_RANGE: {
-            /* Just let the interpreter deal with these */
-            genInterpSingleStep(cUnit, mir);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt35ms_3rms(CompilationUnit *cUnit, MIR *mir,
-                               BasicBlock *bb, ArmLIR *labelList)
-{
-    ArmLIR *retChainingCell = &labelList[bb->fallThrough->id];
-    ArmLIR *predChainingCell = &labelList[bb->taken->id];
-    ArmLIR *pcrLabel = NULL;
-
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    switch (mir->dalvikInsn.opCode) {
-        /* calleeMethod = this->clazz->vtable[BBBB] */
-        case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-        case OP_INVOKE_VIRTUAL_QUICK: {
-            int methodIndex = dInsn->vB;
-            if (mir->dalvikInsn.opCode == OP_INVOKE_VIRTUAL_QUICK)
-                genProcessArgsNoRange(cUnit, mir, dInsn, &pcrLabel);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn, &pcrLabel);
-
-            genInvokeVirtualCommon(cUnit, mir, methodIndex,
-                                   retChainingCell,
-                                   predChainingCell,
-                                   pcrLabel);
-            break;
-        }
-        /* calleeMethod = method->clazz->super->vtable[BBBB] */
-        case OP_INVOKE_SUPER_QUICK:
-        case OP_INVOKE_SUPER_QUICK_RANGE: {
-            const Method *calleeMethod =
-                cUnit->method->clazz->super->vtable[dInsn->vB];
-
-            if (mir->dalvikInsn.opCode == OP_INVOKE_SUPER_QUICK)
-                genProcessArgsNoRange(cUnit, mir, dInsn, &pcrLabel);
-            else
-                genProcessArgsRange(cUnit, mir, dInsn, &pcrLabel);
-
-            /* r0 = calleeMethod */
-            loadConstant(cUnit, r0, (int) calleeMethod);
-
-            genInvokeSingletonCommon(cUnit, mir, bb, labelList, pcrLabel,
-                                     calleeMethod);
-            /* Handle exceptions using the interpreter */
-            genTrap(cUnit, mir->offset, pcrLabel);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-/*
- * NOTE: We assume here that the special native inline routines
- * are side-effect free.  By making this assumption, we can safely
- * re-execute the routine from the interpreter if it decides it
- * wants to throw an exception. We still need to EXPORT_PC(), though.
- */
-static bool handleFmt3inline(CompilationUnit *cUnit, MIR *mir)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    switch( mir->dalvikInsn.opCode) {
-        case OP_EXECUTE_INLINE: {
-            unsigned int i;
-            const InlineOperation* inLineTable = dvmGetInlineOpsTable();
-            int offset = offsetof(InterpState, retval);
-            int operation = dInsn->vB;
-
-            switch (operation) {
-                case INLINE_EMPTYINLINEMETHOD:
-                    return false;  /* Nop */
-                case INLINE_STRING_LENGTH:
-                    return genInlinedStringLength(cUnit, mir);
-                case INLINE_MATH_ABS_INT:
-                    return genInlinedAbsInt(cUnit, mir);
-                case INLINE_MATH_ABS_LONG:
-                    return genInlinedAbsLong(cUnit, mir);
-                case INLINE_MATH_MIN_INT:
-                    return genInlinedMinMaxInt(cUnit, mir, true);
-                case INLINE_MATH_MAX_INT:
-                    return genInlinedMinMaxInt(cUnit, mir, false);
-                case INLINE_STRING_CHARAT:
-                    return genInlinedStringCharAt(cUnit, mir);
-                case INLINE_MATH_SQRT:
-                    if (genInlineSqrt(cUnit, mir))
-                        return false;
-                    else
-                        break;   /* Handle with C routine */
-                case INLINE_MATH_COS:
-                    if (genInlineCos(cUnit, mir))
-                        return false;
-                    else
-                        break;   /* Handle with C routine */
-                case INLINE_MATH_SIN:
-                    if (genInlineSin(cUnit, mir))
-                        return false;
-                    else
-                        break;   /* Handle with C routine */
-                case INLINE_MATH_ABS_FLOAT:
-                    return genInlinedAbsFloat(cUnit, mir);
-                case INLINE_MATH_ABS_DOUBLE:
-                    return genInlinedAbsDouble(cUnit, mir);
-                case INLINE_STRING_COMPARETO:
-                case INLINE_STRING_EQUALS:
-                    break;
-                default:
-                    dvmAbort();
-            }
-
-            /* Materialize pointer to retval & push */
-            newLIR2(cUnit, THUMB_MOV_RR, r4PC, rGLUE);
-            newLIR2(cUnit, THUMB_ADD_RI8, r4PC, offset);
-            /* Push r4 and (just to take up space) r5) */
-            newLIR1(cUnit, THUMB_PUSH, (1<<r4PC | 1<<rFP));
-
-            /* Get code pointer to inline routine */
-            loadConstant(cUnit, r4PC, (int)inLineTable[operation].func);
-
-            /* Export PC */
-            genExportPC(cUnit, mir, r0, r1 );
-
-            /* Load arguments to r0 through r3 as applicable */
-            for (i=0; i < dInsn->vA; i++) {
-                loadValue(cUnit, dInsn->arg[i], i);
-            }
-            /* Call inline routine */
-            newLIR1(cUnit, THUMB_BLX_R, r4PC);
-
-            /* Strip frame */
-            newLIR1(cUnit, THUMB_ADD_SPI7, 2);
-
-            /* Did we throw? If so, redo under interpreter*/
-            genZeroCheck(cUnit, r0, mir->offset, NULL);
-
-            resetRegisterScoreboard(cUnit);
-            break;
-        }
-        default:
-            return true;
-    }
-    return false;
-}
-
-static bool handleFmt51l(CompilationUnit *cUnit, MIR *mir)
-{
-    loadConstant(cUnit, r0, mir->dalvikInsn.vB_wide & 0xFFFFFFFFUL);
-    loadConstant(cUnit, r1, (mir->dalvikInsn.vB_wide>>32) & 0xFFFFFFFFUL);
-    storeValuePair(cUnit, r0, r1, mir->dalvikInsn.vA, r2);
-    return false;
-}
-
-/*****************************************************************************/
-/*
- * The following are special processing routines that handle transfer of
- * controls between compiled code and the interpreter. Certain VM states like
- * Dalvik PC and special-purpose registers are reconstructed here.
- */
-
-/* Chaining cell for code that may need warmup. */
-static void handleNormalChainingCell(CompilationUnit *cUnit,
-                                     unsigned int offset)
-{
-    newLIR3(cUnit, THUMB_LDR_RRI5, r0, rGLUE,
-        offsetof(InterpState, jitToInterpEntries.dvmJitToInterpNormal) >> 2);
-    newLIR1(cUnit, THUMB_BLX_R, r0);
-    addWordData(cUnit, (int) (cUnit->method->insns + offset), true);
-}
-
-/*
- * Chaining cell for instructions that immediately following already translated
- * code.
- */
-static void handleHotChainingCell(CompilationUnit *cUnit,
-                                  unsigned int offset)
-{
-    newLIR3(cUnit, THUMB_LDR_RRI5, r0, rGLUE,
-        offsetof(InterpState, jitToInterpEntries.dvmJitToTraceSelect) >> 2);
-    newLIR1(cUnit, THUMB_BLX_R, r0);
-    addWordData(cUnit, (int) (cUnit->method->insns + offset), true);
-}
-
-/* Chaining cell for monomorphic method invocations. */
-static void handleInvokeSingletonChainingCell(CompilationUnit *cUnit,
-                                              const Method *callee)
-{
-    newLIR3(cUnit, THUMB_LDR_RRI5, r0, rGLUE,
-        offsetof(InterpState, jitToInterpEntries.dvmJitToTraceSelect) >> 2);
-    newLIR1(cUnit, THUMB_BLX_R, r0);
-    addWordData(cUnit, (int) (callee->insns), true);
-}
-
-/* Chaining cell for monomorphic method invocations. */
-static void handleInvokePredictedChainingCell(CompilationUnit *cUnit)
-{
-
-    /* Should not be executed in the initial state */
-    addWordData(cUnit, PREDICTED_CHAIN_BX_PAIR_INIT, true);
-    /* To be filled: class */
-    addWordData(cUnit, PREDICTED_CHAIN_CLAZZ_INIT, true);
-    /* To be filled: method */
-    addWordData(cUnit, PREDICTED_CHAIN_METHOD_INIT, true);
-    /*
-     * Rechain count. The initial value of 0 here will trigger chaining upon
-     * the first invocation of this callsite.
-     */
-    addWordData(cUnit, PREDICTED_CHAIN_COUNTER_INIT, true);
-}
-
-/* Load the Dalvik PC into r0 and jump to the specified target */
-static void handlePCReconstruction(CompilationUnit *cUnit,
-                                   ArmLIR *targetLabel)
-{
-    ArmLIR **pcrLabel =
-        (ArmLIR **) cUnit->pcReconstructionList.elemList;
-    int numElems = cUnit->pcReconstructionList.numUsed;
-    int i;
-    for (i = 0; i < numElems; i++) {
-        dvmCompilerAppendLIR(cUnit, (LIR *) pcrLabel[i]);
-        /* r0 = dalvik PC */
-        loadConstant(cUnit, r0, pcrLabel[i]->operands[0]);
-        genUnconditionalBranch(cUnit, targetLabel);
-    }
-}
-
-/* Entry function to invoke the backend of the JIT compiler */
-void dvmCompilerMIR2LIR(CompilationUnit *cUnit)
-{
-    /* Used to hold the labels of each block */
-    ArmLIR *labelList =
-        dvmCompilerNew(sizeof(ArmLIR) * cUnit->numBlocks, true);
-    GrowableList chainingListByType[CHAINING_CELL_LAST];
-    int i;
-
-    /*
-     * Initialize various types chaining lists.
-     */
-    for (i = 0; i < CHAINING_CELL_LAST; i++) {
-        dvmInitGrowableList(&chainingListByType[i], 2);
-    }
-
-    BasicBlock **blockList = cUnit->blockList;
-
-    if (cUnit->executionCount) {
-        /*
-         * Reserve 6 bytes at the beginning of the trace
-         *        +----------------------------+
-         *        | execution count (4 bytes)  |
-         *        +----------------------------+
-         *        | chain cell offset (2 bytes)|
-         *        +----------------------------+
-         * ...and then code to increment the execution
-         * count:
-         *       mov   r0, pc       @ move adr of "mov r0,pc" + 4 to r0
-         *       sub   r0, #10      @ back up to addr of executionCount
-         *       ldr   r1, [r0]
-         *       add   r1, #1
-         *       str   r1, [r0]
-         */
-        newLIR1(cUnit, ARM_16BIT_DATA, 0);
-        newLIR1(cUnit, ARM_16BIT_DATA, 0);
-        cUnit->chainCellOffsetLIR =
-            (LIR *) newLIR1(cUnit, ARM_16BIT_DATA, CHAIN_CELL_OFFSET_TAG);
-        cUnit->headerSize = 6;
-        newLIR2(cUnit, THUMB_MOV_RR_H2L, r0, rpc & THUMB_REG_MASK);
-        newLIR2(cUnit, THUMB_SUB_RI8, r0, 10);
-        newLIR3(cUnit, THUMB_LDR_RRI5, r1, r0, 0);
-        newLIR2(cUnit, THUMB_ADD_RI8, r1, 1);
-        newLIR3(cUnit, THUMB_STR_RRI5, r1, r0, 0);
-    } else {
-         /* Just reserve 2 bytes for the chain cell offset */
-        cUnit->chainCellOffsetLIR =
-            (LIR *) newLIR1(cUnit, ARM_16BIT_DATA, CHAIN_CELL_OFFSET_TAG);
-        cUnit->headerSize = 2;
-    }
-
-    /* Handle the content in each basic block */
-    for (i = 0; i < cUnit->numBlocks; i++) {
-        blockList[i]->visited = true;
-        MIR *mir;
-
-        labelList[i].operands[0] = blockList[i]->startOffset;
-
-        if (blockList[i]->blockType >= CHAINING_CELL_LAST) {
-            /*
-             * Append the label pseudo LIR first. Chaining cells will be handled
-             * separately afterwards.
-             */
-            dvmCompilerAppendLIR(cUnit, (LIR *) &labelList[i]);
-        }
-
-        if (blockList[i]->blockType == DALVIK_BYTECODE) {
-            labelList[i].opCode = ARM_PSEUDO_NORMAL_BLOCK_LABEL;
-            /* Reset the register state */
-            resetRegisterScoreboard(cUnit);
-        } else {
-            switch (blockList[i]->blockType) {
-                case CHAINING_CELL_NORMAL:
-                    labelList[i].opCode = ARM_PSEUDO_CHAINING_CELL_NORMAL;
-                    /* handle the codegen later */
-                    dvmInsertGrowableList(
-                        &chainingListByType[CHAINING_CELL_NORMAL], (void *) i);
-                    break;
-                case CHAINING_CELL_INVOKE_SINGLETON:
-                    labelList[i].opCode =
-                        ARM_PSEUDO_CHAINING_CELL_INVOKE_SINGLETON;
-                    labelList[i].operands[0] =
-                        (int) blockList[i]->containingMethod;
-                    /* handle the codegen later */
-                    dvmInsertGrowableList(
-                        &chainingListByType[CHAINING_CELL_INVOKE_SINGLETON],
-                        (void *) i);
-                    break;
-                case CHAINING_CELL_INVOKE_PREDICTED:
-                    labelList[i].opCode =
-                        ARM_PSEUDO_CHAINING_CELL_INVOKE_PREDICTED;
-                    /* handle the codegen later */
-                    dvmInsertGrowableList(
-                        &chainingListByType[CHAINING_CELL_INVOKE_PREDICTED],
-                        (void *) i);
-                    break;
-                case CHAINING_CELL_HOT:
-                    labelList[i].opCode =
-                        ARM_PSEUDO_CHAINING_CELL_HOT;
-                    /* handle the codegen later */
-                    dvmInsertGrowableList(
-                        &chainingListByType[CHAINING_CELL_HOT],
-                        (void *) i);
-                    break;
-                case PC_RECONSTRUCTION:
-                    /* Make sure exception handling block is next */
-                    labelList[i].opCode =
-                        ARM_PSEUDO_PC_RECONSTRUCTION_BLOCK_LABEL;
-                    assert (i == cUnit->numBlocks - 2);
-                    handlePCReconstruction(cUnit, &labelList[i+1]);
-                    break;
-                case EXCEPTION_HANDLING:
-                    labelList[i].opCode = ARM_PSEUDO_EH_BLOCK_LABEL;
-                    if (cUnit->pcReconstructionList.numUsed) {
-                        newLIR3(cUnit, THUMB_LDR_RRI5, r1, rGLUE,
-                            offsetof(InterpState,
-                                     jitToInterpEntries.dvmJitToInterpPunt)
-                            >> 2);
-                        newLIR1(cUnit, THUMB_BLX_R, r1);
-                    }
-                    break;
-                default:
-                    break;
-            }
-            continue;
-        }
-
-        ArmLIR *headLIR = NULL;
-
-        for (mir = blockList[i]->firstMIRInsn; mir; mir = mir->next) {
-            OpCode dalvikOpCode = mir->dalvikInsn.opCode;
-            InstructionFormat dalvikFormat =
-                dexGetInstrFormat(gDvm.instrFormat, dalvikOpCode);
-            ArmLIR *boundaryLIR =
-                newLIR2(cUnit, ARM_PSEUDO_DALVIK_BYTECODE_BOUNDARY,
-                        mir->offset,dalvikOpCode);
-            /* Remember the first LIR for this block */
-            if (headLIR == NULL) {
-                headLIR = boundaryLIR;
-            }
-            bool notHandled;
-            /*
-             * Debugging: screen the opcode first to see if it is in the
-             * do[-not]-compile list
-             */
-            bool singleStepMe =
-                gDvmJit.includeSelectedOp !=
-                ((gDvmJit.opList[dalvikOpCode >> 3] &
-                  (1 << (dalvikOpCode & 0x7))) !=
-                 0);
-            if (singleStepMe || cUnit->allSingleStep) {
-                notHandled = false;
-                genInterpSingleStep(cUnit, mir);
-            } else {
-                opcodeCoverage[dalvikOpCode]++;
-                switch (dalvikFormat) {
-                    case kFmt10t:
-                    case kFmt20t:
-                    case kFmt30t:
-                        notHandled = handleFmt10t_Fmt20t_Fmt30t(cUnit,
-                                  mir, blockList[i], labelList);
-                        break;
-                    case kFmt10x:
-                        notHandled = handleFmt10x(cUnit, mir);
-                        break;
-                    case kFmt11n:
-                    case kFmt31i:
-                        notHandled = handleFmt11n_Fmt31i(cUnit, mir);
-                        break;
-                    case kFmt11x:
-                        notHandled = handleFmt11x(cUnit, mir);
-                        break;
-                    case kFmt12x:
-                        notHandled = handleFmt12x(cUnit, mir);
-                        break;
-                    case kFmt20bc:
-                        notHandled = handleFmt20bc(cUnit, mir);
-                        break;
-                    case kFmt21c:
-                    case kFmt31c:
-                        notHandled = handleFmt21c_Fmt31c(cUnit, mir);
-                        break;
-                    case kFmt21h:
-                        notHandled = handleFmt21h(cUnit, mir);
-                        break;
-                    case kFmt21s:
-                        notHandled = handleFmt21s(cUnit, mir);
-                        break;
-                    case kFmt21t:
-                        notHandled = handleFmt21t(cUnit, mir, blockList[i],
-                                                  labelList);
-                        break;
-                    case kFmt22b:
-                    case kFmt22s:
-                        notHandled = handleFmt22b_Fmt22s(cUnit, mir);
-                        break;
-                    case kFmt22c:
-                        notHandled = handleFmt22c(cUnit, mir);
-                        break;
-                    case kFmt22cs:
-                        notHandled = handleFmt22cs(cUnit, mir);
-                        break;
-                    case kFmt22t:
-                        notHandled = handleFmt22t(cUnit, mir, blockList[i],
-                                                  labelList);
-                        break;
-                    case kFmt22x:
-                    case kFmt32x:
-                        notHandled = handleFmt22x_Fmt32x(cUnit, mir);
-                        break;
-                    case kFmt23x:
-                        notHandled = handleFmt23x(cUnit, mir);
-                        break;
-                    case kFmt31t:
-                        notHandled = handleFmt31t(cUnit, mir);
-                        break;
-                    case kFmt3rc:
-                    case kFmt35c:
-                        notHandled = handleFmt35c_3rc(cUnit, mir, blockList[i],
-                                                      labelList);
-                        break;
-                    case kFmt3rms:
-                    case kFmt35ms:
-                        notHandled = handleFmt35ms_3rms(cUnit, mir,blockList[i],
-                                                        labelList);
-                        break;
-                    case kFmt3inline:
-                        notHandled = handleFmt3inline(cUnit, mir);
-                        break;
-                    case kFmt51l:
-                        notHandled = handleFmt51l(cUnit, mir);
-                        break;
-                    default:
-                        notHandled = true;
-                        break;
-                }
-            }
-            if (notHandled) {
-                LOGE("%#06x: Opcode 0x%x (%s) / Fmt %d not handled\n",
-                     mir->offset,
-                     dalvikOpCode, getOpcodeName(dalvikOpCode),
-                     dalvikFormat);
-                dvmAbort();
-                break;
-            }
-        }
-        /* Eliminate redundant loads/stores and delay stores into later slots */
-        dvmCompilerApplyLocalOptimizations(cUnit, (LIR *) headLIR,
-                                           cUnit->lastLIRInsn);
-        /*
-         * Check if the block is terminated due to trace length constraint -
-         * insert an unconditional branch to the chaining cell.
-         */
-        if (blockList[i]->needFallThroughBranch) {
-            genUnconditionalBranch(cUnit,
-                                   &labelList[blockList[i]->fallThrough->id]);
-        }
-
-    }
-
-    /* Handle the chaining cells in predefined order */
-    for (i = 0; i < CHAINING_CELL_LAST; i++) {
-        size_t j;
-        int *blockIdList = (int *) chainingListByType[i].elemList;
-
-        cUnit->numChainingCells[i] = chainingListByType[i].numUsed;
-
-        /* No chaining cells of this type */
-        if (cUnit->numChainingCells[i] == 0)
-            continue;
-
-        /* Record the first LIR for a new type of chaining cell */
-        cUnit->firstChainingLIR[i] = (LIR *) &labelList[blockIdList[0]];
-
-        for (j = 0; j < chainingListByType[i].numUsed; j++) {
-            int blockId = blockIdList[j];
-
-            /* Align this chaining cell first */
-            newLIR0(cUnit, ARM_PSEUDO_ALIGN4);
-
-            /* Insert the pseudo chaining instruction */
-            dvmCompilerAppendLIR(cUnit, (LIR *) &labelList[blockId]);
-
-
-            switch (blockList[blockId]->blockType) {
-                case CHAINING_CELL_NORMAL:
-                    handleNormalChainingCell(cUnit,
-                      blockList[blockId]->startOffset);
-                    break;
-                case CHAINING_CELL_INVOKE_SINGLETON:
-                    handleInvokeSingletonChainingCell(cUnit,
-                        blockList[blockId]->containingMethod);
-                    break;
-                case CHAINING_CELL_INVOKE_PREDICTED:
-                    handleInvokePredictedChainingCell(cUnit);
-                    break;
-                case CHAINING_CELL_HOT:
-                    handleHotChainingCell(cUnit,
-                        blockList[blockId]->startOffset);
-                    break;
-                default:
-                    dvmAbort();
-                    break;
-            }
-        }
-    }
-
-    dvmCompilerApplyGlobalOptimizations(cUnit);
-}
-
-/* Accept the work and start compiling */
-bool dvmCompilerDoWork(CompilerWorkOrder *work)
-{
-   bool res;
-
-   if (gDvmJit.codeCacheFull) {
-       return false;
-   }
-
-   switch (work->kind) {
-       case kWorkOrderMethod:
-           res = dvmCompileMethod(work->info, &work->result);
-           break;
-       case kWorkOrderTrace:
-           /* Start compilation with maximally allowed trace length */
-           res = dvmCompileTrace(work->info, JIT_MAX_TRACE_LEN, &work->result);
-           break;
-       default:
-           res = false;
-           dvmAbort();
-   }
-   return res;
-}
-
-/* Architectural-specific debugging helpers go here */
-void dvmCompilerArchDump(void)
-{
-    /* Print compiled opcode in this VM instance */
-    int i, start, streak;
-    char buf[1024];
-
-    streak = i = 0;
-    buf[0] = 0;
-    while (opcodeCoverage[i] == 0 && i < 256) {
-        i++;
-    }
-    if (i == 256) {
-        return;
-    }
-    for (start = i++, streak = 1; i < 256; i++) {
-        if (opcodeCoverage[i]) {
-            streak++;
-        } else {
-            if (streak == 1) {
-                sprintf(buf+strlen(buf), "%x,", start);
-            } else {
-                sprintf(buf+strlen(buf), "%x-%x,", start, start + streak - 1);
-            }
-            streak = 0;
-            while (opcodeCoverage[i] == 0 && i < 256) {
-                i++;
-            }
-            if (i < 256) {
-                streak = 1;
-                start = i;
-            }
-        }
-    }
-    if (streak) {
-        if (streak == 1) {
-            sprintf(buf+strlen(buf), "%x", start);
-        } else {
-            sprintf(buf+strlen(buf), "%x-%x", start, start + streak - 1);
-        }
-    }
-    if (strlen(buf)) {
-        LOGD("dalvik.vm.jit.op = %s", buf);
-    }
-}
diff --git a/vm/compiler/codegen/arm/Codegen.h b/vm/compiler/codegen/arm/Codegen.h
index 4016075..da65bb5 100644
--- a/vm/compiler/codegen/arm/Codegen.h
+++ b/vm/compiler/codegen/arm/Codegen.h
@@ -14,44 +14,66 @@
  * limitations under the License.
  */
 
-#ifndef _DALVIK_VM_COMPILER_CODEGEN_ARM_CODEGEN_H
-#define _DALVIK_VM_COMPILER_CODEGEN_ARM_CODEGEN_H
+/*
+ * This file contains register alloction support and is intended to be
+ * included by:
+ *
+ *        Codegen-$(TARGET_ARCH_VARIANT).c
+ *
+ */
+
+#include "compiler/CompilerIR.h"
+#include "CalloutHelper.h"
 
 /*
- * Forward declarations for common routines in Codegen.c used by ISA
- * variant code such as ThumbUtilty.c
+ * loadConstant() sometimes needs to add a small imm to a pre-existing constant
  */
+static ArmLIR *opRegImm(CompilationUnit *cUnit, OpKind op, int rDestSrc1,
+                        int value);
+static ArmLIR *opRegReg(CompilationUnit *cUnit, OpKind op, int rDestSrc1,
+                        int rSrc2);
+
+/* Forward decalraton the portable versions due to circular dependency */
+static bool genArithOpFloatPortable(CompilationUnit *cUnit, MIR *mir,
+                                    RegLocation rlDest, RegLocation rlSrc1,
+                                    RegLocation rlSrc2);
+
+static bool genArithOpDoublePortable(CompilationUnit *cUnit, MIR *mir,
+                                     RegLocation rlDest, RegLocation rlSrc1,
+                                     RegLocation rlSrc2);
+
+static bool genConversionPortable(CompilationUnit *cUnit, MIR *mir);
+
+static void genMonitorPortable(CompilationUnit *cUnit, MIR *mir);
+
+static void genInterpSingleStep(CompilationUnit *cUnit, MIR *mir);
+
+#if defined(WITH_SELF_VERIFICATION)
+/* Self Verification memory instruction decoder */
+void dvmSelfVerificationMemOpDecode(int lr, int* sp);
+#endif
+
+/*
+ * Architecture-dependent register allocation routines implemented in
+ * Thumb[2]/Ralloc.c
+ */
+extern int dvmCompilerAllocTypedTempPair(CompilationUnit *cUnit,
+                                         bool fpHint, int regClass);
+
+extern int dvmCompilerAllocTypedTemp(CompilationUnit *cUnit, bool fpHint,
+                                     int regClass);
+
+extern ArmLIR* dvmCompilerRegCopyNoInsert(CompilationUnit *cUnit, int rDest,
+                                          int rSrc);
+
+extern ArmLIR* dvmCompilerRegCopy(CompilationUnit *cUnit, int rDest, int rSrc);
+
+extern void dvmCompilerRegCopyWide(CompilationUnit *cUnit, int destLo,
+                                   int destHi, int srcLo, int srcHi);
+
+extern void dvmCompilerFlushRegImpl(CompilationUnit *cUnit, int rBase,
+                                    int displacement, int rSrc, OpSize size);
 
-static ArmLIR *newLIR0(CompilationUnit *cUnit, ArmOpCode opCode);
-static ArmLIR *newLIR1(CompilationUnit *cUnit, ArmOpCode opCode,
-                           int dest);
-static ArmLIR *newLIR2(CompilationUnit *cUnit, ArmOpCode opCode,
-                           int dest, int src1);
-static ArmLIR *newLIR3(CompilationUnit *cUnit, ArmOpCode opCode,
-                           int dest, int src1, int src2);
-static ArmLIR *newLIR23(CompilationUnit *cUnit, ArmOpCode opCode,
-                            int srcdest, int src2);
-static ArmLIR *scanLiteralPool(CompilationUnit *cUnit, int value,
-                                   unsigned int delta);
-static ArmLIR *addWordData(CompilationUnit *cUnit, int value, bool inPlace);
-static inline ArmLIR *genCheckCommon(CompilationUnit *cUnit, int dOffset,
-                                         ArmLIR *branch,
-                                         ArmLIR *pcrLabel);
-
-/* Routines which must be supplied by the variant-specific code */
-static void genDispatchToHandler(CompilationUnit *cUnit, TemplateOpCode opCode);
-bool dvmCompilerArchInit(void);
-static bool genInlineSqrt(CompilationUnit *cUnit, MIR *mir);
-static bool genInlineCos(CompilationUnit *cUnit, MIR *mir);
-static bool genInlineSin(CompilationUnit *cUnit, MIR *mir);
-static bool genConversion(CompilationUnit *cUnit, MIR *mir);
-static bool genArithOpFloat(CompilationUnit *cUnit, MIR *mir, int vDest,
-                            int vSrc1, int vSrc2);
-static bool genArithOpDouble(CompilationUnit *cUnit, MIR *mir, int vDest,
-                             int vSrc1, int vSrc2);
-static bool genCmpX(CompilationUnit *cUnit, MIR *mir, int vDest, int vSrc1,
-                    int vSrc2);
-
-
-
-#endif /* _DALVIK_VM_COMPILER_CODEGEN_ARM_CODEGEN_H */
+extern void dvmCompilerFlushRegWideImpl(CompilationUnit *cUnit, int rBase,
+                                        int displacement, int rSrcLo,
+                                        int rSrcHi);
diff --git a/vm/compiler/codegen/arm/GlobalOptimizations.c b/vm/compiler/codegen/arm/GlobalOptimizations.c
index 40e1f07..1cfa32b 100644
--- a/vm/compiler/codegen/arm/GlobalOptimizations.c
+++ b/vm/compiler/codegen/arm/GlobalOptimizations.c
@@ -31,7 +31,7 @@ static void applyRedundantBranchElimination(CompilationUnit *cUnit)
          thisLIR = NEXT_LIR(thisLIR)) {
 
         /* Branch to the next instruction */
-        if (thisLIR->opCode == THUMB_B_UNCOND) {
+        if (thisLIR->opCode == kThumbBUncond) {
             ArmLIR *nextLIR = thisLIR;
 
             while (true) {
@@ -48,8 +48,7 @@ static void applyRedundantBranchElimination(CompilationUnit *cUnit)
                 /*
                  * Found real useful stuff between the branch and the target
                  */
-                if (!isPseudoOpCode(nextLIR->opCode) ||
-                    nextLIR->opCode == ARM_PSEUDO_ALIGN4)
+                if (!isPseudoOpCode(nextLIR->opCode))
                     break;
             }
         }
diff --git a/vm/compiler/codegen/arm/LocalOptimizations.c b/vm/compiler/codegen/arm/LocalOptimizations.c
index 11aaedd..85dfa8e 100644
--- a/vm/compiler/codegen/arm/LocalOptimizations.c
+++ b/vm/compiler/codegen/arm/LocalOptimizations.c
@@ -17,26 +17,45 @@
 #include "Dalvik.h"
 #include "vm/compiler/CompilerInternals.h"
 #include "ArmLIR.h"
+#include "Codegen.h"
+
+#define DEBUG_OPT(X)
 
 ArmLIR* dvmCompilerGenCopy(CompilationUnit *cUnit, int rDest, int rSrc);
 
 /* Is this a Dalvik register access? */
 static inline bool isDalvikLoad(ArmLIR *lir)
 {
-    return ((lir->operands[1] == rFP) &&
-            ((lir->opCode == THUMB_LDR_RRI5) ||
-             (lir->opCode == THUMB2_LDR_RRI12) ||
-             (lir->opCode == THUMB2_VLDRS) ||
-             (lir->opCode == THUMB2_VLDRD)));
+    return (lir->useMask != ENCODE_ALL) && (lir->useMask & ENCODE_DALVIK_REG);
+}
+
+/* Is this a load from the literal pool? */
+static inline bool isLiteralLoad(ArmLIR *lir)
+{
+    return (lir->useMask != ENCODE_ALL) && (lir->useMask & ENCODE_LITERAL);
 }
 
 static inline bool isDalvikStore(ArmLIR *lir)
 {
-    return ((lir->operands[1] == rFP) &&
-            ((lir->opCode == THUMB_STR_RRI5) ||
-             (lir->opCode == THUMB2_STR_RRI12) ||
-             (lir->opCode == THUMB2_VSTRS) ||
-             (lir->opCode == THUMB2_VSTRD)));
+    return (lir->defMask != ENCODE_ALL) && (lir->defMask & ENCODE_DALVIK_REG);
+}
+
+static inline bool isDalvikRegisterClobbered(ArmLIR *lir1, ArmLIR *lir2)
+{
+  int reg1Lo = DECODE_ALIAS_INFO_REG(lir1->aliasInfo);
+  int reg1Hi = reg1Lo + DECODE_ALIAS_INFO_WIDE(lir1->aliasInfo);
+  int reg2Lo = DECODE_ALIAS_INFO_REG(lir2->aliasInfo);
+  int reg2Hi = reg2Lo + DECODE_ALIAS_INFO_WIDE(lir2->aliasInfo);
+
+  return (reg1Lo == reg2Lo) || (reg1Lo == reg2Hi) || (reg1Hi == reg2Lo);
+}
+
+static void dumpDependentInsnPair(ArmLIR *thisLIR, ArmLIR *checkLIR,
+                                  const char *optimization)
+{
+    LOGD("************ %s ************", optimization);
+    dvmDumpLIRInsn((LIR *) thisLIR, 0);
+    dvmDumpLIRInsn((LIR *) checkLIR, 0);
 }
 
 /*
@@ -59,10 +78,18 @@ static void applyLoadStoreElimination(CompilationUnit *cUnit,
             continue;
         }
         if (isDalvikStore(thisLIR)) {
-            int dRegId = thisLIR->operands[2];
+            int dRegId = DECODE_ALIAS_INFO_REG(thisLIR->aliasInfo);
+            int dRegIdHi = dRegId + DECODE_ALIAS_INFO_WIDE(thisLIR->aliasInfo);
             int nativeRegId = thisLIR->operands[0];
             ArmLIR *checkLIR;
             int sinkDistance = 0;
+            /*
+             * Add r15 (pc) to the mask to prevent this instruction
+             * from sinking past branch instructions. Unset the Dalvik register
+             * bit when checking with native resource constraints.
+             */
+            u8 stopMask = (ENCODE_REG_PC | thisLIR->useMask) &
+                          ~ENCODE_DALVIK_REG;
 
             for (checkLIR = NEXT_LIR(thisLIR);
                  checkLIR != tailLIR;
@@ -70,64 +97,52 @@ static void applyLoadStoreElimination(CompilationUnit *cUnit,
 
                 /* Check if a Dalvik register load is redundant */
                 if (isDalvikLoad(checkLIR) &&
-                    checkLIR->operands[2] == dRegId ) {
-                    if (FPREG(nativeRegId) != FPREG(checkLIR->operands[0])) {
-                        break;  // TODO: handle gen<=>float copies
-                    }
+                    (checkLIR->aliasInfo == thisLIR->aliasInfo) &&
+                    (REGTYPE(checkLIR->operands[0]) == REGTYPE(nativeRegId))) {
                     /* Insert a move to replace the load */
                     if (checkLIR->operands[0] != nativeRegId) {
                         ArmLIR *moveLIR;
-                        moveLIR = dvmCompilerRegCopy(cUnit,
-                                                    checkLIR->operands[0],
-                                                    nativeRegId);
+                        moveLIR = dvmCompilerRegCopyNoInsert(
+                                    cUnit, checkLIR->operands[0], nativeRegId);
                         /*
-                         * Insertion is guaranteed to succeed since checkLIR
-                         * is never the first LIR on the list
+                         * Insert the converted checkLIR instruction after the
+                         * the original checkLIR since the optimization is
+                         * scannng in the top-down order and the new instruction
+                         * will need to be checked.
                          */
-                        dvmCompilerInsertLIRBefore((LIR *) checkLIR,
-                                                   (LIR *) moveLIR);
+                        dvmCompilerInsertLIRAfter((LIR *) checkLIR,
+                                                  (LIR *) moveLIR);
                     }
                     checkLIR->isNop = true;
                     continue;
 
-                /* Found a true output dependency - nuke the previous store */
+                /*
+                 * Found a true output dependency - nuke the previous store.
+                 * The register type doesn't matter here.
+                 */
                 } else if (isDalvikStore(checkLIR) &&
-                           checkLIR->operands[2] == dRegId) {
+                           (checkLIR->aliasInfo == thisLIR->aliasInfo)) {
                     thisLIR->isNop = true;
                     break;
                 /* Find out the latest slot that the store can be sunk into */
                 } else {
-                    bool stopHere = false;
-
                     /* Last instruction reached */
-                    stopHere |= checkLIR->generic.next == NULL;
+                    bool stopHere = (NEXT_LIR(checkLIR) == tailLIR);
 
-                    /*
-                     * Conservatively assume there is a memory dependency
-                     * for st/ld multiples and reg+reg address mode
-                     */
-                    stopHere |= checkLIR->opCode == THUMB_STMIA ||
-                                checkLIR->opCode == THUMB_LDMIA ||
-                                checkLIR->opCode == THUMB_STR_RRR ||
-                                checkLIR->opCode == THUMB_LDR_RRR ||
-                                checkLIR->opCode == THUMB2_VLDRD ||
-                                checkLIR->opCode == THUMB2_VSTRD;
-;
+                    /* Store data is clobbered */
+                    stopHere |= ((stopMask & checkLIR->defMask) != 0);
 
-                    if (!isPseudoOpCode(checkLIR->opCode)) {
-
-                        /* Store data is clobbered */
-                        stopHere |= (EncodingMap[checkLIR->opCode].flags &
-                                     CLOBBER_DEST) != 0 &&
-                                    checkLIR->operands[0] == nativeRegId;
-
-                        stopHere |= (EncodingMap[checkLIR->opCode].flags &
-                                     IS_BRANCH) != 0;
+                    /* Store data partially clobbers the Dalvik register */
+                    if (stopHere == false &&
+                        ((checkLIR->useMask | checkLIR->defMask) &
+                         ENCODE_DALVIK_REG)) {
+                        stopHere = isDalvikRegisterClobbered(thisLIR, checkLIR);
                     }
 
                     /* Found a new place to put the store - move it here */
                     if (stopHere == true) {
-
+                        DEBUG_OPT(dumpDependentInsnPair(thisLIR, checkLIR,
+                                                        "SINK STORE"));
                         /* The store can be sunk for at least one cycle */
                         if (sinkDistance != 0) {
                             ArmLIR *newStoreLIR =
@@ -135,8 +150,9 @@ static void applyLoadStoreElimination(CompilationUnit *cUnit,
                             *newStoreLIR = *thisLIR;
                             newStoreLIR->age = cUnit->optRound;
                             /*
-                             * Insertion is guaranteed to succeed since checkLIR
-                             * is never the first LIR on the list
+                             * Stop point found - insert *before* the checkLIR
+                             * since the instruction list is scanned in the
+                             * top-down order.
                              */
                             dvmCompilerInsertLIRBefore((LIR *) checkLIR,
                                                        (LIR *) newStoreLIR);
@@ -157,10 +173,340 @@ static void applyLoadStoreElimination(CompilationUnit *cUnit,
     }
 }
 
+static void applyLoadHoisting(CompilationUnit *cUnit,
+                              ArmLIR *headLIR,
+                              ArmLIR *tailLIR)
+{
+    ArmLIR *thisLIR;
+    /*
+     * Don't want to hoist in front of first load following a barrier (or
+     * first instruction of the block.
+     */
+    bool firstLoad = true;
+    int maxHoist = dvmCompilerTargetOptHint(kMaxHoistDistance);
+
+    cUnit->optRound++;
+    for (thisLIR = headLIR;
+         thisLIR != tailLIR;
+         thisLIR = NEXT_LIR(thisLIR)) {
+        /* Skip newly added instructions */
+        if (thisLIR->age >= cUnit->optRound ||
+            thisLIR->isNop == true) {
+            continue;
+        }
+
+        if (firstLoad && (EncodingMap[thisLIR->opCode].flags & IS_LOAD)) {
+            /*
+             * Ensure nothing will be hoisted in front of this load because
+             * it's result will likely be needed soon.
+             */
+            thisLIR->defMask |= ENCODE_MEM_USE;
+            firstLoad = false;
+        }
+
+        firstLoad |= (thisLIR->defMask == ENCODE_ALL);
+
+        if (isDalvikLoad(thisLIR)) {
+            int dRegId = DECODE_ALIAS_INFO_REG(thisLIR->aliasInfo);
+            int dRegIdHi = dRegId + DECODE_ALIAS_INFO_WIDE(thisLIR->aliasInfo);
+            int nativeRegId = thisLIR->operands[0];
+            ArmLIR *checkLIR;
+            int hoistDistance = 0;
+            u8 stopUseMask = (ENCODE_REG_PC | thisLIR->useMask);
+            u8 stopDefMask = thisLIR->defMask;
+            u8 checkResult;
+
+            /* First check if the load can be completely elinimated */
+            for (checkLIR = PREV_LIR(thisLIR);
+                 checkLIR != headLIR;
+                 checkLIR = PREV_LIR(checkLIR)) {
+
+                if (checkLIR->isNop) continue;
+
+                /*
+                 * Check if the Dalvik register is previously accessed
+                 * with exactly the same type.
+                 */
+                if ((isDalvikLoad(checkLIR) || isDalvikStore(checkLIR)) &&
+                    (checkLIR->aliasInfo == thisLIR->aliasInfo) &&
+                    (checkLIR->operands[0] == nativeRegId)) {
+                    /*
+                     * If it is previously accessed but with a different type,
+                     * the search will terminate later at the point checking
+                     * for partially overlapping stores.
+                     */
+                    thisLIR->isNop = true;
+                    break;
+                }
+
+                /*
+                 * No earlier use/def can reach this load if:
+                 * 1) Head instruction is reached
+                 */
+                if (checkLIR == headLIR) {
+                    break;
+                }
+
+                checkResult = (stopUseMask | stopDefMask) & checkLIR->defMask;
+
+                /*
+                 * If both instructions are verified Dalvik accesses, clear the
+                 * may- and must-alias bits to detect true resource
+                 * dependencies.
+                 */
+                if (checkResult & ENCODE_DALVIK_REG) {
+                    checkResult &= ~(ENCODE_DALVIK_REG | ENCODE_FRAME_REF);
+                }
+
+                /*
+                 * 2) load target register is clobbered
+                 * 3) A branch is seen (stopUseMask has the PC bit set).
+                 */
+                if (checkResult) {
+                    break;
+                }
+
+                /* Store data partially clobbers the Dalvik register */
+                if (isDalvikStore(checkLIR) &&
+                    isDalvikRegisterClobbered(thisLIR, checkLIR)) {
+                    break;
+                }
+            }
+
+            /* The load has been eliminated */
+            if (thisLIR->isNop) continue;
+
+            /*
+             * The load cannot be eliminated. See if it can be hoisted to an
+             * earlier spot.
+             */
+            for (checkLIR = PREV_LIR(thisLIR);
+                 /* empty by intention */;
+                 checkLIR = PREV_LIR(checkLIR)) {
+
+                if (checkLIR->isNop) continue;
+
+                /*
+                 * Check if the "thisLIR" load is redundant
+                 * NOTE: At one point, we also triggered if the checkLIR
+                 * instruction was a load.  However, that tended to insert
+                 * a load/use dependency because the full scheduler is
+                 * not yet complete.  When it is, we chould also trigger
+                 * on loads.
+                 */
+                if (isDalvikStore(checkLIR) &&
+                    (checkLIR->aliasInfo == thisLIR->aliasInfo) &&
+                    (REGTYPE(checkLIR->operands[0]) == REGTYPE(nativeRegId))) {
+                    /* Insert a move to replace the load */
+                    if (checkLIR->operands[0] != nativeRegId) {
+                        ArmLIR *moveLIR;
+                        moveLIR = dvmCompilerRegCopyNoInsert(
+                                    cUnit, nativeRegId, checkLIR->operands[0]);
+                        /*
+                         * Convert *thisLIR* load into a move
+                         */
+                        dvmCompilerInsertLIRAfter((LIR *) checkLIR,
+                                                  (LIR *) moveLIR);
+                    }
+                    thisLIR->isNop = true;
+                    break;
+
+                /* Find out if the load can be yanked past the checkLIR */
+                } else {
+                    /* Last instruction reached */
+                    bool stopHere = (checkLIR == headLIR);
+
+                    /* Base address is clobbered by checkLIR */
+                    checkResult = stopUseMask & checkLIR->defMask;
+                    if (checkResult & ENCODE_DALVIK_REG) {
+                        checkResult &= ~(ENCODE_DALVIK_REG | ENCODE_FRAME_REF);
+                    }
+                    stopHere |= (checkResult != 0);
+
+                    /* Load target clobbers use/def in checkLIR */
+                    checkResult = stopDefMask &
+                                  (checkLIR->useMask | checkLIR->defMask);
+                    if (checkResult & ENCODE_DALVIK_REG) {
+                        checkResult &= ~(ENCODE_DALVIK_REG | ENCODE_FRAME_REF);
+                    }
+                    stopHere |= (checkResult != 0);
+
+                    /* Store data partially clobbers the Dalvik register */
+                    if (stopHere == false &&
+                        (checkLIR->defMask & ENCODE_DALVIK_REG)) {
+                        stopHere = isDalvikRegisterClobbered(thisLIR, checkLIR);
+                    }
+
+                    /*
+                     * Stop at an earlier Dalvik load if the offset of checkLIR
+                     * is not less than thisLIR
+                     *
+                     * Experiments show that doing
+                     *
+                     * ldr     r1, [r5, #16]
+                     * ldr     r0, [r5, #20]
+                     *
+                     * is much faster than
+                     *
+                     * ldr     r0, [r5, #20]
+                     * ldr     r1, [r5, #16]
+                     */
+                    if (isDalvikLoad(checkLIR)) {
+                        int dRegId2 =
+                            DECODE_ALIAS_INFO_REG(checkLIR->aliasInfo);
+                        if (dRegId2 <= dRegId) {
+                            stopHere = true;
+                        }
+                    }
+
+                    /* Don't go too far */
+                    stopHere |= (hoistDistance >= maxHoist);
+
+                    /* Found a new place to put the load - move it here */
+                    if (stopHere == true) {
+                        DEBUG_OPT(dumpDependentInsnPair(thisLIR, checkLIR,
+                                                        "HOIST LOAD"));
+                        /* The load can be hoisted for at least one cycle */
+                        if (hoistDistance != 0) {
+                            ArmLIR *newLoadLIR =
+                                dvmCompilerNew(sizeof(ArmLIR), true);
+                            *newLoadLIR = *thisLIR;
+                            newLoadLIR->age = cUnit->optRound;
+                            /*
+                             * Stop point found - insert *after* the checkLIR
+                             * since the instruction list is scanned in the
+                             * bottom-up order.
+                             */
+                            dvmCompilerInsertLIRAfter((LIR *) checkLIR,
+                                                      (LIR *) newLoadLIR);
+                            thisLIR->isNop = true;
+                        }
+                        break;
+                    }
+
+                    /*
+                     * Saw a real instruction that hosting the load is
+                     * beneficial
+                     */
+                    if (!isPseudoOpCode(checkLIR->opCode)) {
+                        hoistDistance++;
+                    }
+                }
+            }
+        } else if (isLiteralLoad(thisLIR)) {
+            int litVal = thisLIR->aliasInfo;
+            int nativeRegId = thisLIR->operands[0];
+            ArmLIR *checkLIR;
+            int hoistDistance = 0;
+            u8 stopUseMask = (ENCODE_REG_PC | thisLIR->useMask) &
+                             ~ENCODE_LITPOOL_REF;
+            u8 stopDefMask = thisLIR->defMask & ~ENCODE_LITPOOL_REF;
+
+            /* First check if the load can be completely elinimated */
+            for (checkLIR = PREV_LIR(thisLIR);
+                 checkLIR != headLIR;
+                 checkLIR = PREV_LIR(checkLIR)) {
+
+                if (checkLIR->isNop) continue;
+
+                /* Reloading same literal into same tgt reg? Eliminate if so */
+                if (isLiteralLoad(checkLIR) &&
+                    (checkLIR->aliasInfo == litVal) &&
+                    (checkLIR->operands[0] == nativeRegId)) {
+                    thisLIR->isNop = true;
+                    break;
+                }
+
+                /*
+                 * No earlier use/def can reach this load if:
+                 * 1) Head instruction is reached
+                 * 2) load target register is clobbered
+                 * 3) A branch is seen (stopUseMask has the PC bit set).
+                 */
+                if ((checkLIR == headLIR) ||
+                    (stopUseMask | stopDefMask) & checkLIR->defMask) {
+                    break;
+                }
+            }
+
+            /* The load has been eliminated */
+            if (thisLIR->isNop) continue;
+
+            /*
+             * The load cannot be eliminated. See if it can be hoisted to an
+             * earlier spot.
+             */
+            for (checkLIR = PREV_LIR(thisLIR);
+                 /* empty by intention */;
+                 checkLIR = PREV_LIR(checkLIR)) {
+
+                if (checkLIR->isNop) continue;
+
+                /*
+                 * TUNING: once a full scheduler exists, check here
+                 * for conversion of a redundant load into a copy similar
+                 * to the way redundant loads are handled above.
+                 */
+
+                /* Find out if the load can be yanked past the checkLIR */
+
+                /* Last instruction reached */
+                bool stopHere = (checkLIR == headLIR);
+
+                /* Base address is clobbered by checkLIR */
+                stopHere |= ((stopUseMask & checkLIR->defMask) != 0);
+
+                /* Load target clobbers use/def in checkLIR */
+                stopHere |= ((stopDefMask &
+                             (checkLIR->useMask | checkLIR->defMask)) != 0);
+
+                /* Avoid re-ordering literal pool loads */
+                stopHere |= isLiteralLoad(checkLIR);
+
+                /* Don't go too far */
+                stopHere |= (hoistDistance >= maxHoist);
+
+                /* Found a new place to put the load - move it here */
+                if (stopHere == true) {
+                    DEBUG_OPT(dumpDependentInsnPair(thisLIR, checkLIR,
+                                                    "HOIST LOAD"));
+                    /* The store can be hoisted for at least one cycle */
+                    if (hoistDistance != 0) {
+                        ArmLIR *newLoadLIR =
+                            dvmCompilerNew(sizeof(ArmLIR), true);
+                        *newLoadLIR = *thisLIR;
+                        newLoadLIR->age = cUnit->optRound;
+                        /*
+                         * Insertion is guaranteed to succeed since checkLIR
+                         * is never the first LIR on the list
+                         */
+                        dvmCompilerInsertLIRAfter((LIR *) checkLIR,
+                                                  (LIR *) newLoadLIR);
+                        thisLIR->isNop = true;
+                    }
+                    break;
+                }
+
+                /*
+                 * Saw a real instruction that hosting the load is
+                 * beneficial
+                 */
+                if (!isPseudoOpCode(checkLIR->opCode)) {
+                    hoistDistance++;
+                }
+            }
+        }
+    }
+}
+
 void dvmCompilerApplyLocalOptimizations(CompilationUnit *cUnit, LIR *headLIR,
                                         LIR *tailLIR)
 {
-    applyLoadStoreElimination(cUnit,
-                              (ArmLIR *) headLIR,
-                              (ArmLIR *) tailLIR);
+    if (!(gDvmJit.disableOpt & (1 << kLoadStoreElimination))) {
+        applyLoadStoreElimination(cUnit, (ArmLIR *) headLIR,
+                                  (ArmLIR *) tailLIR);
+    }
+    if (!(gDvmJit.disableOpt & (1 << kLoadHoisting))) {
+        applyLoadHoisting(cUnit, (ArmLIR *) headLIR, (ArmLIR *) tailLIR);
+    }
 }
diff --git a/vm/compiler/codegen/arm/Thumb2Util.c b/vm/compiler/codegen/arm/Thumb2Util.c
deleted file mode 100644
index 3a9f1de..0000000
--- a/vm/compiler/codegen/arm/Thumb2Util.c
+++ /dev/null
@@ -1,488 +0,0 @@
-/*
- * Copyright (C) 2009 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
- * This file contains codegen for the Thumb ISA and is intended to be
- * includes by:and support common to all supported
- *
- *        Codegen-$(TARGET_ARCH_VARIANT).c
- *
- */
-
-#include "Codegen.h"
-
-/* Routines which must be supplied here */
-static void loadConstant(CompilationUnit *cUnit, int rDest, int value);
-static void genExportPC(CompilationUnit *cUnit, MIR *mir, int rDPC, int rAddr);
-static void genConditionalBranch(CompilationUnit *cUnit,
-                                 ArmConditionCode cond,
-                                 ArmLIR *target);
-static ArmLIR *genUnconditionalBranch(CompilationUnit *cUnit, ArmLIR *target);
-static void loadValuePair(CompilationUnit *cUnit, int vSrc, int rDestLo,
-                          int rDestHi);
-static void storeValuePair(CompilationUnit *cUnit, int rSrcLo, int rSrcHi,
-                           int vDest, int rScratch);
-static void loadValueAddress(CompilationUnit *cUnit, int vSrc, int vDest);
-static void loadValue(CompilationUnit *cUnit, int vSrc, int rDest);
-static void loadWordDisp(CompilationUnit *cUnit, int rBase, int displacement,
-                         int rDest);
-static void storeValue(CompilationUnit *cUnit, int rSrc, int vDest,
-                       int rScratch);
-static inline ArmLIR *genRegImmCheck(CompilationUnit *cUnit,
-                                         ArmConditionCode cond, int reg,
-                                         int checkValue, int dOffset,
-                                         ArmLIR *pcrLabel);
-ArmLIR* dvmCompilerRegCopy(CompilationUnit *cUnit, int rDest, int rSrc);
-
-/*****************************************************************************/
-
-/*
- * Support for register allocation
- */
-
-/* non-existent register */
-#define vNone   (-1)
-
-/* get the next register in r0..r3 in a round-robin fashion */
-#define NEXT_REG(reg) ((reg + 1) & 3)
-/*
- * The following are utility routines to help maintain the RegisterScoreboard
- * state to facilitate register renaming.
- */
-
-/* Reset the tracker to unknown state */
-static inline void resetRegisterScoreboard(CompilationUnit *cUnit)
-{
-    RegisterScoreboard *registerScoreboard = &cUnit->registerScoreboard;
-
-    dvmClearAllBits(registerScoreboard->nullCheckedRegs);
-    registerScoreboard->liveDalvikReg = vNone;
-    registerScoreboard->nativeReg = vNone;
-    registerScoreboard->nativeRegHi = vNone;
-}
-
-/* Kill the corresponding bit in the null-checked register list */
-static inline void killNullCheckedRegister(CompilationUnit *cUnit, int vReg)
-{
-    dvmClearBit(cUnit->registerScoreboard.nullCheckedRegs, vReg);
-}
-
-/* The Dalvik register pair held in native registers have changed */
-static inline void updateLiveRegisterPair(CompilationUnit *cUnit,
-                                          int vReg, int mRegLo, int mRegHi)
-{
-    cUnit->registerScoreboard.liveDalvikReg = vReg;
-    cUnit->registerScoreboard.nativeReg = mRegLo;
-    cUnit->registerScoreboard.nativeRegHi = mRegHi;
-    cUnit->registerScoreboard.isWide = true;
-}
-
-/* The Dalvik register held in a native register has changed */
-static inline void updateLiveRegister(CompilationUnit *cUnit,
-                                      int vReg, int mReg)
-{
-    cUnit->registerScoreboard.liveDalvikReg = vReg;
-    cUnit->registerScoreboard.nativeReg = mReg;
-    cUnit->registerScoreboard.isWide = false;
-}
-
-/*
- * Given a Dalvik register id vSrc, use a very simple algorithm to increase
- * the lifetime of cached Dalvik value in a native register.
- */
-static inline int selectFirstRegister(CompilationUnit *cUnit, int vSrc,
-                                      bool isWide)
-{
-    RegisterScoreboard *registerScoreboard = &cUnit->registerScoreboard;
-
-    /* No live value - suggest to use r0 */
-    if (registerScoreboard->liveDalvikReg == vNone)
-        return r0;
-
-    /* Reuse the previously used native reg */
-    if (registerScoreboard->liveDalvikReg == vSrc) {
-        if (isWide != true) {
-            return registerScoreboard->nativeReg;
-        } else {
-            /* Return either r0 or r2 */
-            return (registerScoreboard->nativeReg + 1) & 2;
-        }
-    }
-
-    /* No reuse - choose the next one among r0..r3 in the round-robin fashion */
-    if (isWide) {
-        return (registerScoreboard->nativeReg + 2) & 2;
-    } else {
-        return (registerScoreboard->nativeReg + 1) & 3;
-    }
-
-}
-
-/*****************************************************************************/
-
-ArmLIR* dvmCompilerRegCopy(CompilationUnit *cUnit, int rDest, int rSrc)
-{
-    ArmLIR* res = dvmCompilerNew(sizeof(ArmLIR), true);
-    res->operands[0] = rDest;
-    res->operands[1] = rSrc;
-    if (rDest == rSrc) {
-        res->isNop = true;
-    } else {
-        if (LOWREG(rDest) && LOWREG(rSrc)) {
-            res->opCode = THUMB_MOV_RR;
-        } else if (FPREG(rDest) && FPREG(rSrc)) {
-            if (DOUBLEREG(rDest)) {
-                assert(DOUBLEREG(rSrc));
-                res->opCode = THUMB2_VMOVD;
-            } else {
-                assert(SINGLEREG(rSrc));
-                res->opCode = THUMB2_VMOVS;
-            }
-        } else {
-            // TODO: support copy between FP and gen regs.
-            assert(!FPREG(rDest));
-            assert(!FPREG(rSrc));
-            res->opCode = THUMB2_MOV_RR;
-        }
-    }
-    return res;
-}
-
-static int leadingZeros(u4 val)
-{
-    u4 alt;
-    int n;
-    int count;
-
-    count = 16;
-    n = 32;
-    do {
-        alt = val >> count;
-        if (alt != 0) {
-            n = n - count;
-            val = alt;
-        }
-        count >>= 1;
-    } while (count);
-    return n - val;
-}
-
-/*
- * Determine whether value can be encoded as a Thumb modified
- * immediate.  If not, return -1.  If so, return i:imm3:a:bcdefgh form.
- */
-static int modifiedImmediate(u4 value)
-{
-   int zLeading;
-   int zTrailing;
-   u4 b0 = value & 0xff;
-
-   /* Note: case of value==0 must use 0:000:0:0000000 encoding */
-   if (value <= 0xFF)
-       return b0;  // 0:000:a:bcdefgh
-   if (value == ((b0 << 16) | b0))
-       return (0x1 << 8) | b0; /* 0:001:a:bcdefgh */
-   if (value == ((b0 << 24) | (b0 << 16) | (b0 << 8) | b0))
-       return (0x3 << 8) | b0; /* 0:011:a:bcdefgh */
-   b0 = (value >> 8) & 0xff;
-   if (value == ((b0 << 24) | (b0 << 8)))
-       return (0x2 << 8) | b0; /* 0:010:a:bcdefgh */
-   /* Can we do it with rotation? */
-   zLeading = leadingZeros(value);
-   zTrailing = 32 - leadingZeros(~value & (value - 1));
-   /* A run of eight or fewer active bits? */
-   if ((zLeading + zTrailing) < 24)
-       return -1;  /* No - bail */
-   /* left-justify the constant, discarding msb (known to be 1) */
-   value <<= zLeading + 1;
-   /* Create bcdefgh */
-   value >>= 25;
-   /* Put it all together */
-   return value | ((0x8 + zLeading) << 7); /* [01000..11111]:bcdefgh */
-}
-
-/*
- * Load a immediate using a shortcut if possible; otherwise
- * grab from the per-translation literal pool
- */
-static void loadConstant(CompilationUnit *cUnit, int rDest, int value)
-{
-    int modImm;
-    /* See if the value can be constructed cheaply */
-    if ((value & 0xff) == value) {
-        newLIR2(cUnit, THUMB_MOV_IMM, rDest, value);
-        return;
-    } else if ((value & 0xFFFFFF00) == 0xFFFFFF00) {
-        newLIR2(cUnit, THUMB_MOV_IMM, rDest, ~value);
-        newLIR2(cUnit, THUMB_MVN, rDest, rDest);
-        return;
-    }
-    /* Check Modified immediate special cases */
-    modImm = modifiedImmediate(value);
-    if (modImm >= 0) {
-        newLIR2(cUnit, THUMB2_MOV_IMM_SHIFT, rDest, modImm);
-        return;
-    }
-    /* 16-bit immediate? */
-    if ((value & 0xffff) == value) {
-        newLIR2(cUnit, THUMB2_MOV_IMM16, rDest, value);
-        return;
-    }
-    /* No shortcut - go ahead and use literal pool */
-    ArmLIR *dataTarget = scanLiteralPool(cUnit, value, 255);
-    if (dataTarget == NULL) {
-        dataTarget = addWordData(cUnit, value, false);
-    }
-    ArmLIR *loadPcRel = dvmCompilerNew(sizeof(ArmLIR), true);
-    loadPcRel->opCode = THUMB_LDR_PC_REL;
-    loadPcRel->generic.target = (LIR *) dataTarget;
-    loadPcRel->operands[0] = rDest;
-    dvmCompilerAppendLIR(cUnit, (LIR *) loadPcRel);
-
-    /*
-     * To save space in the constant pool, we use the ADD_RRI8 instruction to
-     * add up to 255 to an existing constant value.
-     */
-    if (dataTarget->operands[0] != value) {
-        newLIR2(cUnit, THUMB_ADD_RI8, rDest, value - dataTarget->operands[0]);
-    }
-}
-
-/* Export the Dalvik PC assicated with an instruction to the StackSave area */
-static void genExportPC(CompilationUnit *cUnit, MIR *mir, int rDPC, int rAddr)
-{
-    int offset = offsetof(StackSaveArea, xtra.currentPc);
-    loadConstant(cUnit, rDPC, (int) (cUnit->method->insns + mir->offset));
-    newLIR3(cUnit, THUMB2_STR_RRI8_PREDEC, rDPC, rFP,
-            sizeof(StackSaveArea) - offset);
-}
-
-/* Generate conditional branch instructions */
-static void genConditionalBranch(CompilationUnit *cUnit,
-                                 ArmConditionCode cond,
-                                 ArmLIR *target)
-{
-    ArmLIR *branch = newLIR2(cUnit, THUMB_B_COND, 0, cond);
-    branch->generic.target = (LIR *) target;
-}
-
-/* Generate unconditional branch instructions */
-static ArmLIR *genUnconditionalBranch(CompilationUnit *cUnit, ArmLIR *target)
-{
-    ArmLIR *branch = newLIR0(cUnit, THUMB_B_UNCOND);
-    branch->generic.target = (LIR *) target;
-    return branch;
-}
-
-/*
- * Load a pair of values of rFP[src..src+1] and store them into rDestLo and
- * rDestHi
- */
-static void loadValuePair(CompilationUnit *cUnit, int vSrc, int rDestLo,
-                          int rDestHi)
-{
-    bool allLowRegs = (LOWREG(rDestLo) && LOWREG(rDestHi));
-
-    /* Use reg + imm5*4 to load the values if possible */
-    if (allLowRegs && vSrc <= 30) {
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDestLo, rFP, vSrc);
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDestHi, rFP, vSrc+1);
-    } else {
-        assert(rDestLo < rDestHi);
-        loadValueAddress(cUnit, vSrc, rDestLo);
-        if (allLowRegs) {
-            newLIR2(cUnit, THUMB_LDMIA, rDestLo, (1<<rDestLo) | (1<<(rDestHi)));
-        } else {
-            assert(0); // Unimp - need Thumb2 ldmia
-        }
-    }
-}
-
-/*
- * Store a pair of values of rSrc and rSrc+1 and store them into vDest and
- * vDest+1
- */
-static void storeValuePair(CompilationUnit *cUnit, int rSrcLo, int rSrcHi,
-                           int vDest, int rScratch)
-{
-    bool allLowRegs = (LOWREG(rSrcLo) && LOWREG(rSrcHi));
-    killNullCheckedRegister(cUnit, vDest);
-    killNullCheckedRegister(cUnit, vDest+1);
-    updateLiveRegisterPair(cUnit, vDest, rSrcLo, rSrcHi);
-
-    /* Use reg + imm5*4 to store the values if possible */
-    if (allLowRegs && vDest <= 30) {
-        newLIR3(cUnit, THUMB_STR_RRI5, rSrcLo, rFP, vDest);
-        newLIR3(cUnit, THUMB_STR_RRI5, rSrcHi, rFP, vDest+1);
-    } else {
-        assert(rSrcLo < rSrcHi);
-        loadValueAddress(cUnit, vDest, rScratch);
-        if (allLowRegs) {
-            newLIR2(cUnit, THUMB_STMIA, rScratch,
-                    (1<<rSrcLo) | (1 << (rSrcHi)));
-        } else {
-            assert(0); // Unimp - need Thumb2 stmia
-        }
-    }
-}
-
-static void addRegisterRegister(CompilationUnit *cUnit, int rDest,
-                                int rSrc1, int rSrc2)
-{
-    if (!LOWREG(rDest) || !LOWREG(rSrc1) || !LOWREG(rSrc2)) {
-        assert(0); // Unimp
-        //newLIR3(cUnit, THUMB2_ADD_RRR, rDest, rFP, rDest);
-    } else {
-        newLIR3(cUnit, THUMB_ADD_RRR, rDest, rFP, rDest);
-    }
-}
-
-/* Add in immediate to a register. */
-static void addRegisterImmediate(CompilationUnit *cUnit, int rDest, int rSrc,
-                                 int value)
-{
-// TODO: check for modified immediate form
-    if (LOWREG(rDest) && LOWREG(rSrc) && (value <= 7)) {
-        newLIR3(cUnit, THUMB_ADD_RRI3, rDest, rSrc, value);
-    } else if (LOWREG(rDest) && (rDest == rSrc) && ((value & 0xff) == 0xff)) {
-        newLIR2(cUnit, THUMB_ADD_RI8, rDest, value);
-    } else if (value <= 4095) {
-        newLIR3(cUnit, THUMB2_ADD_RRI12, rDest, rSrc, value);
-    } else {
-        loadConstant(cUnit, rDest, value);
-        addRegisterRegister(cUnit, rDest, rDest, rFP);
-    }
-}
-
-/* Load the address of a Dalvik register on the frame */
-static void loadValueAddress(CompilationUnit *cUnit, int vSrc, int rDest)
-{
-    addRegisterImmediate(cUnit, rDest, rFP, vSrc*4);
-}
-
-/*
- * FIXME: We need a general register temp for all of these coprocessor
- * operations in case we can't reach in 1 shot.  Might just want to
- * designate a hot temp that all codegen routines could use in their
- * scope.  Alternately, callers will need to allocate a temp and
- * pass it in to each of these.
- */
-
-/* Load a float from a Dalvik register */
-static void loadFloat(CompilationUnit *cUnit, int vSrc, int rDest)
-{
-    assert(vSrc <= 255); // FIXME - temp limit to 1st 256
-    assert(SINGLEREG(rDest));
-    newLIR3(cUnit, THUMB2_VLDRS, rDest, rFP, vSrc);
-}
-
-/* Store a float to a Dalvik register */
-static void storeFloat(CompilationUnit *cUnit, int rSrc, int vDest,
-                       int rScratch)
-{
-    assert(vSrc <= 255); // FIXME - temp limit to 1st 256
-    assert(SINGLEREG(rSrc));
-    newLIR3(cUnit, THUMB2_VSTRS, rSrc, rFP, vDest);
-}
-
-/* Load a double from a Dalvik register */
-static void loadDouble(CompilationUnit *cUnit, int vSrc, int rDest)
-{
-    assert(vSrc <= 255); // FIXME - temp limit to 1st 256
-    assert(DOUBLEREG(rDest));
-    newLIR3(cUnit, THUMB2_VLDRD, rDest, rFP, vSrc);
-}
-
-/* Store a double to a Dalvik register */
-static void storeDouble(CompilationUnit *cUnit, int rSrc, int vDest,
-                       int rScratch)
-{
-    assert(vSrc <= 255); // FIXME - temp limit to 1st 256
-    assert(DOUBLEREG(rSrc));
-    newLIR3(cUnit, THUMB2_VSTRD, rSrc, rFP, vDest);
-}
-
-
-/* Load a single value from rFP[src] and store them into rDest */
-static void loadValue(CompilationUnit *cUnit, int vSrc, int rDest)
-{
-    loadWordDisp(cUnit, rFP, vSrc * 4, rDest);
-}
-
-/* Load a word at base + displacement.  Displacement must be word multiple */
-static void loadWordDisp(CompilationUnit *cUnit, int rBase, int displacement,
-                         int rDest)
-{
-    bool allLowRegs = (LOWREG(rBase) && LOWREG(rDest));
-    assert((displacement & 0x3) == 0);
-    /* Can it fit in a RRI5? */
-    if (allLowRegs && displacement < 128) {
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDest, rBase, displacement >> 2);
-    } else if (displacement < 4092) {
-        newLIR3(cUnit, THUMB2_LDR_RRI12, rDest, rFP, displacement);
-    } else {
-        loadConstant(cUnit, rDest, displacement);
-        if (allLowRegs) {
-            newLIR3(cUnit, THUMB_LDR_RRR, rDest, rBase, rDest);
-        } else {
-            assert(0); // Unimp - need Thumb2 ldr_rrr
-        }
-    }
-}
-
-/* Store a value from rSrc to vDest */
-static void storeValue(CompilationUnit *cUnit, int rSrc, int vDest,
-                       int rScratch)
-{
-    killNullCheckedRegister(cUnit, vDest);
-    updateLiveRegister(cUnit, vDest, rSrc);
-
-    /* Use reg + imm5*4 to store the value if possible */
-    if (LOWREG(rSrc) && vDest <= 31) {
-        newLIR3(cUnit, THUMB_STR_RRI5, rSrc, rFP, vDest);
-    } else if (vDest <= 1023) {
-        newLIR3(cUnit, THUMB2_STR_RRI12, rSrc, rFP, vDest*4);
-    } else {
-        loadConstant(cUnit, rScratch, vDest*4);
-        if (LOWREG(rSrc)) {
-            newLIR3(cUnit, THUMB_STR_RRR, rSrc, rFP, rScratch);
-        } else {
-            assert(0); // Unimp: Need generic str_rrr routine
-        }
-    }
-}
-
-/*
- * Perform a "reg cmp imm" operation and jump to the PCR region if condition
- * satisfies.
- */
-static ArmLIR *genRegImmCheck(CompilationUnit *cUnit,
-                                         ArmConditionCode cond, int reg,
-                                         int checkValue, int dOffset,
-                                         ArmLIR *pcrLabel)
-{
-    ArmLIR *branch;
-    if ((LOWREG(reg)) && (checkValue == 0) &&
-       ((cond == ARM_COND_EQ) || (cond == ARM_COND_NE))) {
-        branch = newLIR2(cUnit,
-                         (cond == ARM_COND_EQ) ? THUMB2_CBZ : THUMB2_CBNZ,
-                         reg, 0);
-    } else {
-        newLIR2(cUnit, THUMB_CMP_RI8, reg, checkValue);
-        branch = newLIR2(cUnit, THUMB_B_COND, 0, cond);
-    }
-    return genCheckCommon(cUnit, dOffset, branch, pcrLabel);
-}
diff --git a/vm/compiler/codegen/arm/ThumbUtil.c b/vm/compiler/codegen/arm/ThumbUtil.c
deleted file mode 100644
index 8be50ad..0000000
--- a/vm/compiler/codegen/arm/ThumbUtil.c
+++ /dev/null
@@ -1,337 +0,0 @@
-/*
- * Copyright (C) 2009 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*
- * This file contains codegen for the Thumb ISA and is intended to be
- * includes by:and support common to all supported
- *
- *        Codegen-$(TARGET_ARCH_VARIANT).c
- *
- */
-
-#include "Codegen.h"
-
-/* Routines which must be supplied here */
-static void loadConstant(CompilationUnit *cUnit, int rDest, int value);
-static void genExportPC(CompilationUnit *cUnit, MIR *mir, int rDPC, int rAddr);
-static void genConditionalBranch(CompilationUnit *cUnit,
-                                 ArmConditionCode cond,
-                                 ArmLIR *target);
-static ArmLIR *genUnconditionalBranch(CompilationUnit *cUnit, ArmLIR *target);
-static void loadValuePair(CompilationUnit *cUnit, int vSrc, int rDestLo,
-                          int rDestHi);
-static void storeValuePair(CompilationUnit *cUnit, int rSrcLo, int rSrcHi,
-                           int vDest, int rScratch);
-static void loadValueAddress(CompilationUnit *cUnit, int vSrc, int vDest);
-static void loadValue(CompilationUnit *cUnit, int vSrc, int rDest);
-static void loadWordDisp(CompilationUnit *cUnit, int rBase, int displacement,
-                         int rDest);
-static void storeValue(CompilationUnit *cUnit, int rSrc, int vDest,
-                       int rScratch);
-static inline ArmLIR *genRegImmCheck(CompilationUnit *cUnit,
-                                         ArmConditionCode cond, int reg,
-                                         int checkValue, int dOffset,
-                                         ArmLIR *pcrLabel);
-ArmLIR* dvmCompilerRegCopy(CompilationUnit *cUnit, int rDest, int rSrc);
-
-/*****************************************************************************/
-
-/*
- * Support for register allocation
- */
-
-/* non-existent register */
-#define vNone   (-1)
-
-/* get the next register in r0..r3 in a round-robin fashion */
-#define NEXT_REG(reg) ((reg + 1) & 3)
-/*
- * The following are utility routines to help maintain the RegisterScoreboard
- * state to facilitate register renaming.
- */
-
-/* Reset the tracker to unknown state */
-static inline void resetRegisterScoreboard(CompilationUnit *cUnit)
-{
-    RegisterScoreboard *registerScoreboard = &cUnit->registerScoreboard;
-
-    dvmClearAllBits(registerScoreboard->nullCheckedRegs);
-    registerScoreboard->liveDalvikReg = vNone;
-    registerScoreboard->nativeReg = vNone;
-    registerScoreboard->nativeRegHi = vNone;
-}
-
-/* Kill the corresponding bit in the null-checked register list */
-static inline void killNullCheckedRegister(CompilationUnit *cUnit, int vReg)
-{
-    dvmClearBit(cUnit->registerScoreboard.nullCheckedRegs, vReg);
-}
-
-/* The Dalvik register pair held in native registers have changed */
-static inline void updateLiveRegisterPair(CompilationUnit *cUnit,
-                                          int vReg, int mRegLo, int mRegHi)
-{
-    cUnit->registerScoreboard.liveDalvikReg = vReg;
-    cUnit->registerScoreboard.nativeReg = mRegLo;
-    cUnit->registerScoreboard.nativeRegHi = mRegHi;
-    cUnit->registerScoreboard.isWide = true;
-}
-
-/* The Dalvik register held in a native register has changed */
-static inline void updateLiveRegister(CompilationUnit *cUnit,
-                                      int vReg, int mReg)
-{
-    cUnit->registerScoreboard.liveDalvikReg = vReg;
-    cUnit->registerScoreboard.nativeReg = mReg;
-    cUnit->registerScoreboard.isWide = false;
-}
-
-/*
- * Given a Dalvik register id vSrc, use a very simple algorithm to increase
- * the lifetime of cached Dalvik value in a native register.
- */
-static inline int selectFirstRegister(CompilationUnit *cUnit, int vSrc,
-                                      bool isWide)
-{
-    RegisterScoreboard *registerScoreboard = &cUnit->registerScoreboard;
-
-    /* No live value - suggest to use r0 */
-    if (registerScoreboard->liveDalvikReg == vNone)
-        return r0;
-
-    /* Reuse the previously used native reg */
-    if (registerScoreboard->liveDalvikReg == vSrc) {
-        if (isWide != true) {
-            return registerScoreboard->nativeReg;
-        } else {
-            /* Return either r0 or r2 */
-            return (registerScoreboard->nativeReg + 1) & 2;
-        }
-    }
-
-    /* No reuse - choose the next one among r0..r3 in the round-robin fashion */
-    if (isWide) {
-        return (registerScoreboard->nativeReg + 2) & 2;
-    } else {
-        return (registerScoreboard->nativeReg + 1) & 3;
-    }
-
-}
-
-/*****************************************************************************/
-
-ArmLIR* dvmCompilerRegCopy(CompilationUnit *cUnit, int rDest, int rSrc)
-{
-    ArmLIR* res = dvmCompilerNew(sizeof(ArmLIR), true);
-    assert(LOWREG(rDest) && LOWREG(rSrc));
-    res->operands[0] = rDest;
-    res->operands[1] = rSrc;
-    res->opCode = THUMB_MOV_RR;
-    if (rDest == rSrc) {
-        res->isNop = true;
-    }
-    return res;
-}
-
-/*
- * Load a immediate using a shortcut if possible; otherwise
- * grab from the per-translation literal pool
- */
-static void loadConstant(CompilationUnit *cUnit, int rDest, int value)
-{
-    /* See if the value can be constructed cheaply */
-    if ((value >= 0) && (value <= 255)) {
-        newLIR2(cUnit, THUMB_MOV_IMM, rDest, value);
-        return;
-    } else if ((value & 0xFFFFFF00) == 0xFFFFFF00) {
-        newLIR2(cUnit, THUMB_MOV_IMM, rDest, ~value);
-        newLIR2(cUnit, THUMB_MVN, rDest, rDest);
-        return;
-    }
-    /* No shortcut - go ahead and use literal pool */
-    ArmLIR *dataTarget = scanLiteralPool(cUnit, value, 255);
-    if (dataTarget == NULL) {
-        dataTarget = addWordData(cUnit, value, false);
-    }
-    ArmLIR *loadPcRel = dvmCompilerNew(sizeof(ArmLIR), true);
-    loadPcRel->opCode = THUMB_LDR_PC_REL;
-    loadPcRel->generic.target = (LIR *) dataTarget;
-    loadPcRel->operands[0] = rDest;
-    dvmCompilerAppendLIR(cUnit, (LIR *) loadPcRel);
-
-    /*
-     * To save space in the constant pool, we use the ADD_RRI8 instruction to
-     * add up to 255 to an existing constant value.
-     */
-    if (dataTarget->operands[0] != value) {
-        newLIR2(cUnit, THUMB_ADD_RI8, rDest, value - dataTarget->operands[0]);
-    }
-}
-
-/* Export the Dalvik PC assicated with an instruction to the StackSave area */
-static void genExportPC(CompilationUnit *cUnit, MIR *mir, int rDPC, int rAddr)
-{
-    int offset = offsetof(StackSaveArea, xtra.currentPc);
-    loadConstant(cUnit, rDPC, (int) (cUnit->method->insns + mir->offset));
-    newLIR2(cUnit, THUMB_MOV_RR, rAddr, rFP);
-    newLIR2(cUnit, THUMB_SUB_RI8, rAddr, sizeof(StackSaveArea) - offset);
-    newLIR3(cUnit, THUMB_STR_RRI5, rDPC, rAddr, 0);
-}
-
-/* Generate conditional branch instructions */
-static void genConditionalBranch(CompilationUnit *cUnit,
-                                 ArmConditionCode cond,
-                                 ArmLIR *target)
-{
-    ArmLIR *branch = newLIR2(cUnit, THUMB_B_COND, 0, cond);
-    branch->generic.target = (LIR *) target;
-}
-
-/* Generate unconditional branch instructions */
-static ArmLIR *genUnconditionalBranch(CompilationUnit *cUnit, ArmLIR *target)
-{
-    ArmLIR *branch = newLIR0(cUnit, THUMB_B_UNCOND);
-    branch->generic.target = (LIR *) target;
-    return branch;
-}
-
-/*
- * Load a pair of values of rFP[src..src+1] and store them into rDestLo and
- * rDestHi
- */
-static void loadValuePair(CompilationUnit *cUnit, int vSrc, int rDestLo,
-                          int rDestHi)
-{
-    /* Use reg + imm5*4 to load the values if possible */
-    if (vSrc <= 30) {
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDestLo, rFP, vSrc);
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDestHi, rFP, vSrc+1);
-    } else {
-        if (vSrc <= 64) {
-            /* Sneak 4 into the base address first */
-            newLIR3(cUnit, THUMB_ADD_RRI3, rDestLo, rFP, 4);
-            newLIR2(cUnit, THUMB_ADD_RI8, rDestLo, (vSrc-1)*4);
-        } else {
-            /* Offset too far from rFP */
-            loadConstant(cUnit, rDestLo, vSrc*4);
-            newLIR3(cUnit, THUMB_ADD_RRR, rDestLo, rFP, rDestLo);
-        }
-        assert(rDestLo < rDestHi);
-        newLIR2(cUnit, THUMB_LDMIA, rDestLo, (1<<rDestLo) | (1<<(rDestHi)));
-    }
-}
-
-/*
- * Store a pair of values of rSrc and rSrc+1 and store them into vDest and
- * vDest+1
- */
-static void storeValuePair(CompilationUnit *cUnit, int rSrcLo, int rSrcHi,
-                           int vDest, int rScratch)
-{
-    killNullCheckedRegister(cUnit, vDest);
-    killNullCheckedRegister(cUnit, vDest+1);
-    updateLiveRegisterPair(cUnit, vDest, rSrcLo, rSrcHi);
-
-    /* Use reg + imm5*4 to store the values if possible */
-    if (vDest <= 30) {
-        newLIR3(cUnit, THUMB_STR_RRI5, rSrcLo, rFP, vDest);
-        newLIR3(cUnit, THUMB_STR_RRI5, rSrcHi, rFP, vDest+1);
-    } else {
-        if (vDest <= 64) {
-            /* Sneak 4 into the base address first */
-            newLIR3(cUnit, THUMB_ADD_RRI3, rScratch, rFP, 4);
-            newLIR2(cUnit, THUMB_ADD_RI8, rScratch, (vDest-1)*4);
-        } else {
-            /* Offset too far from rFP */
-            loadConstant(cUnit, rScratch, vDest*4);
-            newLIR3(cUnit, THUMB_ADD_RRR, rScratch, rFP, rScratch);
-        }
-        assert(rSrcLo < rSrcHi);
-        newLIR2(cUnit, THUMB_STMIA, rScratch, (1<<rSrcLo) | (1 << (rSrcHi)));
-    }
-}
-
-/* Load the address of a Dalvik register on the frame */
-static void loadValueAddress(CompilationUnit *cUnit, int vSrc, int rDest)
-{
-    /* RRI3 can add up to 7 */
-    if (vSrc <= 1) {
-        newLIR3(cUnit, THUMB_ADD_RRI3, rDest, rFP, vSrc*4);
-    } else if (vSrc <= 64) {
-        /* Sneak 4 into the base address first */
-        newLIR3(cUnit, THUMB_ADD_RRI3, rDest, rFP, 4);
-        newLIR2(cUnit, THUMB_ADD_RI8, rDest, (vSrc-1)*4);
-    } else {
-        loadConstant(cUnit, rDest, vSrc*4);
-        newLIR3(cUnit, THUMB_ADD_RRR, rDest, rFP, rDest);
-    }
-}
-
-/* Load a single value from rFP[src] and store them into rDest */
-static void loadValue(CompilationUnit *cUnit, int vSrc, int rDest)
-{
-    /* Use reg + imm5*4 to load the value if possible */
-    if (vSrc <= 31) {
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDest, rFP, vSrc);
-    } else {
-        loadConstant(cUnit, rDest, vSrc*4);
-        newLIR3(cUnit, THUMB_LDR_RRR, rDest, rFP, rDest);
-    }
-}
-
-/* Load a word at base + displacement.  Displacement must be word multiple */
-static void loadWordDisp(CompilationUnit *cUnit, int rBase, int displacement,
-                         int rDest)
-{
-    assert((displacement & 0x3) == 0);
-    /* Can it fit in a RRI5? */
-    if (displacement < 128) {
-        newLIR3(cUnit, THUMB_LDR_RRI5, rDest, rBase, displacement >> 2);
-    } else {
-        loadConstant(cUnit, rDest, displacement);
-        newLIR3(cUnit, THUMB_LDR_RRR, rDest, rBase, rDest);
-    }
-}
-
-/* Store a value from rSrc to vDest */
-static void storeValue(CompilationUnit *cUnit, int rSrc, int vDest,
-                       int rScratch)
-{
-    killNullCheckedRegister(cUnit, vDest);
-    updateLiveRegister(cUnit, vDest, rSrc);
-
-    /* Use reg + imm5*4 to store the value if possible */
-    if (vDest <= 31) {
-        newLIR3(cUnit, THUMB_STR_RRI5, rSrc, rFP, vDest);
-    } else {
-        loadConstant(cUnit, rScratch, vDest*4);
-        newLIR3(cUnit, THUMB_STR_RRR, rSrc, rFP, rScratch);
-    }
-}
-
-/*
- * Perform a "reg cmp imm" operation and jump to the PCR region if condition
- * satisfies.
- */
-static inline ArmLIR *genRegImmCheck(CompilationUnit *cUnit,
-                                         ArmConditionCode cond, int reg,
-                                         int checkValue, int dOffset,
-                                         ArmLIR *pcrLabel)
-{
-    newLIR2(cUnit, THUMB_CMP_RI8, reg, checkValue);
-    ArmLIR *branch = newLIR2(cUnit, THUMB_B_COND, 0, cond);
-    return genCheckCommon(cUnit, dOffset, branch, pcrLabel);
-}
diff --git a/vm/compiler/codegen/arm/armv5te-vfp/ArchVariant.c b/vm/compiler/codegen/arm/armv5te-vfp/ArchVariant.c
index 6c5b010..7f9fa3b 100644
--- a/vm/compiler/codegen/arm/armv5te-vfp/ArchVariant.c
+++ b/vm/compiler/codegen/arm/armv5te-vfp/ArchVariant.c
@@ -19,62 +19,17 @@
  * variant-specific code.
  */
 
-#define USE_IN_CACHE_HANDLER 1
-
 /*
  * Determine the initial instruction set to be used for this trace.
  * Later components may decide to change this.
  */
-JitInstructionSetType dvmCompilerInstructionSet(CompilationUnit *cUnit)
+JitInstructionSetType dvmCompilerInstructionSet(void)
 {
     return DALVIK_JIT_THUMB;
 }
 
-/*
- * Jump to the out-of-line handler in ARM mode to finish executing the
- * remaining of more complex instructions.
- */
-static void genDispatchToHandler(CompilationUnit *cUnit, TemplateOpCode opCode)
-{
-#if USE_IN_CACHE_HANDLER
-    /*
-     * NOTE - In practice BLX only needs one operand, but since the assembler
-     * may abort itself and retry due to other out-of-range conditions we
-     * cannot really use operand[0] to store the absolute target address since
-     * it may get clobbered by the final relative offset. Therefore,
-     * we fake BLX_1 is a two operand instruction and the absolute target
-     * address is stored in operand[1].
-     */
-    newLIR2(cUnit, THUMB_BLX_1,
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode],
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode]);
-    newLIR2(cUnit, THUMB_BLX_2,
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode],
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode]);
-#else
-    /*
-     * In case we want to access the statically compiled handlers for
-     * debugging purposes, define USE_IN_CACHE_HANDLER to 0
-     */
-    void *templatePtr;
-
-#define JIT_TEMPLATE(X) extern void dvmCompiler_TEMPLATE_##X();
-#include "../../../template/armv5te-vfp/TemplateOpList.h"
-#undef JIT_TEMPLATE
-    switch (opCode) {
-#define JIT_TEMPLATE(X) \
-        case TEMPLATE_##X: { templatePtr = dvmCompiler_TEMPLATE_##X; break; }
-#include "../../../template/armv5te-vfp/TemplateOpList.h"
-#undef JIT_TEMPLATE
-        default: templatePtr = NULL;
-    }
-    loadConstant(cUnit, r7, (int) templatePtr);
-    newLIR1(cUnit, THUMB_BLX_R, r7);
-#endif
-}
-
 /* Architecture-specific initializations and checks go here */
-bool dvmCompilerArchInit(void)
+bool dvmCompilerArchVariantInit(void)
 {
     /* First, declare dvmCompiler_TEMPLATE_XXX for each template */
 #define JIT_TEMPLATE(X) extern void dvmCompiler_TEMPLATE_##X();
@@ -93,6 +48,18 @@ bool dvmCompilerArchInit(void)
 #include "../../../template/armv5te-vfp/TemplateOpList.h"
 #undef JIT_TEMPLATE
 
+    /* Target-specific configuration */
+    gDvmJit.jitTableSize = 1 << 9; // 512
+    gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
+    gDvmJit.threshold = 200;
+    gDvmJit.codeCacheSize = 512*1024;
+
+#if defined(WITH_SELF_VERIFICATION)
+    /* Force into blocking mode */
+    gDvmJit.blockingMode = true;
+    gDvm.nativeDebuggerActive = true;
+#endif
+
     /* Codegen-specific assumptions */
     assert(offsetof(ClassObject, vtable) < 128 &&
            (offsetof(ClassObject, vtable) & 0x3) == 0);
@@ -104,188 +71,24 @@ bool dvmCompilerArchInit(void)
     assert(sizeof(StackSaveArea) < 236);
 
     /*
-     * EA is calculated by doing "Rn + imm5 << 2", and there are 5 entry points
-     * that codegen may access, make sure that the offset from the top of the
-     * struct is less than 108.
+     * EA is calculated by doing "Rn + imm5 << 2", make sure that the last
+     * offset from the struct is less than 128.
      */
-    assert(offsetof(InterpState, jitToInterpEntries) < 108);
+    assert((offsetof(InterpState, jitToInterpEntries) +
+            sizeof(struct JitToInterpEntries)) <= 128);
     return true;
 }
 
-static bool genInlineSqrt(CompilationUnit *cUnit, MIR *mir)
-{
-    int offset = offsetof(InterpState, retval);
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vSrc = mir->dalvikInsn.vA;
-    loadValueAddress(cUnit, vSrc, r2);
-    genDispatchToHandler(cUnit, TEMPLATE_SQRT_DOUBLE_VFP);
-    newLIR3(cUnit, THUMB_STR_RRI5, r0, rGLUE, offset >> 2);
-    newLIR3(cUnit, THUMB_STR_RRI5, r1, rGLUE, (offset >> 2) + 1);
-    return false;
-}
-
-static bool genInlineCos(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;
-}
-
-static bool genInlineSin(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;
-}
-
-static bool genArithOpFloat(CompilationUnit *cUnit, MIR *mir, int vDest,
-                                int vSrc1, int vSrc2)
-{
-    TemplateOpCode opCode;
-
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch (mir->dalvikInsn.opCode) {
-        case OP_ADD_FLOAT_2ADDR:
-        case OP_ADD_FLOAT:
-            opCode = TEMPLATE_ADD_FLOAT_VFP;
-            break;
-        case OP_SUB_FLOAT_2ADDR:
-        case OP_SUB_FLOAT:
-            opCode = TEMPLATE_SUB_FLOAT_VFP;
-            break;
-        case OP_DIV_FLOAT_2ADDR:
-        case OP_DIV_FLOAT:
-            opCode = TEMPLATE_DIV_FLOAT_VFP;
-            break;
-        case OP_MUL_FLOAT_2ADDR:
-        case OP_MUL_FLOAT:
-            opCode = TEMPLATE_MUL_FLOAT_VFP;
-            break;
-        case OP_REM_FLOAT_2ADDR:
-        case OP_REM_FLOAT:
-        case OP_NEG_FLOAT: {
-            return genArithOpFloatPortable(cUnit, mir, vDest,
-                                                      vSrc1, vSrc2);
-        }
-        default:
-            return true;
-    }
-    loadValueAddress(cUnit, vDest, r0);
-    loadValueAddress(cUnit, vSrc1, r1);
-    loadValueAddress(cUnit, vSrc2, r2);
-    genDispatchToHandler(cUnit, opCode);
-    return false;
-}
-
-static bool genArithOpDouble(CompilationUnit *cUnit, MIR *mir, int vDest,
-                             int vSrc1, int vSrc2)
-{
-    TemplateOpCode opCode;
-
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch (mir->dalvikInsn.opCode) {
-        case OP_ADD_DOUBLE_2ADDR:
-        case OP_ADD_DOUBLE:
-            opCode = TEMPLATE_ADD_DOUBLE_VFP;
-            break;
-        case OP_SUB_DOUBLE_2ADDR:
-        case OP_SUB_DOUBLE:
-            opCode = TEMPLATE_SUB_DOUBLE_VFP;
-            break;
-        case OP_DIV_DOUBLE_2ADDR:
-        case OP_DIV_DOUBLE:
-            opCode = TEMPLATE_DIV_DOUBLE_VFP;
-            break;
-        case OP_MUL_DOUBLE_2ADDR:
-        case OP_MUL_DOUBLE:
-            opCode = TEMPLATE_MUL_DOUBLE_VFP;
-            break;
-        case OP_REM_DOUBLE_2ADDR:
-        case OP_REM_DOUBLE:
-        case OP_NEG_DOUBLE: {
-            return genArithOpDoublePortable(cUnit, mir, vDest,
-                                                       vSrc1, vSrc2);
-        }
-        default:
-            return true;
-    }
-    loadValueAddress(cUnit, vDest, r0);
-    loadValueAddress(cUnit, vSrc1, r1);
-    loadValueAddress(cUnit, vSrc2, r2);
-    genDispatchToHandler(cUnit, opCode);
-    return false;
-}
-
-static bool genConversion(CompilationUnit *cUnit, MIR *mir)
+int dvmCompilerTargetOptHint(int key)
 {
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vSrc1Dest = mir->dalvikInsn.vA;
-    int vSrc2 = mir->dalvikInsn.vB;
-    TemplateOpCode template;
-
-    switch (opCode) {
-        case OP_INT_TO_FLOAT:
-            template = TEMPLATE_INT_TO_FLOAT_VFP;
-            break;
-        case OP_FLOAT_TO_INT:
-            template = TEMPLATE_FLOAT_TO_INT_VFP;
-            break;
-        case OP_DOUBLE_TO_FLOAT:
-            template = TEMPLATE_DOUBLE_TO_FLOAT_VFP;
-            break;
-        case OP_FLOAT_TO_DOUBLE:
-            template = TEMPLATE_FLOAT_TO_DOUBLE_VFP;
-            break;
-        case OP_INT_TO_DOUBLE:
-            template = TEMPLATE_INT_TO_DOUBLE_VFP;
-            break;
-        case OP_DOUBLE_TO_INT:
-            template = TEMPLATE_DOUBLE_TO_INT_VFP;
-            break;
-        case OP_FLOAT_TO_LONG:
-        case OP_LONG_TO_FLOAT:
-        case OP_DOUBLE_TO_LONG:
-        case OP_LONG_TO_DOUBLE:
-            return genConversionPortable(cUnit, mir);
-        default:
-            return true;
-    }
-    loadValueAddress(cUnit, vSrc1Dest, r0);
-    loadValueAddress(cUnit, vSrc2, r1);
-    genDispatchToHandler(cUnit, template);
-    return false;
-}
-
-static bool genCmpX(CompilationUnit *cUnit, MIR *mir, int vDest, int vSrc1,
-                    int vSrc2)
-{
-    TemplateOpCode template;
-
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch(mir->dalvikInsn.opCode) {
-        case OP_CMPL_FLOAT:
-            template = TEMPLATE_CMPL_FLOAT_VFP;
-            break;
-        case OP_CMPG_FLOAT:
-            template = TEMPLATE_CMPG_FLOAT_VFP;
-            break;
-        case OP_CMPL_DOUBLE:
-            template = TEMPLATE_CMPL_DOUBLE_VFP;
-            break;
-        case OP_CMPG_DOUBLE:
-            template = TEMPLATE_CMPG_DOUBLE_VFP;
+    int res;
+    switch (key) {
+        case kMaxHoistDistance:
+            res = 2;
             break;
         default:
-            return true;
+            LOGE("Unknown target optimization hint key: %d",key);
+            res = 0;
     }
-    loadValueAddress(cUnit, vSrc1, r0);
-    loadValueAddress(cUnit, vSrc2, r1);
-    genDispatchToHandler(cUnit, template);
-    storeValue(cUnit, r0, vDest, r1);
-    return false;
+    return res;
 }
diff --git a/vm/compiler/codegen/arm/armv5te/ArchVariant.c b/vm/compiler/codegen/arm/armv5te/ArchVariant.c
index a1f2b00..e018ea1 100644
--- a/vm/compiler/codegen/arm/armv5te/ArchVariant.c
+++ b/vm/compiler/codegen/arm/armv5te/ArchVariant.c
@@ -19,62 +19,17 @@
  * variant-specific code.
  */
 
-#define USE_IN_CACHE_HANDLER 1
-
 /*
  * Determine the initial instruction set to be used for this trace.
  * Later components may decide to change this.
  */
-JitInstructionSetType dvmCompilerInstructionSet(CompilationUnit *cUnit)
+JitInstructionSetType dvmCompilerInstructionSet(void)
 {
     return DALVIK_JIT_THUMB;
 }
 
-/*
- * Jump to the out-of-line handler in ARM mode to finish executing the
- * remaining of more complex instructions.
- */
-static void genDispatchToHandler(CompilationUnit *cUnit, TemplateOpCode opCode)
-{
-#if USE_IN_CACHE_HANDLER
-    /*
-     * NOTE - In practice BLX only needs one operand, but since the assembler
-     * may abort itself and retry due to other out-of-range conditions we
-     * cannot really use operand[0] to store the absolute target address since
-     * it may get clobbered by the final relative offset. Therefore,
-     * we fake BLX_1 is a two operand instruction and the absolute target
-     * address is stored in operand[1].
-     */
-    newLIR2(cUnit, THUMB_BLX_1,
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode],
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode]);
-    newLIR2(cUnit, THUMB_BLX_2,
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode],
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode]);
-#else
-    /*
-     * In case we want to access the statically compiled handlers for
-     * debugging purposes, define USE_IN_CACHE_HANDLER to 0
-     */
-    void *templatePtr;
-
-#define JIT_TEMPLATE(X) extern void dvmCompiler_TEMPLATE_##X();
-#include "../../../template/armv5te/TemplateOpList.h"
-#undef JIT_TEMPLATE
-    switch (opCode) {
-#define JIT_TEMPLATE(X) \
-        case TEMPLATE_##X: { templatePtr = dvmCompiler_TEMPLATE_##X; break; }
-#include "../../../template/armv5te/TemplateOpList.h"
-#undef JIT_TEMPLATE
-        default: templatePtr = NULL;
-    }
-    loadConstant(cUnit, r7, (int) templatePtr);
-    newLIR1(cUnit, THUMB_BLX_R, r7);
-#endif
-}
-
 /* Architecture-specific initializations and checks go here */
-bool dvmCompilerArchInit(void)
+bool dvmCompilerArchVariantInit(void)
 {
     /* First, declare dvmCompiler_TEMPLATE_XXX for each template */
 #define JIT_TEMPLATE(X) extern void dvmCompiler_TEMPLATE_##X();
@@ -93,6 +48,18 @@ bool dvmCompilerArchInit(void)
 #include "../../../template/armv5te/TemplateOpList.h"
 #undef JIT_TEMPLATE
 
+    /* Target-specific configuration */
+    gDvmJit.jitTableSize = 1 << 9; // 512
+    gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
+    gDvmJit.threshold = 200;
+    gDvmJit.codeCacheSize = 512*1024;
+
+#if defined(WITH_SELF_VERIFICATION)
+    /* Force into blocking mode */
+    gDvmJit.blockingMode = true;
+    gDvm.nativeDebuggerActive = true;
+#endif
+
     /* Codegen-specific assumptions */
     assert(offsetof(ClassObject, vtable) < 128 &&
            (offsetof(ClassObject, vtable) & 0x3) == 0);
@@ -104,80 +71,24 @@ bool dvmCompilerArchInit(void)
     assert(sizeof(StackSaveArea) < 236);
 
     /*
-     * EA is calculated by doing "Rn + imm5 << 2", and there are 5 entry points
-     * that codegen may access, make sure that the offset from the top of the
-     * struct is less than 108.
+     * EA is calculated by doing "Rn + imm5 << 2", make sure that the last
+     * offset from the struct is less than 128.
      */
-    assert(offsetof(InterpState, jitToInterpEntries) < 108);
+    assert((offsetof(InterpState, jitToInterpEntries) +
+            sizeof(struct JitToInterpEntries)) <= 128);
     return true;
 }
 
-static bool genInlineSqrt(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;   /* punt to C handler */
-}
-
-static bool genInlineCos(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;   /* punt to C handler */
-}
-
-static bool genInlineSin(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;   /* punt to C handler */
-}
-
-static bool genConversion(CompilationUnit *cUnit, MIR *mir)
-{
-    return genConversionPortable(cUnit, mir);
-}
-
-static bool genArithOpFloat(CompilationUnit *cUnit, MIR *mir, int vDest,
-                        int vSrc1, int vSrc2)
+int dvmCompilerTargetOptHint(int key)
 {
-    return genArithOpFloatPortable(cUnit, mir, vDest, vSrc1, vSrc2);
-}
-
-static bool genArithOpDouble(CompilationUnit *cUnit, MIR *mir, int vDest,
-                      int vSrc1, int vSrc2)
-{
-    return genArithOpDoublePortable(cUnit, mir, vDest, vSrc1, vSrc2);
-}
-
-static bool genCmpX(CompilationUnit *cUnit, MIR *mir, int vDest, int vSrc1,
-                    int vSrc2)
-{
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch (mir->dalvikInsn.opCode) {
-        case OP_CMPL_FLOAT:
-            loadValue(cUnit, vSrc1, r0);
-            loadValue(cUnit, vSrc2, r1);
-            genDispatchToHandler(cUnit, TEMPLATE_CMPL_FLOAT);
-            storeValue(cUnit, r0, vDest, r1);
-            break;
-        case OP_CMPG_FLOAT:
-            loadValue(cUnit, vSrc1, r0);
-            loadValue(cUnit, vSrc2, r1);
-            genDispatchToHandler(cUnit, TEMPLATE_CMPG_FLOAT);
-            storeValue(cUnit, r0, vDest, r1);
-            break;
-        case OP_CMPL_DOUBLE:
-            loadValueAddress(cUnit, vSrc1, r0);
-            loadValueAddress(cUnit, vSrc2, r1);
-            genDispatchToHandler(cUnit, TEMPLATE_CMPL_DOUBLE);
-            storeValue(cUnit, r0, vDest, r1);
-            break;
-        case OP_CMPG_DOUBLE:
-            loadValueAddress(cUnit, vSrc1, r0);
-            loadValueAddress(cUnit, vSrc2, r1);
-            genDispatchToHandler(cUnit, TEMPLATE_CMPG_DOUBLE);
-            storeValue(cUnit, r0, vDest, r1);
+    int res;
+    switch (key) {
+        case kMaxHoistDistance:
+            res = 2;
             break;
         default:
-            return true;
+            LOGE("Unknown target optimization hint key: %d",key);
+            res = 0;
     }
-    return false;
+    return res;
 }
diff --git a/vm/compiler/codegen/arm/armv7-a/ArchVariant.c b/vm/compiler/codegen/arm/armv7-a/ArchVariant.c
index 92097af..5a14774 100644
--- a/vm/compiler/codegen/arm/armv7-a/ArchVariant.c
+++ b/vm/compiler/codegen/arm/armv7-a/ArchVariant.c
@@ -14,70 +14,17 @@
  * limitations under the License.
  */
 
-
-static void loadFloat(CompilationUnit *cUnit, int vSrc, int rDest);
-
-/*
- * This file is included by Codegen-armv5te-vfp.c, and implements architecture
- * variant-specific code.
- */
-
-#define USE_IN_CACHE_HANDLER 1
-
 /*
  * Determine the initial instruction set to be used for this trace.
  * Later components may decide to change this.
  */
-JitInstructionSetType dvmCompilerInstructionSet(CompilationUnit *cUnit)
+JitInstructionSetType dvmCompilerInstructionSet(void)
 {
     return DALVIK_JIT_THUMB2;
 }
 
-/*
- * Jump to the out-of-line handler in ARM mode to finish executing the
- * remaining of more complex instructions.
- */
-static void genDispatchToHandler(CompilationUnit *cUnit, TemplateOpCode opCode)
-{
-#if USE_IN_CACHE_HANDLER
-    /*
-     * NOTE - In practice BLX only needs one operand, but since the assembler
-     * may abort itself and retry due to other out-of-range conditions we
-     * cannot really use operand[0] to store the absolute target address since
-     * it may get clobbered by the final relative offset. Therefore,
-     * we fake BLX_1 is a two operand instruction and the absolute target
-     * address is stored in operand[1].
-     */
-    newLIR2(cUnit, THUMB_BLX_1,
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode],
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode]);
-    newLIR2(cUnit, THUMB_BLX_2,
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode],
-            (int) gDvmJit.codeCache + templateEntryOffsets[opCode]);
-#else
-    /*
-     * In case we want to access the statically compiled handlers for
-     * debugging purposes, define USE_IN_CACHE_HANDLER to 0
-     */
-    void *templatePtr;
-
-#define JIT_TEMPLATE(X) extern void dvmCompiler_TEMPLATE_##X();
-#include "../../../template/armv5te-vfp/TemplateOpList.h"
-#undef JIT_TEMPLATE
-    switch (opCode) {
-#define JIT_TEMPLATE(X) \
-        case TEMPLATE_##X: { templatePtr = dvmCompiler_TEMPLATE_##X; break; }
-#include "../../../template/armv5te-vfp/TemplateOpList.h"
-#undef JIT_TEMPLATE
-        default: templatePtr = NULL;
-    }
-    loadConstant(cUnit, r7, (int) templatePtr);
-    newLIR1(cUnit, THUMB_BLX_R, r7);
-#endif
-}
-
 /* Architecture-specific initializations and checks go here */
-bool dvmCompilerArchInit(void)
+bool dvmCompilerArchVariantInit(void)
 {
     /* First, declare dvmCompiler_TEMPLATE_XXX for each template */
 #define JIT_TEMPLATE(X) extern void dvmCompiler_TEMPLATE_##X();
@@ -96,6 +43,18 @@ bool dvmCompilerArchInit(void)
 #include "../../../template/armv5te-vfp/TemplateOpList.h"
 #undef JIT_TEMPLATE
 
+    /* Target-specific configuration */
+    gDvmJit.jitTableSize = 1 << 12; // 4096
+    gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
+    gDvmJit.threshold = 40;
+    gDvmJit.codeCacheSize = 1024*1024;
+
+#if defined(WITH_SELF_VERIFICATION)
+    /* Force into blocking */
+    gDvmJit.blockingMode = true;
+    gDvm.nativeDebuggerActive = true;
+#endif
+
     /* Codegen-specific assumptions */
     assert(offsetof(ClassObject, vtable) < 128 &&
            (offsetof(ClassObject, vtable) & 0x3) == 0);
@@ -107,213 +66,24 @@ bool dvmCompilerArchInit(void)
     assert(sizeof(StackSaveArea) < 236);
 
     /*
-     * EA is calculated by doing "Rn + imm5 << 2", and there are 5 entry points
-     * that codegen may access, make sure that the offset from the top of the
-     * struct is less than 108.
+     * EA is calculated by doing "Rn + imm5 << 2", make sure that the last
+     * offset from the struct is less than 128.
      */
-    assert(offsetof(InterpState, jitToInterpEntries) < 108);
-    return true;
-}
-
-static bool genInlineSqrt(CompilationUnit *cUnit, MIR *mir)
-{
-    int offset = offsetof(InterpState, retval);
-    int vSrc = mir->dalvikInsn.vA;
-    loadDouble(cUnit, vSrc, dr1);
-    newLIR2(cUnit, THUMB2_VSQRTD, dr0, dr1);
-    assert(offset & 0x3 == 0);  /* Must be word aligned */
-    assert(offset < 1024);
-    newLIR3(cUnit, THUMB2_VSTRD, dr0, rGLUE, offset >> 2);
+    assert((offsetof(InterpState, jitToInterpEntries) +
+            sizeof(struct JitToInterpEntries)) <= 128);
     return true;
 }
 
-static bool genInlineCos(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;
-}
-
-static bool genInlineSin(CompilationUnit *cUnit, MIR *mir)
-{
-    return false;
-}
-
-static bool genArithOpFloat(CompilationUnit *cUnit, MIR *mir, int vDest,
-                                int vSrc1, int vSrc2)
-{
-    int op = THUMB_BKPT;
-
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch (mir->dalvikInsn.opCode) {
-        case OP_ADD_FLOAT_2ADDR:
-        case OP_ADD_FLOAT:
-            op = THUMB2_VADDS;
-            break;
-        case OP_SUB_FLOAT_2ADDR:
-        case OP_SUB_FLOAT:
-            op = THUMB2_VSUBS;
-            break;
-        case OP_DIV_FLOAT_2ADDR:
-        case OP_DIV_FLOAT:
-            op = THUMB2_VDIVS;
-            break;
-        case OP_MUL_FLOAT_2ADDR:
-        case OP_MUL_FLOAT:
-            op = THUMB2_VMULS;
-            break;
-        case OP_REM_FLOAT_2ADDR:
-        case OP_REM_FLOAT:
-        case OP_NEG_FLOAT: {
-            return genArithOpFloatPortable(cUnit, mir, vDest, vSrc1, vSrc2);
-        }
-        default:
-            return true;
-    }
-    loadFloat(cUnit, vSrc1, fr2);
-    loadFloat(cUnit, vSrc2, fr4);
-    newLIR3(cUnit, op, fr0, fr2, fr4);
-    storeFloat(cUnit, fr0, vDest, 0);
-    return false;
-}
-
-static bool genArithOpDouble(CompilationUnit *cUnit, MIR *mir, int vDest,
-                             int vSrc1, int vSrc2)
-{
-    int op = THUMB_BKPT;
-
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch (mir->dalvikInsn.opCode) {
-        case OP_ADD_DOUBLE_2ADDR:
-        case OP_ADD_DOUBLE:
-            op = THUMB2_VADDD;
-            break;
-        case OP_SUB_DOUBLE_2ADDR:
-        case OP_SUB_DOUBLE:
-            op = THUMB2_VSUBD;
-            break;
-        case OP_DIV_DOUBLE_2ADDR:
-        case OP_DIV_DOUBLE:
-            op = THUMB2_VDIVD;
-            break;
-        case OP_MUL_DOUBLE_2ADDR:
-        case OP_MUL_DOUBLE:
-            op = THUMB2_VMULD;
-            break;
-        case OP_REM_DOUBLE_2ADDR:
-        case OP_REM_DOUBLE:
-        case OP_NEG_DOUBLE: {
-            return genArithOpDoublePortable(cUnit, mir, vDest, vSrc1, vSrc2);
-        }
-        default:
-            return true;
-    }
-    loadDouble(cUnit, vSrc1, dr1);
-    loadDouble(cUnit, vSrc2, dr2);
-    newLIR3(cUnit, op, dr0, dr1, dr2);
-    storeDouble(cUnit, dr0, vDest, 0);
-    return false;
-}
-
-static bool genConversion(CompilationUnit *cUnit, MIR *mir)
+int dvmCompilerTargetOptHint(int key)
 {
-    OpCode opCode = mir->dalvikInsn.opCode;
-    int vSrc1Dest = mir->dalvikInsn.vA;
-    int vSrc2 = mir->dalvikInsn.vB;
-    int op = THUMB_BKPT;
-    bool longSrc = false;
-    bool longDest = false;
-    int srcReg;
-    int tgtReg;
-
-    switch (opCode) {
-        case OP_INT_TO_FLOAT:
-            longSrc = false;
-            longDest = false;
-            op = THUMB2_VCVTIF;
-            break;
-        case OP_FLOAT_TO_INT:
-            longSrc = false;
-            longDest = false;
-            op = THUMB2_VCVTFI;
-            break;
-        case OP_DOUBLE_TO_FLOAT:
-            longSrc = true;
-            longDest = false;
-            op = THUMB2_VCVTDF;
-            break;
-        case OP_FLOAT_TO_DOUBLE:
-            longSrc = false;
-            longDest = true;
-            op = THUMB2_VCVTFD;
-            break;
-        case OP_INT_TO_DOUBLE:
-            longSrc = false;
-            longDest = true;
-            op = THUMB2_VCVTID;
-            break;
-        case OP_DOUBLE_TO_INT:
-            longSrc = true;
-            longDest = false;
-            op = THUMB2_VCVTDI;
-            break;
-        case OP_FLOAT_TO_LONG:
-        case OP_LONG_TO_FLOAT:
-        case OP_DOUBLE_TO_LONG:
-        case OP_LONG_TO_DOUBLE:
-            return genConversionPortable(cUnit, mir);
-        default:
-            return true;
-    }
-    if (longSrc) {
-        srcReg = dr1;
-        loadDouble(cUnit, vSrc2, srcReg);
-    } else {
-        srcReg = fr2;
-        loadFloat(cUnit, vSrc2, srcReg);
-    }
-    if (longDest) {
-        newLIR2(cUnit, op, dr0, srcReg);
-        storeDouble(cUnit, dr0, vSrc1Dest, 0);
-    } else {
-        newLIR2(cUnit, op, fr0, srcReg);
-        storeFloat(cUnit, fr0, vSrc1Dest, 0);
-    }
-    return false;
-}
-
-static bool genCmpX(CompilationUnit *cUnit, MIR *mir, int vDest, int vSrc1,
-                    int vSrc2)
-{
-    TemplateOpCode template;
-
-    /*
-     * Don't attempt to optimize register usage since these opcodes call out to
-     * the handlers.
-     */
-    switch(mir->dalvikInsn.opCode) {
-        case OP_CMPL_FLOAT:
-            template = TEMPLATE_CMPL_FLOAT_VFP;
-            break;
-        case OP_CMPG_FLOAT:
-            template = TEMPLATE_CMPG_FLOAT_VFP;
-            break;
-        case OP_CMPL_DOUBLE:
-            template = TEMPLATE_CMPL_DOUBLE_VFP;
-            break;
-        case OP_CMPG_DOUBLE:
-            template = TEMPLATE_CMPG_DOUBLE_VFP;
+    int res;
+    switch (key) {
+        case kMaxHoistDistance:
+            res = 7;
             break;
         default:
-            return true;
+            LOGE("Unknown target optimization hint key: %d",key);
+            res = 0;
     }
-    loadValueAddress(cUnit, vSrc1, r0);
-    loadValueAddress(cUnit, vSrc2, r1);
-    genDispatchToHandler(cUnit, template);
-    storeValue(cUnit, r0, vDest, r1);
-    return false;
+    return res;
 }
diff --git a/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_DOUBLE_VFP.S b/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_DOUBLE_VFP.S
index 3801f49..1b143a9 100644
--- a/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_DOUBLE_VFP.S
+++ b/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_DOUBLE_VFP.S
@@ -25,7 +25,7 @@
     /* op vAA, vBB, vCC */
     fldd    d0, [r0]                    @ d0<- vBB
     fldd    d1, [r1]                    @ d1<- vCC
-    fcmped  d0, d1                      @ compare (vBB, vCC)
+    fcmpd  d0, d1                       @ compare (vBB, vCC)
     mov     r0, #1                      @ r0<- 1 (default)
     fmstat                              @ export status flags
     mvnmi   r0, #0                      @ (less than) r0<- -1
diff --git a/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_FLOAT_VFP.S b/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_FLOAT_VFP.S
index 1faafa1..0510ef6 100644
--- a/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_FLOAT_VFP.S
+++ b/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPG_FLOAT_VFP.S
@@ -24,7 +24,7 @@
     /* op vAA, vBB, vCC */
     flds    s0, [r0]                    @ d0<- vBB
     flds    s1, [r1]                    @ d1<- vCC
-    fcmpes  s0, s1                      @ compare (vBB, vCC)
+    fcmps  s0, s1                      @ compare (vBB, vCC)
     mov     r0, #1                      @ r0<- 1 (default)
     fmstat                              @ export status flags
     mvnmi   r0, #0                      @ (less than) r0<- -1
diff --git a/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPL_FLOAT_VFP.S b/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPL_FLOAT_VFP.S
index 014f160..bdb42d6 100644
--- a/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPL_FLOAT_VFP.S
+++ b/vm/compiler/template/armv5te-vfp/TEMPLATE_CMPL_FLOAT_VFP.S
@@ -24,7 +24,7 @@
     /* op vAA, vBB, vCC */
     flds    s0, [r0]                    @ d0<- vBB
     flds    s1, [r1]                    @ d1<- vCC
-    fcmpes  s0, s1                      @ compare (vBB, vCC)
+    fcmps  s0, s1                      @ compare (vBB, vCC)
     mvn     r0, #0                      @ r0<- -1 (default)
     fmstat                              @ export status flags
     movgt   r0, #1                      @ (greater than) r0<- 1
diff --git a/vm/compiler/template/armv5te-vfp/TemplateOpList.h b/vm/compiler/template/armv5te-vfp/TemplateOpList.h
index c95163c..d991bed 100644
--- a/vm/compiler/template/armv5te-vfp/TemplateOpList.h
+++ b/vm/compiler/template/armv5te-vfp/TemplateOpList.h
@@ -50,3 +50,10 @@ JIT_TEMPLATE(CMPL_DOUBLE_VFP)
 JIT_TEMPLATE(CMPG_FLOAT_VFP)
 JIT_TEMPLATE(CMPL_FLOAT_VFP)
 JIT_TEMPLATE(SQRT_DOUBLE_VFP)
+JIT_TEMPLATE(THROW_EXCEPTION_COMMON)
+JIT_TEMPLATE(MEM_OP_DECODE)
+JIT_TEMPLATE(STRING_COMPARETO)
+JIT_TEMPLATE(STRING_INDEXOF)
+JIT_TEMPLATE(INTERPRET)
+JIT_TEMPLATE(MONITOR_ENTER)
+JIT_TEMPLATE(MONITOR_ENTER_DEBUG)
diff --git a/vm/compiler/template/armv5te/TEMPLATE_CMPL_DOUBLE.S b/vm/compiler/template/armv5te/TEMPLATE_CMPL_DOUBLE.S
index dfafd2c..01772b4 100644
--- a/vm/compiler/template/armv5te/TEMPLATE_CMPL_DOUBLE.S
+++ b/vm/compiler/template/armv5te/TEMPLATE_CMPL_DOUBLE.S
@@ -1,6 +1,6 @@
 %default { "naninst":"mvn     r0, #0" }
     /*
-     * For the JIT: incoming arguments are pointers to the arguments in r0/r1
+     * For the JIT: incoming arguments in r0-r1, r2-r3
      *              result in r0
      *
      * Compare two floating-point values.  Puts 0, 1, or -1 into the
@@ -14,26 +14,24 @@
      * For: cmpl-double, cmpg-double
      */
     /* op vAA, vBB, vCC */
-    mov     r4, lr                      @ save return address
-    mov     r9, r0                      @ save copy of &arg1
-    mov     r10, r1                     @ save copy of &arg2
-    ldmia   r9, {r0-r1}                 @ r0/r1<- vBB/vBB+1
-    ldmia   r10, {r2-r3}                @ r2/r3<- vCC/vCC+1
+    push    {r0-r3}                     @ save operands
+    mov     r11, lr                     @ save return address
     LDR_PC_LR ".L__aeabi_cdcmple"       @ PIC way of "bl __aeabi_cdcmple"
     bhi     .L${opcode}_gt_or_nan       @ C set and Z clear, disambiguate
     mvncc   r0, #0                      @ (less than) r1<- -1
     moveq   r0, #0                      @ (equal) r1<- 0, trumps less than
-    bx      r4
+    add     sp, #16                     @ drop unused operands
+    bx      r11
 
     @ Test for NaN with a second comparison.  EABI forbids testing bit
     @ patterns, and we can't represent 0x7fc00000 in immediate form, so
     @ make the library call.
 .L${opcode}_gt_or_nan:
-    ldmia   r10, {r0-r1}                @ reverse order
-    ldmia   r9, {r2-r3}
+    pop     {r2-r3}                     @ restore operands in reverse order
+    pop     {r0-r1}                     @ restore operands in reverse order
     LDR_PC_LR ".L__aeabi_cdcmple"       @ r0<- Z set if eq, C clear if <
     movcc   r0, #1                      @ (greater than) r1<- 1
-    bxcc    r4
+    bxcc    r11
     $naninst                            @ r1<- 1 or -1 for NaN
-    bx      r4
+    bx      r11
 
diff --git a/vm/compiler/template/armv5te/TEMPLATE_CMPL_FLOAT.S b/vm/compiler/template/armv5te/TEMPLATE_CMPL_FLOAT.S
index 31d4cd8..b63780f 100644
--- a/vm/compiler/template/armv5te/TEMPLATE_CMPL_FLOAT.S
+++ b/vm/compiler/template/armv5te/TEMPLATE_CMPL_FLOAT.S
@@ -1,6 +1,6 @@
 %default { "naninst":"mvn     r0, #0" }
     /*
-     * For the JIT: incoming arguments in r0, r1
+     * For the JIT: incoming arguments in r0-r1, r2-r3
      *              result in r0
      *
      * Compare two floating-point values.  Puts 0, 1, or -1 into the
@@ -33,24 +33,24 @@
      * for: cmpl-float, cmpg-float
      */
     /* op vAA, vBB, vCC */
-    mov     r4, lr                      @ save return address
     mov     r9, r0                      @ Save copies - we may need to redo
     mov     r10, r1
+    mov     r11, lr                     @ save return address
     LDR_PC_LR ".L__aeabi_cfcmple"       @ cmp <=: C clear if <, Z set if eq
     bhi     .L${opcode}_gt_or_nan       @ C set and Z clear, disambiguate
     mvncc   r0, #0                      @ (less than) r0<- -1
     moveq   r0, #0                      @ (equal) r0<- 0, trumps less than
-    bx      r4
+    bx      r11
     @ Test for NaN with a second comparison.  EABI forbids testing bit
     @ patterns, and we can't represent 0x7fc00000 in immediate form, so
     @ make the library call.
 .L${opcode}_gt_or_nan:
-    mov     r1, r9                      @ reverse order
-    mov     r0, r10
+    mov     r0, r10                     @ restore in reverse order
+    mov     r1, r9
     LDR_PC_LR ".L__aeabi_cfcmple"       @ r0<- Z set if eq, C clear if <
     movcc   r0, #1                      @ (greater than) r1<- 1
-    bxcc    r4
+    bxcc    r11
     $naninst                            @ r1<- 1 or -1 for NaN
-    bx      r4
+    bx      r11
 
 
diff --git a/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_CHAIN.S b/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_CHAIN.S
index 20bb3ab..d6e6763 100644
--- a/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_CHAIN.S
+++ b/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_CHAIN.S
@@ -16,7 +16,7 @@
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     add     r12, lr, #2                 @ setup the punt-to-interp address
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    r12                         @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
diff --git a/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NATIVE.S b/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NATIVE.S
index b20d564..0dbd6c0 100644
--- a/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NATIVE.S
+++ b/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NATIVE.S
@@ -22,11 +22,17 @@
     str     r0, [r1, #(offStackSaveArea_method - sizeofStackSaveArea)]
     cmp     r8, #0                      @ suspendCount != 0
     ldr     r8, [r0, #offMethod_nativeFunc] @ r8<- method->nativeFunc
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    lr                          @ bail to the interpreter
+#else
+    bx      lr                          @ bail to interpreter unconditionally
+#endif
 
     @ go ahead and transfer control to the native code
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    mov     r2, #0
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in the jit code cache
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
     mov     r9, r3                      @ r9<- glue->self (preserve)
@@ -46,6 +52,22 @@
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [rFP, #(offStackSaveArea_currentPc - sizeofStackSaveArea)]
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
+
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the mode properly
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+
 
diff --git a/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NO_OPT.S b/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NO_OPT.S
index 0ac7cf8..facce51 100644
--- a/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NO_OPT.S
+++ b/vm/compiler/template/armv5te/TEMPLATE_INVOKE_METHOD_NO_OPT.S
@@ -13,7 +13,7 @@
     sub     r1, r1, r7, lsl #2          @ r1<- newFp (old savearea - regsSize)
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    lr                          @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -31,9 +31,13 @@
     cmp     r8, #0                      @ suspendCount != 0
     bxne    lr                          @ bail to the interpreter
     tst     r10, #ACC_NATIVE
+#if !defined(WITH_SELF_VERIFICATION)
     bne     .LinvokeNative
+#else
+    bxne    lr                          @ bail to the interpreter
+#endif
 
-    ldr     r10, .LdvmJitToInterpNoChain
+    ldr     r10, .LdvmJitToInterpTraceSelectNoChain
     ldr     r3, [r9, #offClassObject_pDvmDex] @ r3<- method->clazz->pDvmDex
     ldr     r2, [rGLUE, #offGlue_self]      @ r2<- glue->self
 
@@ -44,4 +48,7 @@
     str     rFP, [r2, #offThread_curFrame]  @ self->curFrame = newFp
 
     @ Start executing the callee
-    mov     pc, r10                         @ dvmJitToInterpNoChain
+#if defined(JIT_STATS)
+    mov     r0, #kInlineCacheMiss
+#endif
+    mov     pc, r10                         @ dvmJitToInterpTraceSelectNoChain
diff --git a/vm/compiler/template/armv5te/TEMPLATE_RETURN.S b/vm/compiler/template/armv5te/TEMPLATE_RETURN.S
index f0a4623..502c493 100644
--- a/vm/compiler/template/armv5te/TEMPLATE_RETURN.S
+++ b/vm/compiler/template/armv5te/TEMPLATE_RETURN.S
@@ -9,27 +9,39 @@
     ldr     r10, [r0, #offStackSaveArea_prevFrame] @ r10<- saveArea->prevFrame
     ldr     r8, [rGLUE, #offGlue_pSelfSuspendCount] @ r8<- &suspendCount
     ldr     rPC, [r0, #offStackSaveArea_savedPc] @ rPC<- saveArea->savedPc
+#if !defined(WITH_SELF_VERIFICATION)
     ldr     r9,  [r0, #offStackSaveArea_returnAddr] @ r9<- chaining cell ret
+#else
+    mov     r9, #0                      @ disable chaining
+#endif
     ldr     r2, [r10, #(offStackSaveArea_method - sizeofStackSaveArea)]
                                         @ r2<- method we're returning to
     ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
     cmp     r2, #0                      @ break frame?
+#if !defined(WITH_SELF_VERIFICATION)
     beq     1f                          @ bail to interpreter
-    ldr     r0, .LdvmJitToInterpNoChain @ defined in footer.S
+#else
+    blxeq   lr                          @ punt to interpreter and compare state
+#endif
+    ldr     r1, .LdvmJitToInterpNoChain @ defined in footer.S
     mov     rFP, r10                    @ publish new FP
     ldrne   r10, [r2, #offMethod_clazz] @ r10<- method->clazz
     ldr     r8, [r8]                    @ r8<- suspendCount
 
     str     r2, [rGLUE, #offGlue_method]@ glue->method = newSave->method
-    ldr     r1, [r10, #offClassObject_pDvmDex] @ r1<- method->clazz->pDvmDex
+    ldr     r0, [r10, #offClassObject_pDvmDex] @ r0<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame] @ self->curFrame = fp
     add     rPC, rPC, #6                @ publish new rPC (advance 6 bytes)
-    str     r1, [rGLUE, #offGlue_methodClassDex]
+    str     r0, [rGLUE, #offGlue_methodClassDex]
     cmp     r8, #0                      @ check the suspendCount
     movne   r9, #0                      @ clear the chaining cell address
+    str     r9, [r3, #offThread_inJitCodeCache] @ in code cache or not
     cmp     r9, #0                      @ chaining cell exists?
     blxne   r9                          @ jump to the chaining cell
-    mov     pc, r0                      @ callsite is interpreted
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1                      @ callsite is interpreted
 1:
     stmia   rGLUE, {rPC, rFP}           @ SAVE_PC_FP_TO_GLUE()
     ldr     r2, .LdvmMterpStdBail       @ defined in footer.S
diff --git a/vm/compiler/template/armv5te/TemplateOpList.h b/vm/compiler/template/armv5te/TemplateOpList.h
index 39cd07a..e81383c 100644
--- a/vm/compiler/template/armv5te/TemplateOpList.h
+++ b/vm/compiler/template/armv5te/TemplateOpList.h
@@ -35,3 +35,10 @@ JIT_TEMPLATE(MUL_LONG)
 JIT_TEMPLATE(SHL_LONG)
 JIT_TEMPLATE(SHR_LONG)
 JIT_TEMPLATE(USHR_LONG)
+JIT_TEMPLATE(THROW_EXCEPTION_COMMON)
+JIT_TEMPLATE(MEM_OP_DECODE)
+JIT_TEMPLATE(STRING_COMPARETO)
+JIT_TEMPLATE(STRING_INDEXOF)
+JIT_TEMPLATE(INTERPRET)
+JIT_TEMPLATE(MONITOR_ENTER)
+JIT_TEMPLATE(MONITOR_ENTER_DEBUG)
diff --git a/vm/compiler/template/armv5te/footer.S b/vm/compiler/template/armv5te/footer.S
index 3b0b149..b93eee3 100644
--- a/vm/compiler/template/armv5te/footer.S
+++ b/vm/compiler/template/armv5te/footer.S
@@ -10,7 +10,9 @@
     @ Prep for the native call
     @ r1 = newFP, r0 = methodToCall
     ldr     r3, [rGLUE, #offGlue_self]      @ r3<- glue->self
+    mov     r2, #0
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in jit code cache
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
@@ -23,33 +25,71 @@
 
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+    @ Refresh Jit's on/off status
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable]
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r2, [r10, #offStackSaveArea_returnAddr] @ r2 = chaining cell ret
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved->top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+    ldr     r3, [r3]    @ r1 <- pointer to Jit profile table
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [r10, #offStackSaveArea_savedPc] @ reload rPC
+    str     r3, [rGLUE, #offGlue_pJitProfTable]  @ cache current JitProfTable
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
+
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the new mode
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
 
-/* NOTE - this path can be exercised if the JIT threshold is set to 5 */
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+/*
+ * On entry:
+ * r0  Faulting Dalvik PC
+ */
 .LhandleException:
-    ldr     r0, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
+#if defined(WITH_SELF_VERIFICATION)
+    ldr     pc, .LdeadFood @ should not see this under self-verification mode
+.LdeadFood:
+    .word   0xdeadf00d
+#endif
+    ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
+    mov     r2, #0
+    str     r2, [r3, #offThread_inJitCodeCache] @ in interpreter land
+    ldr     r1, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
     ldr     rIBASE, .LdvmAsmInstructionStart    @ same as above
-    ldr     rPC, [r10, #offStackSaveArea_savedPc] @ reload rPC
-    mov     pc, r0                  @ branch to dvmMterpCommonExceptionThrown
+    mov     rPC, r0                 @ reload the faulting Dalvik address
+    mov     pc, r1                  @ branch to dvmMterpCommonExceptionThrown
 
     .align  2
 .LdvmAsmInstructionStart:
     .word   dvmAsmInstructionStart
+.LdvmJitToInterpTraceSelectNoChain:
+    .word   dvmJitToInterpTraceSelectNoChain
 .LdvmJitToInterpNoChain:
     .word   dvmJitToInterpNoChain
 .LdvmMterpStdBail:
     .word   dvmMterpStdBail
 .LdvmMterpCommonExceptionThrown:
     .word   dvmMterpCommonExceptionThrown
+.LdvmLockObject:
+    .word   dvmLockObject
+#if defined(WITH_SELF_VERIFICATION)
+.LdvmSelfVerificationMemOpDecode:
+    .word   dvmSelfVerificationMemOpDecode
+#endif
 .L__aeabi_cdcmple:
     .word   __aeabi_cdcmple
 .L__aeabi_cfcmple:
diff --git a/vm/compiler/template/armv5te/header.S b/vm/compiler/template/armv5te/header.S
index 9651032..c257105 100644
--- a/vm/compiler/template/armv5te/header.S
+++ b/vm/compiler/template/armv5te/header.S
@@ -85,6 +85,9 @@ unspecified registers or condition codes.
 #define SAVEAREA_FROM_FP(_reg, _fpreg) \
     sub     _reg, _fpreg, #sizeofStackSaveArea
 
+#define EXPORT_PC() \
+    str     rPC, [rFP, #(-sizeofStackSaveArea + offStackSaveArea_currentPc)]
+
 /*
  * This is a #include, not a %include, because we want the C pre-processor
  * to expand the macros into assembler assignment statements.
diff --git a/vm/compiler/template/armv5te/platform.S b/vm/compiler/template/armv5te/platform.S
index b960a93..880e875 100644
--- a/vm/compiler/template/armv5te/platform.S
+++ b/vm/compiler/template/armv5te/platform.S
@@ -1,6 +1,6 @@
 /*
  * ===========================================================================
- *  CPU-version-specific defines
+ *  CPU-version-specific defines and utility
  * ===========================================================================
  */
 
diff --git a/vm/compiler/template/armv7-a/TemplateOpList.h b/vm/compiler/template/armv7-a/TemplateOpList.h
index c95163c..d991bed 100644
--- a/vm/compiler/template/armv7-a/TemplateOpList.h
+++ b/vm/compiler/template/armv7-a/TemplateOpList.h
@@ -50,3 +50,10 @@ JIT_TEMPLATE(CMPL_DOUBLE_VFP)
 JIT_TEMPLATE(CMPG_FLOAT_VFP)
 JIT_TEMPLATE(CMPL_FLOAT_VFP)
 JIT_TEMPLATE(SQRT_DOUBLE_VFP)
+JIT_TEMPLATE(THROW_EXCEPTION_COMMON)
+JIT_TEMPLATE(MEM_OP_DECODE)
+JIT_TEMPLATE(STRING_COMPARETO)
+JIT_TEMPLATE(STRING_INDEXOF)
+JIT_TEMPLATE(INTERPRET)
+JIT_TEMPLATE(MONITOR_ENTER)
+JIT_TEMPLATE(MONITOR_ENTER_DEBUG)
diff --git a/vm/compiler/template/config-armv5te-vfp b/vm/compiler/template/config-armv5te-vfp
index 628e75f..1b02261 100644
--- a/vm/compiler/template/config-armv5te-vfp
+++ b/vm/compiler/template/config-armv5te-vfp
@@ -25,7 +25,7 @@ import armv5te/header.S
 #import cstubs/stubdefs.c
 
 # highly-platform-specific defs
-import armv5te/platform.S
+import armv5te-vfp/platform.S
 
 # common defs for the C helpers; include this before the instruction handlers
 #import c/opcommon.c
@@ -42,6 +42,12 @@ op-start armv5te-vfp
     op TEMPLATE_SHL_LONG armv5te
     op TEMPLATE_SHR_LONG armv5te
     op TEMPLATE_USHR_LONG armv5te
+    op TEMPLATE_THROW_EXCEPTION_COMMON armv5te
+    op TEMPLATE_STRING_COMPARETO armv5te
+    op TEMPLATE_STRING_INDEXOF armv5te
+    op TEMPLATE_INTERPRET armv5te
+    op TEMPLATE_MONITOR_ENTER armv5te
+    op TEMPLATE_MONITOR_ENTER_DEBUG armv5te
 
 op-end
 
diff --git a/vm/compiler/template/config-armv7-a b/vm/compiler/template/config-armv7-a
index ecf0b3a..be7af31 100644
--- a/vm/compiler/template/config-armv7-a
+++ b/vm/compiler/template/config-armv7-a
@@ -25,7 +25,7 @@ import armv5te/header.S
 #import cstubs/stubdefs.c
 
 # highly-platform-specific defs
-import armv5te/platform.S
+import armv5te-vfp/platform.S
 
 # common defs for the C helpers; include this before the instruction handlers
 #import c/opcommon.c
@@ -42,7 +42,12 @@ op-start armv5te-vfp
     op TEMPLATE_SHL_LONG armv5te
     op TEMPLATE_SHR_LONG armv5te
     op TEMPLATE_USHR_LONG armv5te
-
+    op TEMPLATE_THROW_EXCEPTION_COMMON armv5te
+    op TEMPLATE_STRING_COMPARETO armv5te
+    op TEMPLATE_STRING_INDEXOF armv5te
+    op TEMPLATE_INTERPRET armv5te
+    op TEMPLATE_MONITOR_ENTER armv5te
+    op TEMPLATE_MONITOR_ENTER_DEBUG armv5te
 op-end
 
 # "helper" code for C; include if you use any of the C stubs (this generates
diff --git a/vm/compiler/template/out/CompilerTemplateAsm-armv5te-vfp.S b/vm/compiler/template/out/CompilerTemplateAsm-armv5te-vfp.S
index 0c433a1..d8a2784 100644
--- a/vm/compiler/template/out/CompilerTemplateAsm-armv5te-vfp.S
+++ b/vm/compiler/template/out/CompilerTemplateAsm-armv5te-vfp.S
@@ -92,6 +92,9 @@ unspecified registers or condition codes.
 #define SAVEAREA_FROM_FP(_reg, _fpreg) \
     sub     _reg, _fpreg, #sizeofStackSaveArea
 
+#define EXPORT_PC() \
+    str     rPC, [rFP, #(-sizeofStackSaveArea + offStackSaveArea_currentPc)]
+
 /*
  * This is a #include, not a %include, because we want the C pre-processor
  * to expand the macros into assembler assignment statements.
@@ -99,10 +102,10 @@ unspecified registers or condition codes.
 #include "../../../mterp/common/asm-constants.h"
 
 
-/* File: armv5te/platform.S */
+/* File: armv5te-vfp/platform.S */
 /*
  * ===========================================================================
- *  CPU-version-specific defines
+ *  CPU-version-specific defines and utility
  * ===========================================================================
  */
 
@@ -180,27 +183,39 @@ dvmCompiler_TEMPLATE_RETURN:
     ldr     r10, [r0, #offStackSaveArea_prevFrame] @ r10<- saveArea->prevFrame
     ldr     r8, [rGLUE, #offGlue_pSelfSuspendCount] @ r8<- &suspendCount
     ldr     rPC, [r0, #offStackSaveArea_savedPc] @ rPC<- saveArea->savedPc
+#if !defined(WITH_SELF_VERIFICATION)
     ldr     r9,  [r0, #offStackSaveArea_returnAddr] @ r9<- chaining cell ret
+#else
+    mov     r9, #0                      @ disable chaining
+#endif
     ldr     r2, [r10, #(offStackSaveArea_method - sizeofStackSaveArea)]
                                         @ r2<- method we're returning to
     ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
     cmp     r2, #0                      @ break frame?
+#if !defined(WITH_SELF_VERIFICATION)
     beq     1f                          @ bail to interpreter
-    ldr     r0, .LdvmJitToInterpNoChain @ defined in footer.S
+#else
+    blxeq   lr                          @ punt to interpreter and compare state
+#endif
+    ldr     r1, .LdvmJitToInterpNoChain @ defined in footer.S
     mov     rFP, r10                    @ publish new FP
     ldrne   r10, [r2, #offMethod_clazz] @ r10<- method->clazz
     ldr     r8, [r8]                    @ r8<- suspendCount
 
     str     r2, [rGLUE, #offGlue_method]@ glue->method = newSave->method
-    ldr     r1, [r10, #offClassObject_pDvmDex] @ r1<- method->clazz->pDvmDex
+    ldr     r0, [r10, #offClassObject_pDvmDex] @ r0<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame] @ self->curFrame = fp
     add     rPC, rPC, #6                @ publish new rPC (advance 6 bytes)
-    str     r1, [rGLUE, #offGlue_methodClassDex]
+    str     r0, [rGLUE, #offGlue_methodClassDex]
     cmp     r8, #0                      @ check the suspendCount
     movne   r9, #0                      @ clear the chaining cell address
+    str     r9, [r3, #offThread_inJitCodeCache] @ in code cache or not
     cmp     r9, #0                      @ chaining cell exists?
     blxne   r9                          @ jump to the chaining cell
-    mov     pc, r0                      @ callsite is interpreted
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1                      @ callsite is interpreted
 1:
     stmia   rGLUE, {rPC, rFP}           @ SAVE_PC_FP_TO_GLUE()
     ldr     r2, .LdvmMterpStdBail       @ defined in footer.S
@@ -228,7 +243,7 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     sub     r1, r1, r7, lsl #2          @ r1<- newFp (old savearea - regsSize)
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    lr                          @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -246,9 +261,13 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     cmp     r8, #0                      @ suspendCount != 0
     bxne    lr                          @ bail to the interpreter
     tst     r10, #ACC_NATIVE
+#if !defined(WITH_SELF_VERIFICATION)
     bne     .LinvokeNative
+#else
+    bxne    lr                          @ bail to the interpreter
+#endif
 
-    ldr     r10, .LdvmJitToInterpNoChain
+    ldr     r10, .LdvmJitToInterpTraceSelectNoChain
     ldr     r3, [r9, #offClassObject_pDvmDex] @ r3<- method->clazz->pDvmDex
     ldr     r2, [rGLUE, #offGlue_self]      @ r2<- glue->self
 
@@ -259,7 +278,10 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     str     rFP, [r2, #offThread_curFrame]  @ self->curFrame = newFp
 
     @ Start executing the callee
-    mov     pc, r10                         @ dvmJitToInterpNoChain
+#if defined(JIT_STATS)
+    mov     r0, #kInlineCacheMiss
+#endif
+    mov     pc, r10                         @ dvmJitToInterpTraceSelectNoChain
 
 /* ------------------------------ */
     .balign 4
@@ -284,7 +306,7 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_CHAIN:
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     add     r12, lr, #2                 @ setup the punt-to-interp address
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    r12                         @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -391,11 +413,17 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NATIVE:
     str     r0, [r1, #(offStackSaveArea_method - sizeofStackSaveArea)]
     cmp     r8, #0                      @ suspendCount != 0
     ldr     r8, [r0, #offMethod_nativeFunc] @ r8<- method->nativeFunc
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    lr                          @ bail to the interpreter
+#else
+    bx      lr                          @ bail to interpreter unconditionally
+#endif
 
     @ go ahead and transfer control to the native code
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    mov     r2, #0
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in the jit code cache
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
     mov     r9, r3                      @ r9<- glue->self (preserve)
@@ -415,8 +443,24 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NATIVE:
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [rFP, #(offStackSaveArea_currentPc - sizeofStackSaveArea)]
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
+
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the mode properly
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+
 
 
 /* ------------------------------ */
@@ -860,7 +904,7 @@ dvmCompiler_TEMPLATE_CMPG_DOUBLE_VFP:
     /* op vAA, vBB, vCC */
     fldd    d0, [r0]                    @ d0<- vBB
     fldd    d1, [r1]                    @ d1<- vCC
-    fcmped  d0, d1                      @ compare (vBB, vCC)
+    fcmpd  d0, d1                       @ compare (vBB, vCC)
     mov     r0, #1                      @ r0<- 1 (default)
     fmstat                              @ export status flags
     mvnmi   r0, #0                      @ (less than) r0<- -1
@@ -929,7 +973,7 @@ dvmCompiler_TEMPLATE_CMPG_FLOAT_VFP:
     /* op vAA, vBB, vCC */
     flds    s0, [r0]                    @ d0<- vBB
     flds    s1, [r1]                    @ d1<- vCC
-    fcmpes  s0, s1                      @ compare (vBB, vCC)
+    fcmps  s0, s1                      @ compare (vBB, vCC)
     mov     r0, #1                      @ r0<- 1 (default)
     fmstat                              @ export status flags
     mvnmi   r0, #0                      @ (less than) r0<- -1
@@ -963,7 +1007,7 @@ dvmCompiler_TEMPLATE_CMPL_FLOAT_VFP:
     /* op vAA, vBB, vCC */
     flds    s0, [r0]                    @ d0<- vBB
     flds    s1, [r1]                    @ d1<- vCC
-    fcmpes  s0, s1                      @ compare (vBB, vCC)
+    fcmps  s0, s1                      @ compare (vBB, vCC)
     mvn     r0, #0                      @ r0<- -1 (default)
     fmstat                              @ export status flags
     movgt   r0, #1                      @ (greater than) r0<- 1
@@ -998,6 +1042,401 @@ dvmCompiler_TEMPLATE_SQRT_DOUBLE_VFP:
 .Lsqrt:
     .word   sqrt
 
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_THROW_EXCEPTION_COMMON
+dvmCompiler_TEMPLATE_THROW_EXCEPTION_COMMON:
+/* File: armv5te/TEMPLATE_THROW_EXCEPTION_COMMON.S */
+    /*
+     * Throw an exception from JIT'ed code.
+     * On entry:
+     *    r0    Dalvik PC that raises the exception
+     */
+    b       .LhandleException
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MEM_OP_DECODE
+dvmCompiler_TEMPLATE_MEM_OP_DECODE:
+/* File: armv5te-vfp/TEMPLATE_MEM_OP_DECODE.S */
+#if defined(WITH_SELF_VERIFICATION)
+    /*
+     * This handler encapsulates heap memory ops for selfVerification mode.
+     *
+     * The call to the handler is inserted prior to a heap memory operation.
+     * This handler then calls a function to decode the memory op, and process
+     * it accordingly. Afterwards, the handler changes the return address to
+     * skip the memory op so it never gets executed.
+     */
+    vpush   {d0-d15}                    @ save out all fp registers
+    push    {r0-r12,lr}                 @ save out all registers
+    mov     r0, lr                      @ arg0 <- link register
+    mov     r1, sp                      @ arg1 <- stack pointer
+    ldr     r2, .LdvmSelfVerificationMemOpDecode @ defined in footer.S
+    blx     r2                          @ decode and handle the mem op
+    pop     {r0-r12,lr}                 @ restore all registers
+    vpop    {d0-d15}                    @ restore all fp registers
+    bx      lr                          @ return to compiled code
+#endif
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_STRING_COMPARETO
+dvmCompiler_TEMPLATE_STRING_COMPARETO:
+/* File: armv5te/TEMPLATE_STRING_COMPARETO.S */
+    /*
+     * String's compareTo.
+     *
+     * Requires r0/r1 to have been previously checked for null.  Will
+     * return negative if this's string is < comp, 0 if they are the
+     * same and positive if >.
+     *
+     * IMPORTANT NOTE:
+     *
+     * This code relies on hard-coded offsets for string objects, and must be
+     * kept in sync with definitions in UtfString.h.  See asm-constants.h
+     *
+     * On entry:
+     *    r0:   this object pointer
+     *    r1:   comp object pointer
+     *
+     */
+
+    mov    r2, r0         @ this to r2, opening up r0 for return value
+    subs   r0, r2, r1     @ Same?
+    bxeq   lr
+
+    ldr    r4, [r2, #STRING_FIELDOFF_OFFSET]
+    ldr    r9, [r1, #STRING_FIELDOFF_OFFSET]
+    ldr    r7, [r2, #STRING_FIELDOFF_COUNT]
+    ldr    r10, [r1, #STRING_FIELDOFF_COUNT]
+    ldr    r2, [r2, #STRING_FIELDOFF_VALUE]
+    ldr    r1, [r1, #STRING_FIELDOFF_VALUE]
+
+    /*
+     * At this point, we have:
+     *    value:  r2/r1
+     *    offset: r4/r9
+     *    count:  r7/r10
+     * We're going to compute
+     *    r11 <- countDiff
+     *    r10 <- minCount
+     */
+     subs  r11, r7, r10
+     movls r10, r7
+
+     /* Now, build pointers to the string data */
+     add   r2, r2, r4, lsl #1
+     add   r1, r1, r9, lsl #1
+     /*
+      * Note: data pointers point to previous element so we can use pre-index
+      * mode with base writeback.
+      */
+     add   r2, #16-2   @ offset to contents[-1]
+     add   r1, #16-2   @ offset to contents[-1]
+
+     /*
+      * At this point we have:
+      *   r2: *this string data
+      *   r1: *comp string data
+      *   r10: iteration count for comparison
+      *   r11: value to return if the first part of the string is equal
+      *   r0: reserved for result
+      *   r3, r4, r7, r8, r9, r12 available for loading string data
+      */
+
+    subs  r10, #2
+    blt   do_remainder2
+
+      /*
+       * Unroll the first two checks so we can quickly catch early mismatch
+       * on long strings (but preserve incoming alignment)
+       */
+
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    ldrh  r7, [r2, #2]!
+    ldrh  r8, [r1, #2]!
+    subs  r0, r3, r4
+    subeqs  r0, r7, r8
+    bxne  lr
+    cmp   r10, #28
+    bgt   do_memcmp16
+    subs  r10, #3
+    blt   do_remainder
+
+loopback_triple:
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    ldrh  r7, [r2, #2]!
+    ldrh  r8, [r1, #2]!
+    ldrh  r9, [r2, #2]!
+    ldrh  r12,[r1, #2]!
+    subs  r0, r3, r4
+    subeqs  r0, r7, r8
+    subeqs  r0, r9, r12
+    bxne  lr
+    subs  r10, #3
+    bge   loopback_triple
+
+do_remainder:
+    adds  r10, #3
+    beq   returnDiff
+
+loopback_single:
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    subs  r0, r3, r4
+    bxne  lr
+    subs  r10, #1
+    bne     loopback_single
+
+returnDiff:
+    mov   r0, r11
+    bx    lr
+
+do_remainder2:
+    adds  r10, #2
+    bne   loopback_single
+    mov   r0, r11
+    bx    lr
+
+    /* Long string case */
+do_memcmp16:
+    mov   r4, lr
+    ldr   lr, .Lmemcmp16
+    mov   r7, r11
+    add   r0, r2, #2
+    add   r1, r1, #2
+    mov   r2, r10
+    blx   lr
+    cmp   r0, #0
+    bxne  r4
+    mov   r0, r7
+    bx    r4
+
+.Lmemcmp16:
+    .word __memcmp16
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_STRING_INDEXOF
+dvmCompiler_TEMPLATE_STRING_INDEXOF:
+/* File: armv5te/TEMPLATE_STRING_INDEXOF.S */
+    /*
+     * String's indexOf.
+     *
+     * Requires r0 to have been previously checked for null.  Will
+     * return index of match of r1 in r0.
+     *
+     * IMPORTANT NOTE:
+     *
+     * This code relies on hard-coded offsets for string objects, and must be
+     * kept in sync wth definitions in UtfString.h  See asm-constants.h
+     *
+     * On entry:
+     *    r0:   string object pointer
+     *    r1:   char to match
+     *    r2:   Starting offset in string data
+     */
+
+    ldr    r7, [r0, #STRING_FIELDOFF_OFFSET]
+    ldr    r8, [r0, #STRING_FIELDOFF_COUNT]
+    ldr    r0, [r0, #STRING_FIELDOFF_VALUE]
+
+    /*
+     * At this point, we have:
+     *    r0: object pointer
+     *    r1: char to match
+     *    r2: starting offset
+     *    r7: offset
+     *    r8: string length
+     */
+
+     /* Build pointer to start of string data */
+     add   r0, #16
+     add   r0, r0, r7, lsl #1
+
+     /* Save a copy of starting data in r7 */
+     mov   r7, r0
+
+     /* Clamp start to [0..count] */
+     cmp   r2, #0
+     movlt r2, #0
+     cmp   r2, r8
+     movgt r2, r8
+
+     /* Build pointer to start of data to compare and pre-bias */
+     add   r0, r0, r2, lsl #1
+     sub   r0, #2
+
+     /* Compute iteration count */
+     sub   r8, r2
+
+     /*
+      * At this point we have:
+      *   r0: start of data to test
+      *   r1: chat to compare
+      *   r8: iteration count
+      *   r7: original start of string
+      *   r3, r4, r9, r10, r11, r12 available for loading string data
+      */
+
+    subs  r8, #4
+    blt   indexof_remainder
+
+indexof_loop4:
+    ldrh  r3, [r0, #2]!
+    ldrh  r4, [r0, #2]!
+    ldrh  r10, [r0, #2]!
+    ldrh  r11, [r0, #2]!
+    cmp   r3, r1
+    beq   match_0
+    cmp   r4, r1
+    beq   match_1
+    cmp   r10, r1
+    beq   match_2
+    cmp   r11, r1
+    beq   match_3
+    subs  r8, #4
+    bge   indexof_loop4
+
+indexof_remainder:
+    adds    r8, #4
+    beq     indexof_nomatch
+
+indexof_loop1:
+    ldrh  r3, [r0, #2]!
+    cmp   r3, r1
+    beq   match_3
+    subs  r8, #1
+    bne   indexof_loop1
+
+indexof_nomatch:
+    mov   r0, #-1
+    bx    lr
+
+match_0:
+    sub   r0, #6
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_1:
+    sub   r0, #4
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_2:
+    sub   r0, #2
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_3:
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_INTERPRET
+dvmCompiler_TEMPLATE_INTERPRET:
+/* File: armv5te/TEMPLATE_INTERPRET.S */
+    /*
+     * This handler transfers control to the interpeter without performing
+     * any lookups.  It may be called either as part of a normal chaining
+     * operation, or from the transition code in header.S.  We distinquish
+     * the two cases by looking at the link register.  If called from a
+     * translation chain, it will point to the chaining Dalvik PC + 1.
+     * On entry:
+     *    lr - if NULL:
+     *        r1 - the Dalvik PC to begin interpretation.
+     *    else
+     *        [lr, #-1] contains Dalvik PC to begin interpretation
+     *    rGLUE - pointer to interpState
+     *    rFP - Dalvik frame pointer
+     */
+    cmp     lr, #0
+    ldrne   r1,[lr, #-1]
+    ldr     r2, .LinterpPunt
+    mov     r0, r1                       @ set Dalvik PC
+    bx      r2
+    @ doesn't return
+
+.LinterpPunt:
+    .word   dvmJitToInterpPunt
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MONITOR_ENTER
+dvmCompiler_TEMPLATE_MONITOR_ENTER:
+/* File: armv5te/TEMPLATE_MONITOR_ENTER.S */
+    /*
+     * Call out to the runtime to lock an object.  Because this thread
+     * may have been suspended in THREAD_MONITOR state and the Jit's
+     * translation cache subsequently cleared, we cannot return directly.
+     * Instead, unconditionally transition to the interpreter to resume.
+     *
+     * On entry:
+     *    r0 - self pointer
+     *    r1 - the object (which has already been null-checked by the caller
+     *    r4 - the Dalvik PC of the following instruction.
+     */
+    ldr     r2, .LdvmLockObject
+    mov     r3, #0                       @ Record that we're not returning
+    str     r3, [r0, #offThread_inJitCodeCache]
+    blx     r2                           @ dvmLockObject(self, obj)
+    @ refresh Jit's on/off status
+    ldr     r0, [rGLUE, #offGlue_ppJitProfTable]
+    ldr     r0, [r0]
+    ldr     r2, .LdvmJitToInterpNoChain
+    str     r0, [rGLUE, #offGlue_pJitProfTable]
+    @ Bail to interpreter - no chain [note - r4 still contains rPC]
+#if defined(JIT_STATS)
+    mov     r0, #kHeavyweightMonitor
+#endif
+    bx      r2
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MONITOR_ENTER_DEBUG
+dvmCompiler_TEMPLATE_MONITOR_ENTER_DEBUG:
+/* File: armv5te/TEMPLATE_MONITOR_ENTER_DEBUG.S */
+    /*
+     * To support deadlock prediction, this version of MONITOR_ENTER
+     * will always call the heavyweight dvmLockObject, check for an
+     * exception and then bail out to the interpreter.
+     *
+     * On entry:
+     *    r0 - self pointer
+     *    r1 - the object (which has already been null-checked by the caller
+     *    r4 - the Dalvik PC of the following instruction.
+     *
+     */
+    ldr     r2, .LdvmLockObject
+    mov     r3, #0                       @ Record that we're not returning
+    str     r3, [r0, #offThread_inJitCodeCache]
+    blx     r2             @ dvmLockObject(self, obj)
+    @ refresh Jit's on/off status & test for exception
+    ldr     r0, [rGLUE, #offGlue_ppJitProfTable]
+    ldr     r1, [rGLUE, #offGlue_self]
+    ldr     r0, [r0]
+    ldr     r1, [r1, #offThread_exception]
+    str     r0, [rGLUE, #offGlue_pJitProfTable]
+    cmp     r1, #0
+    beq     1f
+    ldr     r2, .LhandleException
+    sub     r0, r4, #2     @ roll dPC back to this monitor instruction
+    bx      r2
+1:
+    @ Bail to interpreter - no chain [note - r4 still contains rPC]
+#if defined(JIT_STATS)
+    mov     r0, #kHeavyweightMonitor
+#endif
+    ldr     pc, .LdvmJitToInterpNoChain
+
     .size   dvmCompilerTemplateStart, .-dvmCompilerTemplateStart
 /* File: armv5te/footer.S */
 /*
@@ -1012,7 +1451,9 @@ dvmCompiler_TEMPLATE_SQRT_DOUBLE_VFP:
     @ Prep for the native call
     @ r1 = newFP, r0 = methodToCall
     ldr     r3, [rGLUE, #offGlue_self]      @ r3<- glue->self
+    mov     r2, #0
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in jit code cache
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
@@ -1025,33 +1466,71 @@ dvmCompiler_TEMPLATE_SQRT_DOUBLE_VFP:
 
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+    @ Refresh Jit's on/off status
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable]
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r2, [r10, #offStackSaveArea_returnAddr] @ r2 = chaining cell ret
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved->top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+    ldr     r3, [r3]    @ r1 <- pointer to Jit profile table
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [r10, #offStackSaveArea_savedPc] @ reload rPC
+    str     r3, [rGLUE, #offGlue_pJitProfTable]  @ cache current JitProfTable
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
 
-/* NOTE - this path can be exercised if the JIT threshold is set to 5 */
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the new mode
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+/*
+ * On entry:
+ * r0  Faulting Dalvik PC
+ */
 .LhandleException:
-    ldr     r0, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
+#if defined(WITH_SELF_VERIFICATION)
+    ldr     pc, .LdeadFood @ should not see this under self-verification mode
+.LdeadFood:
+    .word   0xdeadf00d
+#endif
+    ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
+    mov     r2, #0
+    str     r2, [r3, #offThread_inJitCodeCache] @ in interpreter land
+    ldr     r1, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
     ldr     rIBASE, .LdvmAsmInstructionStart    @ same as above
-    ldr     rPC, [r10, #offStackSaveArea_savedPc] @ reload rPC
-    mov     pc, r0                  @ branch to dvmMterpCommonExceptionThrown
+    mov     rPC, r0                 @ reload the faulting Dalvik address
+    mov     pc, r1                  @ branch to dvmMterpCommonExceptionThrown
 
     .align  2
 .LdvmAsmInstructionStart:
     .word   dvmAsmInstructionStart
+.LdvmJitToInterpTraceSelectNoChain:
+    .word   dvmJitToInterpTraceSelectNoChain
 .LdvmJitToInterpNoChain:
     .word   dvmJitToInterpNoChain
 .LdvmMterpStdBail:
     .word   dvmMterpStdBail
 .LdvmMterpCommonExceptionThrown:
     .word   dvmMterpCommonExceptionThrown
+.LdvmLockObject:
+    .word   dvmLockObject
+#if defined(WITH_SELF_VERIFICATION)
+.LdvmSelfVerificationMemOpDecode:
+    .word   dvmSelfVerificationMemOpDecode
+#endif
 .L__aeabi_cdcmple:
     .word   __aeabi_cdcmple
 .L__aeabi_cfcmple:
diff --git a/vm/compiler/template/out/CompilerTemplateAsm-armv5te.S b/vm/compiler/template/out/CompilerTemplateAsm-armv5te.S
index 29dde74..eab49cf 100644
--- a/vm/compiler/template/out/CompilerTemplateAsm-armv5te.S
+++ b/vm/compiler/template/out/CompilerTemplateAsm-armv5te.S
@@ -92,6 +92,9 @@ unspecified registers or condition codes.
 #define SAVEAREA_FROM_FP(_reg, _fpreg) \
     sub     _reg, _fpreg, #sizeofStackSaveArea
 
+#define EXPORT_PC() \
+    str     rPC, [rFP, #(-sizeofStackSaveArea + offStackSaveArea_currentPc)]
+
 /*
  * This is a #include, not a %include, because we want the C pre-processor
  * to expand the macros into assembler assignment statements.
@@ -102,7 +105,7 @@ unspecified registers or condition codes.
 /* File: armv5te/platform.S */
 /*
  * ===========================================================================
- *  CPU-version-specific defines
+ *  CPU-version-specific defines and utility
  * ===========================================================================
  */
 
@@ -180,27 +183,39 @@ dvmCompiler_TEMPLATE_RETURN:
     ldr     r10, [r0, #offStackSaveArea_prevFrame] @ r10<- saveArea->prevFrame
     ldr     r8, [rGLUE, #offGlue_pSelfSuspendCount] @ r8<- &suspendCount
     ldr     rPC, [r0, #offStackSaveArea_savedPc] @ rPC<- saveArea->savedPc
+#if !defined(WITH_SELF_VERIFICATION)
     ldr     r9,  [r0, #offStackSaveArea_returnAddr] @ r9<- chaining cell ret
+#else
+    mov     r9, #0                      @ disable chaining
+#endif
     ldr     r2, [r10, #(offStackSaveArea_method - sizeofStackSaveArea)]
                                         @ r2<- method we're returning to
     ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
     cmp     r2, #0                      @ break frame?
+#if !defined(WITH_SELF_VERIFICATION)
     beq     1f                          @ bail to interpreter
-    ldr     r0, .LdvmJitToInterpNoChain @ defined in footer.S
+#else
+    blxeq   lr                          @ punt to interpreter and compare state
+#endif
+    ldr     r1, .LdvmJitToInterpNoChain @ defined in footer.S
     mov     rFP, r10                    @ publish new FP
     ldrne   r10, [r2, #offMethod_clazz] @ r10<- method->clazz
     ldr     r8, [r8]                    @ r8<- suspendCount
 
     str     r2, [rGLUE, #offGlue_method]@ glue->method = newSave->method
-    ldr     r1, [r10, #offClassObject_pDvmDex] @ r1<- method->clazz->pDvmDex
+    ldr     r0, [r10, #offClassObject_pDvmDex] @ r0<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame] @ self->curFrame = fp
     add     rPC, rPC, #6                @ publish new rPC (advance 6 bytes)
-    str     r1, [rGLUE, #offGlue_methodClassDex]
+    str     r0, [rGLUE, #offGlue_methodClassDex]
     cmp     r8, #0                      @ check the suspendCount
     movne   r9, #0                      @ clear the chaining cell address
+    str     r9, [r3, #offThread_inJitCodeCache] @ in code cache or not
     cmp     r9, #0                      @ chaining cell exists?
     blxne   r9                          @ jump to the chaining cell
-    mov     pc, r0                      @ callsite is interpreted
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1                      @ callsite is interpreted
 1:
     stmia   rGLUE, {rPC, rFP}           @ SAVE_PC_FP_TO_GLUE()
     ldr     r2, .LdvmMterpStdBail       @ defined in footer.S
@@ -228,7 +243,7 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     sub     r1, r1, r7, lsl #2          @ r1<- newFp (old savearea - regsSize)
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    lr                          @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -246,9 +261,13 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     cmp     r8, #0                      @ suspendCount != 0
     bxne    lr                          @ bail to the interpreter
     tst     r10, #ACC_NATIVE
+#if !defined(WITH_SELF_VERIFICATION)
     bne     .LinvokeNative
+#else
+    bxne    lr                          @ bail to the interpreter
+#endif
 
-    ldr     r10, .LdvmJitToInterpNoChain
+    ldr     r10, .LdvmJitToInterpTraceSelectNoChain
     ldr     r3, [r9, #offClassObject_pDvmDex] @ r3<- method->clazz->pDvmDex
     ldr     r2, [rGLUE, #offGlue_self]      @ r2<- glue->self
 
@@ -259,7 +278,10 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     str     rFP, [r2, #offThread_curFrame]  @ self->curFrame = newFp
 
     @ Start executing the callee
-    mov     pc, r10                         @ dvmJitToInterpNoChain
+#if defined(JIT_STATS)
+    mov     r0, #kInlineCacheMiss
+#endif
+    mov     pc, r10                         @ dvmJitToInterpTraceSelectNoChain
 
 /* ------------------------------ */
     .balign 4
@@ -284,7 +306,7 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_CHAIN:
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     add     r12, lr, #2                 @ setup the punt-to-interp address
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    r12                         @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -391,11 +413,17 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NATIVE:
     str     r0, [r1, #(offStackSaveArea_method - sizeofStackSaveArea)]
     cmp     r8, #0                      @ suspendCount != 0
     ldr     r8, [r0, #offMethod_nativeFunc] @ r8<- method->nativeFunc
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    lr                          @ bail to the interpreter
+#else
+    bx      lr                          @ bail to interpreter unconditionally
+#endif
 
     @ go ahead and transfer control to the native code
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    mov     r2, #0
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in the jit code cache
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
     mov     r9, r3                      @ r9<- glue->self (preserve)
@@ -415,8 +443,24 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NATIVE:
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [rFP, #(offStackSaveArea_currentPc - sizeofStackSaveArea)]
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
+
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the mode properly
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+
 
 
 /* ------------------------------ */
@@ -426,7 +470,7 @@ dvmCompiler_TEMPLATE_CMPG_DOUBLE:
 /* File: armv5te/TEMPLATE_CMPG_DOUBLE.S */
 /* File: armv5te/TEMPLATE_CMPL_DOUBLE.S */
     /*
-     * For the JIT: incoming arguments are pointers to the arguments in r0/r1
+     * For the JIT: incoming arguments in r0-r1, r2-r3
      *              result in r0
      *
      * Compare two floating-point values.  Puts 0, 1, or -1 into the
@@ -440,28 +484,26 @@ dvmCompiler_TEMPLATE_CMPG_DOUBLE:
      * For: cmpl-double, cmpg-double
      */
     /* op vAA, vBB, vCC */
-    mov     r4, lr                      @ save return address
-    mov     r9, r0                      @ save copy of &arg1
-    mov     r10, r1                     @ save copy of &arg2
-    ldmia   r9, {r0-r1}                 @ r0/r1<- vBB/vBB+1
-    ldmia   r10, {r2-r3}                @ r2/r3<- vCC/vCC+1
+    push    {r0-r3}                     @ save operands
+    mov     r11, lr                     @ save return address
     LDR_PC_LR ".L__aeabi_cdcmple"       @ PIC way of "bl __aeabi_cdcmple"
     bhi     .LTEMPLATE_CMPG_DOUBLE_gt_or_nan       @ C set and Z clear, disambiguate
     mvncc   r0, #0                      @ (less than) r1<- -1
     moveq   r0, #0                      @ (equal) r1<- 0, trumps less than
-    bx      r4
+    add     sp, #16                     @ drop unused operands
+    bx      r11
 
     @ Test for NaN with a second comparison.  EABI forbids testing bit
     @ patterns, and we can't represent 0x7fc00000 in immediate form, so
     @ make the library call.
 .LTEMPLATE_CMPG_DOUBLE_gt_or_nan:
-    ldmia   r10, {r0-r1}                @ reverse order
-    ldmia   r9, {r2-r3}
+    pop     {r2-r3}                     @ restore operands in reverse order
+    pop     {r0-r1}                     @ restore operands in reverse order
     LDR_PC_LR ".L__aeabi_cdcmple"       @ r0<- Z set if eq, C clear if <
     movcc   r0, #1                      @ (greater than) r1<- 1
-    bxcc    r4
+    bxcc    r11
     mov     r0, #1                            @ r1<- 1 or -1 for NaN
-    bx      r4
+    bx      r11
 
 
 
@@ -471,7 +513,7 @@ dvmCompiler_TEMPLATE_CMPG_DOUBLE:
 dvmCompiler_TEMPLATE_CMPL_DOUBLE:
 /* File: armv5te/TEMPLATE_CMPL_DOUBLE.S */
     /*
-     * For the JIT: incoming arguments are pointers to the arguments in r0/r1
+     * For the JIT: incoming arguments in r0-r1, r2-r3
      *              result in r0
      *
      * Compare two floating-point values.  Puts 0, 1, or -1 into the
@@ -485,28 +527,26 @@ dvmCompiler_TEMPLATE_CMPL_DOUBLE:
      * For: cmpl-double, cmpg-double
      */
     /* op vAA, vBB, vCC */
-    mov     r4, lr                      @ save return address
-    mov     r9, r0                      @ save copy of &arg1
-    mov     r10, r1                     @ save copy of &arg2
-    ldmia   r9, {r0-r1}                 @ r0/r1<- vBB/vBB+1
-    ldmia   r10, {r2-r3}                @ r2/r3<- vCC/vCC+1
+    push    {r0-r3}                     @ save operands
+    mov     r11, lr                     @ save return address
     LDR_PC_LR ".L__aeabi_cdcmple"       @ PIC way of "bl __aeabi_cdcmple"
     bhi     .LTEMPLATE_CMPL_DOUBLE_gt_or_nan       @ C set and Z clear, disambiguate
     mvncc   r0, #0                      @ (less than) r1<- -1
     moveq   r0, #0                      @ (equal) r1<- 0, trumps less than
-    bx      r4
+    add     sp, #16                     @ drop unused operands
+    bx      r11
 
     @ Test for NaN with a second comparison.  EABI forbids testing bit
     @ patterns, and we can't represent 0x7fc00000 in immediate form, so
     @ make the library call.
 .LTEMPLATE_CMPL_DOUBLE_gt_or_nan:
-    ldmia   r10, {r0-r1}                @ reverse order
-    ldmia   r9, {r2-r3}
+    pop     {r2-r3}                     @ restore operands in reverse order
+    pop     {r0-r1}                     @ restore operands in reverse order
     LDR_PC_LR ".L__aeabi_cdcmple"       @ r0<- Z set if eq, C clear if <
     movcc   r0, #1                      @ (greater than) r1<- 1
-    bxcc    r4
+    bxcc    r11
     mvn     r0, #0                            @ r1<- 1 or -1 for NaN
-    bx      r4
+    bx      r11
 
 
 /* ------------------------------ */
@@ -516,7 +556,7 @@ dvmCompiler_TEMPLATE_CMPG_FLOAT:
 /* File: armv5te/TEMPLATE_CMPG_FLOAT.S */
 /* File: armv5te/TEMPLATE_CMPL_FLOAT.S */
     /*
-     * For the JIT: incoming arguments in r0, r1
+     * For the JIT: incoming arguments in r0-r1, r2-r3
      *              result in r0
      *
      * Compare two floating-point values.  Puts 0, 1, or -1 into the
@@ -549,25 +589,25 @@ dvmCompiler_TEMPLATE_CMPG_FLOAT:
      * for: cmpl-float, cmpg-float
      */
     /* op vAA, vBB, vCC */
-    mov     r4, lr                      @ save return address
     mov     r9, r0                      @ Save copies - we may need to redo
     mov     r10, r1
+    mov     r11, lr                     @ save return address
     LDR_PC_LR ".L__aeabi_cfcmple"       @ cmp <=: C clear if <, Z set if eq
     bhi     .LTEMPLATE_CMPG_FLOAT_gt_or_nan       @ C set and Z clear, disambiguate
     mvncc   r0, #0                      @ (less than) r0<- -1
     moveq   r0, #0                      @ (equal) r0<- 0, trumps less than
-    bx      r4
+    bx      r11
     @ Test for NaN with a second comparison.  EABI forbids testing bit
     @ patterns, and we can't represent 0x7fc00000 in immediate form, so
     @ make the library call.
 .LTEMPLATE_CMPG_FLOAT_gt_or_nan:
-    mov     r1, r9                      @ reverse order
-    mov     r0, r10
+    mov     r0, r10                     @ restore in reverse order
+    mov     r1, r9
     LDR_PC_LR ".L__aeabi_cfcmple"       @ r0<- Z set if eq, C clear if <
     movcc   r0, #1                      @ (greater than) r1<- 1
-    bxcc    r4
+    bxcc    r11
     mov     r0, #1                            @ r1<- 1 or -1 for NaN
-    bx      r4
+    bx      r11
 
 
 
@@ -578,7 +618,7 @@ dvmCompiler_TEMPLATE_CMPG_FLOAT:
 dvmCompiler_TEMPLATE_CMPL_FLOAT:
 /* File: armv5te/TEMPLATE_CMPL_FLOAT.S */
     /*
-     * For the JIT: incoming arguments in r0, r1
+     * For the JIT: incoming arguments in r0-r1, r2-r3
      *              result in r0
      *
      * Compare two floating-point values.  Puts 0, 1, or -1 into the
@@ -611,25 +651,25 @@ dvmCompiler_TEMPLATE_CMPL_FLOAT:
      * for: cmpl-float, cmpg-float
      */
     /* op vAA, vBB, vCC */
-    mov     r4, lr                      @ save return address
     mov     r9, r0                      @ Save copies - we may need to redo
     mov     r10, r1
+    mov     r11, lr                     @ save return address
     LDR_PC_LR ".L__aeabi_cfcmple"       @ cmp <=: C clear if <, Z set if eq
     bhi     .LTEMPLATE_CMPL_FLOAT_gt_or_nan       @ C set and Z clear, disambiguate
     mvncc   r0, #0                      @ (less than) r0<- -1
     moveq   r0, #0                      @ (equal) r0<- 0, trumps less than
-    bx      r4
+    bx      r11
     @ Test for NaN with a second comparison.  EABI forbids testing bit
     @ patterns, and we can't represent 0x7fc00000 in immediate form, so
     @ make the library call.
 .LTEMPLATE_CMPL_FLOAT_gt_or_nan:
-    mov     r1, r9                      @ reverse order
-    mov     r0, r10
+    mov     r0, r10                     @ restore in reverse order
+    mov     r1, r9
     LDR_PC_LR ".L__aeabi_cfcmple"       @ r0<- Z set if eq, C clear if <
     movcc   r0, #1                      @ (greater than) r1<- 1
-    bxcc    r4
+    bxcc    r11
     mvn     r0, #0                            @ r1<- 1 or -1 for NaN
-    bx      r4
+    bx      r11
 
 
 
@@ -732,6 +772,399 @@ dvmCompiler_TEMPLATE_USHR_LONG:
     bx      lr
 
 
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_THROW_EXCEPTION_COMMON
+dvmCompiler_TEMPLATE_THROW_EXCEPTION_COMMON:
+/* File: armv5te/TEMPLATE_THROW_EXCEPTION_COMMON.S */
+    /*
+     * Throw an exception from JIT'ed code.
+     * On entry:
+     *    r0    Dalvik PC that raises the exception
+     */
+    b       .LhandleException
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MEM_OP_DECODE
+dvmCompiler_TEMPLATE_MEM_OP_DECODE:
+/* File: armv5te/TEMPLATE_MEM_OP_DECODE.S */
+#if defined(WITH_SELF_VERIFICATION)
+    /*
+     * This handler encapsulates heap memory ops for selfVerification mode.
+     *
+     * The call to the handler is inserted prior to a heap memory operation.
+     * This handler then calls a function to decode the memory op, and process
+     * it accordingly. Afterwards, the handler changes the return address to
+     * skip the memory op so it never gets executed.
+     */
+    push    {r0-r12,lr}                 @ save out all registers
+    mov     r0, lr                      @ arg0 <- link register
+    mov     r1, sp                      @ arg1 <- stack pointer
+    ldr     r2, .LdvmSelfVerificationMemOpDecode @ defined in footer.S
+    blx     r2                          @ decode and handle the mem op
+    pop     {r0-r12,lr}                 @ restore all registers
+    bx      lr                          @ return to compiled code
+#endif
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_STRING_COMPARETO
+dvmCompiler_TEMPLATE_STRING_COMPARETO:
+/* File: armv5te/TEMPLATE_STRING_COMPARETO.S */
+    /*
+     * String's compareTo.
+     *
+     * Requires r0/r1 to have been previously checked for null.  Will
+     * return negative if this's string is < comp, 0 if they are the
+     * same and positive if >.
+     *
+     * IMPORTANT NOTE:
+     *
+     * This code relies on hard-coded offsets for string objects, and must be
+     * kept in sync with definitions in UtfString.h.  See asm-constants.h
+     *
+     * On entry:
+     *    r0:   this object pointer
+     *    r1:   comp object pointer
+     *
+     */
+
+    mov    r2, r0         @ this to r2, opening up r0 for return value
+    subs   r0, r2, r1     @ Same?
+    bxeq   lr
+
+    ldr    r4, [r2, #STRING_FIELDOFF_OFFSET]
+    ldr    r9, [r1, #STRING_FIELDOFF_OFFSET]
+    ldr    r7, [r2, #STRING_FIELDOFF_COUNT]
+    ldr    r10, [r1, #STRING_FIELDOFF_COUNT]
+    ldr    r2, [r2, #STRING_FIELDOFF_VALUE]
+    ldr    r1, [r1, #STRING_FIELDOFF_VALUE]
+
+    /*
+     * At this point, we have:
+     *    value:  r2/r1
+     *    offset: r4/r9
+     *    count:  r7/r10
+     * We're going to compute
+     *    r11 <- countDiff
+     *    r10 <- minCount
+     */
+     subs  r11, r7, r10
+     movls r10, r7
+
+     /* Now, build pointers to the string data */
+     add   r2, r2, r4, lsl #1
+     add   r1, r1, r9, lsl #1
+     /*
+      * Note: data pointers point to previous element so we can use pre-index
+      * mode with base writeback.
+      */
+     add   r2, #16-2   @ offset to contents[-1]
+     add   r1, #16-2   @ offset to contents[-1]
+
+     /*
+      * At this point we have:
+      *   r2: *this string data
+      *   r1: *comp string data
+      *   r10: iteration count for comparison
+      *   r11: value to return if the first part of the string is equal
+      *   r0: reserved for result
+      *   r3, r4, r7, r8, r9, r12 available for loading string data
+      */
+
+    subs  r10, #2
+    blt   do_remainder2
+
+      /*
+       * Unroll the first two checks so we can quickly catch early mismatch
+       * on long strings (but preserve incoming alignment)
+       */
+
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    ldrh  r7, [r2, #2]!
+    ldrh  r8, [r1, #2]!
+    subs  r0, r3, r4
+    subeqs  r0, r7, r8
+    bxne  lr
+    cmp   r10, #28
+    bgt   do_memcmp16
+    subs  r10, #3
+    blt   do_remainder
+
+loopback_triple:
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    ldrh  r7, [r2, #2]!
+    ldrh  r8, [r1, #2]!
+    ldrh  r9, [r2, #2]!
+    ldrh  r12,[r1, #2]!
+    subs  r0, r3, r4
+    subeqs  r0, r7, r8
+    subeqs  r0, r9, r12
+    bxne  lr
+    subs  r10, #3
+    bge   loopback_triple
+
+do_remainder:
+    adds  r10, #3
+    beq   returnDiff
+
+loopback_single:
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    subs  r0, r3, r4
+    bxne  lr
+    subs  r10, #1
+    bne     loopback_single
+
+returnDiff:
+    mov   r0, r11
+    bx    lr
+
+do_remainder2:
+    adds  r10, #2
+    bne   loopback_single
+    mov   r0, r11
+    bx    lr
+
+    /* Long string case */
+do_memcmp16:
+    mov   r4, lr
+    ldr   lr, .Lmemcmp16
+    mov   r7, r11
+    add   r0, r2, #2
+    add   r1, r1, #2
+    mov   r2, r10
+    blx   lr
+    cmp   r0, #0
+    bxne  r4
+    mov   r0, r7
+    bx    r4
+
+.Lmemcmp16:
+    .word __memcmp16
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_STRING_INDEXOF
+dvmCompiler_TEMPLATE_STRING_INDEXOF:
+/* File: armv5te/TEMPLATE_STRING_INDEXOF.S */
+    /*
+     * String's indexOf.
+     *
+     * Requires r0 to have been previously checked for null.  Will
+     * return index of match of r1 in r0.
+     *
+     * IMPORTANT NOTE:
+     *
+     * This code relies on hard-coded offsets for string objects, and must be
+     * kept in sync wth definitions in UtfString.h  See asm-constants.h
+     *
+     * On entry:
+     *    r0:   string object pointer
+     *    r1:   char to match
+     *    r2:   Starting offset in string data
+     */
+
+    ldr    r7, [r0, #STRING_FIELDOFF_OFFSET]
+    ldr    r8, [r0, #STRING_FIELDOFF_COUNT]
+    ldr    r0, [r0, #STRING_FIELDOFF_VALUE]
+
+    /*
+     * At this point, we have:
+     *    r0: object pointer
+     *    r1: char to match
+     *    r2: starting offset
+     *    r7: offset
+     *    r8: string length
+     */
+
+     /* Build pointer to start of string data */
+     add   r0, #16
+     add   r0, r0, r7, lsl #1
+
+     /* Save a copy of starting data in r7 */
+     mov   r7, r0
+
+     /* Clamp start to [0..count] */
+     cmp   r2, #0
+     movlt r2, #0
+     cmp   r2, r8
+     movgt r2, r8
+
+     /* Build pointer to start of data to compare and pre-bias */
+     add   r0, r0, r2, lsl #1
+     sub   r0, #2
+
+     /* Compute iteration count */
+     sub   r8, r2
+
+     /*
+      * At this point we have:
+      *   r0: start of data to test
+      *   r1: chat to compare
+      *   r8: iteration count
+      *   r7: original start of string
+      *   r3, r4, r9, r10, r11, r12 available for loading string data
+      */
+
+    subs  r8, #4
+    blt   indexof_remainder
+
+indexof_loop4:
+    ldrh  r3, [r0, #2]!
+    ldrh  r4, [r0, #2]!
+    ldrh  r10, [r0, #2]!
+    ldrh  r11, [r0, #2]!
+    cmp   r3, r1
+    beq   match_0
+    cmp   r4, r1
+    beq   match_1
+    cmp   r10, r1
+    beq   match_2
+    cmp   r11, r1
+    beq   match_3
+    subs  r8, #4
+    bge   indexof_loop4
+
+indexof_remainder:
+    adds    r8, #4
+    beq     indexof_nomatch
+
+indexof_loop1:
+    ldrh  r3, [r0, #2]!
+    cmp   r3, r1
+    beq   match_3
+    subs  r8, #1
+    bne   indexof_loop1
+
+indexof_nomatch:
+    mov   r0, #-1
+    bx    lr
+
+match_0:
+    sub   r0, #6
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_1:
+    sub   r0, #4
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_2:
+    sub   r0, #2
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_3:
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_INTERPRET
+dvmCompiler_TEMPLATE_INTERPRET:
+/* File: armv5te/TEMPLATE_INTERPRET.S */
+    /*
+     * This handler transfers control to the interpeter without performing
+     * any lookups.  It may be called either as part of a normal chaining
+     * operation, or from the transition code in header.S.  We distinquish
+     * the two cases by looking at the link register.  If called from a
+     * translation chain, it will point to the chaining Dalvik PC + 1.
+     * On entry:
+     *    lr - if NULL:
+     *        r1 - the Dalvik PC to begin interpretation.
+     *    else
+     *        [lr, #-1] contains Dalvik PC to begin interpretation
+     *    rGLUE - pointer to interpState
+     *    rFP - Dalvik frame pointer
+     */
+    cmp     lr, #0
+    ldrne   r1,[lr, #-1]
+    ldr     r2, .LinterpPunt
+    mov     r0, r1                       @ set Dalvik PC
+    bx      r2
+    @ doesn't return
+
+.LinterpPunt:
+    .word   dvmJitToInterpPunt
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MONITOR_ENTER
+dvmCompiler_TEMPLATE_MONITOR_ENTER:
+/* File: armv5te/TEMPLATE_MONITOR_ENTER.S */
+    /*
+     * Call out to the runtime to lock an object.  Because this thread
+     * may have been suspended in THREAD_MONITOR state and the Jit's
+     * translation cache subsequently cleared, we cannot return directly.
+     * Instead, unconditionally transition to the interpreter to resume.
+     *
+     * On entry:
+     *    r0 - self pointer
+     *    r1 - the object (which has already been null-checked by the caller
+     *    r4 - the Dalvik PC of the following instruction.
+     */
+    ldr     r2, .LdvmLockObject
+    mov     r3, #0                       @ Record that we're not returning
+    str     r3, [r0, #offThread_inJitCodeCache]
+    blx     r2                           @ dvmLockObject(self, obj)
+    @ refresh Jit's on/off status
+    ldr     r0, [rGLUE, #offGlue_ppJitProfTable]
+    ldr     r0, [r0]
+    ldr     r2, .LdvmJitToInterpNoChain
+    str     r0, [rGLUE, #offGlue_pJitProfTable]
+    @ Bail to interpreter - no chain [note - r4 still contains rPC]
+#if defined(JIT_STATS)
+    mov     r0, #kHeavyweightMonitor
+#endif
+    bx      r2
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MONITOR_ENTER_DEBUG
+dvmCompiler_TEMPLATE_MONITOR_ENTER_DEBUG:
+/* File: armv5te/TEMPLATE_MONITOR_ENTER_DEBUG.S */
+    /*
+     * To support deadlock prediction, this version of MONITOR_ENTER
+     * will always call the heavyweight dvmLockObject, check for an
+     * exception and then bail out to the interpreter.
+     *
+     * On entry:
+     *    r0 - self pointer
+     *    r1 - the object (which has already been null-checked by the caller
+     *    r4 - the Dalvik PC of the following instruction.
+     *
+     */
+    ldr     r2, .LdvmLockObject
+    mov     r3, #0                       @ Record that we're not returning
+    str     r3, [r0, #offThread_inJitCodeCache]
+    blx     r2             @ dvmLockObject(self, obj)
+    @ refresh Jit's on/off status & test for exception
+    ldr     r0, [rGLUE, #offGlue_ppJitProfTable]
+    ldr     r1, [rGLUE, #offGlue_self]
+    ldr     r0, [r0]
+    ldr     r1, [r1, #offThread_exception]
+    str     r0, [rGLUE, #offGlue_pJitProfTable]
+    cmp     r1, #0
+    beq     1f
+    ldr     r2, .LhandleException
+    sub     r0, r4, #2     @ roll dPC back to this monitor instruction
+    bx      r2
+1:
+    @ Bail to interpreter - no chain [note - r4 still contains rPC]
+#if defined(JIT_STATS)
+    mov     r0, #kHeavyweightMonitor
+#endif
+    ldr     pc, .LdvmJitToInterpNoChain
+
     .size   dvmCompilerTemplateStart, .-dvmCompilerTemplateStart
 /* File: armv5te/footer.S */
 /*
@@ -746,7 +1179,9 @@ dvmCompiler_TEMPLATE_USHR_LONG:
     @ Prep for the native call
     @ r1 = newFP, r0 = methodToCall
     ldr     r3, [rGLUE, #offGlue_self]      @ r3<- glue->self
+    mov     r2, #0
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in jit code cache
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
@@ -759,33 +1194,71 @@ dvmCompiler_TEMPLATE_USHR_LONG:
 
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+    @ Refresh Jit's on/off status
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable]
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r2, [r10, #offStackSaveArea_returnAddr] @ r2 = chaining cell ret
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved->top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+    ldr     r3, [r3]    @ r1 <- pointer to Jit profile table
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [r10, #offStackSaveArea_savedPc] @ reload rPC
+    str     r3, [rGLUE, #offGlue_pJitProfTable]  @ cache current JitProfTable
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
 
-/* NOTE - this path can be exercised if the JIT threshold is set to 5 */
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the new mode
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+/*
+ * On entry:
+ * r0  Faulting Dalvik PC
+ */
 .LhandleException:
-    ldr     r0, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
+#if defined(WITH_SELF_VERIFICATION)
+    ldr     pc, .LdeadFood @ should not see this under self-verification mode
+.LdeadFood:
+    .word   0xdeadf00d
+#endif
+    ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
+    mov     r2, #0
+    str     r2, [r3, #offThread_inJitCodeCache] @ in interpreter land
+    ldr     r1, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
     ldr     rIBASE, .LdvmAsmInstructionStart    @ same as above
-    ldr     rPC, [r10, #offStackSaveArea_savedPc] @ reload rPC
-    mov     pc, r0                  @ branch to dvmMterpCommonExceptionThrown
+    mov     rPC, r0                 @ reload the faulting Dalvik address
+    mov     pc, r1                  @ branch to dvmMterpCommonExceptionThrown
 
     .align  2
 .LdvmAsmInstructionStart:
     .word   dvmAsmInstructionStart
+.LdvmJitToInterpTraceSelectNoChain:
+    .word   dvmJitToInterpTraceSelectNoChain
 .LdvmJitToInterpNoChain:
     .word   dvmJitToInterpNoChain
 .LdvmMterpStdBail:
     .word   dvmMterpStdBail
 .LdvmMterpCommonExceptionThrown:
     .word   dvmMterpCommonExceptionThrown
+.LdvmLockObject:
+    .word   dvmLockObject
+#if defined(WITH_SELF_VERIFICATION)
+.LdvmSelfVerificationMemOpDecode:
+    .word   dvmSelfVerificationMemOpDecode
+#endif
 .L__aeabi_cdcmple:
     .word   __aeabi_cdcmple
 .L__aeabi_cfcmple:
diff --git a/vm/compiler/template/out/CompilerTemplateAsm-armv7-a.S b/vm/compiler/template/out/CompilerTemplateAsm-armv7-a.S
index 9e97b74..cf6fff9 100644
--- a/vm/compiler/template/out/CompilerTemplateAsm-armv7-a.S
+++ b/vm/compiler/template/out/CompilerTemplateAsm-armv7-a.S
@@ -92,6 +92,9 @@ unspecified registers or condition codes.
 #define SAVEAREA_FROM_FP(_reg, _fpreg) \
     sub     _reg, _fpreg, #sizeofStackSaveArea
 
+#define EXPORT_PC() \
+    str     rPC, [rFP, #(-sizeofStackSaveArea + offStackSaveArea_currentPc)]
+
 /*
  * This is a #include, not a %include, because we want the C pre-processor
  * to expand the macros into assembler assignment statements.
@@ -99,10 +102,10 @@ unspecified registers or condition codes.
 #include "../../../mterp/common/asm-constants.h"
 
 
-/* File: armv5te/platform.S */
+/* File: armv5te-vfp/platform.S */
 /*
  * ===========================================================================
- *  CPU-version-specific defines
+ *  CPU-version-specific defines and utility
  * ===========================================================================
  */
 
@@ -180,27 +183,39 @@ dvmCompiler_TEMPLATE_RETURN:
     ldr     r10, [r0, #offStackSaveArea_prevFrame] @ r10<- saveArea->prevFrame
     ldr     r8, [rGLUE, #offGlue_pSelfSuspendCount] @ r8<- &suspendCount
     ldr     rPC, [r0, #offStackSaveArea_savedPc] @ rPC<- saveArea->savedPc
+#if !defined(WITH_SELF_VERIFICATION)
     ldr     r9,  [r0, #offStackSaveArea_returnAddr] @ r9<- chaining cell ret
+#else
+    mov     r9, #0                      @ disable chaining
+#endif
     ldr     r2, [r10, #(offStackSaveArea_method - sizeofStackSaveArea)]
                                         @ r2<- method we're returning to
     ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
     cmp     r2, #0                      @ break frame?
+#if !defined(WITH_SELF_VERIFICATION)
     beq     1f                          @ bail to interpreter
-    ldr     r0, .LdvmJitToInterpNoChain @ defined in footer.S
+#else
+    blxeq   lr                          @ punt to interpreter and compare state
+#endif
+    ldr     r1, .LdvmJitToInterpNoChain @ defined in footer.S
     mov     rFP, r10                    @ publish new FP
     ldrne   r10, [r2, #offMethod_clazz] @ r10<- method->clazz
     ldr     r8, [r8]                    @ r8<- suspendCount
 
     str     r2, [rGLUE, #offGlue_method]@ glue->method = newSave->method
-    ldr     r1, [r10, #offClassObject_pDvmDex] @ r1<- method->clazz->pDvmDex
+    ldr     r0, [r10, #offClassObject_pDvmDex] @ r0<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame] @ self->curFrame = fp
     add     rPC, rPC, #6                @ publish new rPC (advance 6 bytes)
-    str     r1, [rGLUE, #offGlue_methodClassDex]
+    str     r0, [rGLUE, #offGlue_methodClassDex]
     cmp     r8, #0                      @ check the suspendCount
     movne   r9, #0                      @ clear the chaining cell address
+    str     r9, [r3, #offThread_inJitCodeCache] @ in code cache or not
     cmp     r9, #0                      @ chaining cell exists?
     blxne   r9                          @ jump to the chaining cell
-    mov     pc, r0                      @ callsite is interpreted
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1                      @ callsite is interpreted
 1:
     stmia   rGLUE, {rPC, rFP}           @ SAVE_PC_FP_TO_GLUE()
     ldr     r2, .LdvmMterpStdBail       @ defined in footer.S
@@ -228,7 +243,7 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     sub     r1, r1, r7, lsl #2          @ r1<- newFp (old savearea - regsSize)
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    lr                          @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -246,9 +261,13 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     cmp     r8, #0                      @ suspendCount != 0
     bxne    lr                          @ bail to the interpreter
     tst     r10, #ACC_NATIVE
+#if !defined(WITH_SELF_VERIFICATION)
     bne     .LinvokeNative
+#else
+    bxne    lr                          @ bail to the interpreter
+#endif
 
-    ldr     r10, .LdvmJitToInterpNoChain
+    ldr     r10, .LdvmJitToInterpTraceSelectNoChain
     ldr     r3, [r9, #offClassObject_pDvmDex] @ r3<- method->clazz->pDvmDex
     ldr     r2, [rGLUE, #offGlue_self]      @ r2<- glue->self
 
@@ -259,7 +278,10 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NO_OPT:
     str     rFP, [r2, #offThread_curFrame]  @ self->curFrame = newFp
 
     @ Start executing the callee
-    mov     pc, r10                         @ dvmJitToInterpNoChain
+#if defined(JIT_STATS)
+    mov     r0, #kInlineCacheMiss
+#endif
+    mov     pc, r10                         @ dvmJitToInterpTraceSelectNoChain
 
 /* ------------------------------ */
     .balign 4
@@ -284,7 +306,7 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_CHAIN:
     SAVEAREA_FROM_FP(r10, r1)           @ r10<- stack save area
     add     r12, lr, #2                 @ setup the punt-to-interp address
     sub     r10, r10, r2, lsl #2        @ r10<- bottom (newsave - outsSize)
-    ldr     r8, [r8]                    @ r3<- suspendCount (int)
+    ldr     r8, [r8]                    @ r8<- suspendCount (int)
     cmp     r10, r9                     @ bottom < interpStackEnd?
     bxlt    r12                         @ return to raise stack overflow excep.
     @ r1 = newFP, r0 = methodToCall, r3 = returnCell, rPC = dalvikCallsite
@@ -391,11 +413,17 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NATIVE:
     str     r0, [r1, #(offStackSaveArea_method - sizeofStackSaveArea)]
     cmp     r8, #0                      @ suspendCount != 0
     ldr     r8, [r0, #offMethod_nativeFunc] @ r8<- method->nativeFunc
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    lr                          @ bail to the interpreter
+#else
+    bx      lr                          @ bail to interpreter unconditionally
+#endif
 
     @ go ahead and transfer control to the native code
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    mov     r2, #0
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in the jit code cache
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
     mov     r9, r3                      @ r9<- glue->self (preserve)
@@ -415,8 +443,24 @@ dvmCompiler_TEMPLATE_INVOKE_METHOD_NATIVE:
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [rFP, #(offStackSaveArea_currentPc - sizeofStackSaveArea)]
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
+
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the mode properly
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+
 
 
 /* ------------------------------ */
@@ -860,7 +904,7 @@ dvmCompiler_TEMPLATE_CMPG_DOUBLE_VFP:
     /* op vAA, vBB, vCC */
     fldd    d0, [r0]                    @ d0<- vBB
     fldd    d1, [r1]                    @ d1<- vCC
-    fcmped  d0, d1                      @ compare (vBB, vCC)
+    fcmpd  d0, d1                       @ compare (vBB, vCC)
     mov     r0, #1                      @ r0<- 1 (default)
     fmstat                              @ export status flags
     mvnmi   r0, #0                      @ (less than) r0<- -1
@@ -929,7 +973,7 @@ dvmCompiler_TEMPLATE_CMPG_FLOAT_VFP:
     /* op vAA, vBB, vCC */
     flds    s0, [r0]                    @ d0<- vBB
     flds    s1, [r1]                    @ d1<- vCC
-    fcmpes  s0, s1                      @ compare (vBB, vCC)
+    fcmps  s0, s1                      @ compare (vBB, vCC)
     mov     r0, #1                      @ r0<- 1 (default)
     fmstat                              @ export status flags
     mvnmi   r0, #0                      @ (less than) r0<- -1
@@ -963,7 +1007,7 @@ dvmCompiler_TEMPLATE_CMPL_FLOAT_VFP:
     /* op vAA, vBB, vCC */
     flds    s0, [r0]                    @ d0<- vBB
     flds    s1, [r1]                    @ d1<- vCC
-    fcmpes  s0, s1                      @ compare (vBB, vCC)
+    fcmps  s0, s1                      @ compare (vBB, vCC)
     mvn     r0, #0                      @ r0<- -1 (default)
     fmstat                              @ export status flags
     movgt   r0, #1                      @ (greater than) r0<- 1
@@ -998,6 +1042,401 @@ dvmCompiler_TEMPLATE_SQRT_DOUBLE_VFP:
 .Lsqrt:
     .word   sqrt
 
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_THROW_EXCEPTION_COMMON
+dvmCompiler_TEMPLATE_THROW_EXCEPTION_COMMON:
+/* File: armv5te/TEMPLATE_THROW_EXCEPTION_COMMON.S */
+    /*
+     * Throw an exception from JIT'ed code.
+     * On entry:
+     *    r0    Dalvik PC that raises the exception
+     */
+    b       .LhandleException
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MEM_OP_DECODE
+dvmCompiler_TEMPLATE_MEM_OP_DECODE:
+/* File: armv5te-vfp/TEMPLATE_MEM_OP_DECODE.S */
+#if defined(WITH_SELF_VERIFICATION)
+    /*
+     * This handler encapsulates heap memory ops for selfVerification mode.
+     *
+     * The call to the handler is inserted prior to a heap memory operation.
+     * This handler then calls a function to decode the memory op, and process
+     * it accordingly. Afterwards, the handler changes the return address to
+     * skip the memory op so it never gets executed.
+     */
+    vpush   {d0-d15}                    @ save out all fp registers
+    push    {r0-r12,lr}                 @ save out all registers
+    mov     r0, lr                      @ arg0 <- link register
+    mov     r1, sp                      @ arg1 <- stack pointer
+    ldr     r2, .LdvmSelfVerificationMemOpDecode @ defined in footer.S
+    blx     r2                          @ decode and handle the mem op
+    pop     {r0-r12,lr}                 @ restore all registers
+    vpop    {d0-d15}                    @ restore all fp registers
+    bx      lr                          @ return to compiled code
+#endif
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_STRING_COMPARETO
+dvmCompiler_TEMPLATE_STRING_COMPARETO:
+/* File: armv5te/TEMPLATE_STRING_COMPARETO.S */
+    /*
+     * String's compareTo.
+     *
+     * Requires r0/r1 to have been previously checked for null.  Will
+     * return negative if this's string is < comp, 0 if they are the
+     * same and positive if >.
+     *
+     * IMPORTANT NOTE:
+     *
+     * This code relies on hard-coded offsets for string objects, and must be
+     * kept in sync with definitions in UtfString.h.  See asm-constants.h
+     *
+     * On entry:
+     *    r0:   this object pointer
+     *    r1:   comp object pointer
+     *
+     */
+
+    mov    r2, r0         @ this to r2, opening up r0 for return value
+    subs   r0, r2, r1     @ Same?
+    bxeq   lr
+
+    ldr    r4, [r2, #STRING_FIELDOFF_OFFSET]
+    ldr    r9, [r1, #STRING_FIELDOFF_OFFSET]
+    ldr    r7, [r2, #STRING_FIELDOFF_COUNT]
+    ldr    r10, [r1, #STRING_FIELDOFF_COUNT]
+    ldr    r2, [r2, #STRING_FIELDOFF_VALUE]
+    ldr    r1, [r1, #STRING_FIELDOFF_VALUE]
+
+    /*
+     * At this point, we have:
+     *    value:  r2/r1
+     *    offset: r4/r9
+     *    count:  r7/r10
+     * We're going to compute
+     *    r11 <- countDiff
+     *    r10 <- minCount
+     */
+     subs  r11, r7, r10
+     movls r10, r7
+
+     /* Now, build pointers to the string data */
+     add   r2, r2, r4, lsl #1
+     add   r1, r1, r9, lsl #1
+     /*
+      * Note: data pointers point to previous element so we can use pre-index
+      * mode with base writeback.
+      */
+     add   r2, #16-2   @ offset to contents[-1]
+     add   r1, #16-2   @ offset to contents[-1]
+
+     /*
+      * At this point we have:
+      *   r2: *this string data
+      *   r1: *comp string data
+      *   r10: iteration count for comparison
+      *   r11: value to return if the first part of the string is equal
+      *   r0: reserved for result
+      *   r3, r4, r7, r8, r9, r12 available for loading string data
+      */
+
+    subs  r10, #2
+    blt   do_remainder2
+
+      /*
+       * Unroll the first two checks so we can quickly catch early mismatch
+       * on long strings (but preserve incoming alignment)
+       */
+
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    ldrh  r7, [r2, #2]!
+    ldrh  r8, [r1, #2]!
+    subs  r0, r3, r4
+    subeqs  r0, r7, r8
+    bxne  lr
+    cmp   r10, #28
+    bgt   do_memcmp16
+    subs  r10, #3
+    blt   do_remainder
+
+loopback_triple:
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    ldrh  r7, [r2, #2]!
+    ldrh  r8, [r1, #2]!
+    ldrh  r9, [r2, #2]!
+    ldrh  r12,[r1, #2]!
+    subs  r0, r3, r4
+    subeqs  r0, r7, r8
+    subeqs  r0, r9, r12
+    bxne  lr
+    subs  r10, #3
+    bge   loopback_triple
+
+do_remainder:
+    adds  r10, #3
+    beq   returnDiff
+
+loopback_single:
+    ldrh  r3, [r2, #2]!
+    ldrh  r4, [r1, #2]!
+    subs  r0, r3, r4
+    bxne  lr
+    subs  r10, #1
+    bne     loopback_single
+
+returnDiff:
+    mov   r0, r11
+    bx    lr
+
+do_remainder2:
+    adds  r10, #2
+    bne   loopback_single
+    mov   r0, r11
+    bx    lr
+
+    /* Long string case */
+do_memcmp16:
+    mov   r4, lr
+    ldr   lr, .Lmemcmp16
+    mov   r7, r11
+    add   r0, r2, #2
+    add   r1, r1, #2
+    mov   r2, r10
+    blx   lr
+    cmp   r0, #0
+    bxne  r4
+    mov   r0, r7
+    bx    r4
+
+.Lmemcmp16:
+    .word __memcmp16
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_STRING_INDEXOF
+dvmCompiler_TEMPLATE_STRING_INDEXOF:
+/* File: armv5te/TEMPLATE_STRING_INDEXOF.S */
+    /*
+     * String's indexOf.
+     *
+     * Requires r0 to have been previously checked for null.  Will
+     * return index of match of r1 in r0.
+     *
+     * IMPORTANT NOTE:
+     *
+     * This code relies on hard-coded offsets for string objects, and must be
+     * kept in sync wth definitions in UtfString.h  See asm-constants.h
+     *
+     * On entry:
+     *    r0:   string object pointer
+     *    r1:   char to match
+     *    r2:   Starting offset in string data
+     */
+
+    ldr    r7, [r0, #STRING_FIELDOFF_OFFSET]
+    ldr    r8, [r0, #STRING_FIELDOFF_COUNT]
+    ldr    r0, [r0, #STRING_FIELDOFF_VALUE]
+
+    /*
+     * At this point, we have:
+     *    r0: object pointer
+     *    r1: char to match
+     *    r2: starting offset
+     *    r7: offset
+     *    r8: string length
+     */
+
+     /* Build pointer to start of string data */
+     add   r0, #16
+     add   r0, r0, r7, lsl #1
+
+     /* Save a copy of starting data in r7 */
+     mov   r7, r0
+
+     /* Clamp start to [0..count] */
+     cmp   r2, #0
+     movlt r2, #0
+     cmp   r2, r8
+     movgt r2, r8
+
+     /* Build pointer to start of data to compare and pre-bias */
+     add   r0, r0, r2, lsl #1
+     sub   r0, #2
+
+     /* Compute iteration count */
+     sub   r8, r2
+
+     /*
+      * At this point we have:
+      *   r0: start of data to test
+      *   r1: chat to compare
+      *   r8: iteration count
+      *   r7: original start of string
+      *   r3, r4, r9, r10, r11, r12 available for loading string data
+      */
+
+    subs  r8, #4
+    blt   indexof_remainder
+
+indexof_loop4:
+    ldrh  r3, [r0, #2]!
+    ldrh  r4, [r0, #2]!
+    ldrh  r10, [r0, #2]!
+    ldrh  r11, [r0, #2]!
+    cmp   r3, r1
+    beq   match_0
+    cmp   r4, r1
+    beq   match_1
+    cmp   r10, r1
+    beq   match_2
+    cmp   r11, r1
+    beq   match_3
+    subs  r8, #4
+    bge   indexof_loop4
+
+indexof_remainder:
+    adds    r8, #4
+    beq     indexof_nomatch
+
+indexof_loop1:
+    ldrh  r3, [r0, #2]!
+    cmp   r3, r1
+    beq   match_3
+    subs  r8, #1
+    bne   indexof_loop1
+
+indexof_nomatch:
+    mov   r0, #-1
+    bx    lr
+
+match_0:
+    sub   r0, #6
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_1:
+    sub   r0, #4
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_2:
+    sub   r0, #2
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+match_3:
+    sub   r0, r7
+    asr   r0, r0, #1
+    bx    lr
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_INTERPRET
+dvmCompiler_TEMPLATE_INTERPRET:
+/* File: armv5te/TEMPLATE_INTERPRET.S */
+    /*
+     * This handler transfers control to the interpeter without performing
+     * any lookups.  It may be called either as part of a normal chaining
+     * operation, or from the transition code in header.S.  We distinquish
+     * the two cases by looking at the link register.  If called from a
+     * translation chain, it will point to the chaining Dalvik PC + 1.
+     * On entry:
+     *    lr - if NULL:
+     *        r1 - the Dalvik PC to begin interpretation.
+     *    else
+     *        [lr, #-1] contains Dalvik PC to begin interpretation
+     *    rGLUE - pointer to interpState
+     *    rFP - Dalvik frame pointer
+     */
+    cmp     lr, #0
+    ldrne   r1,[lr, #-1]
+    ldr     r2, .LinterpPunt
+    mov     r0, r1                       @ set Dalvik PC
+    bx      r2
+    @ doesn't return
+
+.LinterpPunt:
+    .word   dvmJitToInterpPunt
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MONITOR_ENTER
+dvmCompiler_TEMPLATE_MONITOR_ENTER:
+/* File: armv5te/TEMPLATE_MONITOR_ENTER.S */
+    /*
+     * Call out to the runtime to lock an object.  Because this thread
+     * may have been suspended in THREAD_MONITOR state and the Jit's
+     * translation cache subsequently cleared, we cannot return directly.
+     * Instead, unconditionally transition to the interpreter to resume.
+     *
+     * On entry:
+     *    r0 - self pointer
+     *    r1 - the object (which has already been null-checked by the caller
+     *    r4 - the Dalvik PC of the following instruction.
+     */
+    ldr     r2, .LdvmLockObject
+    mov     r3, #0                       @ Record that we're not returning
+    str     r3, [r0, #offThread_inJitCodeCache]
+    blx     r2                           @ dvmLockObject(self, obj)
+    @ refresh Jit's on/off status
+    ldr     r0, [rGLUE, #offGlue_ppJitProfTable]
+    ldr     r0, [r0]
+    ldr     r2, .LdvmJitToInterpNoChain
+    str     r0, [rGLUE, #offGlue_pJitProfTable]
+    @ Bail to interpreter - no chain [note - r4 still contains rPC]
+#if defined(JIT_STATS)
+    mov     r0, #kHeavyweightMonitor
+#endif
+    bx      r2
+
+
+/* ------------------------------ */
+    .balign 4
+    .global dvmCompiler_TEMPLATE_MONITOR_ENTER_DEBUG
+dvmCompiler_TEMPLATE_MONITOR_ENTER_DEBUG:
+/* File: armv5te/TEMPLATE_MONITOR_ENTER_DEBUG.S */
+    /*
+     * To support deadlock prediction, this version of MONITOR_ENTER
+     * will always call the heavyweight dvmLockObject, check for an
+     * exception and then bail out to the interpreter.
+     *
+     * On entry:
+     *    r0 - self pointer
+     *    r1 - the object (which has already been null-checked by the caller
+     *    r4 - the Dalvik PC of the following instruction.
+     *
+     */
+    ldr     r2, .LdvmLockObject
+    mov     r3, #0                       @ Record that we're not returning
+    str     r3, [r0, #offThread_inJitCodeCache]
+    blx     r2             @ dvmLockObject(self, obj)
+    @ refresh Jit's on/off status & test for exception
+    ldr     r0, [rGLUE, #offGlue_ppJitProfTable]
+    ldr     r1, [rGLUE, #offGlue_self]
+    ldr     r0, [r0]
+    ldr     r1, [r1, #offThread_exception]
+    str     r0, [rGLUE, #offGlue_pJitProfTable]
+    cmp     r1, #0
+    beq     1f
+    ldr     r2, .LhandleException
+    sub     r0, r4, #2     @ roll dPC back to this monitor instruction
+    bx      r2
+1:
+    @ Bail to interpreter - no chain [note - r4 still contains rPC]
+#if defined(JIT_STATS)
+    mov     r0, #kHeavyweightMonitor
+#endif
+    ldr     pc, .LdvmJitToInterpNoChain
+
     .size   dvmCompilerTemplateStart, .-dvmCompilerTemplateStart
 /* File: armv5te/footer.S */
 /*
@@ -1012,7 +1451,9 @@ dvmCompiler_TEMPLATE_SQRT_DOUBLE_VFP:
     @ Prep for the native call
     @ r1 = newFP, r0 = methodToCall
     ldr     r3, [rGLUE, #offGlue_self]      @ r3<- glue->self
+    mov     r2, #0
     ldr     r9, [r3, #offThread_jniLocal_topCookie] @ r9<- thread->localRef->...
+    str     r2, [r3, #offThread_inJitCodeCache] @ not in jit code cache
     str     r1, [r3, #offThread_curFrame]   @ self->curFrame = newFp
     str     r9, [r1, #(offStackSaveArea_localRefCookie - sizeofStackSaveArea)]
                                         @ newFp->localRefCookie=top
@@ -1025,33 +1466,71 @@ dvmCompiler_TEMPLATE_SQRT_DOUBLE_VFP:
 
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+    @ Refresh Jit's on/off status
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable]
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r2, [r10, #offStackSaveArea_returnAddr] @ r2 = chaining cell ret
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved->top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+    ldr     r3, [r3]    @ r1 <- pointer to Jit profile table
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
-    bne     .LhandleException             @ no, handle exception
-    bx      r2
+    ldr     r0, [r10, #offStackSaveArea_savedPc] @ reload rPC
+    str     r3, [rGLUE, #offGlue_pJitProfTable]  @ cache current JitProfTable
+
+    @ r0 = dalvikCallsitePC
+    bne     .LhandleException           @ no, handle exception
 
-/* NOTE - this path can be exercised if the JIT threshold is set to 5 */
+    str     r2, [r9, #offThread_inJitCodeCache] @ set the new mode
+    cmp     r2, #0                      @ return chaining cell still exists?
+    bxne    r2                          @ yes - go ahead
+
+    @ continue executing the next instruction through the interpreter
+    ldr     r1, .LdvmJitToInterpTraceSelectNoChain @ defined in footer.S
+    add     rPC, r0, #6                 @ reconstruct new rPC (advance 6 bytes)
+#if defined(JIT_STATS)
+    mov     r0, #kCallsiteInterpreted
+#endif
+    mov     pc, r1
+
+/*
+ * On entry:
+ * r0  Faulting Dalvik PC
+ */
 .LhandleException:
-    ldr     r0, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
+#if defined(WITH_SELF_VERIFICATION)
+    ldr     pc, .LdeadFood @ should not see this under self-verification mode
+.LdeadFood:
+    .word   0xdeadf00d
+#endif
+    ldr     r3, [rGLUE, #offGlue_self]  @ r3<- glue->self
+    mov     r2, #0
+    str     r2, [r3, #offThread_inJitCodeCache] @ in interpreter land
+    ldr     r1, .LdvmMterpCommonExceptionThrown @ PIC way of getting &func
     ldr     rIBASE, .LdvmAsmInstructionStart    @ same as above
-    ldr     rPC, [r10, #offStackSaveArea_savedPc] @ reload rPC
-    mov     pc, r0                  @ branch to dvmMterpCommonExceptionThrown
+    mov     rPC, r0                 @ reload the faulting Dalvik address
+    mov     pc, r1                  @ branch to dvmMterpCommonExceptionThrown
 
     .align  2
 .LdvmAsmInstructionStart:
     .word   dvmAsmInstructionStart
+.LdvmJitToInterpTraceSelectNoChain:
+    .word   dvmJitToInterpTraceSelectNoChain
 .LdvmJitToInterpNoChain:
     .word   dvmJitToInterpNoChain
 .LdvmMterpStdBail:
     .word   dvmMterpStdBail
 .LdvmMterpCommonExceptionThrown:
     .word   dvmMterpCommonExceptionThrown
+.LdvmLockObject:
+    .word   dvmLockObject
+#if defined(WITH_SELF_VERIFICATION)
+.LdvmSelfVerificationMemOpDecode:
+    .word   dvmSelfVerificationMemOpDecode
+#endif
 .L__aeabi_cdcmple:
     .word   __aeabi_cdcmple
 .L__aeabi_cfcmple:
diff --git a/vm/compiler/template/rebuild.sh b/vm/compiler/template/rebuild.sh
index 8a3861e..78ff84f 100755
--- a/vm/compiler/template/rebuild.sh
+++ b/vm/compiler/template/rebuild.sh
@@ -19,5 +19,5 @@
 # generated as part of the build.
 #
 set -e
-for arch in armv5te armv5te-vfp armv7-a; do TARGET_ARCH_EXT=$arch make -f Makefile-template; done
+for arch in armv5te armv5te-vfp armv7-a armv7-a-neon; do TARGET_ARCH_EXT=$arch make -f Makefile-template; done
 
diff --git a/vm/hprof/Hprof.c b/vm/hprof/Hprof.c
index 2e6f7c9..8380fd8 100644
--- a/vm/hprof/Hprof.c
+++ b/vm/hprof/Hprof.c
@@ -33,15 +33,13 @@
 #define kHeadSuffix "-hptemp"
 
 hprof_context_t *
-hprofStartup(const char *outputFileName)
+hprofStartup(const char *outputFileName, bool directToDdms)
 {
-    hprof_context_t *ctx;
+    FILE* fp = NULL;
 
-    ctx = malloc(sizeof(*ctx));
-    if (ctx != NULL) {
+    if (!directToDdms) {
         int len = strlen(outputFileName);
         char fileName[len + sizeof(kHeadSuffix)];
-        FILE *fp;
 
         /* Construct the temp file name.  This wasn't handed to us by the
          * application, so we need to be careful about stomping on it.
@@ -49,14 +47,12 @@ hprofStartup(const char *outputFileName)
         sprintf(fileName, "%s" kHeadSuffix, outputFileName);
         if (access(fileName, F_OK) == 0) {
             LOGE("hprof: temp file %s exists, bailing\n", fileName);
-            free(ctx);
             return NULL;
         }
 
         fp = fopen(fileName, "w+");
         if (fp == NULL) {
             LOGE("hprof: can't open %s: %s.\n", fileName, strerror(errno));
-            free(ctx);
             return NULL;
         }
         if (unlink(fileName) != 0) {
@@ -64,20 +60,28 @@ hprofStartup(const char *outputFileName)
             /* keep going */
         }
         LOGI("hprof: dumping VM heap to \"%s\".\n", fileName);
+    }
 
-        hprofStartup_String();
-        hprofStartup_Class();
+    hprofStartup_String();
+    hprofStartup_Class();
 #if WITH_HPROF_STACK
-        hprofStartup_StackFrame();
-        hprofStartup_Stack();
+    hprofStartup_StackFrame();
+    hprofStartup_Stack();
 #endif
 
-        /* pass in "fp" for the temp file, and the name of the output file */
-        hprofContextInit(ctx, strdup(outputFileName), fp, false);
-    } else {
+    hprof_context_t *ctx = malloc(sizeof(*ctx));
+    if (ctx == NULL) {
         LOGE("hprof: can't allocate context.\n");
+        if (fp != NULL)
+            fclose(fp);
+        return NULL;
     }
 
+    /* pass in "fp" for the temp file, and the name of the output file */
+    hprofContextInit(ctx, strdup(outputFileName), fp, false, directToDdms);
+
+    assert(ctx->fp != NULL);
+
     return ctx;
 }
 
@@ -115,46 +119,55 @@ copyFileToFile(FILE *dstFp, FILE *srcFp)
  * Finish up the hprof dump.  Returns true on success.
  */
 bool
-hprofShutdown(hprof_context_t *ctx)
+hprofShutdown(hprof_context_t *tailCtx)
 {
-    FILE *tempFp = ctx->fp;
-    FILE *fp;
+    FILE *fp = NULL;
 
     /* flush output to the temp file, then prepare the output file */
-    hprofFlushCurrentRecord(ctx);
-    free(ctx->curRec.body);
-    ctx->curRec.body = NULL;
-    ctx->curRec.allocLen = 0;
-    ctx->fp = NULL;
-
-    LOGI("hprof: dumping heap strings to \"%s\".\n", ctx->fileName);
-    fp = fopen(ctx->fileName, "w");
-    if (fp == NULL) {
-        LOGE("can't open %s: %s\n", ctx->fileName, strerror(errno));
-        fclose(tempFp);
-        free(ctx->fileName);
-        free(ctx);
-        return false;
+    hprofFlushCurrentRecord(tailCtx);
+
+    LOGI("hprof: dumping heap strings to \"%s\".\n", tailCtx->fileName);
+    if (!tailCtx->directToDdms) {
+        fp = fopen(tailCtx->fileName, "w");
+        if (fp == NULL) {
+            LOGE("can't open %s: %s\n", tailCtx->fileName, strerror(errno));
+            hprofFreeContext(tailCtx);
+            return false;
+        }
+    }
+
+    /*
+     * Create a new context struct for the start of the file.  We
+     * heap-allocate it so we can share the "free" function.
+     */
+    hprof_context_t *headCtx = malloc(sizeof(*headCtx));
+    if (headCtx == NULL) {
+        LOGE("hprof: can't allocate context.\n");
+        if (fp != NULL)
+            fclose(fp);
+        hprofFreeContext(tailCtx);
+        return NULL;
     }
-    hprofContextInit(ctx, ctx->fileName, fp, true);
+    hprofContextInit(headCtx, strdup(tailCtx->fileName), fp, true,
+        tailCtx->directToDdms);
 
-    hprofDumpStrings(ctx);
-    hprofDumpClasses(ctx);
+    hprofDumpStrings(headCtx);
+    hprofDumpClasses(headCtx);
 
     /* Write a dummy stack trace record so the analysis
      * tools don't freak out.
      */
-    hprofStartNewRecord(ctx, HPROF_TAG_STACK_TRACE, HPROF_TIME);
-    hprofAddU4ToRecord(&ctx->curRec, HPROF_NULL_STACK_TRACE);
-    hprofAddU4ToRecord(&ctx->curRec, HPROF_NULL_THREAD);
-    hprofAddU4ToRecord(&ctx->curRec, 0);    // no frames
+    hprofStartNewRecord(headCtx, HPROF_TAG_STACK_TRACE, HPROF_TIME);
+    hprofAddU4ToRecord(&headCtx->curRec, HPROF_NULL_STACK_TRACE);
+    hprofAddU4ToRecord(&headCtx->curRec, HPROF_NULL_THREAD);
+    hprofAddU4ToRecord(&headCtx->curRec, 0);    // no frames
 
 #if WITH_HPROF_STACK
-    hprofDumpStackFrames(ctx);
-    hprofDumpStacks(ctx);
+    hprofDumpStackFrames(headCtx);
+    hprofDumpStacks(headCtx);
 #endif
 
-    hprofFlushCurrentRecord(ctx);
+    hprofFlushCurrentRecord(headCtx);
 
     hprofShutdown_Class();
     hprofShutdown_String();
@@ -163,24 +176,52 @@ hprofShutdown(hprof_context_t *ctx)
     hprofShutdown_StackFrame();
 #endif
 
-    /*
-     * Append the contents of the temp file to the output file.  The temp
-     * file was removed immediately after being opened, so it will vanish
-     * when we close it.
-     */
-    rewind(tempFp);
-    if (!copyFileToFile(ctx->fp, tempFp)) {
-        LOGW("hprof: file copy failed, hprof data may be incomplete\n");
-        /* finish up anyway */
+    if (tailCtx->directToDdms) {
+        /* flush to ensure memstream pointer and size are updated */
+        fflush(headCtx->fp);
+        fflush(tailCtx->fp);
+
+        /* send the data off to DDMS */
+        struct iovec iov[2];
+        iov[0].iov_base = headCtx->fileDataPtr;
+        iov[0].iov_len = headCtx->fileDataSize;
+        iov[1].iov_base = tailCtx->fileDataPtr;
+        iov[1].iov_len = tailCtx->fileDataSize;
+        dvmDbgDdmSendChunkV(CHUNK_TYPE("HPDS"), iov, 2);
+    } else {
+        /*
+         * Append the contents of the temp file to the output file.  The temp
+         * file was removed immediately after being opened, so it will vanish
+         * when we close it.
+         */
+        rewind(tailCtx->fp);
+        if (!copyFileToFile(headCtx->fp, tailCtx->fp)) {
+            LOGW("hprof: file copy failed, hprof data may be incomplete\n");
+            /* finish up anyway */
+        }
     }
 
-    fclose(tempFp);
-    fclose(ctx->fp);
-    free(ctx->fileName);
-    free(ctx->curRec.body);
-    free(ctx);
+    hprofFreeContext(headCtx);
+    hprofFreeContext(tailCtx);
 
     /* throw out a log message for the benefit of "runhat" */
     LOGI("hprof: heap dump completed, temp file removed\n");
     return true;
 }
+
+/*
+ * Free any heap-allocated items in "ctx", and then free "ctx" itself.
+ */
+void
+hprofFreeContext(hprof_context_t *ctx)
+{
+    assert(ctx != NULL);
+
+    if (ctx->fp != NULL)
+        fclose(ctx->fp);
+    free(ctx->curRec.body);
+    free(ctx->fileName);
+    free(ctx->fileDataPtr);
+    free(ctx);
+}
+
diff --git a/vm/hprof/Hprof.h b/vm/hprof/Hprof.h
index 696b0a7..db5049f 100644
--- a/vm/hprof/Hprof.h
+++ b/vm/hprof/Hprof.h
@@ -125,13 +125,23 @@ typedef struct hprof_context_t {
      * can cast from a context to a record.
      */
     hprof_record_t curRec;
-    char *fileName;
-    FILE *fp;
+
     u4 gcThreadSerialNumber;
     u1 gcScanState;
     HprofHeapId currentHeap;    // which heap we're currently emitting
     u4 stackTraceSerialNumber;
     size_t objectsInSegment;
+
+    /*
+     * If "directToDdms" is not set, "fileName" is valid, and "fileDataPtr"
+     * and "fileDataSize" are not used.  If "directToDdms" is not set,
+     * it's the other way around.
+     */
+    bool directToDdms;
+    char *fileName;
+    char *fileDataPtr;          // for open_memstream
+    size_t fileDataSize;        // for open_memstream
+    FILE *fp;
 } hprof_context_t;
 
 
@@ -178,7 +188,7 @@ int hprofDumpHeapObject(hprof_context_t *ctx, const Object *obj);
  */
 
 void hprofContextInit(hprof_context_t *ctx, char *fileName, FILE *fp,
-                      bool writeHeader);
+                      bool writeHeader, bool directToDdms);
 
 int hprofFlushRecord(hprof_record_t *rec, FILE *fp);
 int hprofFlushCurrentRecord(hprof_context_t *ctx);
@@ -234,8 +244,9 @@ int hprofShutdown_StackFrame(void);
  * Hprof.c functions
  */
 
-hprof_context_t *hprofStartup(const char *outputFileName);
+hprof_context_t* hprofStartup(const char *outputFileName, bool directToDdms);
 bool hprofShutdown(hprof_context_t *ctx);
+void hprofFreeContext(hprof_context_t *ctx);
 
 /*
  * Heap.c functions
@@ -244,7 +255,7 @@ bool hprofShutdown(hprof_context_t *ctx);
  * the heap implementation; these functions require heap knowledge,
  * so they are implemented in Heap.c.
  */
-int hprofDumpHeap(const char* fileName);
+int hprofDumpHeap(const char* fileName, bool directToDdms);
 void dvmHeapSetHprofGcScanState(hprof_heap_tag_t state, u4 threadSerialNumber);
 
 #endif  // _DALVIK_HPROF_HPROF
diff --git a/vm/hprof/HprofOutput.c b/vm/hprof/HprofOutput.c
index c6d1cbc..0677c85 100644
--- a/vm/hprof/HprofOutput.c
+++ b/vm/hprof/HprofOutput.c
@@ -14,7 +14,9 @@
  * limitations under the License.
  */
 #include <sys/time.h>
+#include <cutils/open_memstream.h>
 #include <time.h>
+#include <errno.h>
 #include "Hprof.h"
 
 #define HPROF_MAGIC_STRING  "JAVA PROFILE 1.0.3"
@@ -54,11 +56,33 @@
         buf_[offset_ + 7] = (unsigned char)(value_      ); \
     } while (0)
 
+/*
+ * Initialize an hprof context struct.
+ *
+ * This will take ownership of "fileName" and "fp".
+ */
 void
 hprofContextInit(hprof_context_t *ctx, char *fileName, FILE *fp,
-    bool writeHeader)
+    bool writeHeader, bool directToDdms)
 {
     memset(ctx, 0, sizeof (*ctx));
+
+    if (directToDdms) {
+        /*
+         * Have to do this here, because it must happen after we
+         * memset the struct (want to treat fileDataPtr/fileDataSize
+         * as read-only while the file is open).
+         */
+        assert(fp == NULL);
+        fp = open_memstream(&ctx->fileDataPtr, &ctx->fileDataSize);
+        if (fp == NULL) {
+            /* not expected */
+            LOGE("hprof: open_memstream failed: %s\n", strerror(errno));
+            dvmAbort();
+        }
+    }
+
+    ctx->directToDdms = directToDdms;
     ctx->fileName = fileName;
     ctx->fp = fp;
 
diff --git a/vm/interp/Interp.c b/vm/interp/Interp.c
index 233ee3f..6ec505d 100644
--- a/vm/interp/Interp.c
+++ b/vm/interp/Interp.c
@@ -35,16 +35,368 @@
  * ===========================================================================
  */
 
+// fwd
+static BreakpointSet* dvmBreakpointSetAlloc(void);
+static void dvmBreakpointSetFree(BreakpointSet* pSet);
+
+/*
+ * Initialize global breakpoint structures.
+ */
+bool dvmBreakpointStartup(void)
+{
+#ifdef WITH_DEBUGGER
+    gDvm.breakpointSet = dvmBreakpointSetAlloc();
+    return (gDvm.breakpointSet != NULL);
+#else
+    return true;
+#endif
+}
+
+/*
+ * Free resources.
+ */
+void dvmBreakpointShutdown(void)
+{
+#ifdef WITH_DEBUGGER
+    dvmBreakpointSetFree(gDvm.breakpointSet);
+#endif
+}
+
+
+#ifdef WITH_DEBUGGER
+/*
+ * This represents a breakpoint inserted in the instruction stream.
+ *
+ * The debugger may ask us to create the same breakpoint multiple times.
+ * We only remove the breakpoint when the last instance is cleared.
+ */
+typedef struct {
+    Method*     method;                 /* method we're associated with */
+    u2*         addr;                   /* absolute memory address */
+    u1          originalOpCode;         /* original 8-bit opcode value */
+    int         setCount;               /* #of times this breakpoint was set */
+} Breakpoint;
+
+/*
+ * Set of breakpoints.
+ */
+struct BreakpointSet {
+    /* grab lock before reading or writing anything else in here */
+    pthread_mutex_t lock;
+
+    /* vector of breakpoint structures */
+    int         alloc;
+    int         count;
+    Breakpoint* breakpoints;
+};
+
+/*
+ * Initialize a BreakpointSet.  Initially empty.
+ */
+static BreakpointSet* dvmBreakpointSetAlloc(void)
+{
+    BreakpointSet* pSet = (BreakpointSet*) calloc(1, sizeof(*pSet));
+
+    dvmInitMutex(&pSet->lock);
+    /* leave the rest zeroed -- will alloc on first use */
+
+    return pSet;
+}
+
+/*
+ * Free storage associated with a BreakpointSet.
+ */
+static void dvmBreakpointSetFree(BreakpointSet* pSet)
+{
+    if (pSet == NULL)
+        return;
+
+    free(pSet->breakpoints);
+    free(pSet);
+}
+
+/*
+ * Lock the breakpoint set.
+ *
+ * It's not currently necessary to switch to VMWAIT in the event of
+ * contention, because nothing in here can block.  However, it's possible
+ * that the bytecode-updater code could become fancier in the future, so
+ * we do the trylock dance as a bit of future-proofing.
+ */
+static void dvmBreakpointSetLock(BreakpointSet* pSet)
+{
+    if (dvmTryLockMutex(&pSet->lock) != 0) {
+        Thread* self = dvmThreadSelf();
+        int oldStatus = dvmChangeStatus(self, THREAD_VMWAIT);
+        dvmLockMutex(&pSet->lock);
+        dvmChangeStatus(self, oldStatus);
+    }
+}
+
 /*
- * Initialize the breakpoint address lookup table when the debugger attaches.
+ * Unlock the breakpoint set.
+ */
+static void dvmBreakpointSetUnlock(BreakpointSet* pSet)
+{
+    dvmUnlockMutex(&pSet->lock);
+}
+
+/*
+ * Return the #of breakpoints.
+ */
+static int dvmBreakpointSetCount(const BreakpointSet* pSet)
+{
+    return pSet->count;
+}
+
+/*
+ * See if we already have an entry for this address.
+ *
+ * The BreakpointSet's lock must be acquired before calling here.
+ *
+ * Returns the index of the breakpoint entry, or -1 if not found.
+ */
+static int dvmBreakpointSetFind(const BreakpointSet* pSet, const u2* addr)
+{
+    int i;
+
+    for (i = 0; i < pSet->count; i++) {
+        Breakpoint* pBreak = &pSet->breakpoints[i];
+        if (pBreak->addr == addr)
+            return i;
+    }
+
+    return -1;
+}
+
+/*
+ * Retrieve the opcode that was originally at the specified location.
  *
- * This shouldn't be necessary -- the global area is initially zeroed out,
- * and the events should be cleaning up after themselves.
+ * The BreakpointSet's lock must be acquired before calling here.
+ *
+ * Returns "true" with the opcode in *pOrig on success.
+ */
+static bool dvmBreakpointSetOriginalOpCode(const BreakpointSet* pSet,
+    const u2* addr, u1* pOrig)
+{
+    int idx = dvmBreakpointSetFind(pSet, addr);
+    if (idx < 0)
+        return false;
+
+    *pOrig = pSet->breakpoints[idx].originalOpCode;
+    return true;
+}
+
+/*
+ * Check the opcode.  If it's a "magic" NOP, indicating the start of
+ * switch or array data in the instruction stream, we don't want to set
+ * a breakpoint.
+ *
+ * This can happen because the line number information dx generates
+ * associates the switch data with the switch statement's line number,
+ * and some debuggers put breakpoints at every address associated with
+ * a given line.  The result is that the breakpoint stomps on the NOP
+ * instruction that doubles as a data table magic number, and an explicit
+ * check in the interpreter results in an exception being thrown.
+ *
+ * We don't want to simply refuse to add the breakpoint to the table,
+ * because that confuses the housekeeping.  We don't want to reject the
+ * debugger's event request, and we want to be sure that there's exactly
+ * one un-set operation for every set op.
+ */
+static bool instructionIsMagicNop(const u2* addr)
+{
+    u2 curVal = *addr;
+    return ((curVal & 0xff) == OP_NOP && (curVal >> 8) != 0);
+}
+
+/*
+ * Add a breakpoint at a specific address.  If the address is already
+ * present in the table, this just increments the count.
+ *
+ * For a new entry, this will extract and preserve the current opcode from
+ * the instruction stream, and replace it with a breakpoint opcode.
+ *
+ * The BreakpointSet's lock must be acquired before calling here.
+ *
+ * Returns "true" on success.
+ */
+static bool dvmBreakpointSetAdd(BreakpointSet* pSet, Method* method,
+    unsigned int instrOffset)
+{
+    const int kBreakpointGrowth = 10;
+    const u2* addr = method->insns + instrOffset;
+    int idx = dvmBreakpointSetFind(pSet, addr);
+    Breakpoint* pBreak;
+
+    if (idx < 0) {
+        if (pSet->count == pSet->alloc) {
+            int newSize = pSet->alloc + kBreakpointGrowth;
+            Breakpoint* newVec;
+
+            LOGV("+++ increasing breakpoint set size to %d\n", newSize);
+
+            /* pSet->breakpoints will be NULL on first entry */
+            newVec = realloc(pSet->breakpoints, newSize * sizeof(Breakpoint));
+            if (newVec == NULL)
+                return false;
+
+            pSet->breakpoints = newVec;
+            pSet->alloc = newSize;
+        }
+
+        pBreak = &pSet->breakpoints[pSet->count++];
+        pBreak->method = method;
+        pBreak->addr = (u2*)addr;
+        pBreak->originalOpCode = *(u1*)addr;
+        pBreak->setCount = 1;
+
+        /*
+         * Change the opcode.  We must ensure that the BreakpointSet
+         * updates happen before we change the opcode.
+         *
+         * If the method has not been verified, we do NOT insert the
+         * breakpoint yet, since that will screw up the verifier.  The
+         * debugger is allowed to insert breakpoints in unverified code,
+         * but since we don't execute unverified code we don't need to
+         * alter the bytecode yet.
+         *
+         * The class init code will "flush" all relevant breakpoints when
+         * verification completes.
+         */
+        MEM_BARRIER();
+        assert(*(u1*)addr != OP_BREAKPOINT);
+        if (dvmIsClassVerified(method->clazz)) {
+            LOGV("Class %s verified, adding breakpoint at %p\n",
+                method->clazz->descriptor, addr);
+            if (instructionIsMagicNop(addr)) {
+                LOGV("Refusing to set breakpoint on %04x at %s.%s + 0x%x\n",
+                    *addr, method->clazz->descriptor, method->name,
+                    instrOffset);
+            } else {
+                dvmDexChangeDex1(method->clazz->pDvmDex, (u1*)addr,
+                    OP_BREAKPOINT);
+            }
+        } else {
+            LOGV("Class %s NOT verified, deferring breakpoint at %p\n",
+                method->clazz->descriptor, addr);
+        }
+    } else {
+        pBreak = &pSet->breakpoints[idx];
+        pBreak->setCount++;
+
+        /*
+         * Instruction stream may not have breakpoint opcode yet -- flush
+         * may be pending during verification of class.
+         */
+        //assert(*(u1*)addr == OP_BREAKPOINT);
+    }
+
+    return true;
+}
+
+/*
+ * Remove one instance of the specified breakpoint.  When the count
+ * reaches zero, the entry is removed from the table, and the original
+ * opcode is restored.
+ *
+ * The BreakpointSet's lock must be acquired before calling here.
+ */
+static void dvmBreakpointSetRemove(BreakpointSet* pSet, Method* method,
+    unsigned int instrOffset)
+{
+    const u2* addr = method->insns + instrOffset;
+    int idx = dvmBreakpointSetFind(pSet, addr);
+
+    if (idx < 0) {
+        /* breakpoint not found in set -- unexpected */
+        if (*(u1*)addr == OP_BREAKPOINT) {
+            LOGE("Unable to restore breakpoint opcode (%s.%s +0x%x)\n",
+                method->clazz->descriptor, method->name, instrOffset);
+            dvmAbort();
+        } else {
+            LOGW("Breakpoint was already restored? (%s.%s +0x%x)\n",
+                method->clazz->descriptor, method->name, instrOffset);
+        }
+    } else {
+        Breakpoint* pBreak = &pSet->breakpoints[idx];
+        if (pBreak->setCount == 1) {
+            /*
+             * Must restore opcode before removing set entry.
+             *
+             * If the breakpoint was never flushed, we could be ovewriting
+             * a value with the same value.  Not a problem, though we
+             * could end up causing a copy-on-write here when we didn't
+             * need to.  (Not worth worrying about.)
+             */
+            dvmDexChangeDex1(method->clazz->pDvmDex, (u1*)addr,
+                pBreak->originalOpCode);
+            MEM_BARRIER();
+
+            if (idx != pSet->count-1) {
+                /* shift down */
+                memmove(&pSet->breakpoints[idx], &pSet->breakpoints[idx+1],
+                    (pSet->count-1 - idx) * sizeof(pSet->breakpoints[0]));
+            }
+            pSet->count--;
+            pSet->breakpoints[pSet->count].addr = (u2*) 0xdecadead; // debug
+        } else {
+            pBreak->setCount--;
+            assert(pBreak->setCount > 0);
+        }
+    }
+}
+
+/*
+ * Flush any breakpoints associated with methods in "clazz".  We want to
+ * change the opcode, which might not have happened when the breakpoint
+ * was initially set because the class was in the process of being
+ * verified.
+ *
+ * The BreakpointSet's lock must be acquired before calling here.
+ */
+static void dvmBreakpointSetFlush(BreakpointSet* pSet, ClassObject* clazz)
+{
+    int i;
+    for (i = 0; i < pSet->count; i++) {
+        Breakpoint* pBreak = &pSet->breakpoints[i];
+        if (pBreak->method->clazz == clazz) {
+            /*
+             * The breakpoint is associated with a method in this class.
+             * It might already be there or it might not; either way,
+             * flush it out.
+             */
+            LOGV("Flushing breakpoint at %p for %s\n",
+                pBreak->addr, clazz->descriptor);
+            if (instructionIsMagicNop(pBreak->addr)) {
+                const Method* method = pBreak->method;
+                LOGV("Refusing to flush breakpoint on %04x at %s.%s + 0x%x\n",
+                    *pBreak->addr, method->clazz->descriptor,
+                    method->name, pBreak->addr - method->insns);
+            } else {
+                dvmDexChangeDex1(clazz->pDvmDex, (u1*)pBreak->addr,
+                    OP_BREAKPOINT);
+            }
+        }
+    }
+}
+#endif /*WITH_DEBUGGER*/
+
+
+/*
+ * Do any debugger-attach-time initialization.
  */
 void dvmInitBreakpoints(void)
 {
 #ifdef WITH_DEBUGGER
-    memset(gDvm.debugBreakAddr, 0, sizeof(gDvm.debugBreakAddr));
+    /* quick sanity check */
+    BreakpointSet* pSet = gDvm.breakpointSet;
+    dvmBreakpointSetLock(pSet);
+    if (dvmBreakpointSetCount(pSet) != 0) {
+        LOGW("WARNING: %d leftover breakpoints\n", dvmBreakpointSetCount(pSet));
+        /* generally not good, but we can keep going */
+    }
+    dvmBreakpointSetUnlock(pSet);
 #else
     assert(false);
 #endif
@@ -64,29 +416,13 @@ void dvmInitBreakpoints(void)
  *
  * "addr" is the absolute address of the breakpoint bytecode.
  */
-void dvmAddBreakAddr(Method* method, int instrOffset)
+void dvmAddBreakAddr(Method* method, unsigned int instrOffset)
 {
 #ifdef WITH_DEBUGGER
-    const u2* addr = method->insns + instrOffset;
-    const u2** ptr = gDvm.debugBreakAddr;
-    int i;
-
-    LOGV("BKP: add %p %s.%s (%s:%d)\n",
-        addr, method->clazz->descriptor, method->name,
-        dvmGetMethodSourceFile(method), dvmLineNumFromPC(method, instrOffset));
-
-    method->debugBreakpointCount++;
-    for (i = 0; i < MAX_BREAKPOINTS; i++, ptr++) {
-        if (*ptr == NULL) {
-            *ptr = addr;
-            break;
-        }
-    }
-    if (i == MAX_BREAKPOINTS) {
-        /* no room; size is too small or we're not cleaning up properly */
-        LOGE("ERROR: max breakpoints exceeded\n");
-        assert(false);
-    }
+    BreakpointSet* pSet = gDvm.breakpointSet;
+    dvmBreakpointSetLock(pSet);
+    dvmBreakpointSetAdd(pSet, method, instrOffset);
+    dvmBreakpointSetUnlock(pSet);
 #else
     assert(false);
 #endif
@@ -102,35 +438,62 @@ void dvmAddBreakAddr(Method* method, int instrOffset)
  * synchronized, so it should not be possible for two threads to be
  * updating breakpoints at the same time.
  */
-void dvmClearBreakAddr(Method* method, int instrOffset)
+void dvmClearBreakAddr(Method* method, unsigned int instrOffset)
 {
 #ifdef WITH_DEBUGGER
-    const u2* addr = method->insns + instrOffset;
-    const u2** ptr = gDvm.debugBreakAddr;
-    int i;
+    BreakpointSet* pSet = gDvm.breakpointSet;
+    dvmBreakpointSetLock(pSet);
+    dvmBreakpointSetRemove(pSet, method, instrOffset);
+    dvmBreakpointSetUnlock(pSet);
 
-    LOGV("BKP: clear %p %s.%s (%s:%d)\n",
-        addr, method->clazz->descriptor, method->name,
-        dvmGetMethodSourceFile(method), dvmLineNumFromPC(method, instrOffset));
-
-    method->debugBreakpointCount--;
-    assert(method->debugBreakpointCount >= 0);
-    for (i = 0; i < MAX_BREAKPOINTS; i++, ptr++) {
-        if (*ptr == addr) {
-            *ptr = NULL;
-            break;
-        }
-    }
-    if (i == MAX_BREAKPOINTS) {
-        /* didn't find it */
-        LOGE("ERROR: breakpoint on %p not found\n", addr);
-        assert(false);
-    }
 #else
     assert(false);
 #endif
 }
 
+#ifdef WITH_DEBUGGER
+/*
+ * Get the original opcode from under a breakpoint.
+ */
+u1 dvmGetOriginalOpCode(const u2* addr)
+{
+    BreakpointSet* pSet = gDvm.breakpointSet;
+    u1 orig = 0;
+
+    dvmBreakpointSetLock(pSet);
+    if (!dvmBreakpointSetOriginalOpCode(pSet, addr, &orig)) {
+        orig = *(u1*)addr;
+        if (orig == OP_BREAKPOINT) {
+            LOGE("GLITCH: can't find breakpoint, opcode is still set\n");
+            dvmAbort();
+        }
+    }
+    dvmBreakpointSetUnlock(pSet);
+
+    return orig;
+}
+
+/*
+ * Flush any breakpoints associated with methods in "clazz".
+ *
+ * We don't want to modify the bytecode of a method before the verifier
+ * gets a chance to look at it, so we postpone opcode replacement until
+ * after verification completes.
+ */
+void dvmFlushBreakpoints(ClassObject* clazz)
+{
+    BreakpointSet* pSet = gDvm.breakpointSet;
+
+    if (pSet == NULL)
+        return;
+
+    assert(dvmIsClassVerified(clazz));
+    dvmBreakpointSetLock(pSet);
+    dvmBreakpointSetFlush(pSet, clazz);
+    dvmBreakpointSetUnlock(pSet);
+}
+#endif
+
 /*
  * Add a single step event.  Currently this is a global item.
  *
@@ -458,7 +821,6 @@ s4 dvmInterpHandleSparseSwitch(const u2* switchData, s4 testVal)
     u2 ident, size;
     const s4* keys;
     const s4* entries;
-    int i;
 
     /*
      * Sparse switch data format:
@@ -493,20 +855,23 @@ s4 dvmInterpHandleSparseSwitch(const u2* switchData, s4 testVal)
     assert(((u4)entries & 0x3) == 0);
 
     /*
-     * Run through the list of keys, which are guaranteed to
+     * Binary-search through the array of keys, which are guaranteed to
      * be sorted low-to-high.
-     *
-     * Most tables have 3-4 entries.  Few have more than 10.  A binary
-     * search here is probably not useful.
      */
-    for (i = 0; i < size; i++) {
-        s4 k = s4FromSwitchData(&keys[i]);
-        if (k == testVal) {
+    int lo = 0;
+    int hi = size - 1;
+    while (lo <= hi) {
+        int mid = (lo + hi) >> 1;
+
+        s4 foundVal = s4FromSwitchData(&keys[mid]);
+        if (testVal < foundVal) {
+            hi = mid - 1;
+        } else if (testVal > foundVal) {
+            lo = mid + 1;
+        } else {
             LOGVV("Value %d found in entry %d (goto 0x%02x)\n",
-                testVal, i, s4FromSwitchData(&entries[i]));
-            return s4FromSwitchData(&entries[i]);
-        } else if (k > testVal) {
-            break;
+                testVal, mid, s4FromSwitchData(&entries[mid]));
+            return s4FromSwitchData(&entries[mid]);
         }
     }
 
@@ -890,13 +1255,20 @@ void dvmInterpret(Thread* self, const Method* method, JValue* pResult)
     InterpState interpState;
     bool change;
 #if defined(WITH_JIT)
+    /* Target-specific save/restore */
+    extern void dvmJitCalleeSave(double *saveArea);
+    extern void dvmJitCalleeRestore(double *saveArea);
     /* Interpreter entry points from compiled code */
     extern void dvmJitToInterpNormal();
     extern void dvmJitToInterpNoChain();
     extern void dvmJitToInterpPunt();
     extern void dvmJitToInterpSingleStep();
-    extern void dvmJitToTraceSelect();
+    extern void dvmJitToInterpTraceSelectNoChain();
+    extern void dvmJitToInterpTraceSelect();
     extern void dvmJitToPatchPredictedChain();
+#if defined(WITH_SELF_VERIFICATION)
+    extern void dvmJitToInterpBackwardBranch();
+#endif
 
     /*
      * Reserve a static entity here to quickly setup runtime contents as
@@ -907,9 +1279,15 @@ void dvmInterpret(Thread* self, const Method* method, JValue* pResult)
         dvmJitToInterpNoChain,
         dvmJitToInterpPunt,
         dvmJitToInterpSingleStep,
-        dvmJitToTraceSelect,
+        dvmJitToInterpTraceSelectNoChain,
+        dvmJitToInterpTraceSelect,
         dvmJitToPatchPredictedChain,
+#if defined(WITH_SELF_VERIFICATION)
+        dvmJitToInterpBackwardBranch,
+#endif
     };
+
+    assert(self->inJitCodeCache == NULL);
 #endif
 
 
@@ -921,7 +1299,9 @@ void dvmInterpret(Thread* self, const Method* method, JValue* pResult)
     interpState.debugIsMethodEntry = true;
 #endif
 #if defined(WITH_JIT)
-    interpState.jitState = gDvmJit.pJitEntryTable ? kJitNormal : kJitOff;
+    dvmJitCalleeSave(interpState.calleeSave);
+    /* Initialize the state to kJitNot */
+    interpState.jitState = kJitNot;
 
     /* Setup the Jit-to-interpreter entry points */
     interpState.jitToInterpEntries = jitToInterpEntries;
@@ -998,4 +1378,7 @@ void dvmInterpret(Thread* self, const Method* method, JValue* pResult)
     }
 
     *pResult = interpState.retval;
+#if defined(WITH_JIT)
+    dvmJitCalleeRestore(interpState.calleeSave);
+#endif
 }
diff --git a/vm/interp/Interp.h b/vm/interp/Interp.h
index cd4c7ec..1964b57 100644
--- a/vm/interp/Interp.h
+++ b/vm/interp/Interp.h
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Dalvik interpreter public definitions.
  */
@@ -34,12 +35,30 @@ void dvmInterpret(Thread* thread, const Method* method, JValue* pResult);
 void dvmThrowVerificationError(const Method* method, int kind, int ref);
 
 /*
- * Breakpoint optimization table.
+ * One-time initialization and shutdown.
+ */
+bool dvmBreakpointStartup(void);
+void dvmBreakpointShutdown(void);
+
+/*
+ * Breakpoint implementation.
  */
 void dvmInitBreakpoints();
-void dvmAddBreakAddr(Method* method, int instrOffset);
-void dvmClearBreakAddr(Method* method, int instrOffset);
+void dvmAddBreakAddr(Method* method, unsigned int instrOffset);
+void dvmClearBreakAddr(Method* method, unsigned int instrOffset);
 bool dvmAddSingleStep(Thread* thread, int size, int depth);
 void dvmClearSingleStep(Thread* thread);
 
+#ifdef WITH_DEBUGGER
+/*
+ * Recover the opcode that was replaced by a breakpoint.
+ */
+u1 dvmGetOriginalOpCode(const u2* addr);
+
+/*
+ * Flush any breakpoints associated with methods in "clazz".
+ */
+void dvmFlushBreakpoints(ClassObject* clazz);
+#endif
+
 #endif /*_DALVIK_INTERP_INTERP*/
diff --git a/vm/interp/InterpDefs.h b/vm/interp/InterpDefs.h
index c9c80e3..826c6d2 100644
--- a/vm/interp/InterpDefs.h
+++ b/vm/interp/InterpDefs.h
@@ -64,17 +64,35 @@ typedef enum InterpEntry {
  *    just been reached).
  * 6) dvmJitToPredictedChain: patch the chaining cell for a virtual call site
  *    to a predicted callee.
+ * 7) dvmJitToBackwardBranch: (WITH_SELF_VERIFICATION ONLY) special case of 1)
+ *    and 5). This is used instead if the ending branch of the trace jumps back
+ *    into the same basic block.
  */
 struct JitToInterpEntries {
     void *dvmJitToInterpNormal;
     void *dvmJitToInterpNoChain;
     void *dvmJitToInterpPunt;
     void *dvmJitToInterpSingleStep;
-    void *dvmJitToTraceSelect;
+    void *dvmJitToInterpTraceSelectNoChain;
+    void *dvmJitToInterpTraceSelect;
     void *dvmJitToPatchPredictedChain;
+#if defined(WITH_SELF_VERIFICATION)
+    void *dvmJitToInterpBackwardBranch;
+#endif
 };
 
-#define JIT_TRACE_THRESH_FILTER_SIZE  16
+/*
+ * Size of save area for callee-save FP regs, which are not automatically
+ * saved by interpreter main because it doesn't use them (but Jit'd code
+ * may). Save/restore routine is defined by target, and size should
+ * be >= max needed by any target.
+ */
+#define JIT_CALLEE_SAVE_DOUBLE_COUNT 8
+
+/* Number of entries in the 2nd level JIT profiler filter cache */
+#define JIT_TRACE_THRESH_FILTER_SIZE 32
+/* Granularity of coverage (power of 2) by each cached entry */
+#define JIT_TRACE_THRESH_FILTER_GRAN_LOG2 6
 #endif
 
 /*
@@ -129,8 +147,17 @@ typedef struct InterpState {
      */
     unsigned char*     pJitProfTable;
     JitState           jitState;
-    void*              jitResume;
-    u2*                jitResumePC;
+    const void*        jitResumeNPC;	// Native PC of compiled code
+    const u2*          jitResumeDPC;	// Dalvik PC corresponding to NPC
+    int                jitThreshold;
+    /*
+     * ppJitProfTable holds the address of gDvmJit.pJitProfTable, which
+     * doubles as an on/off switch for the Jit.  Because a change in
+     * the value of gDvmJit.pJitProfTable isn't reflected in the cached
+     * copy above (pJitProfTable), we need to periodically refresh it.
+     * ppJitProfTable is used for that purpose.
+     */
+    unsigned char**    ppJitProfTable; // Used to refresh pJitProfTable
 #endif
 
 #if defined(WITH_PROFILER) || defined(WITH_DEBUGGER)
@@ -145,12 +172,14 @@ typedef struct InterpState {
 
     int currTraceRun;
     int totalTraceLen;        // Number of Dalvik insts in trace
-    const u2* currTraceHead;        // Start of the trace we're building
-    const u2* currRunHead;          // Start of run we're building
+    const u2* currTraceHead;  // Start of the trace we're building
+    const u2* currRunHead;    // Start of run we're building
     int currRunLen;           // Length of run in 16-bit words
     int lastThreshFilter;
-    const u2* threshFilter[JIT_TRACE_THRESH_FILTER_SIZE];
+    const u2* lastPC;         // Stage the PC first for the threaded interpreter
+    intptr_t threshFilter[JIT_TRACE_THRESH_FILTER_SIZE];
     JitTraceRun trace[MAX_JIT_RUN_LEN];
+    double calleeSave[JIT_CALLEE_SAVE_DOUBLE_COUNT];
 #endif
 
 } InterpState;
@@ -215,9 +244,9 @@ static inline bool dvmDebuggerOrProfilerActive(void)
  * Determine if the jit, debugger or profiler is currently active.  Used when
  * selecting which interpreter to switch to.
  */
-static inline bool dvmJitDebuggerOrProfilerActive(int jitState)
+static inline bool dvmJitDebuggerOrProfilerActive()
 {
-    return jitState != kJitOff
+    return gDvmJit.pProfTable != NULL
 #if defined(WITH_PROFILER)
         || gDvm.activeProfilers != 0
 #endif
diff --git a/vm/interp/Jit.c b/vm/interp/Jit.c
index 2bcb1f5..f475773 100644
--- a/vm/interp/Jit.c
+++ b/vm/interp/Jit.c
@@ -33,58 +33,356 @@
 #include "compiler/CompilerIR.h"
 #include <errno.h>
 
-int dvmJitStartup(void)
+#if defined(WITH_SELF_VERIFICATION)
+/* Allocate space for per-thread ShadowSpace data structures */
+void* dvmSelfVerificationShadowSpaceAlloc(Thread* self)
 {
-    unsigned int i;
-    bool res = true;  /* Assume success */
-
-    // Create the compiler thread and setup miscellaneous chores */
-    res &= dvmCompilerStartup();
-
-    dvmInitMutex(&gDvmJit.tableLock);
-    if (res && gDvm.executionMode == kExecutionModeJit) {
-        JitEntry *pJitTable = NULL;
-        unsigned char *pJitProfTable = NULL;
-        assert(gDvm.jitTableSize &&
-            !(gDvm.jitTableSize & (gDvmJit.jitTableSize - 1))); // Power of 2?
-        dvmLockMutex(&gDvmJit.tableLock);
-        pJitTable = (JitEntry*)
-                    calloc(gDvmJit.jitTableSize, sizeof(*pJitTable));
-        if (!pJitTable) {
-            LOGE("jit table allocation failed\n");
-            res = false;
-            goto done;
+    self->shadowSpace = (ShadowSpace*) calloc(1, sizeof(ShadowSpace));
+    if (self->shadowSpace == NULL)
+        return NULL;
+
+    self->shadowSpace->registerSpaceSize = REG_SPACE;
+    self->shadowSpace->registerSpace =
+        (int*) calloc(self->shadowSpace->registerSpaceSize, sizeof(int));
+
+    return self->shadowSpace->registerSpace;
+}
+
+/* Free per-thread ShadowSpace data structures */
+void dvmSelfVerificationShadowSpaceFree(Thread* self)
+{
+    free(self->shadowSpace->registerSpace);
+    free(self->shadowSpace);
+}
+
+/*
+ * Save out PC, FP, InterpState, and registers to shadow space.
+ * Return a pointer to the shadow space for JIT to use.
+ */
+void* dvmSelfVerificationSaveState(const u2* pc, const void* fp,
+                                   InterpState* interpState, int targetTrace)
+{
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    unsigned preBytes = interpState->method->outsSize*4 + sizeof(StackSaveArea);
+    unsigned postBytes = interpState->method->registersSize*4;
+
+    //LOGD("### selfVerificationSaveState(%d) pc: 0x%x fp: 0x%x",
+    //    self->threadId, (int)pc, (int)fp);
+
+    if (shadowSpace->selfVerificationState != kSVSIdle) {
+        LOGD("~~~ Save: INCORRECT PREVIOUS STATE(%d): %d",
+            self->threadId, shadowSpace->selfVerificationState);
+        LOGD("********** SHADOW STATE DUMP **********");
+        LOGD("PC: 0x%x FP: 0x%x", (int)pc, (int)fp);
+    }
+    shadowSpace->selfVerificationState = kSVSStart;
+
+    if (interpState->entryPoint == kInterpEntryResume) {
+        interpState->entryPoint = kInterpEntryInstr;
+#if 0
+        /* Tracking the success rate of resume after single-stepping */
+        if (interpState->jitResumeDPC == pc) {
+            LOGD("SV single step resumed at %p", pc);
+        }
+        else {
+            LOGD("real %p DPC %p NPC %p", pc, interpState->jitResumeDPC,
+                 interpState->jitResumeNPC);
         }
+#endif
+    }
+
+    // Dynamically grow shadow register space if necessary
+    if (preBytes + postBytes > shadowSpace->registerSpaceSize * sizeof(u4)) {
+        free(shadowSpace->registerSpace);
+        shadowSpace->registerSpaceSize = (preBytes + postBytes) / sizeof(u4);
+        shadowSpace->registerSpace =
+            (int*) calloc(shadowSpace->registerSpaceSize, sizeof(u4));
+    }
+
+    // Remember original state
+    shadowSpace->startPC = pc;
+    shadowSpace->fp = fp;
+    shadowSpace->glue = interpState;
+    /*
+     * Store the original method here in case the trace ends with a
+     * return/invoke, the last method.
+     */
+    shadowSpace->method = interpState->method;
+    shadowSpace->shadowFP = shadowSpace->registerSpace +
+                            shadowSpace->registerSpaceSize - postBytes/4;
+
+    // Create a copy of the InterpState
+    memcpy(&(shadowSpace->interpState), interpState, sizeof(InterpState));
+    shadowSpace->interpState.fp = shadowSpace->shadowFP;
+    shadowSpace->interpState.interpStackEnd = (u1*)shadowSpace->registerSpace;
+
+    // Create a copy of the stack
+    memcpy(((char*)shadowSpace->shadowFP)-preBytes, ((char*)fp)-preBytes,
+        preBytes+postBytes);
+
+    // Setup the shadowed heap space
+    shadowSpace->heapSpaceTail = shadowSpace->heapSpace;
+
+    // Reset trace length
+    shadowSpace->traceLength = 0;
+
+    return shadowSpace;
+}
+
+/*
+ * Save ending PC, FP and compiled code exit point to shadow space.
+ * Return a pointer to the shadow space for JIT to restore state.
+ */
+void* dvmSelfVerificationRestoreState(const u2* pc, const void* fp,
+                                      SelfVerificationState exitPoint)
+{
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    // Official InterpState structure
+    InterpState *realGlue = shadowSpace->glue;
+    shadowSpace->endPC = pc;
+    shadowSpace->endShadowFP = fp;
+
+    //LOGD("### selfVerificationRestoreState(%d) pc: 0x%x fp: 0x%x endPC: 0x%x",
+    //    self->threadId, (int)shadowSpace->startPC, (int)shadowSpace->fp,
+    //    (int)pc);
+
+    if (shadowSpace->selfVerificationState != kSVSStart) {
+        LOGD("~~~ Restore: INCORRECT PREVIOUS STATE(%d): %d",
+            self->threadId, shadowSpace->selfVerificationState);
+        LOGD("********** SHADOW STATE DUMP **********");
+        LOGD("Dalvik PC: 0x%x endPC: 0x%x", (int)shadowSpace->startPC,
+            (int)shadowSpace->endPC);
+        LOGD("Interp FP: 0x%x", (int)shadowSpace->fp);
+        LOGD("Shadow FP: 0x%x endFP: 0x%x", (int)shadowSpace->shadowFP,
+            (int)shadowSpace->endShadowFP);
+    }
+
+    // Move the resume [ND]PC from the shadow space to the real space so that
+    // the debug interpreter can return to the translation
+    if (exitPoint == kSVSSingleStep) {
+        realGlue->jitResumeNPC = shadowSpace->interpState.jitResumeNPC;
+        realGlue->jitResumeDPC = shadowSpace->interpState.jitResumeDPC;
+    } else {
+        realGlue->jitResumeNPC = NULL;
+        realGlue->jitResumeDPC = NULL;
+    }
+
+    // Special case when punting after a single instruction
+    if (exitPoint == kSVSPunt && pc == shadowSpace->startPC) {
+        shadowSpace->selfVerificationState = kSVSIdle;
+    } else {
+        shadowSpace->selfVerificationState = exitPoint;
+    }
+
+    return shadowSpace;
+}
+
+/* Print contents of virtual registers */
+static void selfVerificationPrintRegisters(int* addr, int* addrRef,
+                                           int numWords)
+{
+    int i;
+    for (i = 0; i < numWords; i++) {
+        LOGD("(v%d) 0x%8x%s", i, addr[i], addr[i] != addrRef[i] ? " X" : "");
+    }
+}
+
+/* Print values maintained in shadowSpace */
+static void selfVerificationDumpState(const u2* pc, Thread* self)
+{
+    ShadowSpace* shadowSpace = self->shadowSpace;
+    StackSaveArea* stackSave = SAVEAREA_FROM_FP(self->curFrame);
+    int frameBytes = (int) shadowSpace->registerSpace +
+                     shadowSpace->registerSpaceSize*4 -
+                     (int) shadowSpace->shadowFP;
+    int localRegs = 0;
+    int frameBytes2 = 0;
+    if (self->curFrame < shadowSpace->fp) {
+        localRegs = (stackSave->method->registersSize -
+                     stackSave->method->insSize)*4;
+        frameBytes2 = (int) shadowSpace->fp - (int) self->curFrame - localRegs;
+    }
+    LOGD("********** SHADOW STATE DUMP **********");
+    LOGD("CurrentPC: 0x%x, Offset: 0x%04x", (int)pc,
+        (int)(pc - stackSave->method->insns));
+    LOGD("Class: %s", shadowSpace->method->clazz->descriptor);
+    LOGD("Method: %s", shadowSpace->method->name);
+    LOGD("Dalvik PC: 0x%x endPC: 0x%x", (int)shadowSpace->startPC,
+        (int)shadowSpace->endPC);
+    LOGD("Interp FP: 0x%x endFP: 0x%x", (int)shadowSpace->fp,
+        (int)self->curFrame);
+    LOGD("Shadow FP: 0x%x endFP: 0x%x", (int)shadowSpace->shadowFP,
+        (int)shadowSpace->endShadowFP);
+    LOGD("Frame1 Bytes: %d Frame2 Local: %d Bytes: %d", frameBytes,
+        localRegs, frameBytes2);
+    LOGD("Trace length: %d State: %d", shadowSpace->traceLength,
+        shadowSpace->selfVerificationState);
+}
+
+/* Print decoded instructions in the current trace */
+static void selfVerificationDumpTrace(const u2* pc, Thread* self)
+{
+    ShadowSpace* shadowSpace = self->shadowSpace;
+    StackSaveArea* stackSave = SAVEAREA_FROM_FP(self->curFrame);
+    int i, addr, offset;
+    DecodedInstruction *decInsn;
+
+    LOGD("********** SHADOW TRACE DUMP **********");
+    for (i = 0; i < shadowSpace->traceLength; i++) {
+        addr = shadowSpace->trace[i].addr;
+        offset =  (int)((u2*)addr - stackSave->method->insns);
+        decInsn = &(shadowSpace->trace[i].decInsn);
+        /* Not properly decoding instruction, some registers may be garbage */
+        LOGD("0x%x: (0x%04x) %s", addr, offset, getOpcodeName(decInsn->opCode));
+    }
+}
+
+/* Code is forced into this spin loop when a divergence is detected */
+static void selfVerificationSpinLoop(ShadowSpace *shadowSpace)
+{
+    const u2 *startPC = shadowSpace->startPC;
+    JitTraceDescription* desc = dvmCopyTraceDescriptor(startPC, NULL);
+    if (desc) {
+        dvmCompilerWorkEnqueue(startPC, kWorkOrderTraceDebug, desc);
         /*
-         * NOTE: the profile table must only be allocated once, globally.
-         * Profiling is turned on and off by nulling out gDvm.pJitProfTable
-         * and then restoring its original value.  However, this action
-         * is not syncronized for speed so threads may continue to hold
-         * and update the profile table after profiling has been turned
-         * off by null'ng the global pointer.  Be aware.
+         * This function effectively terminates the VM right here, so not
+         * freeing the desc pointer when the enqueuing fails is acceptable.
          */
-        pJitProfTable = (unsigned char *)malloc(JIT_PROF_SIZE);
-        if (!pJitProfTable) {
-            LOGE("jit prof table allocation failed\n");
-            res = false;
-            goto done;
+    }
+    gDvmJit.selfVerificationSpin = true;
+    while(gDvmJit.selfVerificationSpin) sleep(10);
+}
+
+/* Manage self verification while in the debug interpreter */
+static bool selfVerificationDebugInterp(const u2* pc, Thread* self,
+                                        InterpState *interpState)
+{
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    SelfVerificationState state = shadowSpace->selfVerificationState;
+
+    DecodedInstruction decInsn;
+    dexDecodeInstruction(gDvm.instrFormat, pc, &decInsn);
+
+    //LOGD("### DbgIntp(%d): PC: 0x%x endPC: 0x%x state: %d len: %d %s",
+    //    self->threadId, (int)pc, (int)shadowSpace->endPC, state,
+    //    shadowSpace->traceLength, getOpcodeName(decInsn.opCode));
+
+    if (state == kSVSIdle || state == kSVSStart) {
+        LOGD("~~~ DbgIntrp: INCORRECT PREVIOUS STATE(%d): %d",
+            self->threadId, state);
+        selfVerificationDumpState(pc, self);
+        selfVerificationDumpTrace(pc, self);
+    }
+
+    /*
+     * Skip endPC once when trace has a backward branch. If the SV state is
+     * single step, keep it that way.
+     */
+    if ((state == kSVSBackwardBranch && pc == shadowSpace->endPC) ||
+        (state != kSVSBackwardBranch && state != kSVSSingleStep)) {
+        shadowSpace->selfVerificationState = kSVSDebugInterp;
+    }
+
+    /* Check that the current pc is the end of the trace */
+    if ((state == kSVSDebugInterp || state == kSVSSingleStep) &&
+        pc == shadowSpace->endPC) {
+
+        shadowSpace->selfVerificationState = kSVSIdle;
+
+        /* Check register space */
+        int frameBytes = (int) shadowSpace->registerSpace +
+                         shadowSpace->registerSpaceSize*4 -
+                         (int) shadowSpace->shadowFP;
+        if (memcmp(shadowSpace->fp, shadowSpace->shadowFP, frameBytes)) {
+            LOGD("~~~ DbgIntp(%d): REGISTERS DIVERGENCE!", self->threadId);
+            selfVerificationDumpState(pc, self);
+            selfVerificationDumpTrace(pc, self);
+            LOGD("*** Interp Registers: addr: 0x%x bytes: %d",
+                (int)shadowSpace->fp, frameBytes);
+            selfVerificationPrintRegisters((int*)shadowSpace->fp,
+                                           (int*)shadowSpace->shadowFP,
+                                           frameBytes/4);
+            LOGD("*** Shadow Registers: addr: 0x%x bytes: %d",
+                (int)shadowSpace->shadowFP, frameBytes);
+            selfVerificationPrintRegisters((int*)shadowSpace->shadowFP,
+                                           (int*)shadowSpace->fp,
+                                           frameBytes/4);
+            selfVerificationSpinLoop(shadowSpace);
         }
-        memset(pJitProfTable,0,JIT_PROF_SIZE);
-        for (i=0; i < gDvmJit.jitTableSize; i++) {
-           pJitTable[i].u.info.chain = gDvmJit.jitTableSize;
+        /* Check new frame if it exists (invokes only) */
+        if (self->curFrame < shadowSpace->fp) {
+            StackSaveArea* stackSave = SAVEAREA_FROM_FP(self->curFrame);
+            int localRegs = (stackSave->method->registersSize -
+                             stackSave->method->insSize)*4;
+            int frameBytes2 = (int) shadowSpace->fp -
+                              (int) self->curFrame - localRegs;
+            if (memcmp(((char*)self->curFrame)+localRegs,
+                ((char*)shadowSpace->endShadowFP)+localRegs, frameBytes2)) {
+                LOGD("~~~ DbgIntp(%d): REGISTERS (FRAME2) DIVERGENCE!",
+                    self->threadId);
+                selfVerificationDumpState(pc, self);
+                selfVerificationDumpTrace(pc, self);
+                LOGD("*** Interp Registers: addr: 0x%x l: %d bytes: %d",
+                    (int)self->curFrame, localRegs, frameBytes2);
+                selfVerificationPrintRegisters((int*)self->curFrame,
+                                               (int*)shadowSpace->endShadowFP,
+                                               (frameBytes2+localRegs)/4);
+                LOGD("*** Shadow Registers: addr: 0x%x l: %d bytes: %d",
+                    (int)shadowSpace->endShadowFP, localRegs, frameBytes2);
+                selfVerificationPrintRegisters((int*)shadowSpace->endShadowFP,
+                                               (int*)self->curFrame,
+                                               (frameBytes2+localRegs)/4);
+                selfVerificationSpinLoop(shadowSpace);
+            }
+        }
+
+        /* Check memory space */
+        bool memDiff = false;
+        ShadowHeap* heapSpacePtr;
+        for (heapSpacePtr = shadowSpace->heapSpace;
+             heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+            int memData = *((unsigned int*) heapSpacePtr->addr);
+            if (heapSpacePtr->data != memData) {
+                LOGD("~~~ DbgIntp(%d): MEMORY DIVERGENCE!", self->threadId);
+                LOGD("Addr: 0x%x Intrp Data: 0x%x Jit Data: 0x%x",
+                    heapSpacePtr->addr, memData, heapSpacePtr->data);
+                selfVerificationDumpState(pc, self);
+                selfVerificationDumpTrace(pc, self);
+                memDiff = true;
+            }
         }
-        /* Is chain field wide enough for termination pattern? */
-        assert(pJitTable[0].u.info.chain == gDvm.maxJitTableEntries);
-
-done:
-        gDvmJit.pJitEntryTable = pJitTable;
-        gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
-        gDvmJit.jitTableEntriesUsed = 0;
-        gDvmJit.pProfTableCopy = gDvmJit.pProfTable = pJitProfTable;
-        dvmUnlockMutex(&gDvmJit.tableLock);
+        if (memDiff) selfVerificationSpinLoop(shadowSpace);
+
+        /*
+         * Switch to JIT single step mode to stay in the debug interpreter for
+         * one more instruction
+         */
+        if (state == kSVSSingleStep) {
+            interpState->jitState = kJitSingleStepEnd;
+        }
+        return true;
+
+    /* If end not been reached, make sure max length not exceeded */
+    } else if (shadowSpace->traceLength >= JIT_MAX_TRACE_LEN) {
+        LOGD("~~~ DbgIntp(%d): CONTROL DIVERGENCE!", self->threadId);
+        LOGD("startPC: 0x%x endPC: 0x%x currPC: 0x%x",
+            (int)shadowSpace->startPC, (int)shadowSpace->endPC, (int)pc);
+        selfVerificationDumpState(pc, self);
+        selfVerificationDumpTrace(pc, self);
+        selfVerificationSpinLoop(shadowSpace);
+
+        return true;
     }
-    return res;
+    /* Log the instruction address and decoded instruction for debug */
+    shadowSpace->trace[shadowSpace->traceLength].addr = (int)pc;
+    shadowSpace->trace[shadowSpace->traceLength].decInsn = decInsn;
+    shadowSpace->traceLength++;
+
+    return false;
 }
+#endif
 
 /*
  * If one of our fixed tables or the translation buffer fills up,
@@ -103,26 +401,26 @@ void dvmJitStopTranslationRequests()
      * bytes, and no further attempt will be made to re-allocate it.  Can't
      * free it because some thread may be holding a reference.
      */
-    gDvmJit.pProfTable = gDvmJit.pProfTableCopy = NULL;
+    gDvmJit.pProfTable = NULL;
 }
 
-#if defined(EXIT_STATS)
+#if defined(JIT_STATS)
 /* Convenience function to increment counter from assembly code */
-void dvmBumpNoChain()
+void dvmBumpNoChain(int from)
 {
-    gDvm.jitNoChainExit++;
+    gDvmJit.noChainExit[from]++;
 }
 
 /* Convenience function to increment counter from assembly code */
 void dvmBumpNormal()
 {
-    gDvm.jitNormalExit++;
+    gDvmJit.normalExit++;
 }
 
 /* Convenience function to increment counter from assembly code */
 void dvmBumpPunt(int from)
 {
-    gDvm.jitPuntExit++;
+    gDvmJit.puntExit++;
 }
 #endif
 
@@ -133,34 +431,52 @@ void dvmJitStats()
     int hit;
     int not_hit;
     int chains;
+    int stubs;
     if (gDvmJit.pJitEntryTable) {
-        for (i=0, chains=hit=not_hit=0;
+        for (i=0, stubs=chains=hit=not_hit=0;
              i < (int) gDvmJit.jitTableSize;
              i++) {
-            if (gDvmJit.pJitEntryTable[i].dPC != 0)
+            if (gDvmJit.pJitEntryTable[i].dPC != 0) {
                 hit++;
-            else
+                if (gDvmJit.pJitEntryTable[i].codeAddress ==
+                      gDvmJit.interpretTemplate)
+                    stubs++;
+            } else
                 not_hit++;
             if (gDvmJit.pJitEntryTable[i].u.info.chain != gDvmJit.jitTableSize)
                 chains++;
         }
-        LOGD(
-         "JIT: %d traces, %d slots, %d chains, %d maxQ, %d thresh, %s",
-         hit, not_hit + hit, chains, gDvmJit.compilerMaxQueued,
-         gDvmJit.threshold, gDvmJit.blockingMode ? "Blocking" : "Non-blocking");
-#if defined(EXIT_STATS)
-        LOGD(
-         "JIT: Lookups: %d hits, %d misses; %d NoChain, %d normal, %d punt",
-         gDvmJit.addrLookupsFound, gDvmJit.addrLookupsNotFound,
-         gDvmJit.noChainExit, gDvmJit.normalExit, gDvmJit.puntExit);
-#endif
-        LOGD("JIT: %d Translation chains", gDvmJit.translationChains);
-#if defined(INVOKE_STATS)
-        LOGD("JIT: Invoke: %d chainable, %d pred. chain, %d native, "
-             "%d return",
-             gDvmJit.invokeChain, gDvmJit.invokePredictedChain,
+        LOGD("JIT: table size is %d, entries used is %d",
+             gDvmJit.jitTableSize,  gDvmJit.jitTableEntriesUsed);
+        LOGD("JIT: %d traces, %d slots, %d chains, %d thresh, %s",
+             hit, not_hit + hit, chains, gDvmJit.threshold,
+             gDvmJit.blockingMode ? "Blocking" : "Non-blocking");
+
+#if defined(JIT_STATS)
+        LOGD("JIT: Lookups: %d hits, %d misses; %d normal, %d punt",
+             gDvmJit.addrLookupsFound, gDvmJit.addrLookupsNotFound,
+             gDvmJit.normalExit, gDvmJit.puntExit);
+
+        LOGD("JIT: noChainExit: %d IC miss, %d interp callsite, "
+             "%d switch overflow",
+             gDvmJit.noChainExit[kInlineCacheMiss],
+             gDvmJit.noChainExit[kCallsiteInterpreted],
+             gDvmJit.noChainExit[kSwitchOverflow]);
+
+        LOGD("JIT: ICPatch: %d fast, %d queued; %d dropped",
+             gDvmJit.icPatchFast, gDvmJit.icPatchQueued,
+             gDvmJit.icPatchDropped);
+
+        LOGD("JIT: Invoke: %d mono, %d poly, %d native, %d return",
+             gDvmJit.invokeMonomorphic, gDvmJit.invokePolymorphic,
              gDvmJit.invokeNative, gDvmJit.returnOp);
+        LOGD("JIT: Total compilation time: %llu ms", gDvmJit.jitTime / 1000);
+        LOGD("JIT: Avg unit compilation time: %llu us",
+             gDvmJit.jitTime / gDvmJit.numCompilations);
 #endif
+
+        LOGD("JIT: %d Translation chains, %d interp stubs",
+             gDvmJit.translationChains, stubs);
         if (gDvmJit.profile) {
             dvmCompilerSortAndPrintTraceProfiles();
         }
@@ -168,28 +484,119 @@ void dvmJitStats()
 }
 
 
-/*
- * Final JIT shutdown.  Only do this once, and do not attempt to restart
- * the JIT later.
- */
-void dvmJitShutdown(void)
+void setTraceConstruction(JitEntry *slot, bool value)
 {
-    /* Shutdown the compiler thread */
-    dvmCompilerShutdown();
 
-    dvmCompilerDumpStats();
+    JitEntryInfoUnion oldValue;
+    JitEntryInfoUnion newValue;
+    do {
+        oldValue = slot->u;
+        newValue = oldValue;
+        newValue.info.traceConstruction = value;
+    } while (!ATOMIC_CMP_SWAP( &slot->u.infoWord,
+             oldValue.infoWord, newValue.infoWord));
+}
+
+void resetTracehead(InterpState* interpState, JitEntry *slot)
+{
+    slot->codeAddress = gDvmJit.interpretTemplate;
+    setTraceConstruction(slot, false);
+}
 
-    dvmDestroyMutex(&gDvmJit.tableLock);
+/* Clean up any pending trace builds */
+void dvmJitAbortTraceSelect(InterpState* interpState)
+{
+    if (interpState->jitState == kJitTSelect)
+        interpState->jitState = kJitDone;
+}
 
-    if (gDvmJit.pJitEntryTable) {
-        free(gDvmJit.pJitEntryTable);
-        gDvmJit.pJitEntryTable = NULL;
+/*
+ * Find an entry in the JitTable, creating if necessary.
+ * Returns null if table is full.
+ */
+static JitEntry *lookupAndAdd(const u2* dPC, bool callerLocked)
+{
+    u4 chainEndMarker = gDvmJit.jitTableSize;
+    u4 idx = dvmJitHash(dPC);
+
+    /* Walk the bucket chain to find an exact match for our PC */
+    while ((gDvmJit.pJitEntryTable[idx].u.info.chain != chainEndMarker) &&
+           (gDvmJit.pJitEntryTable[idx].dPC != dPC)) {
+        idx = gDvmJit.pJitEntryTable[idx].u.info.chain;
     }
 
-    if (gDvmJit.pProfTable) {
-        free(gDvmJit.pProfTable);
-        gDvmJit.pProfTable = NULL;
+    if (gDvmJit.pJitEntryTable[idx].dPC != dPC) {
+        /*
+         * No match.  Aquire jitTableLock and find the last
+         * slot in the chain. Possibly continue the chain walk in case
+         * some other thread allocated the slot we were looking
+         * at previuosly (perhaps even the dPC we're trying to enter).
+         */
+        if (!callerLocked)
+            dvmLockMutex(&gDvmJit.tableLock);
+        /*
+         * At this point, if .dPC is NULL, then the slot we're
+         * looking at is the target slot from the primary hash
+         * (the simple, and common case).  Otherwise we're going
+         * to have to find a free slot and chain it.
+         */
+        MEM_BARRIER(); /* Make sure we reload [].dPC after lock */
+        if (gDvmJit.pJitEntryTable[idx].dPC != NULL) {
+            u4 prev;
+            while (gDvmJit.pJitEntryTable[idx].u.info.chain != chainEndMarker) {
+                if (gDvmJit.pJitEntryTable[idx].dPC == dPC) {
+                    /* Another thread got there first for this dPC */
+                    if (!callerLocked)
+                        dvmUnlockMutex(&gDvmJit.tableLock);
+                    return &gDvmJit.pJitEntryTable[idx];
+                }
+                idx = gDvmJit.pJitEntryTable[idx].u.info.chain;
+            }
+            /* Here, idx should be pointing to the last cell of an
+             * active chain whose last member contains a valid dPC */
+            assert(gDvmJit.pJitEntryTable[idx].dPC != NULL);
+            /* Linear walk to find a free cell and add it to the end */
+            prev = idx;
+            while (true) {
+                idx++;
+                if (idx == chainEndMarker)
+                    idx = 0;  /* Wraparound */
+                if ((gDvmJit.pJitEntryTable[idx].dPC == NULL) ||
+                    (idx == prev))
+                    break;
+            }
+            if (idx != prev) {
+                JitEntryInfoUnion oldValue;
+                JitEntryInfoUnion newValue;
+                /*
+                 * Although we hold the lock so that noone else will
+                 * be trying to update a chain field, the other fields
+                 * packed into the word may be in use by other threads.
+                 */
+                do {
+                    oldValue = gDvmJit.pJitEntryTable[prev].u;
+                    newValue = oldValue;
+                    newValue.info.chain = idx;
+                } while (!ATOMIC_CMP_SWAP(
+                         &gDvmJit.pJitEntryTable[prev].u.infoWord,
+                         oldValue.infoWord, newValue.infoWord));
+            }
+        }
+        if (gDvmJit.pJitEntryTable[idx].dPC == NULL) {
+            /*
+             * Initialize codeAddress and allocate the slot.  Must
+             * happen in this order (since dPC is set, the entry is live.
+             */
+            gDvmJit.pJitEntryTable[idx].dPC = dPC;
+            gDvmJit.jitTableEntriesUsed++;
+        } else {
+            /* Table is full */
+            idx = chainEndMarker;
+        }
+        if (!callerLocked)
+            dvmUnlockMutex(&gDvmJit.tableLock);
     }
+    return (idx == chainEndMarker) ? NULL : &gDvmJit.pJitEntryTable[idx];
 }
 
 /*
@@ -212,11 +619,11 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
 {
     int flags,i,len;
     int switchInterp = false;
-    int debugOrProfile = (gDvm.debuggerActive || self->suspendCount
-#if defined(WITH_PROFILER)
-                          || gDvm.activeProfilers
-#endif
-            );
+    bool debugOrProfile = dvmDebuggerOrProfilerActive();
+
+    /* Prepare to handle last PC and stage the current PC */
+    const u2 *lastPC = interpState->lastPC;
+    interpState->lastPC = pc;
 
     switch (interpState->jitState) {
         char* nopStr;
@@ -224,19 +631,38 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
         int offset;
         DecodedInstruction decInsn;
         case kJitTSelect:
-            dexDecodeInstruction(gDvm.instrFormat, pc, &decInsn);
+            /* First instruction - just remember the PC and exit */
+            if (lastPC == NULL) break;
+            /* Grow the trace around the last PC if jitState is kJitTSelect */
+            dexDecodeInstruction(gDvm.instrFormat, lastPC, &decInsn);
+
+            /*
+             * Treat {PACKED,SPARSE}_SWITCH as trace-ending instructions due
+             * to the amount of space it takes to generate the chaining
+             * cells.
+             */
+            if (interpState->totalTraceLen != 0 &&
+                (decInsn.opCode == OP_PACKED_SWITCH ||
+                 decInsn.opCode == OP_SPARSE_SWITCH)) {
+                interpState->jitState = kJitTSelectEnd;
+                break;
+            }
+
+
 #if defined(SHOW_TRACE)
             LOGD("TraceGen: adding %s",getOpcodeName(decInsn.opCode));
 #endif
             flags = dexGetInstrFlags(gDvm.instrFlags, decInsn.opCode);
-            len = dexGetInstrOrTableWidthAbs(gDvm.instrWidth, pc);
-            offset = pc - interpState->method->insns;
-            if (pc != interpState->currRunHead + interpState->currRunLen) {
+            len = dexGetInstrOrTableWidthAbs(gDvm.instrWidth, lastPC);
+            offset = lastPC - interpState->method->insns;
+            assert((unsigned) offset <
+                   dvmGetMethodInsnsSize(interpState->method));
+            if (lastPC != interpState->currRunHead + interpState->currRunLen) {
                 int currTraceRun;
                 /* We need to start a new trace run */
                 currTraceRun = ++interpState->currTraceRun;
                 interpState->currRunLen = 0;
-                interpState->currRunHead = (u2*)pc;
+                interpState->currRunHead = (u2*)lastPC;
                 interpState->trace[currTraceRun].frag.startOffset = offset;
                 interpState->trace[currTraceRun].frag.numInsts = 0;
                 interpState->trace[currTraceRun].frag.runEnd = false;
@@ -245,6 +671,12 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
             interpState->trace[interpState->currTraceRun].frag.numInsts++;
             interpState->totalTraceLen++;
             interpState->currRunLen += len;
+
+            /* Will probably never hit this with the current trace buildier */
+            if (interpState->currTraceRun == (MAX_JIT_RUN_LEN - 1)) {
+                interpState->jitState = kJitTSelectEnd;
+            }
+
             if (  ((flags & kInstrUnconditional) == 0) &&
                   /* don't end trace on INVOKE_DIRECT_EMPTY  */
                   (decInsn.opCode != OP_INVOKE_DIRECT_EMPTY) &&
@@ -258,15 +690,16 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
                  getOpcodeName(decInsn.opCode));
 #endif
             }
-            if (decInsn.opCode == OP_THROW) {
+            /* Break on throw or self-loop */
+            if ((decInsn.opCode == OP_THROW) || (lastPC == pc)){
                 interpState->jitState = kJitTSelectEnd;
             }
             if (interpState->totalTraceLen >= JIT_MAX_TRACE_LEN) {
                 interpState->jitState = kJitTSelectEnd;
             }
+             /* Abandon the trace request if debugger/profiler is attached */
             if (debugOrProfile) {
-                interpState->jitState = kJitTSelectAbort;
-                switchInterp = !debugOrProfile;
+                interpState->jitState = kJitDone;
                 break;
             }
             if ((flags & kInstrCanReturn) != kInstrCanReturn) {
@@ -275,8 +708,11 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
             /* NOTE: intentional fallthrough for returns */
         case kJitTSelectEnd:
             {
+                /* Bad trace */
                 if (interpState->totalTraceLen == 0) {
-                    switchInterp = !debugOrProfile;
+                    /* Bad trace - mark as untranslatable */
+                    interpState->jitState = kJitDone;
+                    switchInterp = true;
                     break;
                 }
                 JitTraceDescription* desc =
@@ -285,13 +721,12 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
                 if (desc == NULL) {
                     LOGE("Out of memory in trace selection");
                     dvmJitStopTranslationRequests();
-                    interpState->jitState = kJitTSelectAbort;
-                    switchInterp = !debugOrProfile;
+                    interpState->jitState = kJitDone;
+                    switchInterp = true;
                     break;
                 }
                 interpState->trace[interpState->currTraceRun].frag.runEnd =
                      true;
-                interpState->jitState = kJitNormal;
                 desc->method = interpState->method;
                 memcpy((char*)&(desc->trace[0]),
                     (char*)&(interpState->trace[0]),
@@ -299,12 +734,30 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
 #if defined(SHOW_TRACE)
                 LOGD("TraceGen:  trace done, adding to queue");
 #endif
-                dvmCompilerWorkEnqueue(
-                       interpState->currTraceHead,kWorkOrderTrace,desc);
-                if (gDvmJit.blockingMode) {
-                    dvmCompilerDrainQueue();
+                if (dvmCompilerWorkEnqueue(
+                       interpState->currTraceHead,kWorkOrderTrace,desc)) {
+                    /* Work order successfully enqueued */
+                    if (gDvmJit.blockingMode) {
+                        dvmCompilerDrainQueue();
+                    }
+                } else {
+                    /*
+                     * Make sure the descriptor for the abandoned work order is
+                     * freed.
+                     */
+                    free(desc);
+                }
+                /*
+                 * Reset "trace in progress" flag whether or not we
+                 * successfully entered a work order.
+                 */
+                JitEntry *jitEntry =
+                    lookupAndAdd(interpState->currTraceHead, false);
+                if (jitEntry) {
+                    setTraceConstruction(jitEntry, false);
                 }
-                switchInterp = !debugOrProfile;
+                interpState->jitState = kJitDone;
+                switchInterp = true;
             }
             break;
         case kJitSingleStep:
@@ -312,25 +765,53 @@ int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState)
             break;
         case kJitSingleStepEnd:
             interpState->entryPoint = kInterpEntryResume;
-            switchInterp = !debugOrProfile;
+            interpState->jitState = kJitDone;
+            switchInterp = true;
             break;
-        case kJitTSelectAbort:
-#if defined(SHOW_TRACE)
-            LOGD("TraceGen:  trace abort");
-#endif
-            interpState->jitState = kJitNormal;
-            switchInterp = !debugOrProfile;
+        case kJitDone:
+            switchInterp = true;
             break;
-        case kJitNormal:
-            switchInterp = !debugOrProfile;
+#if defined(WITH_SELF_VERIFICATION)
+        case kJitSelfVerification:
+            if (selfVerificationDebugInterp(pc, self, interpState)) {
+                /*
+                 * If the next state is not single-step end, we can switch
+                 * interpreter now.
+                 */
+                if (interpState->jitState != kJitSingleStepEnd) {
+                    interpState->jitState = kJitDone;
+                    switchInterp = true;
+                }
+            }
+            break;
+#endif
+        /*
+         * If the debug interpreter was entered for non-JIT reasons, check if
+         * the original reason still holds. If not, we have to force the
+         * interpreter switch here and use dvmDebuggerOrProfilerActive instead
+         * of dvmJitDebuggerOrProfilerActive since the latter will alwasy
+         * return true when the debugger/profiler is already detached and the
+         * JIT profiling table is restored.
+         */
+        case kJitNot:
+            switchInterp = !dvmDebuggerOrProfilerActive();
             break;
         default:
+            LOGE("Unexpected JIT state: %d entry point: %d",
+                 interpState->jitState, interpState->entryPoint);
             dvmAbort();
+            break;
     }
-    return switchInterp;
+    /*
+     * Final check to see if we can really switch the interpreter. Make sure
+     * the jitState is kJitDone or kJitNot when switchInterp is set to true.
+     */
+     assert(switchInterp == false || interpState->jitState == kJitDone ||
+            interpState->jitState == kJitNot);
+     return switchInterp && !debugOrProfile;
 }
 
-static inline JitEntry *findJitEntry(const u2* pc)
+JitEntry *dvmFindJitEntry(const u2* pc)
 {
     int idx = dvmJitHash(pc);
 
@@ -348,11 +829,6 @@ static inline JitEntry *findJitEntry(const u2* pc)
     return NULL;
 }
 
-JitEntry *dvmFindJitEntry(const u2* pc)
-{
-    return findJitEntry(pc);
-}
-
 /*
  * If a translated code address exists for the davik byte code
  * pointer return it.  This routine needs to be fast.
@@ -360,121 +836,41 @@ JitEntry *dvmFindJitEntry(const u2* pc)
 void* dvmJitGetCodeAddr(const u2* dPC)
 {
     int idx = dvmJitHash(dPC);
+    const u2* npc = gDvmJit.pJitEntryTable[idx].dPC;
+    if (npc != NULL) {
+        bool hideTranslation = (gDvm.sumThreadSuspendCount != 0) ||
+                               (gDvmJit.codeCacheFull == true) ||
+                               (gDvmJit.pProfTable == NULL);
 
-    /* If anything is suspended, don't re-enter the code cache */
-    if (gDvm.sumThreadSuspendCount > 0) {
-        return NULL;
-    }
-
-    /* Expect a high hit rate on 1st shot */
-    if (gDvmJit.pJitEntryTable[idx].dPC == dPC) {
-#if defined(EXIT_STATS)
-        gDvmJit.addrLookupsFound++;
+        if (npc == dPC) {
+#if defined(JIT_STATS)
+            gDvmJit.addrLookupsFound++;
 #endif
-        return gDvmJit.pJitEntryTable[idx].codeAddress;
-    } else {
-        int chainEndMarker = gDvmJit.jitTableSize;
-        while (gDvmJit.pJitEntryTable[idx].u.info.chain != chainEndMarker) {
-            idx = gDvmJit.pJitEntryTable[idx].u.info.chain;
-            if (gDvmJit.pJitEntryTable[idx].dPC == dPC) {
-#if defined(EXIT_STATS)
-                gDvmJit.addrLookupsFound++;
+            return hideTranslation ?
+                NULL : gDvmJit.pJitEntryTable[idx].codeAddress;
+        } else {
+            int chainEndMarker = gDvmJit.jitTableSize;
+            while (gDvmJit.pJitEntryTable[idx].u.info.chain != chainEndMarker) {
+                idx = gDvmJit.pJitEntryTable[idx].u.info.chain;
+                if (gDvmJit.pJitEntryTable[idx].dPC == dPC) {
+#if defined(JIT_STATS)
+                    gDvmJit.addrLookupsFound++;
 #endif
-                return gDvmJit.pJitEntryTable[idx].codeAddress;
+                    return hideTranslation ?
+                        NULL : gDvmJit.pJitEntryTable[idx].codeAddress;
+                }
             }
         }
     }
-#if defined(EXIT_STATS)
+#if defined(JIT_STATS)
     gDvmJit.addrLookupsNotFound++;
 #endif
     return NULL;
 }
 
 /*
- * Find an entry in the JitTable, creating if necessary.
- * Returns null if table is full.
- */
-JitEntry *dvmJitLookupAndAdd(const u2* dPC)
-{
-    u4 chainEndMarker = gDvmJit.jitTableSize;
-    u4 idx = dvmJitHash(dPC);
-
-    /* Walk the bucket chain to find an exact match for our PC */
-    while ((gDvmJit.pJitEntryTable[idx].u.info.chain != chainEndMarker) &&
-           (gDvmJit.pJitEntryTable[idx].dPC != dPC)) {
-        idx = gDvmJit.pJitEntryTable[idx].u.info.chain;
-    }
-
-    if (gDvmJit.pJitEntryTable[idx].dPC != dPC) {
-        /*
-         * No match.  Aquire jitTableLock and find the last
-         * slot in the chain. Possibly continue the chain walk in case
-         * some other thread allocated the slot we were looking
-         * at previuosly (perhaps even the dPC we're trying to enter).
-         */
-        dvmLockMutex(&gDvmJit.tableLock);
-        /*
-         * At this point, if .dPC is NULL, then the slot we're
-         * looking at is the target slot from the primary hash
-         * (the simple, and common case).  Otherwise we're going
-         * to have to find a free slot and chain it.
-         */
-        MEM_BARRIER(); /* Make sure we reload [].dPC after lock */
-        if (gDvmJit.pJitEntryTable[idx].dPC != NULL) {
-            u4 prev;
-            while (gDvmJit.pJitEntryTable[idx].u.info.chain != chainEndMarker) {
-                if (gDvmJit.pJitEntryTable[idx].dPC == dPC) {
-                    /* Another thread got there first for this dPC */
-                    dvmUnlockMutex(&gDvmJit.tableLock);
-                    return &gDvmJit.pJitEntryTable[idx];
-                }
-                idx = gDvmJit.pJitEntryTable[idx].u.info.chain;
-            }
-            /* Here, idx should be pointing to the last cell of an
-             * active chain whose last member contains a valid dPC */
-            assert(gDvmJit.pJitEntryTable[idx].dPC != NULL);
-            /* Linear walk to find a free cell and add it to the end */
-            prev = idx;
-            while (true) {
-                idx++;
-                if (idx == chainEndMarker)
-                    idx = 0;  /* Wraparound */
-                if ((gDvmJit.pJitEntryTable[idx].dPC == NULL) ||
-                    (idx == prev))
-                    break;
-            }
-            if (idx != prev) {
-                JitEntryInfoUnion oldValue;
-                JitEntryInfoUnion newValue;
-                /*
-                 * Although we hold the lock so that noone else will
-                 * be trying to update a chain field, the other fields
-                 * packed into the word may be in use by other threads.
-                 */
-                do {
-                    oldValue = gDvmJit.pJitEntryTable[prev].u;
-                    newValue = oldValue;
-                    newValue.info.chain = idx;
-                } while (!ATOMIC_CMP_SWAP(
-                         &gDvmJit.pJitEntryTable[prev].u.infoWord,
-                         oldValue.infoWord, newValue.infoWord));
-            }
-        }
-        if (gDvmJit.pJitEntryTable[idx].dPC == NULL) {
-           /* Allocate the slot */
-            gDvmJit.pJitEntryTable[idx].dPC = dPC;
-            gDvmJit.jitTableEntriesUsed++;
-        } else {
-            /* Table is full */
-            idx = chainEndMarker;
-        }
-        dvmUnlockMutex(&gDvmJit.tableLock);
-    }
-    return (idx == chainEndMarker) ? NULL : &gDvmJit.pJitEntryTable[idx];
-}
-/*
  * Register the translated code pointer into the JitTable.
- * NOTE: Once a codeAddress field transitions from NULL to
+ * NOTE: Once a codeAddress field transitions from initial state to
  * JIT'd code, it must not be altered without first halting all
  * threads.  This routine should only be called by the compiler
  * thread.
@@ -482,7 +878,7 @@ JitEntry *dvmJitLookupAndAdd(const u2* dPC)
 void dvmJitSetCodeAddr(const u2* dPC, void *nPC, JitInstructionSetType set) {
     JitEntryInfoUnion oldValue;
     JitEntryInfoUnion newValue;
-    JitEntry *jitEntry = dvmJitLookupAndAdd(dPC);
+    JitEntry *jitEntry = lookupAndAdd(dPC, false);
     assert(jitEntry);
     /* Note: order of update is important */
     do {
@@ -498,45 +894,51 @@ void dvmJitSetCodeAddr(const u2* dPC, void *nPC, JitInstructionSetType set) {
 /*
  * Determine if valid trace-bulding request is active.  Return true
  * if we need to abort and switch back to the fast interpreter, false
- * otherwise.  NOTE: may be called even when trace selection is not being
- * requested
+ * otherwise.
  */
-
 bool dvmJitCheckTraceRequest(Thread* self, InterpState* interpState)
 {
-    bool res = false;         /* Assume success */
+    bool switchInterp = false;         /* Assume success */
     int i;
-    if (gDvmJit.pJitEntryTable != NULL) {
-        /* Two-level filtering scheme */
-        for (i=0; i< JIT_TRACE_THRESH_FILTER_SIZE; i++) {
-            if (interpState->pc == interpState->threshFilter[i]) {
-                break;
+    intptr_t filterKey = ((intptr_t) interpState->pc) >>
+                         JIT_TRACE_THRESH_FILTER_GRAN_LOG2;
+    bool debugOrProfile = dvmDebuggerOrProfilerActive();
+
+    /* Check if the JIT request can be handled now */
+    if (gDvmJit.pJitEntryTable != NULL && debugOrProfile == false) {
+        /* Bypass the filter for hot trace requests or during stress mode */
+        if (interpState->jitState == kJitTSelectRequest &&
+            gDvmJit.threshold > 6) {
+            /* Two-level filtering scheme */
+            for (i=0; i< JIT_TRACE_THRESH_FILTER_SIZE; i++) {
+                if (filterKey == interpState->threshFilter[i]) {
+                    break;
+                }
+            }
+            if (i == JIT_TRACE_THRESH_FILTER_SIZE) {
+                /*
+                 * Use random replacement policy - otherwise we could miss a
+                 * large loop that contains more traces than the size of our
+                 * filter array.
+                 */
+                i = rand() % JIT_TRACE_THRESH_FILTER_SIZE;
+                interpState->threshFilter[i] = filterKey;
+                interpState->jitState = kJitDone;
             }
         }
-        if (i == JIT_TRACE_THRESH_FILTER_SIZE) {
-            /*
-             * Use random replacement policy - otherwise we could miss a large
-             * loop that contains more traces than the size of our filter array.
-             */
-            i = rand() % JIT_TRACE_THRESH_FILTER_SIZE;
-            interpState->threshFilter[i] = interpState->pc;
-            res = true;
+
+        /* If the compiler is backlogged, cancel any JIT actions */
+        if (gDvmJit.compilerQueueLength >= gDvmJit.compilerHighWater) {
+            interpState->jitState = kJitDone;
         }
+
         /*
-         * If the compiler is backlogged, or if a debugger or profiler is
-         * active, cancel any JIT actions
+         * Check for additional reasons that might force the trace select
+         * request to be dropped
          */
-        if ( res || (gDvmJit.compilerQueueLength >= gDvmJit.compilerHighWater) ||
-              gDvm.debuggerActive || self->suspendCount
-#if defined(WITH_PROFILER)
-                 || gDvm.activeProfilers
-#endif
-                                             ) {
-            if (interpState->jitState != kJitOff) {
-                interpState->jitState = kJitNormal;
-            }
-        } else if (interpState->jitState == kJitTSelectRequest) {
-            JitEntry *slot = dvmJitLookupAndAdd(interpState->pc);
+        if (interpState->jitState == kJitTSelectRequest ||
+            interpState->jitState == kJitTSelectRequestHot) {
+            JitEntry *slot = lookupAndAdd(interpState->pc, false);
             if (slot == NULL) {
                 /*
                  * Table is full.  This should have been
@@ -544,69 +946,100 @@ bool dvmJitCheckTraceRequest(Thread* self, InterpState* interpState)
                  * resized before we run into it here.  Assume bad things
                  * are afoot and disable profiling.
                  */
-                interpState->jitState = kJitTSelectAbort;
+                interpState->jitState = kJitDone;
                 LOGD("JIT: JitTable full, disabling profiling");
                 dvmJitStopTranslationRequests();
-            } else if (slot->u.info.traceRequested) {
-                /* Trace already requested - revert to interpreter */
-                interpState->jitState = kJitTSelectAbort;
+            } else if (slot->u.info.traceConstruction) {
+                /*
+                 * Trace request already in progress, but most likely it
+                 * aborted without cleaning up.  Assume the worst and
+                 * mark trace head as untranslatable.  If we're wrong,
+                 * the compiler thread will correct the entry when the
+                 * translation is completed.  The downside here is that
+                 * some existing translation may chain to the interpret-only
+                 * template instead of the real translation during this
+                 * window.  Performance, but not correctness, issue.
+                 */
+                interpState->jitState = kJitDone;
+                resetTracehead(interpState, slot);
+            } else if (slot->codeAddress) {
+                 /* Nothing to do here - just return */
+                interpState->jitState = kJitDone;
             } else {
-                /* Mark request */
-                JitEntryInfoUnion oldValue;
-                JitEntryInfoUnion newValue;
-                do {
-                    oldValue = slot->u;
-                    newValue = oldValue;
-                    newValue.info.traceRequested = true;
-                } while (!ATOMIC_CMP_SWAP( &slot->u.infoWord,
-                         oldValue.infoWord, newValue.infoWord));
+                /*
+                 * Mark request.  Note, we are not guaranteed exclusivity
+                 * here.  A window exists for another thread to be
+                 * attempting to build this same trace.  Rather than
+                 * bear the cost of locking, we'll just allow that to
+                 * happen.  The compiler thread, if it chooses, can
+                 * discard redundant requests.
+                 */
+                setTraceConstruction(slot, true);
             }
         }
+
         switch (interpState->jitState) {
             case kJitTSelectRequest:
-                 interpState->jitState = kJitTSelect;
-                 interpState->currTraceHead = interpState->pc;
-                 interpState->currTraceRun = 0;
-                 interpState->totalTraceLen = 0;
-                 interpState->currRunHead = interpState->pc;
-                 interpState->currRunLen = 0;
-                 interpState->trace[0].frag.startOffset =
-                       interpState->pc - interpState->method->insns;
-                 interpState->trace[0].frag.numInsts = 0;
-                 interpState->trace[0].frag.runEnd = false;
-                 interpState->trace[0].frag.hint = kJitHintNone;
-                 break;
-            case kJitTSelect:
-            case kJitTSelectAbort:
-                 res = true;
-            case kJitSingleStep:
-            case kJitSingleStepEnd:
-            case kJitOff:
-            case kJitNormal:
+            case kJitTSelectRequestHot:
+                interpState->jitState = kJitTSelect;
+                interpState->currTraceHead = interpState->pc;
+                interpState->currTraceRun = 0;
+                interpState->totalTraceLen = 0;
+                interpState->currRunHead = interpState->pc;
+                interpState->currRunLen = 0;
+                interpState->trace[0].frag.startOffset =
+                     interpState->pc - interpState->method->insns;
+                interpState->trace[0].frag.numInsts = 0;
+                interpState->trace[0].frag.runEnd = false;
+                interpState->trace[0].frag.hint = kJitHintNone;
+                interpState->lastPC = 0;
+                break;
+            /*
+             * For JIT's perspective there is no need to stay in the debug
+             * interpreter unless debugger/profiler is attached.
+             */
+            case kJitDone:
+                switchInterp = true;
                 break;
             default:
+                LOGE("Unexpected JIT state: %d entry point: %d",
+                     interpState->jitState, interpState->entryPoint);
                 dvmAbort();
         }
+    } else {
+        /*
+         * Cannot build trace this time - ready to leave the dbg interpreter
+         */
+        interpState->jitState = kJitDone;
+        switchInterp = true;
     }
-    return res;
+
+    /*
+     * Final check to see if we can really switch the interpreter. Make sure
+     * the jitState is kJitDone when switchInterp is set to true.
+     */
+    assert(switchInterp == false || interpState->jitState == kJitDone);
+    return switchInterp && !debugOrProfile;
 }
 
 /*
  * Resizes the JitTable.  Must be a power of 2, and returns true on failure.
- * Stops all threads, and thus is a heavyweight operation.
+ * Stops all threads, and thus is a heavyweight operation. May only be called
+ * by the compiler thread.
  */
 bool dvmJitResizeJitTable( unsigned int size )
 {
     JitEntry *pNewTable;
     JitEntry *pOldTable;
+    JitEntry tempEntry;
     u4 newMask;
     unsigned int oldSize;
     unsigned int i;
 
-    assert(gDvm.pJitEntryTable != NULL);
+    assert(gDvmJit.pJitEntryTable != NULL);
     assert(size && !(size & (size - 1)));   /* Is power of 2? */
 
-    LOGD("Jit: resizing JitTable from %d to %d", gDvmJit.jitTableSize, size);
+    LOGI("Jit: resizing JitTable from %d to %d", gDvmJit.jitTableSize, size);
 
     newMask = size - 1;
 
@@ -614,6 +1047,13 @@ bool dvmJitResizeJitTable( unsigned int size )
         return true;
     }
 
+    /* Make sure requested size is compatible with chain field width */
+    tempEntry.u.info.chain = size;
+    if (tempEntry.u.info.chain != size) {
+        LOGD("Jit: JitTable request of %d too big", size);
+        return true;
+    }
+
     pNewTable = (JitEntry*)calloc(size, sizeof(*pNewTable));
     if (pNewTable == NULL) {
         return true;
@@ -623,7 +1063,7 @@ bool dvmJitResizeJitTable( unsigned int size )
     }
 
     /* Stop all other interpreting/jit'ng threads */
-    dvmSuspendAllThreads(SUSPEND_FOR_JIT);
+    dvmSuspendAllThreads(SUSPEND_FOR_TBL_RESIZE);
 
     pOldTable = gDvmJit.pJitEntryTable;
     oldSize = gDvmJit.jitTableSize;
@@ -633,39 +1073,48 @@ bool dvmJitResizeJitTable( unsigned int size )
     gDvmJit.jitTableSize = size;
     gDvmJit.jitTableMask = size - 1;
     gDvmJit.jitTableEntriesUsed = 0;
-    dvmUnlockMutex(&gDvmJit.tableLock);
 
     for (i=0; i < oldSize; i++) {
         if (pOldTable[i].dPC) {
             JitEntry *p;
             u2 chain;
-            p = dvmJitLookupAndAdd(pOldTable[i].dPC);
-            p->dPC = pOldTable[i].dPC;
-            /*
-             * Compiler thread may have just updated the new entry's
-             * code address field, so don't blindly copy null.
-             */
-            if (pOldTable[i].codeAddress != NULL) {
-                p->codeAddress = pOldTable[i].codeAddress;
-            }
+            p = lookupAndAdd(pOldTable[i].dPC, true /* holds tableLock*/ );
+            p->codeAddress = pOldTable[i].codeAddress;
             /* We need to preserve the new chain field, but copy the rest */
-            dvmLockMutex(&gDvmJit.tableLock);
             chain = p->u.info.chain;
             p->u = pOldTable[i].u;
             p->u.info.chain = chain;
-            dvmUnlockMutex(&gDvmJit.tableLock);
         }
     }
+    dvmUnlockMutex(&gDvmJit.tableLock);
 
     free(pOldTable);
 
     /* Restart the world */
-    dvmResumeAllThreads(SUSPEND_FOR_JIT);
+    dvmResumeAllThreads(SUSPEND_FOR_TBL_RESIZE);
 
     return false;
 }
 
 /*
+ * Reset the JitTable to the initial clean state.
+ */
+void dvmJitResetTable(void)
+{
+    JitEntry *jitEntry = gDvmJit.pJitEntryTable;
+    unsigned int size = gDvmJit.jitTableSize;
+    unsigned int i;
+
+    dvmLockMutex(&gDvmJit.tableLock);
+    memset((void *) jitEntry, 0, sizeof(JitEntry) * size);
+    for (i=0; i< size; i++) {
+        jitEntry[i].u.info.chain = size;  /* Initialize chain termination */
+    }
+    gDvmJit.jitTableEntriesUsed = 0;
+    dvmUnlockMutex(&gDvmJit.tableLock);
+}
+
+/*
  * Float/double conversion requires clamping to min and max of integer form.  If
  * target doesn't support this normally, use these.
  */
@@ -697,5 +1146,4 @@ s8 dvmJitf2l(float f)
         return (s8)f;
 }
 
-
 #endif /* WITH_JIT */
diff --git a/vm/interp/Jit.h b/vm/interp/Jit.h
index 660b5ec..9d17a52 100644
--- a/vm/interp/Jit.h
+++ b/vm/interp/Jit.h
@@ -20,11 +20,55 @@
 #define _DALVIK_INTERP_JIT
 
 #include "InterpDefs.h"
-
-#define JIT_PROF_SIZE 512
+#include "mterp/common/jit-config.h"
 
 #define JIT_MAX_TRACE_LEN 100
 
+#if defined (WITH_SELF_VERIFICATION)
+
+#define REG_SPACE 256                /* default size of shadow space */
+#define HEAP_SPACE JIT_MAX_TRACE_LEN /* default size of heap space */
+
+typedef struct ShadowHeap {
+    int addr;
+    int data;
+} ShadowHeap;
+
+typedef struct InstructionTrace {
+    int addr;
+    DecodedInstruction decInsn;
+} InstructionTrace;
+
+typedef struct ShadowSpace {
+    const u2* startPC;          /* starting pc of jitted region */
+    const void* fp;             /* starting fp of jitted region */
+    void* glue;                 /* starting glue of jitted region */
+    SelfVerificationState selfVerificationState;  /* self verification state */
+    const u2* endPC;            /* ending pc of jitted region */
+    void* shadowFP;       /* pointer to fp in shadow space */
+    InterpState interpState;    /* copy of interpState */
+    int* registerSpace;         /* copy of register state */
+    int registerSpaceSize;      /* current size of register space */
+    ShadowHeap heapSpace[HEAP_SPACE]; /* copy of heap space */
+    ShadowHeap* heapSpaceTail;        /* tail pointer to heapSpace */
+    const void* endShadowFP;    /* ending fp in shadow space */
+    InstructionTrace trace[JIT_MAX_TRACE_LEN]; /* opcode trace for debugging */
+    int traceLength;            /* counter for current trace length */
+    const Method* method;       /* starting method of jitted region */
+} ShadowSpace;
+
+/*
+ * Self verification functions.
+ */
+void* dvmSelfVerificationShadowSpaceAlloc(Thread* self);
+void dvmSelfVerificationShadowSpaceFree(Thread* self);
+void* dvmSelfVerificationSaveState(const u2* pc, const void* fp,
+                                   InterpState* interpState,
+                                   int targetTrace);
+void* dvmSelfVerificationRestoreState(const u2* pc, const void* fp,
+                                      SelfVerificationState exitPoint);
+#endif
+
 /*
  * JitTable hash function.
  */
@@ -44,13 +88,13 @@ static inline u4 dvmJitHash( const u2* p ) {
  */
 
 typedef struct JitEntryInfo {
-    unsigned int           traceRequested:1;   /* already requested a translation */
+    unsigned int           traceConstruction:1;   /* build underway? */
     unsigned int           isMethodEntry:1;
     unsigned int           inlineCandidate:1;
     unsigned int           profileEnabled:1;
     JitInstructionSetType  instructionSet:4;
     unsigned int           unused:8;
-    u2                     chain;              /* Index of next in chain */
+    u2                     chain;                 /* Index of next in chain */
 } JitEntryInfo;
 
 typedef union JitEntryInfoUnion {
@@ -59,24 +103,22 @@ typedef union JitEntryInfoUnion {
 } JitEntryInfoUnion;
 
 typedef struct JitEntry {
-    JitEntryInfoUnion u;
-    u2                chain;              /* Index of next in chain */
-    const u2*         dPC;                /* Dalvik code address */
-    void*             codeAddress;        /* Code address of native translation */
+    JitEntryInfoUnion   u;
+    const u2*           dPC;            /* Dalvik code address */
+    void*               codeAddress;    /* Code address of native translation */
 } JitEntry;
 
-int dvmJitStartup(void);
-void dvmJitShutdown(void);
 int dvmCheckJit(const u2* pc, Thread* self, InterpState* interpState);
 void* dvmJitGetCodeAddr(const u2* dPC);
 bool dvmJitCheckTraceRequest(Thread* self, InterpState* interpState);
 void dvmJitStopTranslationRequests(void);
 void dvmJitStats(void);
 bool dvmJitResizeJitTable(unsigned int size);
+void dvmJitResetTable(void);
 struct JitEntry *dvmFindJitEntry(const u2* pc);
 s8 dvmJitd2l(double d);
 s8 dvmJitf2l(float f);
 void dvmJitSetCodeAddr(const u2* dPC, void *nPC, JitInstructionSetType set);
-
+void dvmJitAbortTraceSelect(InterpState* interpState);
 
 #endif /*_DALVIK_INTERP_JIT*/
diff --git a/vm/interp/Stack.c b/vm/interp/Stack.c
index 0a25d23..c097102 100644
--- a/vm/interp/Stack.c
+++ b/vm/interp/Stack.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Stacks and their uses (e.g. native --> interpreted method calls).
  *
@@ -80,7 +81,7 @@ static bool dvmPushInterpFrame(Thread* self, const Method* method)
              "(req=%d top=%p cur=%p size=%d %s.%s)\n",
             stackReq, self->interpStackStart, self->curFrame,
             self->interpStackSize, method->clazz->descriptor, method->name);
-        dvmHandleStackOverflow(self);
+        dvmHandleStackOverflow(self, method);
         assert(dvmCheckException(self));
         return false;
     }
@@ -153,7 +154,7 @@ bool dvmPushJNIFrame(Thread* self, const Method* method)
              "(req=%d top=%p cur=%p size=%d '%s')\n",
             stackReq, self->interpStackStart, self->curFrame,
             self->interpStackSize, method->name);
-        dvmHandleStackOverflow(self);
+        dvmHandleStackOverflow(self, method);
         assert(dvmCheckException(self));
         return false;
     }
@@ -227,7 +228,7 @@ bool dvmPushLocalFrame(Thread* self, const Method* method)
              "(req=%d top=%p cur=%p size=%d '%s')\n",
             stackReq, self->interpStackStart, self->curFrame,
             self->interpStackSize, method->name);
-        dvmHandleStackOverflow(self);
+        dvmHandleStackOverflow(self, method);
         assert(dvmCheckException(self));
         return false;
     }
@@ -370,10 +371,10 @@ static ClassObject* callPrep(Thread* self, const Method* method, Object* obj,
 
     assert(self != NULL);
     assert(method != NULL);
-    
+
     if( method == 0xFFFFFFFF ) {
-      LOGE("ERROR: Calling native stub function, returning\n");
-      return NULL;
+          LOGE("ERROR: Calling native stub function, returning\n");
+          return NULL;
     }
 
     if (obj != NULL)
@@ -1019,7 +1020,7 @@ bool dvmCreateStackTraceArray(const void* fp, const Method*** pArray,
  * need to resolve classes, which requires calling into the class loader if
  * the classes aren't already in the "initiating loader" list.
  */
-void dvmHandleStackOverflow(Thread* self)
+void dvmHandleStackOverflow(Thread* self, const Method* method)
 {
     /*
      * Can we make the reserved area available?
@@ -1035,7 +1036,15 @@ void dvmHandleStackOverflow(Thread* self)
     }
 
     /* open it up to the full range */
-    LOGI("Stack overflow, expanding (%p to %p)\n", self->interpStackEnd,
+    LOGI("threadid=%d: stack overflow on call to %s.%s:%s\n",
+        self->threadId,
+        method->clazz->descriptor, method->name, method->shorty);
+    StackSaveArea* saveArea = SAVEAREA_FROM_FP(self->curFrame);
+    LOGI("  method requires %d+%d+%d=%d bytes, fp is %p (%d left)\n",
+        method->registersSize * 4, sizeof(StackSaveArea), method->outsSize * 4,
+        (method->registersSize + method->outsSize) * 4 + sizeof(StackSaveArea),
+        saveArea, (u1*) saveArea - self->interpStackEnd);
+    LOGI("  expanding stack end (%p to %p)\n", self->interpStackEnd,
         self->interpStackStart - self->interpStackSize);
     //dvmDumpThread(self, false);
     self->interpStackEnd = self->interpStackStart - self->interpStackSize;
@@ -1052,19 +1061,25 @@ void dvmHandleStackOverflow(Thread* self)
         LOGW("Stack overflow while throwing exception\n");
         dvmClearException(self);
     }
-    dvmThrowChainedException("Ljava/lang/StackOverflowError;", NULL, excep);
+    dvmThrowChainedExceptionByClass(gDvm.classJavaLangStackOverflowError,
+        NULL, excep);
 }
 
 /*
  * Reduce the available stack size.  By this point we should have finished
  * our overflow processing.
  */
-void dvmCleanupStackOverflow(Thread* self)
+void dvmCleanupStackOverflow(Thread* self, const Object* exception)
 {
     const u1* newStackEnd;
 
     assert(self->stackOverflowed);
 
+    if (exception->clazz != gDvm.classJavaLangStackOverflowError) {
+        /* exception caused during SOE, not the SOE itself */
+        return;
+    }
+
     newStackEnd = (self->interpStackStart - self->interpStackSize)
         + STACK_OVERFLOW_RESERVE;
     if ((u1*)self->curFrame <= newStackEnd) {
@@ -1083,6 +1098,85 @@ void dvmCleanupStackOverflow(Thread* self)
 
 
 /*
+ * Extract the object that is the target of a monitor-enter instruction
+ * in the top stack frame of "thread".
+ *
+ * The other thread might be alive, so this has to work carefully.
+ *
+ * We assume the thread list lock is currently held.
+ *
+ * Returns "true" if we successfully recover the object.  "*pOwner" will
+ * be NULL if we can't determine the owner for some reason (e.g. race
+ * condition on ownership transfer).
+ */
+static bool extractMonitorEnterObject(Thread* thread, Object** pLockObj,
+    Thread** pOwner)
+{
+    void* framePtr = thread->curFrame;
+
+    if (framePtr == NULL || dvmIsBreakFrame(framePtr))
+        return false;
+
+    const StackSaveArea* saveArea = SAVEAREA_FROM_FP(framePtr);
+    const Method* method = saveArea->method;
+    const u2* currentPc = saveArea->xtra.currentPc;
+
+    /* check Method* */
+    if (!dvmLinearAllocContains(method, sizeof(Method))) {
+        LOGD("ExtrMon: method %p not valid\n", method);
+        return false;
+    }
+
+    /* check currentPc */
+    u4 insnsSize = dvmGetMethodInsnsSize(method);
+    if (currentPc < method->insns ||
+        currentPc >= method->insns + insnsSize)
+    {
+        LOGD("ExtrMon: insns %p not valid (%p - %p)\n",
+            currentPc, method->insns, method->insns + insnsSize);
+        return false;
+    }
+
+    /* check the instruction */
+    if ((*currentPc & 0xff) != OP_MONITOR_ENTER) {
+        LOGD("ExtrMon: insn at %p is not monitor-enter (0x%02x)\n",
+            currentPc, *currentPc & 0xff);
+        return false;
+    }
+
+    /* get and check the register index */
+    unsigned int reg = *currentPc >> 8;
+    if (reg >= method->registersSize) {
+        LOGD("ExtrMon: invalid register %d (max %d)\n",
+            reg, method->registersSize);
+        return false;
+    }
+
+    /* get and check the object in that register */
+    u4* fp = (u4*) framePtr;
+    Object* obj = (Object*) fp[reg];
+    if (!dvmIsValidObject(obj)) {
+        LOGD("ExtrMon: invalid object %p at %p[%d]\n", obj, fp, reg);
+        return false;
+    }
+    *pLockObj = obj;
+
+    /*
+     * Try to determine the object's lock holder; it's okay if this fails.
+     *
+     * We're assuming the thread list lock is already held by this thread.
+     * If it's not, we may be living dangerously if we have to scan through
+     * the thread list to find a match.  (The VM will generally be in a
+     * suspended state when executing here, so this is a minor concern
+     * unless we're dumping while threads are running, in which case there's
+     * a good chance of stuff blowing up anyway.)
+     */
+    *pOwner = dvmGetObjectLockHolder(obj);
+
+    return true;
+}
+
+/*
  * Dump stack frames, starting from the specified frame and moving down.
  *
  * Each frame holds a pointer to the currently executing method, and the
@@ -1141,21 +1235,44 @@ static void dumpFrames(const DebugOutputTarget* target, void* framePtr,
             }
             free(className);
 
-            if (first &&
-                (thread->status == THREAD_WAIT ||
-                 thread->status == THREAD_TIMED_WAIT))
-            {
-                /* warning: wait status not stable, even in suspend */
-                Monitor* mon = thread->waitMonitor;
-                Object* obj = dvmGetMonitorObject(mon);
-                if (obj != NULL) {
-                    className = dvmDescriptorToDot(obj->clazz->descriptor);
-                    dvmPrintDebugMessage(target,
-                        "  - waiting on <%p> (a %s)\n", mon, className);
-                    free(className);
+            if (first) {
+                /*
+                 * Decorate WAIT and MONITOR threads with some detail on
+                 * the first frame.
+                 *
+                 * warning: wait status not stable, even in suspend
+                 */
+                if (thread->status == THREAD_WAIT ||
+                    thread->status == THREAD_TIMED_WAIT)
+                {
+                    Monitor* mon = thread->waitMonitor;
+                    Object* obj = dvmGetMonitorObject(mon);
+                    if (obj != NULL) {
+                        className = dvmDescriptorToDot(obj->clazz->descriptor);
+                        dvmPrintDebugMessage(target,
+                            "  - waiting on <%p> (a %s)\n", obj, className);
+                        free(className);
+                    }
+                } else if (thread->status == THREAD_MONITOR) {
+                    Object* obj;
+                    Thread* owner;
+                    if (extractMonitorEnterObject(thread, &obj, &owner)) {
+                        className = dvmDescriptorToDot(obj->clazz->descriptor);
+                        if (owner != NULL) {
+                            char* threadName = dvmGetThreadName(owner);
+                            dvmPrintDebugMessage(target,
+                                "  - waiting to lock <%p> (a %s) held by threadid=%d (%s)\n",
+                                obj, className, owner->threadId, threadName);
+                            free(threadName);
+                        } else {
+                            dvmPrintDebugMessage(target,
+                                "  - waiting to lock <%p> (a %s) held by ???\n",
+                                obj, className);
+                        }
+                        free(className);
+                    }
                 }
             }
-
         }
 
         /*
diff --git a/vm/interp/Stack.h b/vm/interp/Stack.h
index 22f066f..3f76cb1 100644
--- a/vm/interp/Stack.h
+++ b/vm/interp/Stack.h
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Stack frames, and uses thereof.
  */
@@ -166,7 +167,7 @@ struct StackSaveArea {
     ((u4*) ((u1*)SAVEAREA_FROM_FP(_fp) - sizeof(u4) * (_argCount)))
 
 /* reserve this many bytes for handling StackOverflowError */
-#define STACK_OVERFLOW_RESERVE  512
+#define STACK_OVERFLOW_RESERVE  768
 
 /*
  * Determine if the frame pointer points to a "break frame".
@@ -275,8 +276,8 @@ bool dvmCreateStackTraceArray(const void* fp, const Method*** pArray,
 /*
  * Common handling for stack overflow.
  */
-void dvmHandleStackOverflow(Thread* self);
-void dvmCleanupStackOverflow(Thread* self);
+void dvmHandleStackOverflow(Thread* self, const Method* method);
+void dvmCleanupStackOverflow(Thread* self, const Object* exception);
 
 /* debugging; dvmDumpThread() is probably a better starting point */
 void dvmDumpThreadStack(const DebugOutputTarget* target, Thread* thread);
diff --git a/vm/jdwp/Jdwp.h b/vm/jdwp/Jdwp.h
index 0a72a06..7313579 100644
--- a/vm/jdwp/Jdwp.h
+++ b/vm/jdwp/Jdwp.h
@@ -232,6 +232,7 @@ bool dvmJdwpPostVMDeath(JdwpState* state);
 /*
  * Send up a chunk of DDM data.
  */
-void dvmJdwpDdmSendChunk(JdwpState* state, int type, int len, const u1* buf);
+void dvmJdwpDdmSendChunkV(JdwpState* state, int type, const struct iovec* iov,
+    int iovcnt);
 
 #endif /*_DALVIK_JDWP_JDWP*/
diff --git a/vm/jdwp/JdwpAdb.c b/vm/jdwp/JdwpAdb.c
index a74f9e1..dfc7bd7 100644
--- a/vm/jdwp/JdwpAdb.c
+++ b/vm/jdwp/JdwpAdb.c
@@ -174,7 +174,7 @@ static int  receiveClientFd(JdwpNetState*  netState)
             LOGW("receiving file descriptor from ADB failed (socket %d): %s\n",
                  netState->controlSock, strerror(errno));
         } else {
-            LOGI("adbd disconnected\n");
+            LOGD("adbd disconnected\n");
         }
         close(netState->controlSock);
         netState->controlSock = -1;
@@ -434,8 +434,6 @@ static bool handlePacket(JdwpState* state)
 
     cmd = cmdSet = 0;       // shut up gcc
 
-    /*dumpPacket(netState->inputBuffer);*/
-
     length = read4BE(&buf);
     id = read4BE(&buf);
     flags = read1(&buf);
@@ -673,7 +671,6 @@ static bool sendRequest(JdwpState* state, ExpandBuf* pReq)
     JdwpNetState* netState = state->netState;
     int cc;
 
-    /* dumpPacket(expandBufGetBuffer(pReq)); */
     if (netState->clientSock < 0) {
         /* can happen with some DDMS events */
         LOGV("NOT sending request -- no debugger is attached\n");
@@ -697,6 +694,46 @@ static bool sendRequest(JdwpState* state, ExpandBuf* pReq)
     return true;
 }
 
+/*
+ * Send a request that was split into multiple buffers.
+ *
+ * The entire packet must be sent with a single writev() call to avoid
+ * threading issues.
+ *
+ * Returns "true" if it was sent successfully.
+ */
+static bool sendBufferedRequest(JdwpState* state, const struct iovec* iov,
+    int iovcnt)
+{
+    JdwpNetState* netState = state->netState;
+
+    if (netState->clientSock < 0) {
+        /* can happen with some DDMS events */
+        LOGV("NOT sending request -- no debugger is attached\n");
+        return false;
+    }
+
+    size_t expected = 0;
+    int i;
+    for (i = 0; i < iovcnt; i++)
+        expected += iov[i].iov_len;
+
+    /*
+     * TODO: we currently assume the writev() will complete in one
+     * go, which may not be safe for a network socket.  We may need
+     * to mutex this against handlePacket().
+     */
+    ssize_t actual;
+    actual = writev(netState->clientSock, iov, iovcnt);
+    if ((size_t)actual != expected) {
+        LOGE("Failed sending b-req to debugger: %s (%d of %zu)\n",
+            strerror(errno), (int) actual, expected);
+        return false;
+    }
+
+    return true;
+}
+
 
 /*
  * Our functions.
@@ -711,7 +748,8 @@ static const JdwpTransport socketTransport = {
     isConnected,
     awaitingHandshake,
     processIncoming,
-    sendRequest
+    sendRequest,
+    sendBufferedRequest
 };
 
 /*
diff --git a/vm/jdwp/JdwpEvent.c b/vm/jdwp/JdwpEvent.c
index a3ff05a..996d7ad 100644
--- a/vm/jdwp/JdwpEvent.c
+++ b/vm/jdwp/JdwpEvent.c
@@ -1026,6 +1026,10 @@ bool dvmJdwpPostVMDeath(JdwpState* state)
  * Valid mods:
  *  Count, ThreadOnly, ClassOnly, ClassMatch, ClassExclude, LocationOnly,
  *    ExceptionOnly, InstanceOnly
+ *
+ * The "exceptionId" has not been added to the GC-visible object registry,
+ * because there's a pretty good chance that we're not going to send it
+ * up the debugger.
  */
 bool dvmJdwpPostException(JdwpState* state, const JdwpLocation* pThrowLoc,
     ObjectId exceptionId, RefTypeId exceptionClassId,
@@ -1100,6 +1104,9 @@ bool dvmJdwpPostException(JdwpState* state, const JdwpLocation* pThrowLoc,
             expandBufAdd8BE(pReq, exceptionId);
             dvmJdwpAddLocation(pReq, pCatchLoc);
         }
+
+        /* don't let the GC discard it */
+        dvmDbgRegisterObjectId(exceptionId);
     }
 
     cleanupMatchList(state, matchList, matchCount);
@@ -1251,40 +1258,39 @@ bool dvmJdwpPostFieldAccess(JdwpState* state, int STUFF, ObjectId thisPtr,
  * other debugger traffic, and can't suspend the VM, so we skip all of
  * the fun event token gymnastics.
  */
-void dvmJdwpDdmSendChunk(JdwpState* state, int type, int len, const u1* buf)
+void dvmJdwpDdmSendChunkV(JdwpState* state, int type, const struct iovec* iov,
+    int iovcnt)
 {
-    ExpandBuf* pReq;
-    u1* outBuf;
+    u1 header[kJDWPHeaderLen + 8];
+    size_t dataLen = 0;
+    int i;
 
-    /*
-     * Write the chunk header and data into the ExpandBuf.
-     */
-    pReq = expandBufAlloc();
-    expandBufAddSpace(pReq, kJDWPHeaderLen);
-    expandBufAdd4BE(pReq, type);
-    expandBufAdd4BE(pReq, len);
-    if (len > 0) {
-        outBuf = expandBufAddSpace(pReq, len);
-        memcpy(outBuf, buf, len);
-    }
+    assert(iov != NULL);
+    assert(iovcnt > 0 && iovcnt < 10);
 
     /*
-     * Go back and write the JDWP header.
+     * "Wrap" the contents of the iovec with a JDWP/DDMS header.  We do
+     * this by creating a new copy of the vector with space for the header.
      */
-    outBuf = expandBufGetBuffer(pReq);
+    struct iovec wrapiov[iovcnt+1];
+    for (i = 0; i < iovcnt; i++) {
+        wrapiov[i+1].iov_base = iov[i].iov_base;
+        wrapiov[i+1].iov_len = iov[i].iov_len;
+        dataLen += iov[i].iov_len;
+    }
 
-    set4BE(outBuf, expandBufGetLength(pReq));
-    set4BE(outBuf+4, dvmJdwpNextRequestSerial(state));
-    set1(outBuf+8, 0);     /* flags */
-    set1(outBuf+9, kJDWPDdmCmdSet);
-    set1(outBuf+10, kJDWPDdmCmd);
+    /* form the header (JDWP plus DDMS) */
+    set4BE(header, sizeof(header) + dataLen);
+    set4BE(header+4, dvmJdwpNextRequestSerial(state));
+    set1(header+8, 0);     /* flags */
+    set1(header+9, kJDWPDdmCmdSet);
+    set1(header+10, kJDWPDdmCmd);
+    set4BE(header+11, type);
+    set4BE(header+15, dataLen);
 
-    /*
-     * Send it up.
-     */
-    //LOGD("Sending chunk (type=0x%08x len=%d)\n", type, len);
-    dvmJdwpSendRequest(state, pReq);
+    wrapiov[0].iov_base = header;
+    wrapiov[0].iov_len = sizeof(header);
 
-    expandBufFree(pReq);
+    dvmJdwpSendBufferedRequest(state, wrapiov, iovcnt+1);
 }
 
diff --git a/vm/jdwp/JdwpHandler.c b/vm/jdwp/JdwpHandler.c
index ff6ecf4..53b5d26 100644
--- a/vm/jdwp/JdwpHandler.c
+++ b/vm/jdwp/JdwpHandler.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * Handle messages from debugger.
  *
@@ -31,6 +32,7 @@
 
 #include "Bits.h"
 #include "Atomic.h"
+#include "DalvikVersion.h"
 
 #include <stdlib.h>
 #include <string.h>
@@ -110,10 +112,14 @@ static void jdwpWriteValue(ExpandBuf* pReply, int width, u8 value)
 
 /*
  * Common code for *_InvokeMethod requests.
+ *
+ * If "isConstructor" is set, this returns "objectId" rather than the
+ * expected-to-be-void return value of the called function.
  */
 static JdwpError finishInvoke(JdwpState* state,
     const u1* buf, int dataLen, ExpandBuf* pReply,
-    ObjectId threadId, ObjectId objectId, RefTypeId classId, MethodId methodId)
+    ObjectId threadId, ObjectId objectId, RefTypeId classId, MethodId methodId,
+    bool isConstructor)
 {
     JdwpError err = ERR_NONE;
     u8* argArray = NULL;
@@ -121,6 +127,8 @@ static JdwpError finishInvoke(JdwpState* state,
     u4 options;     /* enum InvokeOptions bit flags */
     int i;
 
+    assert(!isConstructor || objectId != 0);
+
     numArgs = read4BE(&buf);
 
     LOGV("    --> threadId=%llx objectId=%llx\n", threadId, objectId);
@@ -163,11 +171,16 @@ static JdwpError finishInvoke(JdwpState* state,
         goto bail;
 
     if (err == ERR_NONE) {
-        int width = dvmDbgGetTagWidth(resultTag);
-
-        expandBufAdd1(pReply, resultTag);
-        if (width != 0)
-            jdwpWriteValue(pReply, width, resultValue);
+        if (isConstructor) {
+            expandBufAdd1(pReply, JT_OBJECT);
+            expandBufAddObjectId(pReply, objectId);
+        } else {
+            int width = dvmDbgGetTagWidth(resultTag);
+
+            expandBufAdd1(pReply, resultTag);
+            if (width != 0)
+                jdwpWriteValue(pReply, width, resultValue);
+        }
         expandBufAdd1(pReply, JT_OBJECT);
         expandBufAddObjectId(pReply, exceptObjId);
 
@@ -198,8 +211,12 @@ bail:
 static JdwpError handleVM_Version(JdwpState* state, const u1* buf,
     int dataLen, ExpandBuf* pReply)
 {
+    char tmpBuf[128];
+
     /* text information on VM version */
-    expandBufAddUtf8String(pReply, (const u1*) "Android DalvikVM 1.0.1");
+    sprintf(tmpBuf, "Android DalvikVM %d.%d.%d",
+        DALVIK_MAJOR_VERSION, DALVIK_MINOR_VERSION, DALVIK_BUG_VERSION);
+    expandBufAddUtf8String(pReply, (const u1*) tmpBuf);
     /* JDWP version numbers */
     expandBufAdd4BE(pReply, 1);        // major
     expandBufAdd4BE(pReply, 5);        // minor
@@ -400,8 +417,10 @@ static JdwpError handleVM_CreateString(JdwpState* state,
     LOGV("  Req to create string '%s'\n", str);
 
     stringId = dvmDbgCreateString(str);
-    expandBufAddObjectId(pReply, stringId);
+    if (stringId == 0)
+        return ERR_OUT_OF_MEMORY;
 
+    expandBufAddObjectId(pReply, stringId);
     return ERR_NONE;
 }
 
@@ -652,8 +671,6 @@ static JdwpError handleRT_Interfaces(JdwpState* state,
     const u1* buf, int dataLen, ExpandBuf* pReply)
 {
     RefTypeId refTypeId;
-    u4 numInterfaces;
-    int i;
 
     refTypeId = dvmReadRefTypeId(&buf);
 
@@ -666,6 +683,25 @@ static JdwpError handleRT_Interfaces(JdwpState* state,
 }
 
 /*
+ * Return the class object corresponding to this type.
+ */
+static JdwpError handleRT_ClassObject(JdwpState* state,
+    const u1* buf, int dataLen, ExpandBuf* pReply)
+{
+    RefTypeId refTypeId;
+    ObjectId classObjId;
+
+    refTypeId = dvmReadRefTypeId(&buf);
+    classObjId = dvmDbgGetClassObject(refTypeId);
+
+    LOGV("  RefTypeId %llx -> ObjectId %llx\n", refTypeId, classObjId);
+
+    expandBufAddObjectId(pReply, classObjId);
+
+    return ERR_NONE;
+}
+
+/*
  * Returns the value of the SourceDebugExtension attribute.
  *
  * JDB seems interested, but DEX files don't currently support this.
@@ -834,7 +870,36 @@ static JdwpError handleCT_InvokeMethod(JdwpState* state,
     methodId = dvmReadMethodId(&buf);
 
     return finishInvoke(state, buf, dataLen, pReply,
-            threadId, 0, classId, methodId);
+            threadId, 0, classId, methodId, false);
+}
+
+/*
+ * Create a new object of the requested type, and invoke the specified
+ * constructor.
+ *
+ * Example: in IntelliJ, create a watch on "new String(myByteArray)" to
+ * see the contents of a byte[] as a string.
+ */
+static JdwpError handleCT_NewInstance(JdwpState* state,
+    const u1* buf, int dataLen, ExpandBuf* pReply)
+{
+    RefTypeId classId;
+    ObjectId threadId;
+    MethodId methodId;
+    ObjectId objectId;
+    u4 numArgs;
+
+    classId = dvmReadRefTypeId(&buf);
+    threadId = dvmReadObjectId(&buf);
+    methodId = dvmReadMethodId(&buf);
+
+    LOGV("Creating instance of %s\n", dvmDbgGetClassDescriptor(classId));
+    objectId = dvmDbgCreateObject(classId);
+    if (objectId == 0)
+        return ERR_OUT_OF_MEMORY;
+
+    return finishInvoke(state, buf, dataLen, pReply,
+            threadId, objectId, classId, methodId, true);
 }
 
 /*
@@ -1011,7 +1076,7 @@ static JdwpError handleOR_InvokeMethod(JdwpState* state,
     methodId = dvmReadMethodId(&buf);
 
     return finishInvoke(state, buf, dataLen, pReply,
-            threadId, objectId, classId, methodId);
+            threadId, objectId, classId, methodId, false);
 }
 
 /*
@@ -1938,7 +2003,7 @@ static const JdwpHandlerMap gHandlerMap[] = {
     //2,    8,  NestedTypes
     { 2,    9,  handleRT_Status,        "ReferenceType.Status" },
     { 2,    10, handleRT_Interfaces,    "ReferenceType.Interfaces" },
-    //2,    11, ClassObject
+    { 2,    11, handleRT_ClassObject,   "ReferenceType.ClassObject" },
     { 2,    12, handleRT_SourceDebugExtension,
                                         "ReferenceType.SourceDebugExtension" },
     { 2,    13, handleRT_SignatureWithGeneric,
@@ -1955,7 +2020,7 @@ static const JdwpHandlerMap gHandlerMap[] = {
     { 3,    1,  handleCT_Superclass,    "ClassType.Superclass" },
     { 3,    2,  handleCT_SetValues,     "ClassType.SetValues" },
     { 3,    3,  handleCT_InvokeMethod,  "ClassType.InvokeMethod" },
-    //3,    4,  NewInstance
+    { 3,    4,  handleCT_NewInstance,   "ClassType.NewInstance" },
 
     /* ArrayType command set (4) */
     //4,    1,  NewInstance
diff --git a/vm/jdwp/JdwpMain.c b/vm/jdwp/JdwpMain.c
index 4166c67..ef24618 100644
--- a/vm/jdwp/JdwpMain.c
+++ b/vm/jdwp/JdwpMain.c
@@ -13,6 +13,7 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 /*
  * JDWP initialization.
  */
@@ -147,7 +148,8 @@ fail:
 
 /*
  * Reset all session-related state.  There should not be an active connection
- * to the client at this point (we may be listening for a new one though).
+ * to the client at this point.  The rest of the VM still thinks there is
+ * a debugger attached.
  *
  * This includes freeing up the debugger event list.
  */
@@ -185,7 +187,8 @@ void dvmJdwpShutdown(JdwpState* state)
         /*
          * Close down the network to inspire the thread to halt.
          */
-        LOGD("JDWP shutting down net...\n");
+        if (gDvm.verboseShutdown)
+            LOGD("JDWP shutting down net...\n");
         dvmJdwpNetShutdown(state);
 
         if (state->debugThreadStarted) {
@@ -195,7 +198,8 @@ void dvmJdwpShutdown(JdwpState* state)
             }
         }
 
-        LOGV("JDWP freeing netstate...\n");
+        if (gDvm.verboseShutdown)
+            LOGD("JDWP freeing netstate...\n");
         dvmJdwpNetFree(state);
         state->netState = NULL;
     }
@@ -315,14 +319,15 @@ static void* jdwpThreadStart(void* arg)
             dvmDbgThreadWaiting();
         }
 
-        /* interpreter can ignore breakpoints */
+        /* release session state, e.g. remove breakpoint instructions */
+        dvmJdwpResetState(state);
+
+        /* tell the interpreter that the debugger is no longer around */
         dvmDbgDisconnected();
 
-        /* if we had stuff suspended, resume it now */
+        /* if we had threads suspended, resume them now */
         dvmUndoDebuggerSuspensions();
 
-        dvmJdwpResetState(state);
-
         /* if we connected out, this was a one-shot deal */
         if (!state->params.server)
             state->run = false;
diff --git a/vm/jdwp/JdwpPriv.h b/vm/jdwp/JdwpPriv.h
index 087b560..87c3fc7 100644
--- a/vm/jdwp/JdwpPriv.h
+++ b/vm/jdwp/JdwpPriv.h
@@ -24,7 +24,9 @@
 #include "jdwp/Jdwp.h"
 #include "jdwp/JdwpEvent.h"
 #include "Debugger.h"
+
 #include <pthread.h>
+#include <sys/uio.h>
 
 /*
  * JDWP constants.
@@ -59,6 +61,8 @@ typedef struct JdwpTransport {
     bool (*awaitingHandshake)(struct JdwpState* state);
     bool (*processIncoming)(struct JdwpState* state);
     bool (*sendRequest)(struct JdwpState* state, ExpandBuf* pReq);
+    bool (*sendBufferedRequest)(struct JdwpState* state,
+        const struct iovec* iov, int iovcnt);
 } JdwpTransport;
 
 const JdwpTransport* dvmJdwpSocketTransport();
@@ -167,5 +171,10 @@ INLINE bool dvmJdwpProcessIncoming(JdwpState* state) {
 INLINE bool dvmJdwpSendRequest(JdwpState* state, ExpandBuf* pReq) {
     return (*state->transport->sendRequest)(state, pReq);
 }
+INLINE bool dvmJdwpSendBufferedRequest(JdwpState* state,
+    const struct iovec* iov, int iovcnt)
+{
+    return (*state->transport->sendBufferedRequest)(state, iov, iovcnt);
+}
 
 #endif /*_DALVIK_JDWP_JDWPPRIV*/
diff --git a/vm/jdwp/JdwpSocket.c b/vm/jdwp/JdwpSocket.c
index 7b1ccfc..42cd189 100644
--- a/vm/jdwp/JdwpSocket.c
+++ b/vm/jdwp/JdwpSocket.c
@@ -825,7 +825,7 @@ static bool sendRequest(JdwpState* state, ExpandBuf* pReq)
     JdwpNetState* netState = state->netState;
     int cc;
 
-    dumpPacket(expandBufGetBuffer(pReq));
+    /*dumpPacket(expandBufGetBuffer(pReq));*/
     if (netState->clientSock < 0) {
         /* can happen with some DDMS events */
         LOGV("NOT sending request -- no debugger is attached\n");
@@ -849,9 +849,53 @@ static bool sendRequest(JdwpState* state, ExpandBuf* pReq)
     return true;
 }
 
+/*
+ * Send a request that was split into multiple buffers.
+ *
+ * The entire packet must be sent with a single writev() call to avoid
+ * threading issues.
+ *
+ * Returns "true" if it was sent successfully.
+ */
+static bool sendBufferedRequest(JdwpState* state, const struct iovec* iov,
+    int iovcnt)
+{
+    JdwpNetState* netState = state->netState;
+
+    if (netState->clientSock < 0) {
+        /* can happen with some DDMS events */
+        LOGV("NOT sending request -- no debugger is attached\n");
+        return false;
+    }
+
+    size_t expected = 0;
+    int i;
+    for (i = 0; i < iovcnt; i++)
+        expected += iov[i].iov_len;
+
+    /*
+     * TODO: we currently assume the writev() will complete in one
+     * go, which may not be safe for a network socket.  We may need
+     * to mutex this against handlePacket().
+     */
+    ssize_t actual;
+    actual = writev(netState->clientSock, iov, iovcnt);
+    if ((size_t)actual != expected) {
+        LOGE("Failed sending b-req to debugger: %s (%d of %zu)\n",
+            strerror(errno), (int) actual, expected);
+        return false;
+    }
+
+    return true;
+}
+
 
 /*
  * Our functions.
+ *
+ * We can't generally share the implementations with other transports,
+ * even if they're also socket-based, because our JdwpNetState will be
+ * different from theirs.
  */
 static const JdwpTransport socketTransport = {
     prepareSocket,
@@ -863,7 +907,8 @@ static const JdwpTransport socketTransport = {
     isConnected,
     awaitingHandshake,
     processIncoming,
-    sendRequest
+    sendRequest,
+    sendBufferedRequest,
 };
 
 /*
diff --git a/vm/mterp/Makefile-mterp b/vm/mterp/Makefile-mterp
index ed77cfc..d30dff2 100644
--- a/vm/mterp/Makefile-mterp
+++ b/vm/mterp/Makefile-mterp
@@ -35,7 +35,8 @@ OUTPUT_DIR := out
 # conservative fashion.  If it's not one of the generated files in "out",
 # assume it's a dependency.
 SOURCE_DEPS := \
-	$(shell find . -path ./$(OUTPUT_DIR) -prune -o -type f -print)
+	$(shell find . -path ./$(OUTPUT_DIR) -prune -o -type f -print) \
+	../Android.mk ../../Android.mk
 
 # Source files generated by the script.  There's always one C and one
 # assembly file, though in practice one or the other could be empty.
diff --git a/vm/mterp/Mterp.c b/vm/mterp/Mterp.c
index 053f544..ca2ca16 100644
--- a/vm/mterp/Mterp.c
+++ b/vm/mterp/Mterp.c
@@ -83,6 +83,8 @@ bool dvmMterpStd(Thread* self, InterpState* glue)
     glue->pSelfSuspendCount = &self->suspendCount;
 #if defined(WITH_JIT)
     glue->pJitProfTable = gDvmJit.pProfTable;
+    glue->ppJitProfTable = &gDvmJit.pProfTable;
+    glue->jitThreshold = gDvmJit.threshold;
 #endif
 #if defined(WITH_DEBUGGER)
     glue->pDebuggerActive = &gDvm.debuggerActive;
@@ -105,6 +107,13 @@ bool dvmMterpStd(Thread* self, InterpState* glue)
     //LOGI("first instruction is 0x%04x\n", glue->pc[0]);
 
     changeInterp = dvmMterpStdRun(glue);
+
+#if defined(WITH_JIT)
+    if (glue->jitState != kJitSingleStep) {
+        glue->self->inJitCodeCache = NULL;
+    }
+#endif
+
     if (!changeInterp) {
         /* this is a "normal" exit; we're not coming back */
 #ifdef LOG_INSTR
diff --git a/vm/mterp/armv5te/OP_EXECUTE_INLINE.S b/vm/mterp/armv5te/OP_EXECUTE_INLINE.S
index eb0b76f..550bb83 100644
--- a/vm/mterp/armv5te/OP_EXECUTE_INLINE.S
+++ b/vm/mterp/armv5te/OP_EXECUTE_INLINE.S
@@ -3,17 +3,18 @@
     /*
      * Execute a "native inline" instruction.
      *
-     * We need to call:
-     *  dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref)
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
      *
-     * The first four args are in r0-r3, but the last two must be pushed
-     * onto the stack.
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
      */
     /* [opt] execute-inline vAA, {vC, vD, vE, vF}, inline@BBBB */
     FETCH(r10, 1)                       @ r10<- BBBB
     add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
     EXPORT_PC()                         @ can throw
-    sub     sp, sp, #8                  @ make room for arg(s)
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
     mov     r0, rINST, lsr #12          @ r0<- B
     str     r1, [sp]                    @ push &glue->retval
     bl      .L${opcode}_continue        @ make call; will return after
diff --git a/vm/mterp/armv5te/OP_IGET_WIDE_QUICK.S b/vm/mterp/armv5te/OP_IGET_WIDE_QUICK.S
index ece7e7a..189b683 100644
--- a/vm/mterp/armv5te/OP_IGET_WIDE_QUICK.S
+++ b/vm/mterp/armv5te/OP_IGET_WIDE_QUICK.S
@@ -3,11 +3,11 @@
     /* iget-wide-quick vA, vB, offset@CCCC */
     mov     r2, rINST, lsr #12          @ r2<- B
     GET_VREG(r3, r2)                    @ r3<- object we're operating on
-    FETCH(r1, 1)                        @ r1<- field byte offset
+    FETCH(ip, 1)                        @ ip<- field byte offset
     cmp     r3, #0                      @ check object for null
     mov     r2, rINST, lsr #8           @ r2<- A(+)
     beq     common_errNullObject        @ object was null
-    ldrd    r0, [r3, r1]                @ r0<- obj.field (64 bits, aligned)
+    ldrd    r0, [r3, ip]                @ r0<- obj.field (64 bits, aligned)
     and     r2, r2, #15
     FETCH_ADVANCE_INST(2)               @ advance rPC, load rINST
     add     r3, rFP, r2, lsl #2         @ r3<- &fp[A]
diff --git a/vm/mterp/armv5te/OP_MONITOR_EXIT.S b/vm/mterp/armv5te/OP_MONITOR_EXIT.S
index b334ae9..c9aedf0 100644
--- a/vm/mterp/armv5te/OP_MONITOR_EXIT.S
+++ b/vm/mterp/armv5te/OP_MONITOR_EXIT.S
@@ -13,12 +13,15 @@
     EXPORT_PC()                         @ before fetch: export the PC
     GET_VREG(r1, r2)                    @ r1<- vAA (object)
     cmp     r1, #0                      @ null object?
-    beq     common_errNullObject        @ yes
+    beq     1f                          @ yes
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
     bl      dvmUnlockObject             @ r0<- success for unlock(self, obj)
     cmp     r0, #0                      @ failed?
-    beq     common_exceptionThrown      @ yes, exception is pending
     FETCH_ADVANCE_INST(1)               @ before throw: advance rPC, load rINST
+    beq     common_exceptionThrown      @ yes, exception is pending
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
+1:
+    FETCH_ADVANCE_INST(1)               @ advance before throw
+    b      common_errNullObject
 
diff --git a/vm/mterp/armv5te/OP_UNUSED_EC.S b/vm/mterp/armv5te/OP_UNUSED_EC.S
deleted file mode 100644
index faa7246..0000000
--- a/vm/mterp/armv5te/OP_UNUSED_EC.S
+++ /dev/null
@@ -1 +0,0 @@
-%include "armv5te/unused.S"
diff --git a/vm/mterp/armv5te/OP_UNUSED_EF.S b/vm/mterp/armv5te/OP_UNUSED_EF.S
deleted file mode 100644
index faa7246..0000000
--- a/vm/mterp/armv5te/OP_UNUSED_EF.S
+++ /dev/null
@@ -1 +0,0 @@
-%include "armv5te/unused.S"
diff --git a/vm/mterp/armv5te/entry.S b/vm/mterp/armv5te/entry.S
index f9e01a3..99e2366 100644
--- a/vm/mterp/armv5te/entry.S
+++ b/vm/mterp/armv5te/entry.S
@@ -60,17 +60,20 @@ dvmMterpStdRun:
 
     /* set up "named" registers, figure out entry point */
     mov     rGLUE, r0                   @ set rGLUE
-    ldrb    r1, [r0, #offGlue_entryPoint]   @ InterpEntry enum is char
+    ldr     r1, [r0, #offGlue_entryPoint]   @ enum is 4 bytes in aapcs-EABI
     LOAD_PC_FP_FROM_GLUE()              @ load rPC and rFP from "glue"
     adr     rIBASE, dvmAsmInstructionStart  @ set rIBASE
     cmp     r1, #kInterpEntryInstr      @ usual case?
     bne     .Lnot_instr                 @ no, handle it
 
 #if defined(WITH_JIT)
-.Lno_singleStep:
+.LentryInstr:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     /* Entry is always a possible trace start */
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
+    mov    r1, #0                       @ prepare the value for the new state
+    str    r1, [r10, #offThread_inJitCodeCache] @ back to the interp land
     cmp    r0,#0
     bne    common_updateProfile
     GET_INST_OPCODE(ip)
@@ -92,18 +95,21 @@ dvmMterpStdRun:
 
 #if defined(WITH_JIT)
 .Lnot_throw:
-    ldr     r0,[rGLUE, #offGlue_jitResume]
-    ldr     r2,[rGLUE, #offGlue_jitResumePC]
+    ldr     r10,[rGLUE, #offGlue_jitResumeNPC]
+    ldr     r2,[rGLUE, #offGlue_jitResumeDPC]
     cmp     r1, #kInterpEntryResume     @ resuming after Jit single-step?
     bne     .Lbad_arg
     cmp     rPC,r2
-    bne     .Lno_singleStep             @ must have branched, don't resume
+    bne     .LentryInstr                @ must have branched, don't resume
+#if defined(WITH_SELF_VERIFICATION)
+    @ glue->entryPoint will be set in dvmSelfVerificationSaveState
+    b       jitSVShadowRunStart         @ re-enter the translation after the
+                                        @ single-stepped instruction
+    @noreturn
+#endif
     mov     r1, #kInterpEntryInstr
-    strb    r1, [rGLUE, #offGlue_entryPoint]
-    ldr     rINST, .LdvmCompilerTemplate
-    bx      r0                          @ re-enter the translation
-.LdvmCompilerTemplate:
-    .word   dvmCompilerTemplateStart
+    str     r1, [rGLUE, #offGlue_entryPoint]
+    bx      r10                         @ re-enter the translation
 #endif
 
 .Lbad_arg:
diff --git a/vm/mterp/armv5te/footer.S b/vm/mterp/armv5te/footer.S
index 90e3750..cf797de 100644
--- a/vm/mterp/armv5te/footer.S
+++ b/vm/mterp/armv5te/footer.S
@@ -11,6 +11,67 @@
     .align  2
 
 #if defined(WITH_JIT)
+#if defined(WITH_SELF_VERIFICATION)
+    .global dvmJitToInterpPunt
+dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r2,#kSVSPunt                 @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpSingleStep
+dvmJitToInterpSingleStep:
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
+    mov    r2,#kSVSSingleStep           @ r2<- interpreter entry point
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSTraceSelectNoChain   @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSTraceSelect          @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpBackwardBranch
+dvmJitToInterpBackwardBranch:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSBackwardBranch       @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNormal
+dvmJitToInterpNormal:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSNormal               @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNoChain
+dvmJitToInterpNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSNoChain              @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+#else
 /*
  * Return from the translation cache to the interpreter when the compiler is
  * having issues translating/executing a Dalvik instruction. We have to skip
@@ -20,12 +81,15 @@
  */
     .global dvmJitToInterpPunt
 dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    rPC, r0
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     mov    r0,lr
     bl     dvmBumpPunt;
 #endif
     EXPORT_PC()
+    mov    r0, #0
+    str    r0, [r10, #offThread_inJitCodeCache] @ Back to the interp land
     adrl   rIBASE, dvmAsmInstructionStart
     FETCH_INST()
     GET_INST_OPCODE(ip)
@@ -40,35 +104,58 @@ dvmJitToInterpPunt:
  */
     .global dvmJitToInterpSingleStep
 dvmJitToInterpSingleStep:
-    str    lr,[rGLUE,#offGlue_jitResume]
-    str    r1,[rGLUE,#offGlue_jitResumePC]
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
     mov    r1,#kInterpEntryInstr
     @ enum is 4 byte in aapcs-EABI
     str    r1, [rGLUE, #offGlue_entryPoint]
     mov    rPC,r0
     EXPORT_PC()
+
     adrl   rIBASE, dvmAsmInstructionStart
     mov    r2,#kJitSingleStep     @ Ask for single step and then revert
     str    r2,[rGLUE,#offGlue_jitState]
     mov    r1,#1                  @ set changeInterp to bail to debug interp
     b      common_gotoBail
 
+/*
+ * Return from the translation cache and immediately request
+ * a translation for the exit target.  Commonly used for callees.
+ */
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+#ifdef JIT_STATS
+    bl     dvmBumpNoChain
+#endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC
+    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
+    cmp    r0,#0
+    bxne   r0                       @ continue native execution if so
+    b      2f
 
 /*
  * Return from the translation cache and immediately request
  * a translation for the exit target.  Commonly used following
  * invokes.
  */
-    .global dvmJitToTraceSelect
-dvmJitToTraceSelect:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
     mov    r0,rPC
-    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    bl     dvmJitGetCodeAddr       @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    2f
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @ in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -79,6 +166,7 @@ dvmJitToTraceSelect:
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
     cmp    r0, #0
+    movne  r2,#kJitTSelectRequestHot   @ ask for trace selection
     bne    common_selectTrace
     GET_INST_OPCODE(ip)
     GOTO_OPCODE(ip)
@@ -99,17 +187,21 @@ dvmJitToTraceSelect:
  */
     .global dvmJitToInterpNormal
 dvmJitToInterpNormal:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
-#ifdef EXIT_STATS
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
+#ifdef JIT_STATS
     bl     dvmBumpNormal
 #endif
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    toInterpreter            @ go if not, otherwise do chain
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -120,13 +212,18 @@ dvmJitToInterpNormal:
  */
     .global dvmJitToInterpNoChain
 dvmJitToInterpNoChain:
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     bl     dvmBumpNoChain
 #endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0
     bxne   r0                       @ continue native execution if so
+#endif
 
 /*
  * No translation, restore interpreter regs and start interpreting.
@@ -154,11 +251,11 @@ common_testUpdateProfile:
 
 common_updateProfile:
     eor     r3,rPC,rPC,lsr #12 @ cheap, but fast hash function
-    lsl     r3,r3,#23          @ shift out excess 511
-    ldrb    r1,[r0,r3,lsr #23] @ get counter
+    lsl     r3,r3,#(32 - JIT_PROF_SIZE_LOG_2)          @ shift out excess bits
+    ldrb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ get counter
     GET_INST_OPCODE(ip)
     subs    r1,r1,#1           @ decrement counter
-    strb    r1,[r0,r3,lsr #23] @ and store it
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ and store it
     GOTO_OPCODE_IFNE(ip)       @ if not threshold, fallthrough otherwise */
 
 /*
@@ -167,20 +264,94 @@ common_updateProfile:
  * is already a native translation in place (and, if so,
  * jump to it now).
  */
-    mov     r1,#255
-    strb    r1,[r0,r3,lsr #23] @ reset counter
+    GET_JIT_THRESHOLD(r1)
+    ldr     r10, [rGLUE, #offGlue_self] @ callee saved r10 <- glue->self
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ reset counter
     EXPORT_PC()
     mov     r0,rPC
     bl      dvmJitGetCodeAddr           @ r0<- dvmJitGetCodeAddr(rPC)
+    str     r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov     r1, rPC                     @ arg1 of translation may need this
+    mov     lr, #0                      @  in case target is HANDLER_INTERPRET
     cmp     r0,#0
-    beq     common_selectTrace
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    r0                          @ jump to the translation
-common_selectTrace:
     mov     r2,#kJitTSelectRequest      @ ask for trace selection
+    @ fall-through to common_selectTrace
+#else
+    moveq   r2,#kJitTSelectRequest      @ ask for trace selection
+    beq     common_selectTrace
+    /*
+     * At this point, we have a target translation.  However, if
+     * that translation is actually the interpret-only pseudo-translation
+     * we want to treat it the same as no translation.
+     */
+    mov     r10, r0                     @ save target
+    bl      dvmCompilerGetInterpretTemplate
+    cmp     r0, r10                     @ special case?
+    bne     jitSVShadowRunStart         @ set up self verification shadow space
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+    /* no return */
+#endif
+
+/*
+ * On entry:
+ *  r2 is jit state, e.g. kJitTSelectRequest or kJitTSelectRequestHot
+ */
+common_selectTrace:
     str     r2,[rGLUE,#offGlue_jitState]
+    mov     r2,#kInterpEntryInstr       @ normal entry reason
+    str     r2,[rGLUE,#offGlue_entryPoint]
     mov     r1,#1                       @ set changeInterp
     b       common_gotoBail
 
+#if defined(WITH_SELF_VERIFICATION)
+/*
+ * Save PC and registers to shadow memory for self verification mode
+ * before jumping to native translation.
+ * On entry:
+ *    rPC, rFP, rGLUE: the values that they should contain
+ *    r10: the address of the target translation.
+ */
+jitSVShadowRunStart:
+    mov     r0,rPC                      @ r0<- program counter
+    mov     r1,rFP                      @ r1<- frame pointer
+    mov     r2,rGLUE                    @ r2<- InterpState pointer
+    mov     r3,r10                      @ r3<- target translation
+    bl      dvmSelfVerificationSaveState @ save registers to shadow space
+    ldr     rFP,[r0,#offShadowSpace_shadowFP] @ rFP<- fp in shadow space
+    add     rGLUE,r0,#offShadowSpace_interpState @ rGLUE<- rGLUE in shadow space
+    bx      r10                         @ jump to the translation
+
+/*
+ * Restore PC, registers, and interpState to original values
+ * before jumping back to the interpreter.
+ */
+jitSVShadowRunEnd:
+    mov    r1,rFP                        @ pass ending fp
+    bl     dvmSelfVerificationRestoreState @ restore pc and fp values
+    ldr    rPC,[r0,#offShadowSpace_startPC] @ restore PC
+    ldr    rFP,[r0,#offShadowSpace_fp]   @ restore FP
+    ldr    rGLUE,[r0,#offShadowSpace_glue] @ restore InterpState
+    ldr    r1,[r0,#offShadowSpace_svState] @ get self verification state
+    cmp    r1,#0                         @ check for punt condition
+    beq    1f
+    mov    r2,#kJitSelfVerification      @ ask for self verification
+    str    r2,[rGLUE,#offGlue_jitState]
+    mov    r2,#kInterpEntryInstr         @ normal entry reason
+    str    r2,[rGLUE,#offGlue_entryPoint]
+    mov    r1,#1                         @ set changeInterp
+    b      common_gotoBail
+
+1:                                       @ exit to interpreter without check
+    EXPORT_PC()
+    adrl   rIBASE, dvmAsmInstructionStart
+    FETCH_INST()
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+#endif
+
 #endif
 
 /*
@@ -222,6 +393,9 @@ common_backwardBranch:
 common_periodicChecks:
     ldr     r3, [rGLUE, #offGlue_pSelfSuspendCount] @ r3<- &suspendCount
 
+    @ speculatively store r0 before it is clobbered by dvmCheckSuspendPending
+    str     r0, [rGLUE, #offGlue_entryPoint]
+
 #if defined(WITH_DEBUGGER)
     ldr     r1, [rGLUE, #offGlue_pDebuggerActive]   @ r1<- &debuggerActive
 #endif
@@ -256,13 +430,24 @@ common_periodicChecks:
     bx      lr                          @ nothing to do, return
 
 2:  @ check suspend
+#if defined(WITH_JIT)
+    /*
+     * Refresh the Jit's cached copy of profile table pointer.  This pointer
+     * doubles as the Jit's on/off switch.
+     */
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ r3<-&gDvmJit.pJitProfTable
+    ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    ldr     r3, [r3] @ r3 <- pJitProfTable
+    EXPORT_PC()                         @ need for precise GC
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh Jit's on/off switch
+#else
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
     EXPORT_PC()                         @ need for precise GC
+#endif
     b       dvmCheckSuspendPending      @ suspend if necessary, then return
 
 3:  @ debugger/profiler enabled, bail out
     add     rPC, rPC, r9                @ update rPC
-    str     r0, [rGLUE, #offGlue_entryPoint]
     mov     r1, #1                      @ "want switch" = true
     b       common_gotoBail
 
@@ -454,20 +639,31 @@ dalvik_mterp:
     @ldr     pc, [r2, #offMethod_nativeFunc] @ pc<- methodToCall->nativeFunc
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+#if defined(WITH_JIT)
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ Refresh Jit's on/off status
+#endif
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+#if defined(WITH_JIT)
+    ldr     r3, [r3]                    @ r3 <- gDvmJit.pProfTable
+#endif
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
+#if defined(WITH_JIT)
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh cached on/off switch
+#endif
     bne     common_exceptionThrown      @ no, handle exception
 
     FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    @ r0=methodToCall
+    mov     r1, r0                      @ r1<- methodToCall
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- self
     bl      dvmHandleStackOverflow
     b       common_exceptionThrown
@@ -531,12 +727,13 @@ common_returnFromMethod:
     ldr     r1, [r10, #offClassObject_pDvmDex]   @ r1<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame]  @ self->curFrame = fp
 #if defined(WITH_JIT)
-    ldr     r3, [r0, #offStackSaveArea_returnAddr] @ r3 = saveArea->returnAddr
+    ldr     r10, [r0, #offStackSaveArea_returnAddr] @ r10 = saveArea->returnAddr
     GET_JIT_PROF_TABLE(r0)
     mov     rPC, r9                     @ publish new rPC
     str     r1, [rGLUE, #offGlue_methodClassDex]
-    cmp     r3, #0                      @ caller is compiled code
-    blxne   r3
+    str     r10, [r3, #offThread_inJitCodeCache]  @ may return to JIT'ed land
+    cmp     r10, #0                      @ caller is compiled code
+    blxne   r10
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     cmp     r0,#0
     bne     common_updateProfile
@@ -577,11 +774,6 @@ common_exceptionThrown:
     mov     r9, #0
     bl      common_periodicChecks
 
-#if defined(WITH_JIT)
-    mov     r2,#kJitTSelectAbort        @ abandon trace selection in progress
-    str     r2,[rGLUE,#offGlue_jitState]
-#endif
-
     ldr     r10, [rGLUE, #offGlue_self] @ r10<- glue->self
     ldr     r9, [r10, #offThread_exception] @ r9<- self->exception
     mov     r1, r10                     @ r1<- self
@@ -612,6 +804,7 @@ common_exceptionThrown:
     beq     1f                          @ no, skip ahead
     mov     rFP, r0                     @ save relPc result in rFP
     mov     r0, r10                     @ r0<- self
+    mov     r1, r9                      @ r1<- exception
     bl      dvmCleanupStackOverflow     @ call(self)
     mov     r0, rFP                     @ restore result
 1:
@@ -649,6 +842,7 @@ common_exceptionThrown:
     ldrb    r1, [r10, #offThread_stackOverflowed]
     cmp     r1, #0                      @ did we overflow earlier?
     movne   r0, r10                     @ if yes: r0<- self
+    movne   r1, r9                      @ if yes: r1<- exception
     blne    dvmCleanupStackOverflow     @ if yes: call(self)
 
     @ may want to show "not caught locally" debug messages here
diff --git a/vm/mterp/armv5te/header.S b/vm/mterp/armv5te/header.S
index e129b15..b6e9891 100644
--- a/vm/mterp/armv5te/header.S
+++ b/vm/mterp/armv5te/header.S
@@ -178,8 +178,8 @@ unspecified registers or condition codes.
 #define SET_VREG(_reg, _vreg)   str     _reg, [rFP, _vreg, lsl #2]
 
 #if defined(WITH_JIT)
-#define GET_JIT_ENABLED(_reg)       ldr     _reg,[rGLUE,#offGlue_jitEnabled]
 #define GET_JIT_PROF_TABLE(_reg)    ldr     _reg,[rGLUE,#offGlue_pJitProfTable]
+#define GET_JIT_THRESHOLD(_reg)     ldr     _reg,[rGLUE,#offGlue_jitThreshold]
 #endif
 
 /*
@@ -194,3 +194,6 @@ unspecified registers or condition codes.
  */
 #include "../common/asm-constants.h"
 
+#if defined(WITH_JIT)
+#include "../common/jit-config.h"
+#endif
diff --git a/vm/mterp/armv6t2/OP_IGET_WIDE_QUICK.S b/vm/mterp/armv6t2/OP_IGET_WIDE_QUICK.S
index 129f424..98abf72 100644
--- a/vm/mterp/armv6t2/OP_IGET_WIDE_QUICK.S
+++ b/vm/mterp/armv6t2/OP_IGET_WIDE_QUICK.S
@@ -2,12 +2,12 @@
 %verify "null object"
     /* iget-wide-quick vA, vB, offset@CCCC */
     mov     r2, rINST, lsr #12          @ r2<- B
-    FETCH(r1, 1)                        @ r1<- field byte offset
+    FETCH(ip, 1)                        @ ip<- field byte offset
     GET_VREG(r3, r2)                    @ r3<- object we're operating on
     ubfx    r2, rINST, #8, #4           @ r2<- A
     cmp     r3, #0                      @ check object for null
     beq     common_errNullObject        @ object was null
-    ldrd    r0, [r3, r1]                @ r0<- obj.field (64 bits, aligned)
+    ldrd    r0, [r3, ip]                @ r0<- obj.field (64 bits, aligned)
     FETCH_ADVANCE_INST(2)               @ advance rPC, load rINST
     add     r3, rFP, r2, lsl #2         @ r3<- &fp[A]
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
diff --git a/vm/mterp/c/OP_EXECUTE_INLINE.c b/vm/mterp/c/OP_EXECUTE_INLINE.c
index 76bc3f6..bc10f1a 100644
--- a/vm/mterp/c/OP_EXECUTE_INLINE.c
+++ b/vm/mterp/c/OP_EXECUTE_INLINE.c
@@ -9,14 +9,15 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
          * the rest uninitialized.  We're assuming that, if the method
          * needs them, they'll be specified in the call.
          *
-         * This annoys gcc when optimizations are enabled, causing a
-         * "may be used uninitialized" warning.  We can quiet the warnings
-         * for a slight penalty (5%: 373ns vs. 393ns on empty method).  Note
-         * that valgrind is perfectly happy with this arrangement, because
-         * the uninitialiezd values are never actually used.
+         * However, this annoys gcc when optimizations are enabled,
+         * causing a "may be used uninitialized" warning.  Quieting
+         * the warnings incurs a slight penalty (5%: 373ns vs. 393ns
+         * on empty method).  Note that valgrind is perfectly happy
+         * either way as the uninitialiezd values are never actually
+         * used.
          */
         u4 arg0, arg1, arg2, arg3;
-        //arg0 = arg1 = arg2 = arg3 = 0;
+        arg0 = arg1 = arg2 = arg3 = 0;
 
         EXPORT_PC();
 
diff --git a/vm/mterp/c/OP_UNUSED_EC.c b/vm/mterp/c/OP_UNUSED_EC.c
deleted file mode 100644
index fcb8c2e..0000000
--- a/vm/mterp/c/OP_UNUSED_EC.c
+++ /dev/null
@@ -1,2 +0,0 @@
-HANDLE_OPCODE(OP_UNUSED_EC)
-OP_END
diff --git a/vm/mterp/c/OP_UNUSED_EF.c b/vm/mterp/c/OP_UNUSED_EF.c
deleted file mode 100644
index c5e1863..0000000
--- a/vm/mterp/c/OP_UNUSED_EF.c
+++ /dev/null
@@ -1,2 +0,0 @@
-HANDLE_OPCODE(OP_UNUSED_EF)
-OP_END
diff --git a/vm/mterp/c/gotoTargets.c b/vm/mterp/c/gotoTargets.c
index 5b93583..534d0b0 100644
--- a/vm/mterp/c/gotoTargets.c
+++ b/vm/mterp/c/gotoTargets.c
@@ -532,6 +532,10 @@ GOTO_TARGET(returnFromMethod)
         if (dvmIsBreakFrame(fp)) {
             /* bail without popping the method frame from stack */
             LOGVV("+++ returned into break frame\n");
+#if defined(WITH_JIT)
+            /* Let the Jit know the return is terminating normally */
+            CHECK_JIT();
+#endif
             GOTO_bail();
         }
 
@@ -576,6 +580,10 @@ GOTO_TARGET(exceptionThrown)
          */
         PERIODIC_CHECKS(kInterpEntryThrow, 0);
 
+#if defined(WITH_JIT)
+        // Something threw during trace selection - abort the current trace
+        ABORT_JIT_TSELECT();
+#endif
         /*
          * We save off the exception and clear the exception status.  While
          * processing the exception we might need to load some Throwable
@@ -626,6 +634,9 @@ GOTO_TARGET(exceptionThrown)
          *
          * If we do find a catch block, we want to transfer execution to
          * that point.
+         *
+         * Note this can cause an exception while resolving classes in
+         * the "catch" blocks.
          */
         catchRelPc = dvmFindCatchBlock(self, pc - curMethod->insns,
                     exception, false, (void*)&fp);
@@ -640,9 +651,19 @@ GOTO_TARGET(exceptionThrown)
          * Note we want to do this *after* the call to dvmFindCatchBlock,
          * because that may need extra stack space to resolve exception
          * classes (e.g. through a class loader).
+         *
+         * It's possible for the stack overflow handling to cause an
+         * exception (specifically, class resolution in a "catch" block
+         * during the call above), so we could see the thread's overflow
+         * flag raised but actually be running in a "nested" interpreter
+         * frame.  We don't allow doubled-up StackOverflowErrors, so
+         * we can check for this by just looking at the exception type
+         * in the cleanup function.  Also, we won't unroll past the SOE
+         * point because the more-recent exception will hit a break frame
+         * as it unrolls to here.
          */
         if (self->stackOverflowed)
-            dvmCleanupStackOverflow(self);
+            dvmCleanupStackOverflow(self, exception);
 
         if (catchRelPc < 0) {
             /* falling through to JNI code or off the bottom of the stack */
@@ -807,10 +828,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             bottom = (u1*) newSaveArea - methodToCall->outsSize * sizeof(u4);
             if (bottom < self->interpStackEnd) {
                 /* stack overflow */
-                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p size=%d '%s')\n",
+                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p(%d) size=%d '%s')\n",
                     self->interpStackStart, self->interpStackEnd, bottom,
-                    self->interpStackSize, methodToCall->name);
-                dvmHandleStackOverflow(self);
+                    (u1*) fp - bottom, self->interpStackSize,
+                    methodToCall->name);
+                dvmHandleStackOverflow(self, methodToCall);
                 assert(dvmCheckException(self));
                 GOTO_exceptionThrown();
             }
@@ -885,6 +907,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             ILOGD("> native <-- %s.%s %s", methodToCall->clazz->descriptor,
                 methodToCall->name, methodToCall->shorty);
 
+#if defined(WITH_JIT)
+            /* Allow the Jit to end any pending trace building */
+            CHECK_JIT();
+#endif
+
             /*
              * Jump through native call bridge.  Because we leave no
              * space for locals on native calls, "newFp" points directly
diff --git a/vm/mterp/c/header.c b/vm/mterp/c/header.c
index 174c226..5e1c5a7 100644
--- a/vm/mterp/c/header.c
+++ b/vm/mterp/c/header.c
@@ -295,6 +295,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -331,8 +336,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
diff --git a/vm/mterp/common/asm-constants.h b/vm/mterp/common/asm-constants.h
index a69247e..135abfd 100644
--- a/vm/mterp/common/asm-constants.h
+++ b/vm/mterp/common/asm-constants.h
@@ -104,8 +104,10 @@ MTERP_OFFSET(offGlue_entryPoint,        MterpGlue, entryPoint, 48)
 #if defined(WITH_JIT)
 MTERP_OFFSET(offGlue_pJitProfTable,     MterpGlue, pJitProfTable, 56)
 MTERP_OFFSET(offGlue_jitState,          MterpGlue, jitState, 60)
-MTERP_OFFSET(offGlue_jitResume,         MterpGlue, jitResume, 64)
-MTERP_OFFSET(offGlue_jitResumePC,       MterpGlue, jitResumePC, 68)
+MTERP_OFFSET(offGlue_jitResumeNPC,      MterpGlue, jitResumeNPC, 64)
+MTERP_OFFSET(offGlue_jitResumeDPC,      MterpGlue, jitResumeDPC, 68)
+MTERP_OFFSET(offGlue_jitThreshold,      MterpGlue, jitThreshold, 72)
+MTERP_OFFSET(offGlue_ppJitProfTable,    MterpGlue, ppJitProfTable, 76)
 #endif
 #elif defined(WITH_DEBUGGER)
 MTERP_OFFSET(offGlue_pDebuggerActive,   MterpGlue, pDebuggerActive, 40)
@@ -113,8 +115,10 @@ MTERP_OFFSET(offGlue_entryPoint,        MterpGlue, entryPoint, 44)
 #if defined(WITH_JIT)
 MTERP_OFFSET(offGlue_pJitProfTable,     MterpGlue, pJitProfTable, 52)
 MTERP_OFFSET(offGlue_jitState,          MterpGlue, jitState, 56)
-MTERP_OFFSET(offGlue_jitResume,         MterpGlue, jitResume, 60)
-MTERP_OFFSET(offGlue_jitResumePC,       MterpGlue, jitResumePC, 64)
+MTERP_OFFSET(offGlue_jitResumeNPC,      MterpGlue, jitResumeNPC, 60)
+MTERP_OFFSET(offGlue_jitResumeDPC,      MterpGlue, jitResumeDPC, 64)
+MTERP_OFFSET(offGlue_jitThreshold,      MterpGlue, jitThreshold, 68)
+MTERP_OFFSET(offGlue_ppJitProfTable,    MterpGlue, ppJitProfTable, 72)
 #endif
 #elif defined(WITH_PROFILER)
 MTERP_OFFSET(offGlue_pActiveProfilers,  MterpGlue, pActiveProfilers, 40)
@@ -122,16 +126,20 @@ MTERP_OFFSET(offGlue_entryPoint,        MterpGlue, entryPoint, 44)
 #if defined(WITH_JIT)
 MTERP_OFFSET(offGlue_pJitProfTable,     MterpGlue, pJitProfTable, 52)
 MTERP_OFFSET(offGlue_jitState,          MterpGlue, jitState, 56)
-MTERP_OFFSET(offGlue_jitResume,         MterpGlue, jitResume, 60)
-MTERP_OFFSET(offGlue_jitResumePC,       MterpGlue, jitResumePC, 64)
+MTERP_OFFSET(offGlue_jitResumeNPC,      MterpGlue, jitResumeNPC, 60)
+MTERP_OFFSET(offGlue_jitResumeDPC,      MterpGlue, jitResumeDPC, 64)
+MTERP_OFFSET(offGlue_jitThreshold,      MterpGlue, jitThreshold, 68)
+MTERP_OFFSET(offGlue_ppJitProfTable,    MterpGlue, ppJitProfTable, 72)
 #endif
 #else
 MTERP_OFFSET(offGlue_entryPoint,        MterpGlue, entryPoint, 40)
 #if defined(WITH_JIT)
 MTERP_OFFSET(offGlue_pJitProfTable,     MterpGlue, pJitProfTable, 48)
 MTERP_OFFSET(offGlue_jitState,          MterpGlue, jitState, 52)
-MTERP_OFFSET(offGlue_jitResume,         MterpGlue, jitResume, 56)
-MTERP_OFFSET(offGlue_jitResumePC,       MterpGlue, jitResumePC, 60)
+MTERP_OFFSET(offGlue_jitResumeNPC,      MterpGlue, jitResumeNPC, 56)
+MTERP_OFFSET(offGlue_jitResumeDPC,      MterpGlue, jitResumeDPC, 60)
+MTERP_OFFSET(offGlue_jitThreshold,      MterpGlue, jitThreshold, 64)
+MTERP_OFFSET(offGlue_ppJitProfTable,    MterpGlue, ppJitProfTable, 68)
 #endif
 #endif
 /* make sure all JValue union members are stored at the same offset */
@@ -169,6 +177,16 @@ MTERP_OFFSET(offStackSaveArea_returnAddr, StackSaveArea, returnAddr, 16)
 MTERP_SIZEOF(sizeofStackSaveArea,       StackSaveArea, 20)
 #endif
 
+  /* ShadowSpace fields */
+#if defined(WITH_JIT) && defined(WITH_SELF_VERIFICATION)
+MTERP_OFFSET(offShadowSpace_startPC, ShadowSpace, startPC, 0)
+MTERP_OFFSET(offShadowSpace_fp, ShadowSpace, fp, 4)
+MTERP_OFFSET(offShadowSpace_glue, ShadowSpace, glue, 8)
+MTERP_OFFSET(offShadowSpace_svState, ShadowSpace, selfVerificationState, 12)
+MTERP_OFFSET(offShadowSpace_shadowFP, ShadowSpace, shadowFP, 20)
+MTERP_OFFSET(offShadowSpace_interpState, ShadowSpace, interpState, 24)
+#endif
+
 /* InstField fields */
 #ifdef PROFILE_FIELD_ACCESS
 MTERP_OFFSET(offInstField_byteOffset,   InstField, byteOffset, 24)
@@ -200,6 +218,17 @@ MTERP_OFFSET(offInlineOperation_func,   InlineOperation, func, 0)
 MTERP_OFFSET(offThread_stackOverflowed, Thread, stackOverflowed, 40)
 MTERP_OFFSET(offThread_curFrame,        Thread, curFrame, 44)
 MTERP_OFFSET(offThread_exception,       Thread, exception, 48)
+
+#if defined(WITH_JIT)
+MTERP_OFFSET(offThread_inJitCodeCache,  Thread, inJitCodeCache, 76)
+#ifdef USE_INDIRECT_REF
+MTERP_OFFSET(offThread_jniLocal_topCookie, \
+                                Thread, jniLocalRefTable.segmentState.all, 80)
+#else
+MTERP_OFFSET(offThread_jniLocal_topCookie, \
+                                Thread, jniLocalRefTable.nextEntry, 80)
+#endif
+#else
 #ifdef USE_INDIRECT_REF
 MTERP_OFFSET(offThread_jniLocal_topCookie, \
                                 Thread, jniLocalRefTable.segmentState.all, 76)
@@ -207,9 +236,15 @@ MTERP_OFFSET(offThread_jniLocal_topCookie, \
 MTERP_OFFSET(offThread_jniLocal_topCookie, \
                                 Thread, jniLocalRefTable.nextEntry, 76)
 #endif
+#endif
 
 /* Object fields */
 MTERP_OFFSET(offObject_clazz,           Object, clazz, 0)
+MTERP_OFFSET(offObject_lock,            Object, lock, 4)
+
+/* Lock shape */
+MTERP_CONSTANT(LW_LOCK_OWNER_SHIFT, 3)
+MTERP_CONSTANT(LW_HASH_STATE_SHIFT, 1)
 
 /* ArrayObject fields */
 MTERP_OFFSET(offArrayObject_length,     ArrayObject, length, 8)
@@ -219,14 +254,34 @@ MTERP_OFFSET(offArrayObject_contents,   ArrayObject, contents, 16)
 MTERP_OFFSET(offArrayObject_contents,   ArrayObject, contents, 12)
 #endif
 
+/* String fields */
+MTERP_CONSTANT(STRING_FIELDOFF_VALUE,     8)
+MTERP_CONSTANT(STRING_FIELDOFF_HASHCODE, 12)
+MTERP_CONSTANT(STRING_FIELDOFF_OFFSET,   16)
+MTERP_CONSTANT(STRING_FIELDOFF_COUNT,    20)
+
+#if defined(WITH_JIT)
+/*
+ * Reasons for the non-chaining interpreter entry points
+ * Enums defined in vm/Globals.h
+ */
+MTERP_CONSTANT(kInlineCacheMiss,        0)
+MTERP_CONSTANT(kCallsiteInterpreted,    1)
+MTERP_CONSTANT(kSwitchOverflow,         2)
+MTERP_CONSTANT(kHeavyweightMonitor,     3)
+
+/* Size of callee save area */
+MTERP_CONSTANT(JIT_CALLEE_SAVE_DOUBLE_COUNT,   8)
+#endif
+
 /* ClassObject fields */
 MTERP_OFFSET(offClassObject_descriptor, ClassObject, descriptor, 24)
 MTERP_OFFSET(offClassObject_accessFlags, ClassObject, accessFlags, 32)
 MTERP_OFFSET(offClassObject_pDvmDex,    ClassObject, pDvmDex, 40)
 MTERP_OFFSET(offClassObject_status,     ClassObject, status, 44)
-MTERP_OFFSET(offClassObject_super,      ClassObject, super, 76)
-MTERP_OFFSET(offClassObject_vtableCount, ClassObject, vtableCount, 116)
-MTERP_OFFSET(offClassObject_vtable,     ClassObject, vtable, 120)
+MTERP_OFFSET(offClassObject_super,      ClassObject, super, 72)
+MTERP_OFFSET(offClassObject_vtableCount, ClassObject, vtableCount, 112)
+MTERP_OFFSET(offClassObject_vtable,     ClassObject, vtable, 116)
 
 /* InterpEntry enumeration */
 MTERP_SIZEOF(sizeofClassStatus,         InterpEntry, MTERP_SMALL_ENUM)
@@ -238,14 +293,28 @@ MTERP_CONSTANT(kInterpEntryResume,  3)
 #endif
 
 #if defined(WITH_JIT)
-MTERP_CONSTANT(kJitOff,             0)
-MTERP_CONSTANT(kJitNormal,          1)
-MTERP_CONSTANT(kJitTSelectRequest,  2)
-MTERP_CONSTANT(kJitTSelect,         3)
-MTERP_CONSTANT(kJitTSelectAbort,    4)
-MTERP_CONSTANT(kJitTSelectEnd,      5)
-MTERP_CONSTANT(kJitSingleStep,      6)
-MTERP_CONSTANT(kJitSingleStepEnd,   7)
+MTERP_CONSTANT(kJitNot,                 0)
+MTERP_CONSTANT(kJitTSelectRequest,      1)
+MTERP_CONSTANT(kJitTSelectRequestHot,   2)
+MTERP_CONSTANT(kJitSelfVerification,    3)
+MTERP_CONSTANT(kJitTSelect,             4)
+MTERP_CONSTANT(kJitTSelectEnd,          5)
+MTERP_CONSTANT(kJitSingleStep,          6)
+MTERP_CONSTANT(kJitSingleStepEnd,       7)
+MTERP_CONSTANT(kJitDone,                8)
+
+#if defined(WITH_SELF_VERIFICATION)
+MTERP_CONSTANT(kSVSIdle, 0)
+MTERP_CONSTANT(kSVSStart, 1)
+MTERP_CONSTANT(kSVSPunt, 2)
+MTERP_CONSTANT(kSVSSingleStep, 3)
+MTERP_CONSTANT(kSVSTraceSelectNoChain, 4)
+MTERP_CONSTANT(kSVSTraceSelect, 5)
+MTERP_CONSTANT(kSVSNormal, 6)
+MTERP_CONSTANT(kSVSNoChain, 7)
+MTERP_CONSTANT(kSVSBackwardBranch, 8)
+MTERP_CONSTANT(kSVSDebugInterp, 9)
+#endif
 #endif
 
 /* ClassStatus enumeration */
diff --git a/vm/mterp/config-x86 b/vm/mterp/config-x86
index b7139ca..cf5adb5 100644
--- a/vm/mterp/config-x86
+++ b/vm/mterp/config-x86
@@ -33,6 +33,8 @@ import c/opcommon.c
 
 # opcode list; argument to op-start is default directory
 op-start x86
+    # stub -- need native impl
+    op OP_EXECUTE_INLINE_RANGE c
 op-end
 
 # arch-specific entry point to interpreter
diff --git a/vm/mterp/cstubs/stubdefs.c b/vm/mterp/cstubs/stubdefs.c
index 8699f05..ba10853 100644
--- a/vm/mterp/cstubs/stubdefs.c
+++ b/vm/mterp/cstubs/stubdefs.c
@@ -2,6 +2,10 @@
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -119,4 +123,3 @@
             GOTO_bail_switch();                                             \
         }                                                                   \
     }
-
diff --git a/vm/mterp/gen-mterp.py b/vm/mterp/gen-mterp.py
index e685350..e0d854d 100755
--- a/vm/mterp/gen-mterp.py
+++ b/vm/mterp/gen-mterp.py
@@ -33,7 +33,6 @@ opcode_locations = {}
 asm_stub_text = []
 label_prefix = ".L"         # use ".L" to hide labels from gdb
 
-
 # Exception class.
 class DataParseError(SyntaxError):
     "Failure when parsing data file"
diff --git a/vm/mterp/out/InterpAsm-armv4t.S b/vm/mterp/out/InterpAsm-armv4t.S
index 2df532f..5909745 100644
--- a/vm/mterp/out/InterpAsm-armv4t.S
+++ b/vm/mterp/out/InterpAsm-armv4t.S
@@ -185,8 +185,8 @@ unspecified registers or condition codes.
 #define SET_VREG(_reg, _vreg)   str     _reg, [rFP, _vreg, lsl #2]
 
 #if defined(WITH_JIT)
-#define GET_JIT_ENABLED(_reg)       ldr     _reg,[rGLUE,#offGlue_jitEnabled]
 #define GET_JIT_PROF_TABLE(_reg)    ldr     _reg,[rGLUE,#offGlue_pJitProfTable]
+#define GET_JIT_THRESHOLD(_reg)     ldr     _reg,[rGLUE,#offGlue_jitThreshold]
 #endif
 
 /*
@@ -201,6 +201,9 @@ unspecified registers or condition codes.
  */
 #include "../common/asm-constants.h"
 
+#if defined(WITH_JIT)
+#include "../common/jit-config.h"
+#endif
 
 /* File: armv5te/platform.S */
 /*
@@ -303,17 +306,20 @@ dvmMterpStdRun:
 
     /* set up "named" registers, figure out entry point */
     mov     rGLUE, r0                   @ set rGLUE
-    ldrb    r1, [r0, #offGlue_entryPoint]   @ InterpEntry enum is char
+    ldr     r1, [r0, #offGlue_entryPoint]   @ enum is 4 bytes in aapcs-EABI
     LOAD_PC_FP_FROM_GLUE()              @ load rPC and rFP from "glue"
     adr     rIBASE, dvmAsmInstructionStart  @ set rIBASE
     cmp     r1, #kInterpEntryInstr      @ usual case?
     bne     .Lnot_instr                 @ no, handle it
 
 #if defined(WITH_JIT)
-.Lno_singleStep:
+.LentryInstr:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     /* Entry is always a possible trace start */
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
+    mov    r1, #0                       @ prepare the value for the new state
+    str    r1, [r10, #offThread_inJitCodeCache] @ back to the interp land
     cmp    r0,#0
     bne    common_updateProfile
     GET_INST_OPCODE(ip)
@@ -335,18 +341,21 @@ dvmMterpStdRun:
 
 #if defined(WITH_JIT)
 .Lnot_throw:
-    ldr     r0,[rGLUE, #offGlue_jitResume]
-    ldr     r2,[rGLUE, #offGlue_jitResumePC]
+    ldr     r10,[rGLUE, #offGlue_jitResumeNPC]
+    ldr     r2,[rGLUE, #offGlue_jitResumeDPC]
     cmp     r1, #kInterpEntryResume     @ resuming after Jit single-step?
     bne     .Lbad_arg
     cmp     rPC,r2
-    bne     .Lno_singleStep             @ must have branched, don't resume
+    bne     .LentryInstr                @ must have branched, don't resume
+#if defined(WITH_SELF_VERIFICATION)
+    @ glue->entryPoint will be set in dvmSelfVerificationSaveState
+    b       jitSVShadowRunStart         @ re-enter the translation after the
+                                        @ single-stepped instruction
+    @noreturn
+#endif
     mov     r1, #kInterpEntryInstr
-    strb    r1, [rGLUE, #offGlue_entryPoint]
-    ldr     rINST, .LdvmCompilerTemplate
-    bx      r0                          @ re-enter the translation
-.LdvmCompilerTemplate:
-    .word   dvmCompilerTemplateStart
+    str     r1, [rGLUE, #offGlue_entryPoint]
+    bx      r10                         @ re-enter the translation
 #endif
 
 .Lbad_arg:
@@ -902,14 +911,17 @@ dalvik_inst:
     EXPORT_PC()                         @ before fetch: export the PC
     GET_VREG(r1, r2)                    @ r1<- vAA (object)
     cmp     r1, #0                      @ null object?
-    beq     common_errNullObject        @ yes
+    beq     1f                          @ yes
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
     bl      dvmUnlockObject             @ r0<- success for unlock(self, obj)
     cmp     r0, #0                      @ failed?
-    beq     common_exceptionThrown      @ yes, exception is pending
     FETCH_ADVANCE_INST(1)               @ before throw: advance rPC, load rINST
+    beq     common_exceptionThrown      @ yes, exception is pending
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
+1:
+    FETCH_ADVANCE_INST(1)               @ advance before throw
+    b      common_errNullObject
 
 
 /* ------------------------------ */
@@ -7617,8 +7629,8 @@ d2i_doconv:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EC: /* 0xec */
-/* File: armv5te/OP_UNUSED_EC.S */
+.L_OP_BREAKPOINT: /* 0xec */
+/* File: armv5te/OP_BREAKPOINT.S */
 /* File: armv5te/unused.S */
     bl      common_abort
 
@@ -7649,17 +7661,18 @@ d2i_doconv:
     /*
      * Execute a "native inline" instruction.
      *
-     * We need to call:
-     *  dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref)
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
      *
-     * The first four args are in r0-r3, but the last two must be pushed
-     * onto the stack.
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
      */
     /* [opt] execute-inline vAA, {vC, vD, vE, vF}, inline@BBBB */
     FETCH(r10, 1)                       @ r10<- BBBB
     add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
     EXPORT_PC()                         @ can throw
-    sub     sp, sp, #8                  @ make room for arg(s)
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
     mov     r0, rINST, lsr #12          @ r0<- B
     str     r1, [sp]                    @ push &glue->retval
     bl      .LOP_EXECUTE_INLINE_continue        @ make call; will return after
@@ -7672,12 +7685,33 @@ d2i_doconv:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EF: /* 0xef */
-/* File: armv5te/OP_UNUSED_EF.S */
-/* File: armv5te/unused.S */
-    bl      common_abort
-
-
+.L_OP_EXECUTE_INLINE_RANGE: /* 0xef */
+/* File: armv5te/OP_EXECUTE_INLINE_RANGE.S */
+    /*
+     * Execute a "native inline" instruction, using "/range" semantics.
+     * Same idea as execute-inline, but we get the args differently.
+     *
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
+     *
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
+     */
+    /* [opt] execute-inline/range {vCCCC..v(CCCC+AA-1)}, inline@BBBB */
+    FETCH(r10, 1)                       @ r10<- BBBB
+    add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
+    EXPORT_PC()                         @ can throw
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
+    mov     r0, rINST, lsr #8           @ r0<- AA
+    str     r1, [sp]                    @ push &glue->retval
+    bl      .LOP_EXECUTE_INLINE_RANGE_continue        @ make call; will return after
+    add     sp, sp, #8                  @ pop stack
+    cmp     r0, #0                      @ test boolean result of inline
+    beq     common_exceptionThrown      @ returned false, handle exception
+    FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
+    GET_INST_OPCODE(ip)                 @ extract opcode from rINST
+    GOTO_OPCODE(ip)                     @ jump to next instruction
 
 /* ------------------------------ */
     .balign 64
@@ -9465,6 +9499,36 @@ d2l_doconv:
     .word   gDvmInlineOpsTable
 
 
+/* continuation for OP_EXECUTE_INLINE_RANGE */
+
+    /*
+     * Extract args, call function.
+     *  r0 = #of args (0-4)
+     *  r10 = call index
+     *  lr = return addr, above  [DO NOT bl out of here w/o preserving LR]
+     */
+.LOP_EXECUTE_INLINE_RANGE_continue:
+    rsb     r0, r0, #4                  @ r0<- 4-r0
+    FETCH(r9, 2)                        @ r9<- CCCC
+    add     pc, pc, r0, lsl #3          @ computed goto, 2 instrs each
+    bl      common_abort                @ (skipped due to ARM prefetch)
+4:  add     ip, r9, #3                  @ base+3
+    GET_VREG(r3, ip)                    @ r3<- vBase[3]
+3:  add     ip, r9, #2                  @ base+2
+    GET_VREG(r2, ip)                    @ r2<- vBase[2]
+2:  add     ip, r9, #1                  @ base+1
+    GET_VREG(r1, ip)                    @ r1<- vBase[1]
+1:  add     ip, r9, #0                  @ (nop)
+    GET_VREG(r0, ip)                    @ r0<- vBase[0]
+0:
+    ldr     r9, .LOP_EXECUTE_INLINE_RANGE_table       @ table of InlineOperation
+    LDR_PC  "[r9, r10, lsl #4]"         @ sizeof=16, "func" is first entry
+    @ (not reached)
+
+.LOP_EXECUTE_INLINE_RANGE_table:
+    .word   gDvmInlineOpsTable
+
+
     .size   dvmAsmSisterStart, .-dvmAsmSisterStart
     .global dvmAsmSisterEnd
 dvmAsmSisterEnd:
@@ -9483,6 +9547,67 @@ dvmAsmSisterEnd:
     .align  2
 
 #if defined(WITH_JIT)
+#if defined(WITH_SELF_VERIFICATION)
+    .global dvmJitToInterpPunt
+dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r2,#kSVSPunt                 @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpSingleStep
+dvmJitToInterpSingleStep:
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
+    mov    r2,#kSVSSingleStep           @ r2<- interpreter entry point
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSTraceSelectNoChain   @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSTraceSelect          @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpBackwardBranch
+dvmJitToInterpBackwardBranch:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSBackwardBranch       @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNormal
+dvmJitToInterpNormal:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSNormal               @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNoChain
+dvmJitToInterpNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSNoChain              @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+#else
 /*
  * Return from the translation cache to the interpreter when the compiler is
  * having issues translating/executing a Dalvik instruction. We have to skip
@@ -9492,12 +9617,15 @@ dvmAsmSisterEnd:
  */
     .global dvmJitToInterpPunt
 dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    rPC, r0
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     mov    r0,lr
     bl     dvmBumpPunt;
 #endif
     EXPORT_PC()
+    mov    r0, #0
+    str    r0, [r10, #offThread_inJitCodeCache] @ Back to the interp land
     adrl   rIBASE, dvmAsmInstructionStart
     FETCH_INST()
     GET_INST_OPCODE(ip)
@@ -9512,35 +9640,58 @@ dvmJitToInterpPunt:
  */
     .global dvmJitToInterpSingleStep
 dvmJitToInterpSingleStep:
-    str    lr,[rGLUE,#offGlue_jitResume]
-    str    r1,[rGLUE,#offGlue_jitResumePC]
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
     mov    r1,#kInterpEntryInstr
     @ enum is 4 byte in aapcs-EABI
     str    r1, [rGLUE, #offGlue_entryPoint]
     mov    rPC,r0
     EXPORT_PC()
+
     adrl   rIBASE, dvmAsmInstructionStart
     mov    r2,#kJitSingleStep     @ Ask for single step and then revert
     str    r2,[rGLUE,#offGlue_jitState]
     mov    r1,#1                  @ set changeInterp to bail to debug interp
     b      common_gotoBail
 
+/*
+ * Return from the translation cache and immediately request
+ * a translation for the exit target.  Commonly used for callees.
+ */
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+#ifdef JIT_STATS
+    bl     dvmBumpNoChain
+#endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC
+    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
+    cmp    r0,#0
+    bxne   r0                       @ continue native execution if so
+    b      2f
 
 /*
  * Return from the translation cache and immediately request
  * a translation for the exit target.  Commonly used following
  * invokes.
  */
-    .global dvmJitToTraceSelect
-dvmJitToTraceSelect:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
     mov    r0,rPC
-    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    bl     dvmJitGetCodeAddr       @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    2f
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @ in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9551,6 +9702,7 @@ dvmJitToTraceSelect:
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
     cmp    r0, #0
+    movne  r2,#kJitTSelectRequestHot   @ ask for trace selection
     bne    common_selectTrace
     GET_INST_OPCODE(ip)
     GOTO_OPCODE(ip)
@@ -9571,17 +9723,21 @@ dvmJitToTraceSelect:
  */
     .global dvmJitToInterpNormal
 dvmJitToInterpNormal:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
-#ifdef EXIT_STATS
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
+#ifdef JIT_STATS
     bl     dvmBumpNormal
 #endif
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    toInterpreter            @ go if not, otherwise do chain
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9592,13 +9748,18 @@ dvmJitToInterpNormal:
  */
     .global dvmJitToInterpNoChain
 dvmJitToInterpNoChain:
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     bl     dvmBumpNoChain
 #endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0
     bxne   r0                       @ continue native execution if so
+#endif
 
 /*
  * No translation, restore interpreter regs and start interpreting.
@@ -9626,11 +9787,11 @@ common_testUpdateProfile:
 
 common_updateProfile:
     eor     r3,rPC,rPC,lsr #12 @ cheap, but fast hash function
-    lsl     r3,r3,#23          @ shift out excess 511
-    ldrb    r1,[r0,r3,lsr #23] @ get counter
+    lsl     r3,r3,#(32 - JIT_PROF_SIZE_LOG_2)          @ shift out excess bits
+    ldrb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ get counter
     GET_INST_OPCODE(ip)
     subs    r1,r1,#1           @ decrement counter
-    strb    r1,[r0,r3,lsr #23] @ and store it
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ and store it
     GOTO_OPCODE_IFNE(ip)       @ if not threshold, fallthrough otherwise */
 
 /*
@@ -9639,20 +9800,94 @@ common_updateProfile:
  * is already a native translation in place (and, if so,
  * jump to it now).
  */
-    mov     r1,#255
-    strb    r1,[r0,r3,lsr #23] @ reset counter
+    GET_JIT_THRESHOLD(r1)
+    ldr     r10, [rGLUE, #offGlue_self] @ callee saved r10 <- glue->self
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ reset counter
     EXPORT_PC()
     mov     r0,rPC
     bl      dvmJitGetCodeAddr           @ r0<- dvmJitGetCodeAddr(rPC)
+    str     r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov     r1, rPC                     @ arg1 of translation may need this
+    mov     lr, #0                      @  in case target is HANDLER_INTERPRET
     cmp     r0,#0
-    beq     common_selectTrace
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    r0                          @ jump to the translation
-common_selectTrace:
     mov     r2,#kJitTSelectRequest      @ ask for trace selection
+    @ fall-through to common_selectTrace
+#else
+    moveq   r2,#kJitTSelectRequest      @ ask for trace selection
+    beq     common_selectTrace
+    /*
+     * At this point, we have a target translation.  However, if
+     * that translation is actually the interpret-only pseudo-translation
+     * we want to treat it the same as no translation.
+     */
+    mov     r10, r0                     @ save target
+    bl      dvmCompilerGetInterpretTemplate
+    cmp     r0, r10                     @ special case?
+    bne     jitSVShadowRunStart         @ set up self verification shadow space
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+    /* no return */
+#endif
+
+/*
+ * On entry:
+ *  r2 is jit state, e.g. kJitTSelectRequest or kJitTSelectRequestHot
+ */
+common_selectTrace:
     str     r2,[rGLUE,#offGlue_jitState]
+    mov     r2,#kInterpEntryInstr       @ normal entry reason
+    str     r2,[rGLUE,#offGlue_entryPoint]
     mov     r1,#1                       @ set changeInterp
     b       common_gotoBail
 
+#if defined(WITH_SELF_VERIFICATION)
+/*
+ * Save PC and registers to shadow memory for self verification mode
+ * before jumping to native translation.
+ * On entry:
+ *    rPC, rFP, rGLUE: the values that they should contain
+ *    r10: the address of the target translation.
+ */
+jitSVShadowRunStart:
+    mov     r0,rPC                      @ r0<- program counter
+    mov     r1,rFP                      @ r1<- frame pointer
+    mov     r2,rGLUE                    @ r2<- InterpState pointer
+    mov     r3,r10                      @ r3<- target translation
+    bl      dvmSelfVerificationSaveState @ save registers to shadow space
+    ldr     rFP,[r0,#offShadowSpace_shadowFP] @ rFP<- fp in shadow space
+    add     rGLUE,r0,#offShadowSpace_interpState @ rGLUE<- rGLUE in shadow space
+    bx      r10                         @ jump to the translation
+
+/*
+ * Restore PC, registers, and interpState to original values
+ * before jumping back to the interpreter.
+ */
+jitSVShadowRunEnd:
+    mov    r1,rFP                        @ pass ending fp
+    bl     dvmSelfVerificationRestoreState @ restore pc and fp values
+    ldr    rPC,[r0,#offShadowSpace_startPC] @ restore PC
+    ldr    rFP,[r0,#offShadowSpace_fp]   @ restore FP
+    ldr    rGLUE,[r0,#offShadowSpace_glue] @ restore InterpState
+    ldr    r1,[r0,#offShadowSpace_svState] @ get self verification state
+    cmp    r1,#0                         @ check for punt condition
+    beq    1f
+    mov    r2,#kJitSelfVerification      @ ask for self verification
+    str    r2,[rGLUE,#offGlue_jitState]
+    mov    r2,#kInterpEntryInstr         @ normal entry reason
+    str    r2,[rGLUE,#offGlue_entryPoint]
+    mov    r1,#1                         @ set changeInterp
+    b      common_gotoBail
+
+1:                                       @ exit to interpreter without check
+    EXPORT_PC()
+    adrl   rIBASE, dvmAsmInstructionStart
+    FETCH_INST()
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+#endif
+
 #endif
 
 /*
@@ -9694,6 +9929,9 @@ common_backwardBranch:
 common_periodicChecks:
     ldr     r3, [rGLUE, #offGlue_pSelfSuspendCount] @ r3<- &suspendCount
 
+    @ speculatively store r0 before it is clobbered by dvmCheckSuspendPending
+    str     r0, [rGLUE, #offGlue_entryPoint]
+
 #if defined(WITH_DEBUGGER)
     ldr     r1, [rGLUE, #offGlue_pDebuggerActive]   @ r1<- &debuggerActive
 #endif
@@ -9728,13 +9966,24 @@ common_periodicChecks:
     bx      lr                          @ nothing to do, return
 
 2:  @ check suspend
+#if defined(WITH_JIT)
+    /*
+     * Refresh the Jit's cached copy of profile table pointer.  This pointer
+     * doubles as the Jit's on/off switch.
+     */
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ r3<-&gDvmJit.pJitProfTable
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    ldr     r3, [r3] @ r3 <- pJitProfTable
     EXPORT_PC()                         @ need for precise GC
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh Jit's on/off switch
+#else
+    ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    EXPORT_PC()                         @ need for precise GC
+#endif
     b       dvmCheckSuspendPending      @ suspend if necessary, then return
 
 3:  @ debugger/profiler enabled, bail out
     add     rPC, rPC, r9                @ update rPC
-    str     r0, [rGLUE, #offGlue_entryPoint]
     mov     r1, #1                      @ "want switch" = true
     b       common_gotoBail
 
@@ -9926,20 +10175,31 @@ dalvik_mterp:
     @ldr     pc, [r2, #offMethod_nativeFunc] @ pc<- methodToCall->nativeFunc
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+#if defined(WITH_JIT)
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ Refresh Jit's on/off status
+#endif
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+#if defined(WITH_JIT)
+    ldr     r3, [r3]                    @ r3 <- gDvmJit.pProfTable
+#endif
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
+#if defined(WITH_JIT)
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh cached on/off switch
+#endif
     bne     common_exceptionThrown      @ no, handle exception
 
     FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    @ r0=methodToCall
+    mov     r1, r0                      @ r1<- methodToCall
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- self
     bl      dvmHandleStackOverflow
     b       common_exceptionThrown
@@ -10003,12 +10263,13 @@ common_returnFromMethod:
     ldr     r1, [r10, #offClassObject_pDvmDex]   @ r1<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame]  @ self->curFrame = fp
 #if defined(WITH_JIT)
-    ldr     r3, [r0, #offStackSaveArea_returnAddr] @ r3 = saveArea->returnAddr
+    ldr     r10, [r0, #offStackSaveArea_returnAddr] @ r10 = saveArea->returnAddr
     GET_JIT_PROF_TABLE(r0)
     mov     rPC, r9                     @ publish new rPC
     str     r1, [rGLUE, #offGlue_methodClassDex]
-    cmp     r3, #0                      @ caller is compiled code
-    blxne   r3
+    str     r10, [r3, #offThread_inJitCodeCache]  @ may return to JIT'ed land
+    cmp     r10, #0                      @ caller is compiled code
+    blxne   r10
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     cmp     r0,#0
     bne     common_updateProfile
@@ -10049,11 +10310,6 @@ common_exceptionThrown:
     mov     r9, #0
     bl      common_periodicChecks
 
-#if defined(WITH_JIT)
-    mov     r2,#kJitTSelectAbort        @ abandon trace selection in progress
-    str     r2,[rGLUE,#offGlue_jitState]
-#endif
-
     ldr     r10, [rGLUE, #offGlue_self] @ r10<- glue->self
     ldr     r9, [r10, #offThread_exception] @ r9<- self->exception
     mov     r1, r10                     @ r1<- self
@@ -10084,6 +10340,7 @@ common_exceptionThrown:
     beq     1f                          @ no, skip ahead
     mov     rFP, r0                     @ save relPc result in rFP
     mov     r0, r10                     @ r0<- self
+    mov     r1, r9                      @ r1<- exception
     bl      dvmCleanupStackOverflow     @ call(self)
     mov     r0, rFP                     @ restore result
 1:
@@ -10121,6 +10378,7 @@ common_exceptionThrown:
     ldrb    r1, [r10, #offThread_stackOverflowed]
     cmp     r1, #0                      @ did we overflow earlier?
     movne   r0, r10                     @ if yes: r0<- self
+    movne   r1, r9                      @ if yes: r1<- exception
     blne    dvmCleanupStackOverflow     @ if yes: call(self)
 
     @ may want to show "not caught locally" debug messages here
diff --git a/vm/mterp/out/InterpAsm-armv5te-vfp.S b/vm/mterp/out/InterpAsm-armv5te-vfp.S
index 61b2697..8244371 100644
--- a/vm/mterp/out/InterpAsm-armv5te-vfp.S
+++ b/vm/mterp/out/InterpAsm-armv5te-vfp.S
@@ -185,8 +185,8 @@ unspecified registers or condition codes.
 #define SET_VREG(_reg, _vreg)   str     _reg, [rFP, _vreg, lsl #2]
 
 #if defined(WITH_JIT)
-#define GET_JIT_ENABLED(_reg)       ldr     _reg,[rGLUE,#offGlue_jitEnabled]
 #define GET_JIT_PROF_TABLE(_reg)    ldr     _reg,[rGLUE,#offGlue_pJitProfTable]
+#define GET_JIT_THRESHOLD(_reg)     ldr     _reg,[rGLUE,#offGlue_jitThreshold]
 #endif
 
 /*
@@ -201,6 +201,9 @@ unspecified registers or condition codes.
  */
 #include "../common/asm-constants.h"
 
+#if defined(WITH_JIT)
+#include "../common/jit-config.h"
+#endif
 
 /* File: armv5te/platform.S */
 /*
@@ -303,17 +306,20 @@ dvmMterpStdRun:
 
     /* set up "named" registers, figure out entry point */
     mov     rGLUE, r0                   @ set rGLUE
-    ldrb    r1, [r0, #offGlue_entryPoint]   @ InterpEntry enum is char
+    ldr     r1, [r0, #offGlue_entryPoint]   @ enum is 4 bytes in aapcs-EABI
     LOAD_PC_FP_FROM_GLUE()              @ load rPC and rFP from "glue"
     adr     rIBASE, dvmAsmInstructionStart  @ set rIBASE
     cmp     r1, #kInterpEntryInstr      @ usual case?
     bne     .Lnot_instr                 @ no, handle it
 
 #if defined(WITH_JIT)
-.Lno_singleStep:
+.LentryInstr:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     /* Entry is always a possible trace start */
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
+    mov    r1, #0                       @ prepare the value for the new state
+    str    r1, [r10, #offThread_inJitCodeCache] @ back to the interp land
     cmp    r0,#0
     bne    common_updateProfile
     GET_INST_OPCODE(ip)
@@ -335,18 +341,21 @@ dvmMterpStdRun:
 
 #if defined(WITH_JIT)
 .Lnot_throw:
-    ldr     r0,[rGLUE, #offGlue_jitResume]
-    ldr     r2,[rGLUE, #offGlue_jitResumePC]
+    ldr     r10,[rGLUE, #offGlue_jitResumeNPC]
+    ldr     r2,[rGLUE, #offGlue_jitResumeDPC]
     cmp     r1, #kInterpEntryResume     @ resuming after Jit single-step?
     bne     .Lbad_arg
     cmp     rPC,r2
-    bne     .Lno_singleStep             @ must have branched, don't resume
+    bne     .LentryInstr                @ must have branched, don't resume
+#if defined(WITH_SELF_VERIFICATION)
+    @ glue->entryPoint will be set in dvmSelfVerificationSaveState
+    b       jitSVShadowRunStart         @ re-enter the translation after the
+                                        @ single-stepped instruction
+    @noreturn
+#endif
     mov     r1, #kInterpEntryInstr
-    strb    r1, [rGLUE, #offGlue_entryPoint]
-    ldr     rINST, .LdvmCompilerTemplate
-    bx      r0                          @ re-enter the translation
-.LdvmCompilerTemplate:
-    .word   dvmCompilerTemplateStart
+    str     r1, [rGLUE, #offGlue_entryPoint]
+    bx      r10                         @ re-enter the translation
 #endif
 
 .Lbad_arg:
@@ -902,14 +911,17 @@ dalvik_inst:
     EXPORT_PC()                         @ before fetch: export the PC
     GET_VREG(r1, r2)                    @ r1<- vAA (object)
     cmp     r1, #0                      @ null object?
-    beq     common_errNullObject        @ yes
+    beq     1f                          @ yes
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
     bl      dvmUnlockObject             @ r0<- success for unlock(self, obj)
     cmp     r0, #0                      @ failed?
-    beq     common_exceptionThrown      @ yes, exception is pending
     FETCH_ADVANCE_INST(1)               @ before throw: advance rPC, load rINST
+    beq     common_exceptionThrown      @ yes, exception is pending
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
+1:
+    FETCH_ADVANCE_INST(1)               @ advance before throw
+    b      common_errNullObject
 
 
 /* ------------------------------ */
@@ -7277,8 +7289,8 @@ dalvik_inst:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EC: /* 0xec */
-/* File: armv5te/OP_UNUSED_EC.S */
+.L_OP_BREAKPOINT: /* 0xec */
+/* File: armv5te/OP_BREAKPOINT.S */
 /* File: armv5te/unused.S */
     bl      common_abort
 
@@ -7309,17 +7321,18 @@ dalvik_inst:
     /*
      * Execute a "native inline" instruction.
      *
-     * We need to call:
-     *  dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref)
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
      *
-     * The first four args are in r0-r3, but the last two must be pushed
-     * onto the stack.
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
      */
     /* [opt] execute-inline vAA, {vC, vD, vE, vF}, inline@BBBB */
     FETCH(r10, 1)                       @ r10<- BBBB
     add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
     EXPORT_PC()                         @ can throw
-    sub     sp, sp, #8                  @ make room for arg(s)
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
     mov     r0, rINST, lsr #12          @ r0<- B
     str     r1, [sp]                    @ push &glue->retval
     bl      .LOP_EXECUTE_INLINE_continue        @ make call; will return after
@@ -7332,12 +7345,33 @@ dalvik_inst:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EF: /* 0xef */
-/* File: armv5te/OP_UNUSED_EF.S */
-/* File: armv5te/unused.S */
-    bl      common_abort
-
-
+.L_OP_EXECUTE_INLINE_RANGE: /* 0xef */
+/* File: armv5te/OP_EXECUTE_INLINE_RANGE.S */
+    /*
+     * Execute a "native inline" instruction, using "/range" semantics.
+     * Same idea as execute-inline, but we get the args differently.
+     *
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
+     *
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
+     */
+    /* [opt] execute-inline/range {vCCCC..v(CCCC+AA-1)}, inline@BBBB */
+    FETCH(r10, 1)                       @ r10<- BBBB
+    add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
+    EXPORT_PC()                         @ can throw
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
+    mov     r0, rINST, lsr #8           @ r0<- AA
+    str     r1, [sp]                    @ push &glue->retval
+    bl      .LOP_EXECUTE_INLINE_RANGE_continue        @ make call; will return after
+    add     sp, sp, #8                  @ pop stack
+    cmp     r0, #0                      @ test boolean result of inline
+    beq     common_exceptionThrown      @ returned false, handle exception
+    FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
+    GET_INST_OPCODE(ip)                 @ extract opcode from rINST
+    GOTO_OPCODE(ip)                     @ jump to next instruction
 
 /* ------------------------------ */
     .balign 64
@@ -7386,11 +7420,11 @@ dalvik_inst:
     /* iget-wide-quick vA, vB, offset@CCCC */
     mov     r2, rINST, lsr #12          @ r2<- B
     GET_VREG(r3, r2)                    @ r3<- object we're operating on
-    FETCH(r1, 1)                        @ r1<- field byte offset
+    FETCH(ip, 1)                        @ ip<- field byte offset
     cmp     r3, #0                      @ check object for null
     mov     r2, rINST, lsr #8           @ r2<- A(+)
     beq     common_errNullObject        @ object was null
-    ldrd    r0, [r3, r1]                @ r0<- obj.field (64 bits, aligned)
+    ldrd    r0, [r3, ip]                @ r0<- obj.field (64 bits, aligned)
     and     r2, r2, #15
     FETCH_ADVANCE_INST(2)               @ advance rPC, load rINST
     add     r3, rFP, r2, lsl #2         @ r3<- &fp[A]
@@ -8983,6 +9017,36 @@ d2l_doconv:
     .word   gDvmInlineOpsTable
 
 
+/* continuation for OP_EXECUTE_INLINE_RANGE */
+
+    /*
+     * Extract args, call function.
+     *  r0 = #of args (0-4)
+     *  r10 = call index
+     *  lr = return addr, above  [DO NOT bl out of here w/o preserving LR]
+     */
+.LOP_EXECUTE_INLINE_RANGE_continue:
+    rsb     r0, r0, #4                  @ r0<- 4-r0
+    FETCH(r9, 2)                        @ r9<- CCCC
+    add     pc, pc, r0, lsl #3          @ computed goto, 2 instrs each
+    bl      common_abort                @ (skipped due to ARM prefetch)
+4:  add     ip, r9, #3                  @ base+3
+    GET_VREG(r3, ip)                    @ r3<- vBase[3]
+3:  add     ip, r9, #2                  @ base+2
+    GET_VREG(r2, ip)                    @ r2<- vBase[2]
+2:  add     ip, r9, #1                  @ base+1
+    GET_VREG(r1, ip)                    @ r1<- vBase[1]
+1:  add     ip, r9, #0                  @ (nop)
+    GET_VREG(r0, ip)                    @ r0<- vBase[0]
+0:
+    ldr     r9, .LOP_EXECUTE_INLINE_RANGE_table       @ table of InlineOperation
+    LDR_PC  "[r9, r10, lsl #4]"         @ sizeof=16, "func" is first entry
+    @ (not reached)
+
+.LOP_EXECUTE_INLINE_RANGE_table:
+    .word   gDvmInlineOpsTable
+
+
     .size   dvmAsmSisterStart, .-dvmAsmSisterStart
     .global dvmAsmSisterEnd
 dvmAsmSisterEnd:
@@ -9001,6 +9065,67 @@ dvmAsmSisterEnd:
     .align  2
 
 #if defined(WITH_JIT)
+#if defined(WITH_SELF_VERIFICATION)
+    .global dvmJitToInterpPunt
+dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r2,#kSVSPunt                 @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpSingleStep
+dvmJitToInterpSingleStep:
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
+    mov    r2,#kSVSSingleStep           @ r2<- interpreter entry point
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSTraceSelectNoChain   @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSTraceSelect          @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpBackwardBranch
+dvmJitToInterpBackwardBranch:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSBackwardBranch       @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNormal
+dvmJitToInterpNormal:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSNormal               @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNoChain
+dvmJitToInterpNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSNoChain              @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+#else
 /*
  * Return from the translation cache to the interpreter when the compiler is
  * having issues translating/executing a Dalvik instruction. We have to skip
@@ -9010,12 +9135,15 @@ dvmAsmSisterEnd:
  */
     .global dvmJitToInterpPunt
 dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    rPC, r0
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     mov    r0,lr
     bl     dvmBumpPunt;
 #endif
     EXPORT_PC()
+    mov    r0, #0
+    str    r0, [r10, #offThread_inJitCodeCache] @ Back to the interp land
     adrl   rIBASE, dvmAsmInstructionStart
     FETCH_INST()
     GET_INST_OPCODE(ip)
@@ -9030,35 +9158,58 @@ dvmJitToInterpPunt:
  */
     .global dvmJitToInterpSingleStep
 dvmJitToInterpSingleStep:
-    str    lr,[rGLUE,#offGlue_jitResume]
-    str    r1,[rGLUE,#offGlue_jitResumePC]
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
     mov    r1,#kInterpEntryInstr
     @ enum is 4 byte in aapcs-EABI
     str    r1, [rGLUE, #offGlue_entryPoint]
     mov    rPC,r0
     EXPORT_PC()
+
     adrl   rIBASE, dvmAsmInstructionStart
     mov    r2,#kJitSingleStep     @ Ask for single step and then revert
     str    r2,[rGLUE,#offGlue_jitState]
     mov    r1,#1                  @ set changeInterp to bail to debug interp
     b      common_gotoBail
 
+/*
+ * Return from the translation cache and immediately request
+ * a translation for the exit target.  Commonly used for callees.
+ */
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+#ifdef JIT_STATS
+    bl     dvmBumpNoChain
+#endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC
+    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
+    cmp    r0,#0
+    bxne   r0                       @ continue native execution if so
+    b      2f
 
 /*
  * Return from the translation cache and immediately request
  * a translation for the exit target.  Commonly used following
  * invokes.
  */
-    .global dvmJitToTraceSelect
-dvmJitToTraceSelect:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
     mov    r0,rPC
-    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    bl     dvmJitGetCodeAddr       @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    2f
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @ in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9069,6 +9220,7 @@ dvmJitToTraceSelect:
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
     cmp    r0, #0
+    movne  r2,#kJitTSelectRequestHot   @ ask for trace selection
     bne    common_selectTrace
     GET_INST_OPCODE(ip)
     GOTO_OPCODE(ip)
@@ -9089,17 +9241,21 @@ dvmJitToTraceSelect:
  */
     .global dvmJitToInterpNormal
 dvmJitToInterpNormal:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
-#ifdef EXIT_STATS
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
+#ifdef JIT_STATS
     bl     dvmBumpNormal
 #endif
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    toInterpreter            @ go if not, otherwise do chain
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9110,13 +9266,18 @@ dvmJitToInterpNormal:
  */
     .global dvmJitToInterpNoChain
 dvmJitToInterpNoChain:
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     bl     dvmBumpNoChain
 #endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0
     bxne   r0                       @ continue native execution if so
+#endif
 
 /*
  * No translation, restore interpreter regs and start interpreting.
@@ -9144,11 +9305,11 @@ common_testUpdateProfile:
 
 common_updateProfile:
     eor     r3,rPC,rPC,lsr #12 @ cheap, but fast hash function
-    lsl     r3,r3,#23          @ shift out excess 511
-    ldrb    r1,[r0,r3,lsr #23] @ get counter
+    lsl     r3,r3,#(32 - JIT_PROF_SIZE_LOG_2)          @ shift out excess bits
+    ldrb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ get counter
     GET_INST_OPCODE(ip)
     subs    r1,r1,#1           @ decrement counter
-    strb    r1,[r0,r3,lsr #23] @ and store it
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ and store it
     GOTO_OPCODE_IFNE(ip)       @ if not threshold, fallthrough otherwise */
 
 /*
@@ -9157,20 +9318,94 @@ common_updateProfile:
  * is already a native translation in place (and, if so,
  * jump to it now).
  */
-    mov     r1,#255
-    strb    r1,[r0,r3,lsr #23] @ reset counter
+    GET_JIT_THRESHOLD(r1)
+    ldr     r10, [rGLUE, #offGlue_self] @ callee saved r10 <- glue->self
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ reset counter
     EXPORT_PC()
     mov     r0,rPC
     bl      dvmJitGetCodeAddr           @ r0<- dvmJitGetCodeAddr(rPC)
+    str     r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov     r1, rPC                     @ arg1 of translation may need this
+    mov     lr, #0                      @  in case target is HANDLER_INTERPRET
     cmp     r0,#0
-    beq     common_selectTrace
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    r0                          @ jump to the translation
-common_selectTrace:
     mov     r2,#kJitTSelectRequest      @ ask for trace selection
+    @ fall-through to common_selectTrace
+#else
+    moveq   r2,#kJitTSelectRequest      @ ask for trace selection
+    beq     common_selectTrace
+    /*
+     * At this point, we have a target translation.  However, if
+     * that translation is actually the interpret-only pseudo-translation
+     * we want to treat it the same as no translation.
+     */
+    mov     r10, r0                     @ save target
+    bl      dvmCompilerGetInterpretTemplate
+    cmp     r0, r10                     @ special case?
+    bne     jitSVShadowRunStart         @ set up self verification shadow space
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+    /* no return */
+#endif
+
+/*
+ * On entry:
+ *  r2 is jit state, e.g. kJitTSelectRequest or kJitTSelectRequestHot
+ */
+common_selectTrace:
     str     r2,[rGLUE,#offGlue_jitState]
+    mov     r2,#kInterpEntryInstr       @ normal entry reason
+    str     r2,[rGLUE,#offGlue_entryPoint]
     mov     r1,#1                       @ set changeInterp
     b       common_gotoBail
 
+#if defined(WITH_SELF_VERIFICATION)
+/*
+ * Save PC and registers to shadow memory for self verification mode
+ * before jumping to native translation.
+ * On entry:
+ *    rPC, rFP, rGLUE: the values that they should contain
+ *    r10: the address of the target translation.
+ */
+jitSVShadowRunStart:
+    mov     r0,rPC                      @ r0<- program counter
+    mov     r1,rFP                      @ r1<- frame pointer
+    mov     r2,rGLUE                    @ r2<- InterpState pointer
+    mov     r3,r10                      @ r3<- target translation
+    bl      dvmSelfVerificationSaveState @ save registers to shadow space
+    ldr     rFP,[r0,#offShadowSpace_shadowFP] @ rFP<- fp in shadow space
+    add     rGLUE,r0,#offShadowSpace_interpState @ rGLUE<- rGLUE in shadow space
+    bx      r10                         @ jump to the translation
+
+/*
+ * Restore PC, registers, and interpState to original values
+ * before jumping back to the interpreter.
+ */
+jitSVShadowRunEnd:
+    mov    r1,rFP                        @ pass ending fp
+    bl     dvmSelfVerificationRestoreState @ restore pc and fp values
+    ldr    rPC,[r0,#offShadowSpace_startPC] @ restore PC
+    ldr    rFP,[r0,#offShadowSpace_fp]   @ restore FP
+    ldr    rGLUE,[r0,#offShadowSpace_glue] @ restore InterpState
+    ldr    r1,[r0,#offShadowSpace_svState] @ get self verification state
+    cmp    r1,#0                         @ check for punt condition
+    beq    1f
+    mov    r2,#kJitSelfVerification      @ ask for self verification
+    str    r2,[rGLUE,#offGlue_jitState]
+    mov    r2,#kInterpEntryInstr         @ normal entry reason
+    str    r2,[rGLUE,#offGlue_entryPoint]
+    mov    r1,#1                         @ set changeInterp
+    b      common_gotoBail
+
+1:                                       @ exit to interpreter without check
+    EXPORT_PC()
+    adrl   rIBASE, dvmAsmInstructionStart
+    FETCH_INST()
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+#endif
+
 #endif
 
 /*
@@ -9212,6 +9447,9 @@ common_backwardBranch:
 common_periodicChecks:
     ldr     r3, [rGLUE, #offGlue_pSelfSuspendCount] @ r3<- &suspendCount
 
+    @ speculatively store r0 before it is clobbered by dvmCheckSuspendPending
+    str     r0, [rGLUE, #offGlue_entryPoint]
+
 #if defined(WITH_DEBUGGER)
     ldr     r1, [rGLUE, #offGlue_pDebuggerActive]   @ r1<- &debuggerActive
 #endif
@@ -9246,13 +9484,24 @@ common_periodicChecks:
     bx      lr                          @ nothing to do, return
 
 2:  @ check suspend
+#if defined(WITH_JIT)
+    /*
+     * Refresh the Jit's cached copy of profile table pointer.  This pointer
+     * doubles as the Jit's on/off switch.
+     */
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ r3<-&gDvmJit.pJitProfTable
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    ldr     r3, [r3] @ r3 <- pJitProfTable
     EXPORT_PC()                         @ need for precise GC
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh Jit's on/off switch
+#else
+    ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    EXPORT_PC()                         @ need for precise GC
+#endif
     b       dvmCheckSuspendPending      @ suspend if necessary, then return
 
 3:  @ debugger/profiler enabled, bail out
     add     rPC, rPC, r9                @ update rPC
-    str     r0, [rGLUE, #offGlue_entryPoint]
     mov     r1, #1                      @ "want switch" = true
     b       common_gotoBail
 
@@ -9444,20 +9693,31 @@ dalvik_mterp:
     @ldr     pc, [r2, #offMethod_nativeFunc] @ pc<- methodToCall->nativeFunc
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+#if defined(WITH_JIT)
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ Refresh Jit's on/off status
+#endif
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+#if defined(WITH_JIT)
+    ldr     r3, [r3]                    @ r3 <- gDvmJit.pProfTable
+#endif
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
+#if defined(WITH_JIT)
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh cached on/off switch
+#endif
     bne     common_exceptionThrown      @ no, handle exception
 
     FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    @ r0=methodToCall
+    mov     r1, r0                      @ r1<- methodToCall
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- self
     bl      dvmHandleStackOverflow
     b       common_exceptionThrown
@@ -9521,12 +9781,13 @@ common_returnFromMethod:
     ldr     r1, [r10, #offClassObject_pDvmDex]   @ r1<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame]  @ self->curFrame = fp
 #if defined(WITH_JIT)
-    ldr     r3, [r0, #offStackSaveArea_returnAddr] @ r3 = saveArea->returnAddr
+    ldr     r10, [r0, #offStackSaveArea_returnAddr] @ r10 = saveArea->returnAddr
     GET_JIT_PROF_TABLE(r0)
     mov     rPC, r9                     @ publish new rPC
     str     r1, [rGLUE, #offGlue_methodClassDex]
-    cmp     r3, #0                      @ caller is compiled code
-    blxne   r3
+    str     r10, [r3, #offThread_inJitCodeCache]  @ may return to JIT'ed land
+    cmp     r10, #0                      @ caller is compiled code
+    blxne   r10
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     cmp     r0,#0
     bne     common_updateProfile
@@ -9567,11 +9828,6 @@ common_exceptionThrown:
     mov     r9, #0
     bl      common_periodicChecks
 
-#if defined(WITH_JIT)
-    mov     r2,#kJitTSelectAbort        @ abandon trace selection in progress
-    str     r2,[rGLUE,#offGlue_jitState]
-#endif
-
     ldr     r10, [rGLUE, #offGlue_self] @ r10<- glue->self
     ldr     r9, [r10, #offThread_exception] @ r9<- self->exception
     mov     r1, r10                     @ r1<- self
@@ -9602,6 +9858,7 @@ common_exceptionThrown:
     beq     1f                          @ no, skip ahead
     mov     rFP, r0                     @ save relPc result in rFP
     mov     r0, r10                     @ r0<- self
+    mov     r1, r9                      @ r1<- exception
     bl      dvmCleanupStackOverflow     @ call(self)
     mov     r0, rFP                     @ restore result
 1:
@@ -9639,6 +9896,7 @@ common_exceptionThrown:
     ldrb    r1, [r10, #offThread_stackOverflowed]
     cmp     r1, #0                      @ did we overflow earlier?
     movne   r0, r10                     @ if yes: r0<- self
+    movne   r1, r9                      @ if yes: r1<- exception
     blne    dvmCleanupStackOverflow     @ if yes: call(self)
 
     @ may want to show "not caught locally" debug messages here
diff --git a/vm/mterp/out/InterpAsm-armv5te.S b/vm/mterp/out/InterpAsm-armv5te.S
index bafd442..2a69ca6 100644
--- a/vm/mterp/out/InterpAsm-armv5te.S
+++ b/vm/mterp/out/InterpAsm-armv5te.S
@@ -185,8 +185,8 @@ unspecified registers or condition codes.
 #define SET_VREG(_reg, _vreg)   str     _reg, [rFP, _vreg, lsl #2]
 
 #if defined(WITH_JIT)
-#define GET_JIT_ENABLED(_reg)       ldr     _reg,[rGLUE,#offGlue_jitEnabled]
 #define GET_JIT_PROF_TABLE(_reg)    ldr     _reg,[rGLUE,#offGlue_pJitProfTable]
+#define GET_JIT_THRESHOLD(_reg)     ldr     _reg,[rGLUE,#offGlue_jitThreshold]
 #endif
 
 /*
@@ -201,6 +201,9 @@ unspecified registers or condition codes.
  */
 #include "../common/asm-constants.h"
 
+#if defined(WITH_JIT)
+#include "../common/jit-config.h"
+#endif
 
 /* File: armv5te/platform.S */
 /*
@@ -303,17 +306,20 @@ dvmMterpStdRun:
 
     /* set up "named" registers, figure out entry point */
     mov     rGLUE, r0                   @ set rGLUE
-    ldrb    r1, [r0, #offGlue_entryPoint]   @ InterpEntry enum is char
+    ldr     r1, [r0, #offGlue_entryPoint]   @ enum is 4 bytes in aapcs-EABI
     LOAD_PC_FP_FROM_GLUE()              @ load rPC and rFP from "glue"
     adr     rIBASE, dvmAsmInstructionStart  @ set rIBASE
     cmp     r1, #kInterpEntryInstr      @ usual case?
     bne     .Lnot_instr                 @ no, handle it
 
 #if defined(WITH_JIT)
-.Lno_singleStep:
+.LentryInstr:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     /* Entry is always a possible trace start */
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
+    mov    r1, #0                       @ prepare the value for the new state
+    str    r1, [r10, #offThread_inJitCodeCache] @ back to the interp land
     cmp    r0,#0
     bne    common_updateProfile
     GET_INST_OPCODE(ip)
@@ -335,18 +341,21 @@ dvmMterpStdRun:
 
 #if defined(WITH_JIT)
 .Lnot_throw:
-    ldr     r0,[rGLUE, #offGlue_jitResume]
-    ldr     r2,[rGLUE, #offGlue_jitResumePC]
+    ldr     r10,[rGLUE, #offGlue_jitResumeNPC]
+    ldr     r2,[rGLUE, #offGlue_jitResumeDPC]
     cmp     r1, #kInterpEntryResume     @ resuming after Jit single-step?
     bne     .Lbad_arg
     cmp     rPC,r2
-    bne     .Lno_singleStep             @ must have branched, don't resume
+    bne     .LentryInstr                @ must have branched, don't resume
+#if defined(WITH_SELF_VERIFICATION)
+    @ glue->entryPoint will be set in dvmSelfVerificationSaveState
+    b       jitSVShadowRunStart         @ re-enter the translation after the
+                                        @ single-stepped instruction
+    @noreturn
+#endif
     mov     r1, #kInterpEntryInstr
-    strb    r1, [rGLUE, #offGlue_entryPoint]
-    ldr     rINST, .LdvmCompilerTemplate
-    bx      r0                          @ re-enter the translation
-.LdvmCompilerTemplate:
-    .word   dvmCompilerTemplateStart
+    str     r1, [rGLUE, #offGlue_entryPoint]
+    bx      r10                         @ re-enter the translation
 #endif
 
 .Lbad_arg:
@@ -902,14 +911,17 @@ dalvik_inst:
     EXPORT_PC()                         @ before fetch: export the PC
     GET_VREG(r1, r2)                    @ r1<- vAA (object)
     cmp     r1, #0                      @ null object?
-    beq     common_errNullObject        @ yes
+    beq     1f                          @ yes
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
     bl      dvmUnlockObject             @ r0<- success for unlock(self, obj)
     cmp     r0, #0                      @ failed?
-    beq     common_exceptionThrown      @ yes, exception is pending
     FETCH_ADVANCE_INST(1)               @ before throw: advance rPC, load rINST
+    beq     common_exceptionThrown      @ yes, exception is pending
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
+1:
+    FETCH_ADVANCE_INST(1)               @ advance before throw
+    b      common_errNullObject
 
 
 /* ------------------------------ */
@@ -7617,8 +7629,8 @@ d2i_doconv:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EC: /* 0xec */
-/* File: armv5te/OP_UNUSED_EC.S */
+.L_OP_BREAKPOINT: /* 0xec */
+/* File: armv5te/OP_BREAKPOINT.S */
 /* File: armv5te/unused.S */
     bl      common_abort
 
@@ -7649,17 +7661,18 @@ d2i_doconv:
     /*
      * Execute a "native inline" instruction.
      *
-     * We need to call:
-     *  dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref)
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
      *
-     * The first four args are in r0-r3, but the last two must be pushed
-     * onto the stack.
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
      */
     /* [opt] execute-inline vAA, {vC, vD, vE, vF}, inline@BBBB */
     FETCH(r10, 1)                       @ r10<- BBBB
     add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
     EXPORT_PC()                         @ can throw
-    sub     sp, sp, #8                  @ make room for arg(s)
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
     mov     r0, rINST, lsr #12          @ r0<- B
     str     r1, [sp]                    @ push &glue->retval
     bl      .LOP_EXECUTE_INLINE_continue        @ make call; will return after
@@ -7672,12 +7685,33 @@ d2i_doconv:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EF: /* 0xef */
-/* File: armv5te/OP_UNUSED_EF.S */
-/* File: armv5te/unused.S */
-    bl      common_abort
-
-
+.L_OP_EXECUTE_INLINE_RANGE: /* 0xef */
+/* File: armv5te/OP_EXECUTE_INLINE_RANGE.S */
+    /*
+     * Execute a "native inline" instruction, using "/range" semantics.
+     * Same idea as execute-inline, but we get the args differently.
+     *
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
+     *
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
+     */
+    /* [opt] execute-inline/range {vCCCC..v(CCCC+AA-1)}, inline@BBBB */
+    FETCH(r10, 1)                       @ r10<- BBBB
+    add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
+    EXPORT_PC()                         @ can throw
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
+    mov     r0, rINST, lsr #8           @ r0<- AA
+    str     r1, [sp]                    @ push &glue->retval
+    bl      .LOP_EXECUTE_INLINE_RANGE_continue        @ make call; will return after
+    add     sp, sp, #8                  @ pop stack
+    cmp     r0, #0                      @ test boolean result of inline
+    beq     common_exceptionThrown      @ returned false, handle exception
+    FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
+    GET_INST_OPCODE(ip)                 @ extract opcode from rINST
+    GOTO_OPCODE(ip)                     @ jump to next instruction
 
 /* ------------------------------ */
     .balign 64
@@ -7726,11 +7760,11 @@ d2i_doconv:
     /* iget-wide-quick vA, vB, offset@CCCC */
     mov     r2, rINST, lsr #12          @ r2<- B
     GET_VREG(r3, r2)                    @ r3<- object we're operating on
-    FETCH(r1, 1)                        @ r1<- field byte offset
+    FETCH(ip, 1)                        @ ip<- field byte offset
     cmp     r3, #0                      @ check object for null
     mov     r2, rINST, lsr #8           @ r2<- A(+)
     beq     common_errNullObject        @ object was null
-    ldrd    r0, [r3, r1]                @ r0<- obj.field (64 bits, aligned)
+    ldrd    r0, [r3, ip]                @ r0<- obj.field (64 bits, aligned)
     and     r2, r2, #15
     FETCH_ADVANCE_INST(2)               @ advance rPC, load rINST
     add     r3, rFP, r2, lsl #2         @ r3<- &fp[A]
@@ -9459,6 +9493,36 @@ d2l_doconv:
     .word   gDvmInlineOpsTable
 
 
+/* continuation for OP_EXECUTE_INLINE_RANGE */
+
+    /*
+     * Extract args, call function.
+     *  r0 = #of args (0-4)
+     *  r10 = call index
+     *  lr = return addr, above  [DO NOT bl out of here w/o preserving LR]
+     */
+.LOP_EXECUTE_INLINE_RANGE_continue:
+    rsb     r0, r0, #4                  @ r0<- 4-r0
+    FETCH(r9, 2)                        @ r9<- CCCC
+    add     pc, pc, r0, lsl #3          @ computed goto, 2 instrs each
+    bl      common_abort                @ (skipped due to ARM prefetch)
+4:  add     ip, r9, #3                  @ base+3
+    GET_VREG(r3, ip)                    @ r3<- vBase[3]
+3:  add     ip, r9, #2                  @ base+2
+    GET_VREG(r2, ip)                    @ r2<- vBase[2]
+2:  add     ip, r9, #1                  @ base+1
+    GET_VREG(r1, ip)                    @ r1<- vBase[1]
+1:  add     ip, r9, #0                  @ (nop)
+    GET_VREG(r0, ip)                    @ r0<- vBase[0]
+0:
+    ldr     r9, .LOP_EXECUTE_INLINE_RANGE_table       @ table of InlineOperation
+    LDR_PC  "[r9, r10, lsl #4]"         @ sizeof=16, "func" is first entry
+    @ (not reached)
+
+.LOP_EXECUTE_INLINE_RANGE_table:
+    .word   gDvmInlineOpsTable
+
+
     .size   dvmAsmSisterStart, .-dvmAsmSisterStart
     .global dvmAsmSisterEnd
 dvmAsmSisterEnd:
@@ -9477,6 +9541,67 @@ dvmAsmSisterEnd:
     .align  2
 
 #if defined(WITH_JIT)
+#if defined(WITH_SELF_VERIFICATION)
+    .global dvmJitToInterpPunt
+dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r2,#kSVSPunt                 @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpSingleStep
+dvmJitToInterpSingleStep:
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
+    mov    r2,#kSVSSingleStep           @ r2<- interpreter entry point
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSTraceSelectNoChain   @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSTraceSelect          @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpBackwardBranch
+dvmJitToInterpBackwardBranch:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSBackwardBranch       @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNormal
+dvmJitToInterpNormal:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSNormal               @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNoChain
+dvmJitToInterpNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSNoChain              @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+#else
 /*
  * Return from the translation cache to the interpreter when the compiler is
  * having issues translating/executing a Dalvik instruction. We have to skip
@@ -9486,12 +9611,15 @@ dvmAsmSisterEnd:
  */
     .global dvmJitToInterpPunt
 dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    rPC, r0
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     mov    r0,lr
     bl     dvmBumpPunt;
 #endif
     EXPORT_PC()
+    mov    r0, #0
+    str    r0, [r10, #offThread_inJitCodeCache] @ Back to the interp land
     adrl   rIBASE, dvmAsmInstructionStart
     FETCH_INST()
     GET_INST_OPCODE(ip)
@@ -9506,35 +9634,58 @@ dvmJitToInterpPunt:
  */
     .global dvmJitToInterpSingleStep
 dvmJitToInterpSingleStep:
-    str    lr,[rGLUE,#offGlue_jitResume]
-    str    r1,[rGLUE,#offGlue_jitResumePC]
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
     mov    r1,#kInterpEntryInstr
     @ enum is 4 byte in aapcs-EABI
     str    r1, [rGLUE, #offGlue_entryPoint]
     mov    rPC,r0
     EXPORT_PC()
+
     adrl   rIBASE, dvmAsmInstructionStart
     mov    r2,#kJitSingleStep     @ Ask for single step and then revert
     str    r2,[rGLUE,#offGlue_jitState]
     mov    r1,#1                  @ set changeInterp to bail to debug interp
     b      common_gotoBail
 
+/*
+ * Return from the translation cache and immediately request
+ * a translation for the exit target.  Commonly used for callees.
+ */
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+#ifdef JIT_STATS
+    bl     dvmBumpNoChain
+#endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC
+    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
+    cmp    r0,#0
+    bxne   r0                       @ continue native execution if so
+    b      2f
 
 /*
  * Return from the translation cache and immediately request
  * a translation for the exit target.  Commonly used following
  * invokes.
  */
-    .global dvmJitToTraceSelect
-dvmJitToTraceSelect:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
     mov    r0,rPC
-    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    bl     dvmJitGetCodeAddr       @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    2f
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @ in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9545,6 +9696,7 @@ dvmJitToTraceSelect:
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
     cmp    r0, #0
+    movne  r2,#kJitTSelectRequestHot   @ ask for trace selection
     bne    common_selectTrace
     GET_INST_OPCODE(ip)
     GOTO_OPCODE(ip)
@@ -9565,17 +9717,21 @@ dvmJitToTraceSelect:
  */
     .global dvmJitToInterpNormal
 dvmJitToInterpNormal:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
-#ifdef EXIT_STATS
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
+#ifdef JIT_STATS
     bl     dvmBumpNormal
 #endif
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    toInterpreter            @ go if not, otherwise do chain
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9586,13 +9742,18 @@ dvmJitToInterpNormal:
  */
     .global dvmJitToInterpNoChain
 dvmJitToInterpNoChain:
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     bl     dvmBumpNoChain
 #endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0
     bxne   r0                       @ continue native execution if so
+#endif
 
 /*
  * No translation, restore interpreter regs and start interpreting.
@@ -9620,11 +9781,11 @@ common_testUpdateProfile:
 
 common_updateProfile:
     eor     r3,rPC,rPC,lsr #12 @ cheap, but fast hash function
-    lsl     r3,r3,#23          @ shift out excess 511
-    ldrb    r1,[r0,r3,lsr #23] @ get counter
+    lsl     r3,r3,#(32 - JIT_PROF_SIZE_LOG_2)          @ shift out excess bits
+    ldrb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ get counter
     GET_INST_OPCODE(ip)
     subs    r1,r1,#1           @ decrement counter
-    strb    r1,[r0,r3,lsr #23] @ and store it
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ and store it
     GOTO_OPCODE_IFNE(ip)       @ if not threshold, fallthrough otherwise */
 
 /*
@@ -9633,20 +9794,94 @@ common_updateProfile:
  * is already a native translation in place (and, if so,
  * jump to it now).
  */
-    mov     r1,#255
-    strb    r1,[r0,r3,lsr #23] @ reset counter
+    GET_JIT_THRESHOLD(r1)
+    ldr     r10, [rGLUE, #offGlue_self] @ callee saved r10 <- glue->self
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ reset counter
     EXPORT_PC()
     mov     r0,rPC
     bl      dvmJitGetCodeAddr           @ r0<- dvmJitGetCodeAddr(rPC)
+    str     r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov     r1, rPC                     @ arg1 of translation may need this
+    mov     lr, #0                      @  in case target is HANDLER_INTERPRET
     cmp     r0,#0
-    beq     common_selectTrace
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    r0                          @ jump to the translation
-common_selectTrace:
     mov     r2,#kJitTSelectRequest      @ ask for trace selection
+    @ fall-through to common_selectTrace
+#else
+    moveq   r2,#kJitTSelectRequest      @ ask for trace selection
+    beq     common_selectTrace
+    /*
+     * At this point, we have a target translation.  However, if
+     * that translation is actually the interpret-only pseudo-translation
+     * we want to treat it the same as no translation.
+     */
+    mov     r10, r0                     @ save target
+    bl      dvmCompilerGetInterpretTemplate
+    cmp     r0, r10                     @ special case?
+    bne     jitSVShadowRunStart         @ set up self verification shadow space
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+    /* no return */
+#endif
+
+/*
+ * On entry:
+ *  r2 is jit state, e.g. kJitTSelectRequest or kJitTSelectRequestHot
+ */
+common_selectTrace:
     str     r2,[rGLUE,#offGlue_jitState]
+    mov     r2,#kInterpEntryInstr       @ normal entry reason
+    str     r2,[rGLUE,#offGlue_entryPoint]
     mov     r1,#1                       @ set changeInterp
     b       common_gotoBail
 
+#if defined(WITH_SELF_VERIFICATION)
+/*
+ * Save PC and registers to shadow memory for self verification mode
+ * before jumping to native translation.
+ * On entry:
+ *    rPC, rFP, rGLUE: the values that they should contain
+ *    r10: the address of the target translation.
+ */
+jitSVShadowRunStart:
+    mov     r0,rPC                      @ r0<- program counter
+    mov     r1,rFP                      @ r1<- frame pointer
+    mov     r2,rGLUE                    @ r2<- InterpState pointer
+    mov     r3,r10                      @ r3<- target translation
+    bl      dvmSelfVerificationSaveState @ save registers to shadow space
+    ldr     rFP,[r0,#offShadowSpace_shadowFP] @ rFP<- fp in shadow space
+    add     rGLUE,r0,#offShadowSpace_interpState @ rGLUE<- rGLUE in shadow space
+    bx      r10                         @ jump to the translation
+
+/*
+ * Restore PC, registers, and interpState to original values
+ * before jumping back to the interpreter.
+ */
+jitSVShadowRunEnd:
+    mov    r1,rFP                        @ pass ending fp
+    bl     dvmSelfVerificationRestoreState @ restore pc and fp values
+    ldr    rPC,[r0,#offShadowSpace_startPC] @ restore PC
+    ldr    rFP,[r0,#offShadowSpace_fp]   @ restore FP
+    ldr    rGLUE,[r0,#offShadowSpace_glue] @ restore InterpState
+    ldr    r1,[r0,#offShadowSpace_svState] @ get self verification state
+    cmp    r1,#0                         @ check for punt condition
+    beq    1f
+    mov    r2,#kJitSelfVerification      @ ask for self verification
+    str    r2,[rGLUE,#offGlue_jitState]
+    mov    r2,#kInterpEntryInstr         @ normal entry reason
+    str    r2,[rGLUE,#offGlue_entryPoint]
+    mov    r1,#1                         @ set changeInterp
+    b      common_gotoBail
+
+1:                                       @ exit to interpreter without check
+    EXPORT_PC()
+    adrl   rIBASE, dvmAsmInstructionStart
+    FETCH_INST()
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+#endif
+
 #endif
 
 /*
@@ -9688,6 +9923,9 @@ common_backwardBranch:
 common_periodicChecks:
     ldr     r3, [rGLUE, #offGlue_pSelfSuspendCount] @ r3<- &suspendCount
 
+    @ speculatively store r0 before it is clobbered by dvmCheckSuspendPending
+    str     r0, [rGLUE, #offGlue_entryPoint]
+
 #if defined(WITH_DEBUGGER)
     ldr     r1, [rGLUE, #offGlue_pDebuggerActive]   @ r1<- &debuggerActive
 #endif
@@ -9722,13 +9960,24 @@ common_periodicChecks:
     bx      lr                          @ nothing to do, return
 
 2:  @ check suspend
+#if defined(WITH_JIT)
+    /*
+     * Refresh the Jit's cached copy of profile table pointer.  This pointer
+     * doubles as the Jit's on/off switch.
+     */
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ r3<-&gDvmJit.pJitProfTable
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    ldr     r3, [r3] @ r3 <- pJitProfTable
     EXPORT_PC()                         @ need for precise GC
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh Jit's on/off switch
+#else
+    ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    EXPORT_PC()                         @ need for precise GC
+#endif
     b       dvmCheckSuspendPending      @ suspend if necessary, then return
 
 3:  @ debugger/profiler enabled, bail out
     add     rPC, rPC, r9                @ update rPC
-    str     r0, [rGLUE, #offGlue_entryPoint]
     mov     r1, #1                      @ "want switch" = true
     b       common_gotoBail
 
@@ -9920,20 +10169,31 @@ dalvik_mterp:
     @ldr     pc, [r2, #offMethod_nativeFunc] @ pc<- methodToCall->nativeFunc
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+#if defined(WITH_JIT)
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ Refresh Jit's on/off status
+#endif
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+#if defined(WITH_JIT)
+    ldr     r3, [r3]                    @ r3 <- gDvmJit.pProfTable
+#endif
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
+#if defined(WITH_JIT)
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh cached on/off switch
+#endif
     bne     common_exceptionThrown      @ no, handle exception
 
     FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    @ r0=methodToCall
+    mov     r1, r0                      @ r1<- methodToCall
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- self
     bl      dvmHandleStackOverflow
     b       common_exceptionThrown
@@ -9997,12 +10257,13 @@ common_returnFromMethod:
     ldr     r1, [r10, #offClassObject_pDvmDex]   @ r1<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame]  @ self->curFrame = fp
 #if defined(WITH_JIT)
-    ldr     r3, [r0, #offStackSaveArea_returnAddr] @ r3 = saveArea->returnAddr
+    ldr     r10, [r0, #offStackSaveArea_returnAddr] @ r10 = saveArea->returnAddr
     GET_JIT_PROF_TABLE(r0)
     mov     rPC, r9                     @ publish new rPC
     str     r1, [rGLUE, #offGlue_methodClassDex]
-    cmp     r3, #0                      @ caller is compiled code
-    blxne   r3
+    str     r10, [r3, #offThread_inJitCodeCache]  @ may return to JIT'ed land
+    cmp     r10, #0                      @ caller is compiled code
+    blxne   r10
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     cmp     r0,#0
     bne     common_updateProfile
@@ -10043,11 +10304,6 @@ common_exceptionThrown:
     mov     r9, #0
     bl      common_periodicChecks
 
-#if defined(WITH_JIT)
-    mov     r2,#kJitTSelectAbort        @ abandon trace selection in progress
-    str     r2,[rGLUE,#offGlue_jitState]
-#endif
-
     ldr     r10, [rGLUE, #offGlue_self] @ r10<- glue->self
     ldr     r9, [r10, #offThread_exception] @ r9<- self->exception
     mov     r1, r10                     @ r1<- self
@@ -10078,6 +10334,7 @@ common_exceptionThrown:
     beq     1f                          @ no, skip ahead
     mov     rFP, r0                     @ save relPc result in rFP
     mov     r0, r10                     @ r0<- self
+    mov     r1, r9                      @ r1<- exception
     bl      dvmCleanupStackOverflow     @ call(self)
     mov     r0, rFP                     @ restore result
 1:
@@ -10115,6 +10372,7 @@ common_exceptionThrown:
     ldrb    r1, [r10, #offThread_stackOverflowed]
     cmp     r1, #0                      @ did we overflow earlier?
     movne   r0, r10                     @ if yes: r0<- self
+    movne   r1, r9                      @ if yes: r1<- exception
     blne    dvmCleanupStackOverflow     @ if yes: call(self)
 
     @ may want to show "not caught locally" debug messages here
diff --git a/vm/mterp/out/InterpAsm-armv7-a.S b/vm/mterp/out/InterpAsm-armv7-a.S
index 8d018c1..082dffd 100644
--- a/vm/mterp/out/InterpAsm-armv7-a.S
+++ b/vm/mterp/out/InterpAsm-armv7-a.S
@@ -185,8 +185,8 @@ unspecified registers or condition codes.
 #define SET_VREG(_reg, _vreg)   str     _reg, [rFP, _vreg, lsl #2]
 
 #if defined(WITH_JIT)
-#define GET_JIT_ENABLED(_reg)       ldr     _reg,[rGLUE,#offGlue_jitEnabled]
 #define GET_JIT_PROF_TABLE(_reg)    ldr     _reg,[rGLUE,#offGlue_pJitProfTable]
+#define GET_JIT_THRESHOLD(_reg)     ldr     _reg,[rGLUE,#offGlue_jitThreshold]
 #endif
 
 /*
@@ -201,6 +201,9 @@ unspecified registers or condition codes.
  */
 #include "../common/asm-constants.h"
 
+#if defined(WITH_JIT)
+#include "../common/jit-config.h"
+#endif
 
 /* File: armv5te/platform.S */
 /*
@@ -303,17 +306,20 @@ dvmMterpStdRun:
 
     /* set up "named" registers, figure out entry point */
     mov     rGLUE, r0                   @ set rGLUE
-    ldrb    r1, [r0, #offGlue_entryPoint]   @ InterpEntry enum is char
+    ldr     r1, [r0, #offGlue_entryPoint]   @ enum is 4 bytes in aapcs-EABI
     LOAD_PC_FP_FROM_GLUE()              @ load rPC and rFP from "glue"
     adr     rIBASE, dvmAsmInstructionStart  @ set rIBASE
     cmp     r1, #kInterpEntryInstr      @ usual case?
     bne     .Lnot_instr                 @ no, handle it
 
 #if defined(WITH_JIT)
-.Lno_singleStep:
+.LentryInstr:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     /* Entry is always a possible trace start */
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
+    mov    r1, #0                       @ prepare the value for the new state
+    str    r1, [r10, #offThread_inJitCodeCache] @ back to the interp land
     cmp    r0,#0
     bne    common_updateProfile
     GET_INST_OPCODE(ip)
@@ -335,18 +341,21 @@ dvmMterpStdRun:
 
 #if defined(WITH_JIT)
 .Lnot_throw:
-    ldr     r0,[rGLUE, #offGlue_jitResume]
-    ldr     r2,[rGLUE, #offGlue_jitResumePC]
+    ldr     r10,[rGLUE, #offGlue_jitResumeNPC]
+    ldr     r2,[rGLUE, #offGlue_jitResumeDPC]
     cmp     r1, #kInterpEntryResume     @ resuming after Jit single-step?
     bne     .Lbad_arg
     cmp     rPC,r2
-    bne     .Lno_singleStep             @ must have branched, don't resume
+    bne     .LentryInstr                @ must have branched, don't resume
+#if defined(WITH_SELF_VERIFICATION)
+    @ glue->entryPoint will be set in dvmSelfVerificationSaveState
+    b       jitSVShadowRunStart         @ re-enter the translation after the
+                                        @ single-stepped instruction
+    @noreturn
+#endif
     mov     r1, #kInterpEntryInstr
-    strb    r1, [rGLUE, #offGlue_entryPoint]
-    ldr     rINST, .LdvmCompilerTemplate
-    bx      r0                          @ re-enter the translation
-.LdvmCompilerTemplate:
-    .word   dvmCompilerTemplateStart
+    str     r1, [rGLUE, #offGlue_entryPoint]
+    bx      r10                         @ re-enter the translation
 #endif
 
 .Lbad_arg:
@@ -899,14 +908,17 @@ dalvik_inst:
     EXPORT_PC()                         @ before fetch: export the PC
     GET_VREG(r1, r2)                    @ r1<- vAA (object)
     cmp     r1, #0                      @ null object?
-    beq     common_errNullObject        @ yes
+    beq     1f                          @ yes
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
     bl      dvmUnlockObject             @ r0<- success for unlock(self, obj)
     cmp     r0, #0                      @ failed?
-    beq     common_exceptionThrown      @ yes, exception is pending
     FETCH_ADVANCE_INST(1)               @ before throw: advance rPC, load rINST
+    beq     common_exceptionThrown      @ yes, exception is pending
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
+1:
+    FETCH_ADVANCE_INST(1)               @ advance before throw
+    b      common_errNullObject
 
 
 /* ------------------------------ */
@@ -7221,8 +7233,8 @@ dalvik_inst:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EC: /* 0xec */
-/* File: armv5te/OP_UNUSED_EC.S */
+.L_OP_BREAKPOINT: /* 0xec */
+/* File: armv5te/OP_BREAKPOINT.S */
 /* File: armv5te/unused.S */
     bl      common_abort
 
@@ -7253,17 +7265,18 @@ dalvik_inst:
     /*
      * Execute a "native inline" instruction.
      *
-     * We need to call:
-     *  dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref)
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
      *
-     * The first four args are in r0-r3, but the last two must be pushed
-     * onto the stack.
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
      */
     /* [opt] execute-inline vAA, {vC, vD, vE, vF}, inline@BBBB */
     FETCH(r10, 1)                       @ r10<- BBBB
     add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
     EXPORT_PC()                         @ can throw
-    sub     sp, sp, #8                  @ make room for arg(s)
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
     mov     r0, rINST, lsr #12          @ r0<- B
     str     r1, [sp]                    @ push &glue->retval
     bl      .LOP_EXECUTE_INLINE_continue        @ make call; will return after
@@ -7276,12 +7289,33 @@ dalvik_inst:
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EF: /* 0xef */
-/* File: armv5te/OP_UNUSED_EF.S */
-/* File: armv5te/unused.S */
-    bl      common_abort
-
-
+.L_OP_EXECUTE_INLINE_RANGE: /* 0xef */
+/* File: armv5te/OP_EXECUTE_INLINE_RANGE.S */
+    /*
+     * Execute a "native inline" instruction, using "/range" semantics.
+     * Same idea as execute-inline, but we get the args differently.
+     *
+     * We need to call an InlineOp4Func:
+     *  bool (func)(u4 arg0, u4 arg1, u4 arg2, u4 arg3, JValue* pResult)
+     *
+     * The first four args are in r0-r3, pointer to return value storage
+     * is on the stack.  The function's return value is a flag that tells
+     * us if an exception was thrown.
+     */
+    /* [opt] execute-inline/range {vCCCC..v(CCCC+AA-1)}, inline@BBBB */
+    FETCH(r10, 1)                       @ r10<- BBBB
+    add     r1, rGLUE, #offGlue_retval  @ r1<- &glue->retval
+    EXPORT_PC()                         @ can throw
+    sub     sp, sp, #8                  @ make room for arg, +64 bit align
+    mov     r0, rINST, lsr #8           @ r0<- AA
+    str     r1, [sp]                    @ push &glue->retval
+    bl      .LOP_EXECUTE_INLINE_RANGE_continue        @ make call; will return after
+    add     sp, sp, #8                  @ pop stack
+    cmp     r0, #0                      @ test boolean result of inline
+    beq     common_exceptionThrown      @ returned false, handle exception
+    FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
+    GET_INST_OPCODE(ip)                 @ extract opcode from rINST
+    GOTO_OPCODE(ip)                     @ jump to next instruction
 
 /* ------------------------------ */
     .balign 64
@@ -7328,12 +7362,12 @@ dalvik_inst:
 /* File: armv6t2/OP_IGET_WIDE_QUICK.S */
     /* iget-wide-quick vA, vB, offset@CCCC */
     mov     r2, rINST, lsr #12          @ r2<- B
-    FETCH(r1, 1)                        @ r1<- field byte offset
+    FETCH(ip, 1)                        @ ip<- field byte offset
     GET_VREG(r3, r2)                    @ r3<- object we're operating on
     ubfx    r2, rINST, #8, #4           @ r2<- A
     cmp     r3, #0                      @ check object for null
     beq     common_errNullObject        @ object was null
-    ldrd    r0, [r3, r1]                @ r0<- obj.field (64 bits, aligned)
+    ldrd    r0, [r3, ip]                @ r0<- obj.field (64 bits, aligned)
     FETCH_ADVANCE_INST(2)               @ advance rPC, load rINST
     add     r3, rFP, r2, lsl #2         @ r3<- &fp[A]
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
@@ -8919,6 +8953,36 @@ d2l_doconv:
     .word   gDvmInlineOpsTable
 
 
+/* continuation for OP_EXECUTE_INLINE_RANGE */
+
+    /*
+     * Extract args, call function.
+     *  r0 = #of args (0-4)
+     *  r10 = call index
+     *  lr = return addr, above  [DO NOT bl out of here w/o preserving LR]
+     */
+.LOP_EXECUTE_INLINE_RANGE_continue:
+    rsb     r0, r0, #4                  @ r0<- 4-r0
+    FETCH(r9, 2)                        @ r9<- CCCC
+    add     pc, pc, r0, lsl #3          @ computed goto, 2 instrs each
+    bl      common_abort                @ (skipped due to ARM prefetch)
+4:  add     ip, r9, #3                  @ base+3
+    GET_VREG(r3, ip)                    @ r3<- vBase[3]
+3:  add     ip, r9, #2                  @ base+2
+    GET_VREG(r2, ip)                    @ r2<- vBase[2]
+2:  add     ip, r9, #1                  @ base+1
+    GET_VREG(r1, ip)                    @ r1<- vBase[1]
+1:  add     ip, r9, #0                  @ (nop)
+    GET_VREG(r0, ip)                    @ r0<- vBase[0]
+0:
+    ldr     r9, .LOP_EXECUTE_INLINE_RANGE_table       @ table of InlineOperation
+    LDR_PC  "[r9, r10, lsl #4]"         @ sizeof=16, "func" is first entry
+    @ (not reached)
+
+.LOP_EXECUTE_INLINE_RANGE_table:
+    .word   gDvmInlineOpsTable
+
+
     .size   dvmAsmSisterStart, .-dvmAsmSisterStart
     .global dvmAsmSisterEnd
 dvmAsmSisterEnd:
@@ -8937,6 +9001,67 @@ dvmAsmSisterEnd:
     .align  2
 
 #if defined(WITH_JIT)
+#if defined(WITH_SELF_VERIFICATION)
+    .global dvmJitToInterpPunt
+dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r2,#kSVSPunt                 @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpSingleStep
+dvmJitToInterpSingleStep:
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
+    mov    r2,#kSVSSingleStep           @ r2<- interpreter entry point
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSTraceSelectNoChain   @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSTraceSelect          @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpBackwardBranch
+dvmJitToInterpBackwardBranch:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSBackwardBranch       @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNormal
+dvmJitToInterpNormal:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    ldr    r0,[lr, #-1]                 @ pass our target PC
+    mov    r2,#kSVSNormal               @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+
+    .global dvmJitToInterpNoChain
+dvmJitToInterpNoChain:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC                       @ pass our target PC
+    mov    r2,#kSVSNoChain              @ r2<- interpreter entry point
+    mov    r3, #0
+    str    r3, [r10, #offThread_inJitCodeCache] @ Back to the interp land
+    b      jitSVShadowRunEnd            @ doesn't return
+#else
 /*
  * Return from the translation cache to the interpreter when the compiler is
  * having issues translating/executing a Dalvik instruction. We have to skip
@@ -8946,12 +9071,15 @@ dvmAsmSisterEnd:
  */
     .global dvmJitToInterpPunt
 dvmJitToInterpPunt:
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    rPC, r0
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     mov    r0,lr
     bl     dvmBumpPunt;
 #endif
     EXPORT_PC()
+    mov    r0, #0
+    str    r0, [r10, #offThread_inJitCodeCache] @ Back to the interp land
     adrl   rIBASE, dvmAsmInstructionStart
     FETCH_INST()
     GET_INST_OPCODE(ip)
@@ -8966,35 +9094,58 @@ dvmJitToInterpPunt:
  */
     .global dvmJitToInterpSingleStep
 dvmJitToInterpSingleStep:
-    str    lr,[rGLUE,#offGlue_jitResume]
-    str    r1,[rGLUE,#offGlue_jitResumePC]
+    str    lr,[rGLUE,#offGlue_jitResumeNPC]
+    str    r1,[rGLUE,#offGlue_jitResumeDPC]
     mov    r1,#kInterpEntryInstr
     @ enum is 4 byte in aapcs-EABI
     str    r1, [rGLUE, #offGlue_entryPoint]
     mov    rPC,r0
     EXPORT_PC()
+
     adrl   rIBASE, dvmAsmInstructionStart
     mov    r2,#kJitSingleStep     @ Ask for single step and then revert
     str    r2,[rGLUE,#offGlue_jitState]
     mov    r1,#1                  @ set changeInterp to bail to debug interp
     b      common_gotoBail
 
+/*
+ * Return from the translation cache and immediately request
+ * a translation for the exit target.  Commonly used for callees.
+ */
+    .global dvmJitToInterpTraceSelectNoChain
+dvmJitToInterpTraceSelectNoChain:
+#ifdef JIT_STATS
+    bl     dvmBumpNoChain
+#endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    mov    r0,rPC
+    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
+    cmp    r0,#0
+    bxne   r0                       @ continue native execution if so
+    b      2f
 
 /*
  * Return from the translation cache and immediately request
  * a translation for the exit target.  Commonly used following
  * invokes.
  */
-    .global dvmJitToTraceSelect
-dvmJitToTraceSelect:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
+    .global dvmJitToInterpTraceSelect
+dvmJitToInterpTraceSelect:
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
     mov    r0,rPC
-    bl     dvmJitGetCodeAddr        @ Is there a translation?
+    bl     dvmJitGetCodeAddr       @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    2f
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @ in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9005,6 +9156,7 @@ dvmJitToTraceSelect:
     GET_JIT_PROF_TABLE(r0)
     FETCH_INST()
     cmp    r0, #0
+    movne  r2,#kJitTSelectRequestHot   @ ask for trace selection
     bne    common_selectTrace
     GET_INST_OPCODE(ip)
     GOTO_OPCODE(ip)
@@ -9025,17 +9177,21 @@ dvmJitToTraceSelect:
  */
     .global dvmJitToInterpNormal
 dvmJitToInterpNormal:
-    ldr    rPC,[r14, #-1]           @ get our target PC
-    add    rINST,r14,#-5            @ save start of chain branch
-#ifdef EXIT_STATS
+    ldr    rPC,[lr, #-1]           @ get our target PC
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
+    add    rINST,lr,#-5            @ save start of chain branch
+#ifdef JIT_STATS
     bl     dvmBumpNormal
 #endif
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
     cmp    r0,#0
     beq    toInterpreter            @ go if not, otherwise do chain
     mov    r1,rINST
     bl     dvmJitChain              @ r0<- dvmJitChain(codeAddr,chainAddr)
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0                    @ successful chain?
     bxne   r0                       @ continue native execution
     b      toInterpreter            @ didn't chain - resume with interpreter
@@ -9046,13 +9202,18 @@ dvmJitToInterpNormal:
  */
     .global dvmJitToInterpNoChain
 dvmJitToInterpNoChain:
-#ifdef EXIT_STATS
+#ifdef JIT_STATS
     bl     dvmBumpNoChain
 #endif
+    ldr    r10, [rGLUE, #offGlue_self]  @ callee saved r10 <- glue->self
     mov    r0,rPC
     bl     dvmJitGetCodeAddr        @ Is there a translation?
+    str    r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov    r1, rPC                  @ arg1 of translation may need this
+    mov    lr, #0                   @  in case target is HANDLER_INTERPRET
     cmp    r0,#0
     bxne   r0                       @ continue native execution if so
+#endif
 
 /*
  * No translation, restore interpreter regs and start interpreting.
@@ -9080,11 +9241,11 @@ common_testUpdateProfile:
 
 common_updateProfile:
     eor     r3,rPC,rPC,lsr #12 @ cheap, but fast hash function
-    lsl     r3,r3,#23          @ shift out excess 511
-    ldrb    r1,[r0,r3,lsr #23] @ get counter
+    lsl     r3,r3,#(32 - JIT_PROF_SIZE_LOG_2)          @ shift out excess bits
+    ldrb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ get counter
     GET_INST_OPCODE(ip)
     subs    r1,r1,#1           @ decrement counter
-    strb    r1,[r0,r3,lsr #23] @ and store it
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ and store it
     GOTO_OPCODE_IFNE(ip)       @ if not threshold, fallthrough otherwise */
 
 /*
@@ -9093,20 +9254,94 @@ common_updateProfile:
  * is already a native translation in place (and, if so,
  * jump to it now).
  */
-    mov     r1,#255
-    strb    r1,[r0,r3,lsr #23] @ reset counter
+    GET_JIT_THRESHOLD(r1)
+    ldr     r10, [rGLUE, #offGlue_self] @ callee saved r10 <- glue->self
+    strb    r1,[r0,r3,lsr #(32 - JIT_PROF_SIZE_LOG_2)] @ reset counter
     EXPORT_PC()
     mov     r0,rPC
     bl      dvmJitGetCodeAddr           @ r0<- dvmJitGetCodeAddr(rPC)
+    str     r0, [r10, #offThread_inJitCodeCache] @ set the inJitCodeCache flag
+    mov     r1, rPC                     @ arg1 of translation may need this
+    mov     lr, #0                      @  in case target is HANDLER_INTERPRET
     cmp     r0,#0
-    beq     common_selectTrace
+#if !defined(WITH_SELF_VERIFICATION)
     bxne    r0                          @ jump to the translation
-common_selectTrace:
     mov     r2,#kJitTSelectRequest      @ ask for trace selection
+    @ fall-through to common_selectTrace
+#else
+    moveq   r2,#kJitTSelectRequest      @ ask for trace selection
+    beq     common_selectTrace
+    /*
+     * At this point, we have a target translation.  However, if
+     * that translation is actually the interpret-only pseudo-translation
+     * we want to treat it the same as no translation.
+     */
+    mov     r10, r0                     @ save target
+    bl      dvmCompilerGetInterpretTemplate
+    cmp     r0, r10                     @ special case?
+    bne     jitSVShadowRunStart         @ set up self verification shadow space
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+    /* no return */
+#endif
+
+/*
+ * On entry:
+ *  r2 is jit state, e.g. kJitTSelectRequest or kJitTSelectRequestHot
+ */
+common_selectTrace:
     str     r2,[rGLUE,#offGlue_jitState]
+    mov     r2,#kInterpEntryInstr       @ normal entry reason
+    str     r2,[rGLUE,#offGlue_entryPoint]
     mov     r1,#1                       @ set changeInterp
     b       common_gotoBail
 
+#if defined(WITH_SELF_VERIFICATION)
+/*
+ * Save PC and registers to shadow memory for self verification mode
+ * before jumping to native translation.
+ * On entry:
+ *    rPC, rFP, rGLUE: the values that they should contain
+ *    r10: the address of the target translation.
+ */
+jitSVShadowRunStart:
+    mov     r0,rPC                      @ r0<- program counter
+    mov     r1,rFP                      @ r1<- frame pointer
+    mov     r2,rGLUE                    @ r2<- InterpState pointer
+    mov     r3,r10                      @ r3<- target translation
+    bl      dvmSelfVerificationSaveState @ save registers to shadow space
+    ldr     rFP,[r0,#offShadowSpace_shadowFP] @ rFP<- fp in shadow space
+    add     rGLUE,r0,#offShadowSpace_interpState @ rGLUE<- rGLUE in shadow space
+    bx      r10                         @ jump to the translation
+
+/*
+ * Restore PC, registers, and interpState to original values
+ * before jumping back to the interpreter.
+ */
+jitSVShadowRunEnd:
+    mov    r1,rFP                        @ pass ending fp
+    bl     dvmSelfVerificationRestoreState @ restore pc and fp values
+    ldr    rPC,[r0,#offShadowSpace_startPC] @ restore PC
+    ldr    rFP,[r0,#offShadowSpace_fp]   @ restore FP
+    ldr    rGLUE,[r0,#offShadowSpace_glue] @ restore InterpState
+    ldr    r1,[r0,#offShadowSpace_svState] @ get self verification state
+    cmp    r1,#0                         @ check for punt condition
+    beq    1f
+    mov    r2,#kJitSelfVerification      @ ask for self verification
+    str    r2,[rGLUE,#offGlue_jitState]
+    mov    r2,#kInterpEntryInstr         @ normal entry reason
+    str    r2,[rGLUE,#offGlue_entryPoint]
+    mov    r1,#1                         @ set changeInterp
+    b      common_gotoBail
+
+1:                                       @ exit to interpreter without check
+    EXPORT_PC()
+    adrl   rIBASE, dvmAsmInstructionStart
+    FETCH_INST()
+    GET_INST_OPCODE(ip)
+    GOTO_OPCODE(ip)
+#endif
+
 #endif
 
 /*
@@ -9148,6 +9383,9 @@ common_backwardBranch:
 common_periodicChecks:
     ldr     r3, [rGLUE, #offGlue_pSelfSuspendCount] @ r3<- &suspendCount
 
+    @ speculatively store r0 before it is clobbered by dvmCheckSuspendPending
+    str     r0, [rGLUE, #offGlue_entryPoint]
+
 #if defined(WITH_DEBUGGER)
     ldr     r1, [rGLUE, #offGlue_pDebuggerActive]   @ r1<- &debuggerActive
 #endif
@@ -9182,13 +9420,24 @@ common_periodicChecks:
     bx      lr                          @ nothing to do, return
 
 2:  @ check suspend
+#if defined(WITH_JIT)
+    /*
+     * Refresh the Jit's cached copy of profile table pointer.  This pointer
+     * doubles as the Jit's on/off switch.
+     */
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ r3<-&gDvmJit.pJitProfTable
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    ldr     r3, [r3] @ r3 <- pJitProfTable
     EXPORT_PC()                         @ need for precise GC
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh Jit's on/off switch
+#else
+    ldr     r0, [rGLUE, #offGlue_self]  @ r0<- glue->self
+    EXPORT_PC()                         @ need for precise GC
+#endif
     b       dvmCheckSuspendPending      @ suspend if necessary, then return
 
 3:  @ debugger/profiler enabled, bail out
     add     rPC, rPC, r9                @ update rPC
-    str     r0, [rGLUE, #offGlue_entryPoint]
     mov     r1, #1                      @ "want switch" = true
     b       common_gotoBail
 
@@ -9380,20 +9629,31 @@ dalvik_mterp:
     @ldr     pc, [r2, #offMethod_nativeFunc] @ pc<- methodToCall->nativeFunc
     LDR_PC_LR "[r2, #offMethod_nativeFunc]"
 
+#if defined(WITH_JIT)
+    ldr     r3, [rGLUE, #offGlue_ppJitProfTable] @ Refresh Jit's on/off status
+#endif
+
     @ native return; r9=self, r10=newSaveArea
     @ equivalent to dvmPopJniLocals
     ldr     r0, [r10, #offStackSaveArea_localRefCookie] @ r0<- saved top
     ldr     r1, [r9, #offThread_exception] @ check for exception
+#if defined(WITH_JIT)
+    ldr     r3, [r3]                    @ r3 <- gDvmJit.pProfTable
+#endif
     str     rFP, [r9, #offThread_curFrame]  @ self->curFrame = fp
     cmp     r1, #0                      @ null?
     str     r0, [r9, #offThread_jniLocal_topCookie] @ new top <- old top
+#if defined(WITH_JIT)
+    str     r3, [rGLUE, #offGlue_pJitProfTable] @ refresh cached on/off switch
+#endif
     bne     common_exceptionThrown      @ no, handle exception
 
     FETCH_ADVANCE_INST(3)               @ advance rPC, load rINST
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     GOTO_OPCODE(ip)                     @ jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    @ r0=methodToCall
+    mov     r1, r0                      @ r1<- methodToCall
     ldr     r0, [rGLUE, #offGlue_self]  @ r0<- self
     bl      dvmHandleStackOverflow
     b       common_exceptionThrown
@@ -9457,12 +9717,13 @@ common_returnFromMethod:
     ldr     r1, [r10, #offClassObject_pDvmDex]   @ r1<- method->clazz->pDvmDex
     str     rFP, [r3, #offThread_curFrame]  @ self->curFrame = fp
 #if defined(WITH_JIT)
-    ldr     r3, [r0, #offStackSaveArea_returnAddr] @ r3 = saveArea->returnAddr
+    ldr     r10, [r0, #offStackSaveArea_returnAddr] @ r10 = saveArea->returnAddr
     GET_JIT_PROF_TABLE(r0)
     mov     rPC, r9                     @ publish new rPC
     str     r1, [rGLUE, #offGlue_methodClassDex]
-    cmp     r3, #0                      @ caller is compiled code
-    blxne   r3
+    str     r10, [r3, #offThread_inJitCodeCache]  @ may return to JIT'ed land
+    cmp     r10, #0                      @ caller is compiled code
+    blxne   r10
     GET_INST_OPCODE(ip)                 @ extract opcode from rINST
     cmp     r0,#0
     bne     common_updateProfile
@@ -9503,11 +9764,6 @@ common_exceptionThrown:
     mov     r9, #0
     bl      common_periodicChecks
 
-#if defined(WITH_JIT)
-    mov     r2,#kJitTSelectAbort        @ abandon trace selection in progress
-    str     r2,[rGLUE,#offGlue_jitState]
-#endif
-
     ldr     r10, [rGLUE, #offGlue_self] @ r10<- glue->self
     ldr     r9, [r10, #offThread_exception] @ r9<- self->exception
     mov     r1, r10                     @ r1<- self
@@ -9538,6 +9794,7 @@ common_exceptionThrown:
     beq     1f                          @ no, skip ahead
     mov     rFP, r0                     @ save relPc result in rFP
     mov     r0, r10                     @ r0<- self
+    mov     r1, r9                      @ r1<- exception
     bl      dvmCleanupStackOverflow     @ call(self)
     mov     r0, rFP                     @ restore result
 1:
@@ -9575,6 +9832,7 @@ common_exceptionThrown:
     ldrb    r1, [r10, #offThread_stackOverflowed]
     cmp     r1, #0                      @ did we overflow earlier?
     movne   r0, r10                     @ if yes: r0<- self
+    movne   r1, r9                      @ if yes: r1<- exception
     blne    dvmCleanupStackOverflow     @ if yes: call(self)
 
     @ may want to show "not caught locally" debug messages here
diff --git a/vm/mterp/out/InterpAsm-x86.S b/vm/mterp/out/InterpAsm-x86.S
index 79c98f6..1d9f36b 100644
--- a/vm/mterp/out/InterpAsm-x86.S
+++ b/vm/mterp/out/InterpAsm-x86.S
@@ -703,7 +703,7 @@ dvmAsmInstructionStart = .L_OP_NOP
     GET_GLUE(%ecx)
     EXPORT_PC()
     testl   %eax,%eax                   # null object?
-    je      common_errNullObject        # go if so
+    je      .LOP_MONITOR_EXIT_errNullObject   # go if so
     movl    offGlue_self(%ecx),%ecx     # ecx<- glue->self
     movl    %eax,OUT_ARG1(%esp)
     SPILL(rPC)
@@ -5797,8 +5797,8 @@ dvmAsmInstructionStart = .L_OP_NOP
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EC: /* 0xec */
-/* File: x86/OP_UNUSED_EC.S */
+.L_OP_BREAKPOINT: /* 0xec */
+/* File: x86/OP_BREAKPOINT.S */
 /* File: x86/unused.S */
     jmp     common_abort
 
@@ -5857,12 +5857,18 @@ dvmAsmInstructionStart = .L_OP_NOP
 
 /* ------------------------------ */
     .balign 64
-.L_OP_UNUSED_EF: /* 0xef */
-/* File: x86/OP_UNUSED_EF.S */
-/* File: x86/unused.S */
-    jmp     common_abort
-
-
+.L_OP_EXECUTE_INLINE_RANGE: /* 0xef */
+    /* (stub) */
+    GET_GLUE(%ecx)
+    SAVE_PC_TO_GLUE(%ecx)            # only need to export these two
+    SAVE_FP_TO_GLUE(%ecx)            # only need to export these two
+    movl %ecx,OUT_ARG0(%esp)         # glue is first arg to function
+    call      dvmMterp_OP_EXECUTE_INLINE_RANGE     # do the real work
+    GET_GLUE(%ecx)
+    LOAD_PC_FROM_GLUE(%ecx)          # retrieve updated values
+    LOAD_FP_FROM_GLUE(%ecx)          # retrieve updated values
+    FETCH_INST()
+    GOTO_NEXT
 /* ------------------------------ */
     .balign 64
 .L_OP_INVOKE_DIRECT_EMPTY: /* 0xf0 */
@@ -6275,9 +6281,12 @@ dvmAsmSisterStart:
     UNSPILL(rPC)
     FETCH_INST_WORD(1)
     testl   %eax,%eax                   # success?
-    je      common_exceptionThrown      # no, exception pending
     ADVANCE_PC(1)
+    je      common_exceptionThrown      # no, exception pending
     GOTO_NEXT
+.LOP_MONITOR_EXIT_errNullObject:
+    ADVANCE_PC(1)                       # advance before throw
+    jmp     common_errNullObject
 
 /* continuation for OP_CHECK_CAST */
 
@@ -8727,11 +8736,12 @@ common_invokeMethodNoRange:
     ADVANCE_PC(3)
     GOTO_NEXT                           # jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    # eax=methodToCall
+    movl        %eax, OUT_ARG1(%esp)    # push parameter methodToCall
     GET_GLUE(%eax)                      # %eax<- pMterpGlue
     movl        offGlue_self(%eax), %eax # %eax<- glue->self
     movl        %eax, OUT_ARG0(%esp)    # push parameter self
-    call        dvmHandleStackOverflow  # call: (Thread* self)
+    call        dvmHandleStackOverflow  # call: (Thread* self, Method* meth)
     UNSPILL(rPC)                        # return: void
     jmp         common_exceptionThrown  # handle exception
 
diff --git a/vm/mterp/out/InterpC-allstubs.c b/vm/mterp/out/InterpC-allstubs.c
index 4e832d8..3dbd6a4 100644
--- a/vm/mterp/out/InterpC-allstubs.c
+++ b/vm/mterp/out/InterpC-allstubs.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -418,6 +422,10 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -536,7 +544,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
@@ -2783,8 +2790,35 @@ OP_END
 HANDLE_OPCODE(OP_UNUSED_EB)
 OP_END
 
-/* File: c/OP_UNUSED_EC.c */
-HANDLE_OPCODE(OP_UNUSED_EC)
+/* File: c/OP_BREAKPOINT.c */
+HANDLE_OPCODE(OP_BREAKPOINT)
+#if (INTERP_TYPE == INTERP_DBG) && defined(WITH_DEBUGGER)
+    {
+        /*
+         * Restart this instruction with the original opcode.  We do
+         * this by simply jumping to the handler.
+         *
+         * It's probably not necessary to update "inst", but we do it
+         * for the sake of anything that needs to do disambiguation in a
+         * common handler with INST_INST.
+         *
+         * The breakpoint itself is handled over in updateDebugger(),
+         * because we need to detect other events (method entry, single
+         * step) and report them in the same event packet, and we're not
+         * yet handling those through breakpoint instructions.  By the
+         * time we get here, the breakpoint has already been handled and
+         * the thread resumed.
+         */
+        u1 originalOpCode = dvmGetOriginalOpCode(pc);
+        LOGV("+++ break 0x%02x (0x%04x -> 0x%04x)\n", originalOpCode, inst,
+            INST_REPLACE_OP(inst, originalOpCode));
+        inst = INST_REPLACE_OP(inst, originalOpCode);
+        FINISH_BKPT(originalOpCode);
+    }
+#else
+    LOGE("Breakpoint hit in non-debug interpreter\n");
+    dvmAbort();
+#endif
 OP_END
 
 /* File: c/OP_THROW_VERIFICATION_ERROR.c */
@@ -2808,14 +2842,15 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
          * the rest uninitialized.  We're assuming that, if the method
          * needs them, they'll be specified in the call.
          *
-         * This annoys gcc when optimizations are enabled, causing a
-         * "may be used uninitialized" warning.  We can quiet the warnings
-         * for a slight penalty (5%: 373ns vs. 393ns on empty method).  Note
-         * that valgrind is perfectly happy with this arrangement, because
-         * the uninitialiezd values are never actually used.
+         * However, this annoys gcc when optimizations are enabled,
+         * causing a "may be used uninitialized" warning.  Quieting
+         * the warnings incurs a slight penalty (5%: 373ns vs. 393ns
+         * on empty method).  Note that valgrind is perfectly happy
+         * either way as the uninitialiezd values are never actually
+         * used.
          */
         u4 arg0, arg1, arg2, arg3;
-        //arg0 = arg1 = arg2 = arg3 = 0;
+        arg0 = arg1 = arg2 = arg3 = 0;
 
         EXPORT_PC();
 
@@ -2856,8 +2891,49 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
     FINISH(3);
 OP_END
 
-/* File: c/OP_UNUSED_EF.c */
-HANDLE_OPCODE(OP_UNUSED_EF)
+/* File: c/OP_EXECUTE_INLINE_RANGE.c */
+HANDLE_OPCODE(OP_EXECUTE_INLINE_RANGE /*{vCCCC..v(CCCC+AA-1)}, inline@BBBB*/)
+    {
+        u4 arg0, arg1, arg2, arg3;
+        arg0 = arg1 = arg2 = arg3 = 0;      /* placate gcc */
+
+        EXPORT_PC();
+
+        vsrc1 = INST_AA(inst);      /* #of args */
+        ref = FETCH(1);             /* inline call "ref" */
+        vdst = FETCH(2);            /* range base */
+        ILOGV("|execute-inline-range args=%d @%d {regs=v%d-v%d}",
+            vsrc1, ref, vdst, vdst+vsrc1-1);
+
+        assert((vdst >> 16) == 0);  // 16-bit type -or- high 16 bits clear
+        assert(vsrc1 <= 4);
+
+        switch (vsrc1) {
+        case 4:
+            arg3 = GET_REGISTER(vdst+3);
+            /* fall through */
+        case 3:
+            arg2 = GET_REGISTER(vdst+2);
+            /* fall through */
+        case 2:
+            arg1 = GET_REGISTER(vdst+1);
+            /* fall through */
+        case 1:
+            arg0 = GET_REGISTER(vdst+0);
+            /* fall through */
+        default:        // case 0
+            ;
+        }
+
+#if INTERP_TYPE == INTERP_DBG
+        if (!dvmPerformInlineOp4Dbg(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#else
+        if (!dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#endif
+    }
+    FINISH(3);
 OP_END
 
 /* File: c/OP_INVOKE_DIRECT_EMPTY.c */
@@ -3565,6 +3641,10 @@ GOTO_TARGET(returnFromMethod)
         if (dvmIsBreakFrame(fp)) {
             /* bail without popping the method frame from stack */
             LOGVV("+++ returned into break frame\n");
+#if defined(WITH_JIT)
+            /* Let the Jit know the return is terminating normally */
+            CHECK_JIT();
+#endif
             GOTO_bail();
         }
 
@@ -3609,6 +3689,10 @@ GOTO_TARGET(exceptionThrown)
          */
         PERIODIC_CHECKS(kInterpEntryThrow, 0);
 
+#if defined(WITH_JIT)
+        // Something threw during trace selection - abort the current trace
+        ABORT_JIT_TSELECT();
+#endif
         /*
          * We save off the exception and clear the exception status.  While
          * processing the exception we might need to load some Throwable
@@ -3659,6 +3743,9 @@ GOTO_TARGET(exceptionThrown)
          *
          * If we do find a catch block, we want to transfer execution to
          * that point.
+         *
+         * Note this can cause an exception while resolving classes in
+         * the "catch" blocks.
          */
         catchRelPc = dvmFindCatchBlock(self, pc - curMethod->insns,
                     exception, false, (void*)&fp);
@@ -3673,9 +3760,19 @@ GOTO_TARGET(exceptionThrown)
          * Note we want to do this *after* the call to dvmFindCatchBlock,
          * because that may need extra stack space to resolve exception
          * classes (e.g. through a class loader).
+         *
+         * It's possible for the stack overflow handling to cause an
+         * exception (specifically, class resolution in a "catch" block
+         * during the call above), so we could see the thread's overflow
+         * flag raised but actually be running in a "nested" interpreter
+         * frame.  We don't allow doubled-up StackOverflowErrors, so
+         * we can check for this by just looking at the exception type
+         * in the cleanup function.  Also, we won't unroll past the SOE
+         * point because the more-recent exception will hit a break frame
+         * as it unrolls to here.
          */
         if (self->stackOverflowed)
-            dvmCleanupStackOverflow(self);
+            dvmCleanupStackOverflow(self, exception);
 
         if (catchRelPc < 0) {
             /* falling through to JNI code or off the bottom of the stack */
@@ -3840,10 +3937,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             bottom = (u1*) newSaveArea - methodToCall->outsSize * sizeof(u4);
             if (bottom < self->interpStackEnd) {
                 /* stack overflow */
-                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p size=%d '%s')\n",
+                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p(%d) size=%d '%s')\n",
                     self->interpStackStart, self->interpStackEnd, bottom,
-                    self->interpStackSize, methodToCall->name);
-                dvmHandleStackOverflow(self);
+                    (u1*) fp - bottom, self->interpStackSize,
+                    methodToCall->name);
+                dvmHandleStackOverflow(self, methodToCall);
                 assert(dvmCheckException(self));
                 GOTO_exceptionThrown();
             }
@@ -3918,6 +4016,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             ILOGD("> native <-- %s.%s %s", methodToCall->clazz->descriptor,
                 methodToCall->name, methodToCall->shorty);
 
+#if defined(WITH_JIT)
+            /* Allow the Jit to end any pending trace building */
+            CHECK_JIT();
+#endif
+
             /*
              * Jump through native call bridge.  Because we leave no
              * space for locals on native calls, "newFp" points directly
diff --git a/vm/mterp/out/InterpC-armv4t.c b/vm/mterp/out/InterpC-armv4t.c
index 6b82bc8..6c7c2e7 100644
--- a/vm/mterp/out/InterpC-armv4t.c
+++ b/vm/mterp/out/InterpC-armv4t.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -418,6 +422,10 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -536,7 +544,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
diff --git a/vm/mterp/out/InterpC-armv5te-vfp.c b/vm/mterp/out/InterpC-armv5te-vfp.c
index 7312700..2d2de9a 100644
--- a/vm/mterp/out/InterpC-armv5te-vfp.c
+++ b/vm/mterp/out/InterpC-armv5te-vfp.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -418,6 +422,10 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -536,7 +544,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
diff --git a/vm/mterp/out/InterpC-armv5te.c b/vm/mterp/out/InterpC-armv5te.c
index ea11551..b2a16aa 100644
--- a/vm/mterp/out/InterpC-armv5te.c
+++ b/vm/mterp/out/InterpC-armv5te.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -418,6 +422,10 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -536,7 +544,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
diff --git a/vm/mterp/out/InterpC-armv7-a.c b/vm/mterp/out/InterpC-armv7-a.c
index 97799ec..da058b2 100644
--- a/vm/mterp/out/InterpC-armv7-a.c
+++ b/vm/mterp/out/InterpC-armv7-a.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -418,6 +422,10 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -536,7 +544,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
diff --git a/vm/mterp/out/InterpC-portdbg.c b/vm/mterp/out/InterpC-portdbg.c
index 4b92639..c27582b 100644
--- a/vm/mterp/out/InterpC-portdbg.c
+++ b/vm/mterp/out/InterpC-portdbg.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -421,11 +425,11 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
     checkDebugAndProf(pc, fp, self, curMethod, &debugIsMethodEntry)
 
 #if defined(WITH_JIT)
-#define CHECK_JIT() \
-    if (dvmCheckJit(pc, self, interpState)) GOTO_bail_switch()
+#define CHECK_JIT() (dvmCheckJit(pc, self, interpState))
+#define ABORT_JIT_TSELECT() (dvmJitAbortTraceSelect(interpState))
 #else
-#define CHECK_JIT() \
-    ((void)0)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT(x) ((void)0)
 #endif
 
 /* File: portable/stubdefs.c */
@@ -450,6 +454,8 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
  *
  * Assumes the existence of "const u2* pc" and (for threaded operation)
  * "u2 inst".
+ *
+ * TODO: remove "switch" version.
  */
 #ifdef THREADED_INTERP
 # define H(_op)             &&op_##_op
@@ -459,12 +465,16 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         inst = FETCH(0);                                                    \
         CHECK_DEBUG_AND_PROF();                                             \
         CHECK_TRACKED_REFS();                                               \
-        CHECK_JIT();                                                        \
+        if (CHECK_JIT()) GOTO_bail_switch();                                \
         goto *handlerTable[INST_INST(inst)];                                \
     }
+# define FINISH_BKPT(_opcode) {                                             \
+        goto *handlerTable[_opcode];                                        \
+    }
 #else
 # define HANDLE_OPCODE(_op) case _op:
 # define FINISH(_offset)    { ADJUST_PC(_offset); break; }
+# define FINISH_BKPT(opcode) { > not implemented < }
 #endif
 
 #define OP_END
@@ -520,7 +530,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
@@ -1173,26 +1182,6 @@ GOTO_TARGET_DECL(exceptionThrown);
 /* code in here is only included in portable-debug interpreter */
 
 /*
- * Determine if an address is "interesting" to the debugger.  This allows
- * us to avoid scanning the entire event list before every instruction.
- *
- * The "debugBreakAddr" table is global and not synchronized.
- */
-static bool isInterestingAddr(const u2* pc)
-{
-    const u2** ptr = gDvm.debugBreakAddr;
-    int i;
-
-    for (i = 0; i < MAX_BREAKPOINTS; i++, ptr++) {
-        if (*ptr == pc) {
-            LOGV("BKP: hit on %p\n", pc);
-            return true;
-        }
-    }
-    return false;
-}
-
-/*
  * Update the debugger on interesting events, such as hitting a breakpoint
  * or a single-step point.  This is called from the top of the interpreter
  * loop, before the current instruction is processed.
@@ -1238,14 +1227,10 @@ static void updateDebugger(const Method* method, const u2* pc, const u4* fp,
      *
      * Depending on the "mods" associated with event(s) on this address,
      * we may or may not actually send a message to the debugger.
-     *
-     * Checking method->debugBreakpointCount is slower on the device than
-     * just scanning the table (!).  We could probably work something out
-     * where we just check it on method entry/exit and remember the result,
-     * but that's more fragile and requires passing more stuff around.
      */
 #ifdef WITH_DEBUGGER
-    if (method->debugBreakpointCount > 0 && isInterestingAddr(pc)) {
+    if (INST_INST(*pc) == OP_BREAKPOINT) {
+        LOGV("+++ breakpoint hit at %p\n", pc);
         eventFlags |= DBG_BREAKPOINT;
     }
 #endif
@@ -1493,18 +1478,26 @@ bool INTERP_FUNC_NAME(Thread* self, InterpState* interpState)
          interpState->pc,
          interpState->method->name);
 #endif
-
 #if INTERP_TYPE == INTERP_DBG
-    /* Check to see if we've got a trace selection request.  If we do,
-     * but something is amiss, revert to the fast interpreter.
-     */
-    if (dvmJitCheckTraceRequest(self,interpState)) {
+    /* Check to see if we've got a trace selection request. */
+    if (
+         /*
+          * Only perform dvmJitCheckTraceRequest if the entry point is
+          * EntryInstr and the jit state is either kJitTSelectRequest or
+          * kJitTSelectRequestHot. If debugger/profiler happens to be attached,
+          * dvmJitCheckTraceRequest will change the jitState to kJitDone but
+          * but stay in the dbg interpreter.
+          */
+         (interpState->entryPoint == kInterpEntryInstr) &&
+         (interpState->jitState == kJitTSelectRequest ||
+          interpState->jitState == kJitTSelectRequestHot) &&
+         dvmJitCheckTraceRequest(self, interpState)) {
         interpState->nextMode = INTERP_STD;
-        //LOGD("** something wrong, exiting\n");
+        //LOGD("Invalid trace request, exiting\n");
         return true;
     }
-#endif
-#endif
+#endif /* INTERP_TYPE == INTERP_DBG */
+#endif /* WITH_JIT */
 
     /* copy state in */
     curMethod = interpState->method;
@@ -1537,6 +1530,7 @@ bool INTERP_FUNC_NAME(Thread* self, InterpState* interpState)
         /* just fall through to instruction loop or threaded kickstart */
         break;
     case kInterpEntryReturn:
+        CHECK_JIT();
         goto returnFromMethod;
     case kInterpEntryThrow:
         goto exceptionThrown;
@@ -3157,8 +3151,35 @@ OP_END
 HANDLE_OPCODE(OP_UNUSED_EB)
 OP_END
 
-/* File: c/OP_UNUSED_EC.c */
-HANDLE_OPCODE(OP_UNUSED_EC)
+/* File: c/OP_BREAKPOINT.c */
+HANDLE_OPCODE(OP_BREAKPOINT)
+#if (INTERP_TYPE == INTERP_DBG) && defined(WITH_DEBUGGER)
+    {
+        /*
+         * Restart this instruction with the original opcode.  We do
+         * this by simply jumping to the handler.
+         *
+         * It's probably not necessary to update "inst", but we do it
+         * for the sake of anything that needs to do disambiguation in a
+         * common handler with INST_INST.
+         *
+         * The breakpoint itself is handled over in updateDebugger(),
+         * because we need to detect other events (method entry, single
+         * step) and report them in the same event packet, and we're not
+         * yet handling those through breakpoint instructions.  By the
+         * time we get here, the breakpoint has already been handled and
+         * the thread resumed.
+         */
+        u1 originalOpCode = dvmGetOriginalOpCode(pc);
+        LOGV("+++ break 0x%02x (0x%04x -> 0x%04x)\n", originalOpCode, inst,
+            INST_REPLACE_OP(inst, originalOpCode));
+        inst = INST_REPLACE_OP(inst, originalOpCode);
+        FINISH_BKPT(originalOpCode);
+    }
+#else
+    LOGE("Breakpoint hit in non-debug interpreter\n");
+    dvmAbort();
+#endif
 OP_END
 
 /* File: c/OP_THROW_VERIFICATION_ERROR.c */
@@ -3182,14 +3203,15 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
          * the rest uninitialized.  We're assuming that, if the method
          * needs them, they'll be specified in the call.
          *
-         * This annoys gcc when optimizations are enabled, causing a
-         * "may be used uninitialized" warning.  We can quiet the warnings
-         * for a slight penalty (5%: 373ns vs. 393ns on empty method).  Note
-         * that valgrind is perfectly happy with this arrangement, because
-         * the uninitialiezd values are never actually used.
+         * However, this annoys gcc when optimizations are enabled,
+         * causing a "may be used uninitialized" warning.  Quieting
+         * the warnings incurs a slight penalty (5%: 373ns vs. 393ns
+         * on empty method).  Note that valgrind is perfectly happy
+         * either way as the uninitialiezd values are never actually
+         * used.
          */
         u4 arg0, arg1, arg2, arg3;
-        //arg0 = arg1 = arg2 = arg3 = 0;
+        arg0 = arg1 = arg2 = arg3 = 0;
 
         EXPORT_PC();
 
@@ -3230,8 +3252,49 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
     FINISH(3);
 OP_END
 
-/* File: c/OP_UNUSED_EF.c */
-HANDLE_OPCODE(OP_UNUSED_EF)
+/* File: c/OP_EXECUTE_INLINE_RANGE.c */
+HANDLE_OPCODE(OP_EXECUTE_INLINE_RANGE /*{vCCCC..v(CCCC+AA-1)}, inline@BBBB*/)
+    {
+        u4 arg0, arg1, arg2, arg3;
+        arg0 = arg1 = arg2 = arg3 = 0;      /* placate gcc */
+
+        EXPORT_PC();
+
+        vsrc1 = INST_AA(inst);      /* #of args */
+        ref = FETCH(1);             /* inline call "ref" */
+        vdst = FETCH(2);            /* range base */
+        ILOGV("|execute-inline-range args=%d @%d {regs=v%d-v%d}",
+            vsrc1, ref, vdst, vdst+vsrc1-1);
+
+        assert((vdst >> 16) == 0);  // 16-bit type -or- high 16 bits clear
+        assert(vsrc1 <= 4);
+
+        switch (vsrc1) {
+        case 4:
+            arg3 = GET_REGISTER(vdst+3);
+            /* fall through */
+        case 3:
+            arg2 = GET_REGISTER(vdst+2);
+            /* fall through */
+        case 2:
+            arg1 = GET_REGISTER(vdst+1);
+            /* fall through */
+        case 1:
+            arg0 = GET_REGISTER(vdst+0);
+            /* fall through */
+        default:        // case 0
+            ;
+        }
+
+#if INTERP_TYPE == INTERP_DBG
+        if (!dvmPerformInlineOp4Dbg(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#else
+        if (!dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#endif
+    }
+    FINISH(3);
 OP_END
 
 /* File: c/OP_INVOKE_DIRECT_EMPTY.c */
@@ -3856,6 +3919,10 @@ GOTO_TARGET(returnFromMethod)
         if (dvmIsBreakFrame(fp)) {
             /* bail without popping the method frame from stack */
             LOGVV("+++ returned into break frame\n");
+#if defined(WITH_JIT)
+            /* Let the Jit know the return is terminating normally */
+            CHECK_JIT();
+#endif
             GOTO_bail();
         }
 
@@ -3900,6 +3967,10 @@ GOTO_TARGET(exceptionThrown)
          */
         PERIODIC_CHECKS(kInterpEntryThrow, 0);
 
+#if defined(WITH_JIT)
+        // Something threw during trace selection - abort the current trace
+        ABORT_JIT_TSELECT();
+#endif
         /*
          * We save off the exception and clear the exception status.  While
          * processing the exception we might need to load some Throwable
@@ -3950,6 +4021,9 @@ GOTO_TARGET(exceptionThrown)
          *
          * If we do find a catch block, we want to transfer execution to
          * that point.
+         *
+         * Note this can cause an exception while resolving classes in
+         * the "catch" blocks.
          */
         catchRelPc = dvmFindCatchBlock(self, pc - curMethod->insns,
                     exception, false, (void*)&fp);
@@ -3964,9 +4038,19 @@ GOTO_TARGET(exceptionThrown)
          * Note we want to do this *after* the call to dvmFindCatchBlock,
          * because that may need extra stack space to resolve exception
          * classes (e.g. through a class loader).
+         *
+         * It's possible for the stack overflow handling to cause an
+         * exception (specifically, class resolution in a "catch" block
+         * during the call above), so we could see the thread's overflow
+         * flag raised but actually be running in a "nested" interpreter
+         * frame.  We don't allow doubled-up StackOverflowErrors, so
+         * we can check for this by just looking at the exception type
+         * in the cleanup function.  Also, we won't unroll past the SOE
+         * point because the more-recent exception will hit a break frame
+         * as it unrolls to here.
          */
         if (self->stackOverflowed)
-            dvmCleanupStackOverflow(self);
+            dvmCleanupStackOverflow(self, exception);
 
         if (catchRelPc < 0) {
             /* falling through to JNI code or off the bottom of the stack */
@@ -4131,10 +4215,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             bottom = (u1*) newSaveArea - methodToCall->outsSize * sizeof(u4);
             if (bottom < self->interpStackEnd) {
                 /* stack overflow */
-                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p size=%d '%s')\n",
+                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p(%d) size=%d '%s')\n",
                     self->interpStackStart, self->interpStackEnd, bottom,
-                    self->interpStackSize, methodToCall->name);
-                dvmHandleStackOverflow(self);
+                    (u1*) fp - bottom, self->interpStackSize,
+                    methodToCall->name);
+                dvmHandleStackOverflow(self, methodToCall);
                 assert(dvmCheckException(self));
                 GOTO_exceptionThrown();
             }
@@ -4209,6 +4294,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             ILOGD("> native <-- %s.%s %s", methodToCall->clazz->descriptor,
                 methodToCall->name, methodToCall->shorty);
 
+#if defined(WITH_JIT)
+            /* Allow the Jit to end any pending trace building */
+            CHECK_JIT();
+#endif
+
             /*
              * Jump through native call bridge.  Because we leave no
              * space for locals on native calls, "newFp" points directly
diff --git a/vm/mterp/out/InterpC-portstd.c b/vm/mterp/out/InterpC-portstd.c
index 1db6e87..1e7550c 100644
--- a/vm/mterp/out/InterpC-portstd.c
+++ b/vm/mterp/out/InterpC-portstd.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -419,7 +423,8 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 
-#define CHECK_JIT() ((void)0)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
 
 /* File: portable/stubdefs.c */
 /*
@@ -443,6 +448,8 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
  *
  * Assumes the existence of "const u2* pc" and (for threaded operation)
  * "u2 inst".
+ *
+ * TODO: remove "switch" version.
  */
 #ifdef THREADED_INTERP
 # define H(_op)             &&op_##_op
@@ -452,12 +459,16 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         inst = FETCH(0);                                                    \
         CHECK_DEBUG_AND_PROF();                                             \
         CHECK_TRACKED_REFS();                                               \
-        CHECK_JIT();                                                        \
+        if (CHECK_JIT()) GOTO_bail_switch();                                \
         goto *handlerTable[INST_INST(inst)];                                \
     }
+# define FINISH_BKPT(_opcode) {                                             \
+        goto *handlerTable[_opcode];                                        \
+    }
 #else
 # define HANDLE_OPCODE(_op) case _op:
 # define FINISH(_offset)    { ADJUST_PC(_offset); break; }
+# define FINISH_BKPT(opcode) { > not implemented < }
 #endif
 
 #define OP_END
@@ -513,7 +524,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
@@ -1207,18 +1217,26 @@ bool INTERP_FUNC_NAME(Thread* self, InterpState* interpState)
          interpState->pc,
          interpState->method->name);
 #endif
-
 #if INTERP_TYPE == INTERP_DBG
-    /* Check to see if we've got a trace selection request.  If we do,
-     * but something is amiss, revert to the fast interpreter.
-     */
-    if (dvmJitCheckTraceRequest(self,interpState)) {
+    /* Check to see if we've got a trace selection request. */
+    if (
+         /*
+          * Only perform dvmJitCheckTraceRequest if the entry point is
+          * EntryInstr and the jit state is either kJitTSelectRequest or
+          * kJitTSelectRequestHot. If debugger/profiler happens to be attached,
+          * dvmJitCheckTraceRequest will change the jitState to kJitDone but
+          * but stay in the dbg interpreter.
+          */
+         (interpState->entryPoint == kInterpEntryInstr) &&
+         (interpState->jitState == kJitTSelectRequest ||
+          interpState->jitState == kJitTSelectRequestHot) &&
+         dvmJitCheckTraceRequest(self, interpState)) {
         interpState->nextMode = INTERP_STD;
-        //LOGD("** something wrong, exiting\n");
+        //LOGD("Invalid trace request, exiting\n");
         return true;
     }
-#endif
-#endif
+#endif /* INTERP_TYPE == INTERP_DBG */
+#endif /* WITH_JIT */
 
     /* copy state in */
     curMethod = interpState->method;
@@ -1251,6 +1269,7 @@ bool INTERP_FUNC_NAME(Thread* self, InterpState* interpState)
         /* just fall through to instruction loop or threaded kickstart */
         break;
     case kInterpEntryReturn:
+        CHECK_JIT();
         goto returnFromMethod;
     case kInterpEntryThrow:
         goto exceptionThrown;
@@ -2871,8 +2890,35 @@ OP_END
 HANDLE_OPCODE(OP_UNUSED_EB)
 OP_END
 
-/* File: c/OP_UNUSED_EC.c */
-HANDLE_OPCODE(OP_UNUSED_EC)
+/* File: c/OP_BREAKPOINT.c */
+HANDLE_OPCODE(OP_BREAKPOINT)
+#if (INTERP_TYPE == INTERP_DBG) && defined(WITH_DEBUGGER)
+    {
+        /*
+         * Restart this instruction with the original opcode.  We do
+         * this by simply jumping to the handler.
+         *
+         * It's probably not necessary to update "inst", but we do it
+         * for the sake of anything that needs to do disambiguation in a
+         * common handler with INST_INST.
+         *
+         * The breakpoint itself is handled over in updateDebugger(),
+         * because we need to detect other events (method entry, single
+         * step) and report them in the same event packet, and we're not
+         * yet handling those through breakpoint instructions.  By the
+         * time we get here, the breakpoint has already been handled and
+         * the thread resumed.
+         */
+        u1 originalOpCode = dvmGetOriginalOpCode(pc);
+        LOGV("+++ break 0x%02x (0x%04x -> 0x%04x)\n", originalOpCode, inst,
+            INST_REPLACE_OP(inst, originalOpCode));
+        inst = INST_REPLACE_OP(inst, originalOpCode);
+        FINISH_BKPT(originalOpCode);
+    }
+#else
+    LOGE("Breakpoint hit in non-debug interpreter\n");
+    dvmAbort();
+#endif
 OP_END
 
 /* File: c/OP_THROW_VERIFICATION_ERROR.c */
@@ -2896,14 +2942,15 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
          * the rest uninitialized.  We're assuming that, if the method
          * needs them, they'll be specified in the call.
          *
-         * This annoys gcc when optimizations are enabled, causing a
-         * "may be used uninitialized" warning.  We can quiet the warnings
-         * for a slight penalty (5%: 373ns vs. 393ns on empty method).  Note
-         * that valgrind is perfectly happy with this arrangement, because
-         * the uninitialiezd values are never actually used.
+         * However, this annoys gcc when optimizations are enabled,
+         * causing a "may be used uninitialized" warning.  Quieting
+         * the warnings incurs a slight penalty (5%: 373ns vs. 393ns
+         * on empty method).  Note that valgrind is perfectly happy
+         * either way as the uninitialiezd values are never actually
+         * used.
          */
         u4 arg0, arg1, arg2, arg3;
-        //arg0 = arg1 = arg2 = arg3 = 0;
+        arg0 = arg1 = arg2 = arg3 = 0;
 
         EXPORT_PC();
 
@@ -2944,8 +2991,49 @@ HANDLE_OPCODE(OP_EXECUTE_INLINE /*vB, {vD, vE, vF, vG}, inline@CCCC*/)
     FINISH(3);
 OP_END
 
-/* File: c/OP_UNUSED_EF.c */
-HANDLE_OPCODE(OP_UNUSED_EF)
+/* File: c/OP_EXECUTE_INLINE_RANGE.c */
+HANDLE_OPCODE(OP_EXECUTE_INLINE_RANGE /*{vCCCC..v(CCCC+AA-1)}, inline@BBBB*/)
+    {
+        u4 arg0, arg1, arg2, arg3;
+        arg0 = arg1 = arg2 = arg3 = 0;      /* placate gcc */
+
+        EXPORT_PC();
+
+        vsrc1 = INST_AA(inst);      /* #of args */
+        ref = FETCH(1);             /* inline call "ref" */
+        vdst = FETCH(2);            /* range base */
+        ILOGV("|execute-inline-range args=%d @%d {regs=v%d-v%d}",
+            vsrc1, ref, vdst, vdst+vsrc1-1);
+
+        assert((vdst >> 16) == 0);  // 16-bit type -or- high 16 bits clear
+        assert(vsrc1 <= 4);
+
+        switch (vsrc1) {
+        case 4:
+            arg3 = GET_REGISTER(vdst+3);
+            /* fall through */
+        case 3:
+            arg2 = GET_REGISTER(vdst+2);
+            /* fall through */
+        case 2:
+            arg1 = GET_REGISTER(vdst+1);
+            /* fall through */
+        case 1:
+            arg0 = GET_REGISTER(vdst+0);
+            /* fall through */
+        default:        // case 0
+            ;
+        }
+
+#if INTERP_TYPE == INTERP_DBG
+        if (!dvmPerformInlineOp4Dbg(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#else
+        if (!dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#endif
+    }
+    FINISH(3);
 OP_END
 
 /* File: c/OP_INVOKE_DIRECT_EMPTY.c */
@@ -3570,6 +3658,10 @@ GOTO_TARGET(returnFromMethod)
         if (dvmIsBreakFrame(fp)) {
             /* bail without popping the method frame from stack */
             LOGVV("+++ returned into break frame\n");
+#if defined(WITH_JIT)
+            /* Let the Jit know the return is terminating normally */
+            CHECK_JIT();
+#endif
             GOTO_bail();
         }
 
@@ -3614,6 +3706,10 @@ GOTO_TARGET(exceptionThrown)
          */
         PERIODIC_CHECKS(kInterpEntryThrow, 0);
 
+#if defined(WITH_JIT)
+        // Something threw during trace selection - abort the current trace
+        ABORT_JIT_TSELECT();
+#endif
         /*
          * We save off the exception and clear the exception status.  While
          * processing the exception we might need to load some Throwable
@@ -3664,6 +3760,9 @@ GOTO_TARGET(exceptionThrown)
          *
          * If we do find a catch block, we want to transfer execution to
          * that point.
+         *
+         * Note this can cause an exception while resolving classes in
+         * the "catch" blocks.
          */
         catchRelPc = dvmFindCatchBlock(self, pc - curMethod->insns,
                     exception, false, (void*)&fp);
@@ -3678,9 +3777,19 @@ GOTO_TARGET(exceptionThrown)
          * Note we want to do this *after* the call to dvmFindCatchBlock,
          * because that may need extra stack space to resolve exception
          * classes (e.g. through a class loader).
+         *
+         * It's possible for the stack overflow handling to cause an
+         * exception (specifically, class resolution in a "catch" block
+         * during the call above), so we could see the thread's overflow
+         * flag raised but actually be running in a "nested" interpreter
+         * frame.  We don't allow doubled-up StackOverflowErrors, so
+         * we can check for this by just looking at the exception type
+         * in the cleanup function.  Also, we won't unroll past the SOE
+         * point because the more-recent exception will hit a break frame
+         * as it unrolls to here.
          */
         if (self->stackOverflowed)
-            dvmCleanupStackOverflow(self);
+            dvmCleanupStackOverflow(self, exception);
 
         if (catchRelPc < 0) {
             /* falling through to JNI code or off the bottom of the stack */
@@ -3845,10 +3954,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             bottom = (u1*) newSaveArea - methodToCall->outsSize * sizeof(u4);
             if (bottom < self->interpStackEnd) {
                 /* stack overflow */
-                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p size=%d '%s')\n",
+                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p(%d) size=%d '%s')\n",
                     self->interpStackStart, self->interpStackEnd, bottom,
-                    self->interpStackSize, methodToCall->name);
-                dvmHandleStackOverflow(self);
+                    (u1*) fp - bottom, self->interpStackSize,
+                    methodToCall->name);
+                dvmHandleStackOverflow(self, methodToCall);
                 assert(dvmCheckException(self));
                 GOTO_exceptionThrown();
             }
@@ -3923,6 +4033,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             ILOGD("> native <-- %s.%s %s", methodToCall->clazz->descriptor,
                 methodToCall->name, methodToCall->shorty);
 
+#if defined(WITH_JIT)
+            /* Allow the Jit to end any pending trace building */
+            CHECK_JIT();
+#endif
+
             /*
              * Jump through native call bridge.  Because we leave no
              * space for locals on native calls, "newFp" points directly
diff --git a/vm/mterp/out/InterpC-x86.c b/vm/mterp/out/InterpC-x86.c
index 07536c4..6a3c442 100644
--- a/vm/mterp/out/InterpC-x86.c
+++ b/vm/mterp/out/InterpC-x86.c
@@ -302,6 +302,11 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #define INST_INST(_inst)    ((_inst) & 0xff)
 
 /*
+ * Replace the opcode (used when handling breakpoints).  _opcode is a u1.
+ */
+#define INST_REPLACE_OP(_inst, _opcode) (((_inst) & 0xff00) | _opcode)
+
+/*
  * Extract the "vA, vB" 4-bit registers from the instruction word (_inst is u2).
  */
 #define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
@@ -338,8 +343,7 @@ static inline void putDoubleToArray(u4* ptr, int idx, double dval)
 #if defined(WITH_JIT)
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
-        dvmJitDebuggerOrProfilerActive(interpState->jitState) :             \
-        !dvmJitDebuggerOrProfilerActive(interpState->jitState) )
+        dvmJitDebuggerOrProfilerActive() : !dvmJitDebuggerOrProfilerActive() )
 #else
 # define NEED_INTERP_SWITCH(_current) (                                     \
     (_current == INTERP_STD) ?                                              \
@@ -418,6 +422,10 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
 #define INTERP_TYPE INTERP_STD
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 # define CHECK_TRACKED_REFS() ((void)0)
+#if defined(WITH_JIT)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
+#endif
 
 /*
  * In the C mterp stubs, "goto" is a function call followed immediately
@@ -536,7 +544,6 @@ static inline bool checkForNullExportPC(Object* obj, u4* fp, const u2* pc)
         }                                                                   \
     }
 
-
 /* File: c/opcommon.c */
 /* forward declarations of goto targets */
 GOTO_TARGET_DECL(filledNewArray, bool methodCallRange);
@@ -1185,6 +1192,51 @@ GOTO_TARGET_DECL(exceptionThrown);
     FINISH(2);
 
 
+/* File: c/OP_EXECUTE_INLINE_RANGE.c */
+HANDLE_OPCODE(OP_EXECUTE_INLINE_RANGE /*{vCCCC..v(CCCC+AA-1)}, inline@BBBB*/)
+    {
+        u4 arg0, arg1, arg2, arg3;
+        arg0 = arg1 = arg2 = arg3 = 0;      /* placate gcc */
+
+        EXPORT_PC();
+
+        vsrc1 = INST_AA(inst);      /* #of args */
+        ref = FETCH(1);             /* inline call "ref" */
+        vdst = FETCH(2);            /* range base */
+        ILOGV("|execute-inline-range args=%d @%d {regs=v%d-v%d}",
+            vsrc1, ref, vdst, vdst+vsrc1-1);
+
+        assert((vdst >> 16) == 0);  // 16-bit type -or- high 16 bits clear
+        assert(vsrc1 <= 4);
+
+        switch (vsrc1) {
+        case 4:
+            arg3 = GET_REGISTER(vdst+3);
+            /* fall through */
+        case 3:
+            arg2 = GET_REGISTER(vdst+2);
+            /* fall through */
+        case 2:
+            arg1 = GET_REGISTER(vdst+1);
+            /* fall through */
+        case 1:
+            arg0 = GET_REGISTER(vdst+0);
+            /* fall through */
+        default:        // case 0
+            ;
+        }
+
+#if INTERP_TYPE == INTERP_DBG
+        if (!dvmPerformInlineOp4Dbg(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#else
+        if (!dvmPerformInlineOp4Std(arg0, arg1, arg2, arg3, &retval, ref))
+            GOTO_exceptionThrown();
+#endif
+    }
+    FINISH(3);
+OP_END
+
 /* File: c/gotoTargets.c */
 /*
  * C footer.  This has some common code shared by the various targets.
@@ -1720,6 +1772,10 @@ GOTO_TARGET(returnFromMethod)
         if (dvmIsBreakFrame(fp)) {
             /* bail without popping the method frame from stack */
             LOGVV("+++ returned into break frame\n");
+#if defined(WITH_JIT)
+            /* Let the Jit know the return is terminating normally */
+            CHECK_JIT();
+#endif
             GOTO_bail();
         }
 
@@ -1764,6 +1820,10 @@ GOTO_TARGET(exceptionThrown)
          */
         PERIODIC_CHECKS(kInterpEntryThrow, 0);
 
+#if defined(WITH_JIT)
+        // Something threw during trace selection - abort the current trace
+        ABORT_JIT_TSELECT();
+#endif
         /*
          * We save off the exception and clear the exception status.  While
          * processing the exception we might need to load some Throwable
@@ -1814,6 +1874,9 @@ GOTO_TARGET(exceptionThrown)
          *
          * If we do find a catch block, we want to transfer execution to
          * that point.
+         *
+         * Note this can cause an exception while resolving classes in
+         * the "catch" blocks.
          */
         catchRelPc = dvmFindCatchBlock(self, pc - curMethod->insns,
                     exception, false, (void*)&fp);
@@ -1828,9 +1891,19 @@ GOTO_TARGET(exceptionThrown)
          * Note we want to do this *after* the call to dvmFindCatchBlock,
          * because that may need extra stack space to resolve exception
          * classes (e.g. through a class loader).
+         *
+         * It's possible for the stack overflow handling to cause an
+         * exception (specifically, class resolution in a "catch" block
+         * during the call above), so we could see the thread's overflow
+         * flag raised but actually be running in a "nested" interpreter
+         * frame.  We don't allow doubled-up StackOverflowErrors, so
+         * we can check for this by just looking at the exception type
+         * in the cleanup function.  Also, we won't unroll past the SOE
+         * point because the more-recent exception will hit a break frame
+         * as it unrolls to here.
          */
         if (self->stackOverflowed)
-            dvmCleanupStackOverflow(self);
+            dvmCleanupStackOverflow(self, exception);
 
         if (catchRelPc < 0) {
             /* falling through to JNI code or off the bottom of the stack */
@@ -1995,10 +2068,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             bottom = (u1*) newSaveArea - methodToCall->outsSize * sizeof(u4);
             if (bottom < self->interpStackEnd) {
                 /* stack overflow */
-                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p size=%d '%s')\n",
+                LOGV("Stack overflow on method call (start=%p end=%p newBot=%p(%d) size=%d '%s')\n",
                     self->interpStackStart, self->interpStackEnd, bottom,
-                    self->interpStackSize, methodToCall->name);
-                dvmHandleStackOverflow(self);
+                    (u1*) fp - bottom, self->interpStackSize,
+                    methodToCall->name);
+                dvmHandleStackOverflow(self, methodToCall);
                 assert(dvmCheckException(self));
                 GOTO_exceptionThrown();
             }
@@ -2073,6 +2147,11 @@ GOTO_TARGET(invokeMethod, bool methodCallRange, const Method* _methodToCall,
             ILOGD("> native <-- %s.%s %s", methodToCall->clazz->descriptor,
                 methodToCall->name, methodToCall->shorty);
 
+#if defined(WITH_JIT)
+            /* Allow the Jit to end any pending trace building */
+            CHECK_JIT();
+#endif
+
             /*
              * Jump through native call bridge.  Because we leave no
              * space for locals on native calls, "newFp" points directly
diff --git a/vm/mterp/portable/debug.c b/vm/mterp/portable/debug.c
index 449d49b..6716aba 100644
--- a/vm/mterp/portable/debug.c
+++ b/vm/mterp/portable/debug.c
@@ -1,26 +1,6 @@
 /* code in here is only included in portable-debug interpreter */
 
 /*
- * Determine if an address is "interesting" to the debugger.  This allows
- * us to avoid scanning the entire event list before every instruction.
- *
- * The "debugBreakAddr" table is global and not synchronized.
- */
-static bool isInterestingAddr(const u2* pc)
-{
-    const u2** ptr = gDvm.debugBreakAddr;
-    int i;
-
-    for (i = 0; i < MAX_BREAKPOINTS; i++, ptr++) {
-        if (*ptr == pc) {
-            LOGV("BKP: hit on %p\n", pc);
-            return true;
-        }
-    }
-    return false;
-}
-
-/*
  * Update the debugger on interesting events, such as hitting a breakpoint
  * or a single-step point.  This is called from the top of the interpreter
  * loop, before the current instruction is processed.
@@ -66,14 +46,10 @@ static void updateDebugger(const Method* method, const u2* pc, const u4* fp,
      *
      * Depending on the "mods" associated with event(s) on this address,
      * we may or may not actually send a message to the debugger.
-     *
-     * Checking method->debugBreakpointCount is slower on the device than
-     * just scanning the table (!).  We could probably work something out
-     * where we just check it on method entry/exit and remember the result,
-     * but that's more fragile and requires passing more stuff around.
      */
 #ifdef WITH_DEBUGGER
-    if (method->debugBreakpointCount > 0 && isInterestingAddr(pc)) {
+    if (INST_INST(*pc) == OP_BREAKPOINT) {
+        LOGV("+++ breakpoint hit at %p\n", pc);
         eventFlags |= DBG_BREAKPOINT;
     }
 #endif
diff --git a/vm/mterp/portable/entry.c b/vm/mterp/portable/entry.c
index 9c7c2d6..dbd5561 100644
--- a/vm/mterp/portable/entry.c
+++ b/vm/mterp/portable/entry.c
@@ -42,18 +42,26 @@ bool INTERP_FUNC_NAME(Thread* self, InterpState* interpState)
          interpState->pc,
          interpState->method->name);
 #endif
-
 #if INTERP_TYPE == INTERP_DBG
-    /* Check to see if we've got a trace selection request.  If we do,
-     * but something is amiss, revert to the fast interpreter.
-     */
-    if (dvmJitCheckTraceRequest(self,interpState)) {
+    /* Check to see if we've got a trace selection request. */
+    if (
+         /*
+          * Only perform dvmJitCheckTraceRequest if the entry point is
+          * EntryInstr and the jit state is either kJitTSelectRequest or
+          * kJitTSelectRequestHot. If debugger/profiler happens to be attached,
+          * dvmJitCheckTraceRequest will change the jitState to kJitDone but
+          * but stay in the dbg interpreter.
+          */
+         (interpState->entryPoint == kInterpEntryInstr) &&
+         (interpState->jitState == kJitTSelectRequest ||
+          interpState->jitState == kJitTSelectRequestHot) &&
+         dvmJitCheckTraceRequest(self, interpState)) {
         interpState->nextMode = INTERP_STD;
-        //LOGD("** something wrong, exiting\n");
+        //LOGD("Invalid trace request, exiting\n");
         return true;
     }
-#endif
-#endif
+#endif /* INTERP_TYPE == INTERP_DBG */
+#endif /* WITH_JIT */
 
     /* copy state in */
     curMethod = interpState->method;
@@ -86,6 +94,7 @@ bool INTERP_FUNC_NAME(Thread* self, InterpState* interpState)
         /* just fall through to instruction loop or threaded kickstart */
         break;
     case kInterpEntryReturn:
+        CHECK_JIT();
         goto returnFromMethod;
     case kInterpEntryThrow:
         goto exceptionThrown;
diff --git a/vm/mterp/portable/portdbg.c b/vm/mterp/portable/portdbg.c
index 04132cb..030a515 100644
--- a/vm/mterp/portable/portdbg.c
+++ b/vm/mterp/portable/portdbg.c
@@ -5,9 +5,9 @@
     checkDebugAndProf(pc, fp, self, curMethod, &debugIsMethodEntry)
 
 #if defined(WITH_JIT)
-#define CHECK_JIT() \
-    if (dvmCheckJit(pc, self, interpState)) GOTO_bail_switch()
+#define CHECK_JIT() (dvmCheckJit(pc, self, interpState))
+#define ABORT_JIT_TSELECT() (dvmJitAbortTraceSelect(interpState))
 #else
-#define CHECK_JIT() \
-    ((void)0)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT(x) ((void)0)
 #endif
diff --git a/vm/mterp/portable/portstd.c b/vm/mterp/portable/portstd.c
index f55e8e7..e2d8b10 100644
--- a/vm/mterp/portable/portstd.c
+++ b/vm/mterp/portable/portstd.c
@@ -3,4 +3,5 @@
 
 #define CHECK_DEBUG_AND_PROF() ((void)0)
 
-#define CHECK_JIT() ((void)0)
+#define CHECK_JIT() (0)
+#define ABORT_JIT_TSELECT() ((void)0)
diff --git a/vm/mterp/portable/stubdefs.c b/vm/mterp/portable/stubdefs.c
index 305aebb..29258fc 100644
--- a/vm/mterp/portable/stubdefs.c
+++ b/vm/mterp/portable/stubdefs.c
@@ -19,6 +19,8 @@
  *
  * Assumes the existence of "const u2* pc" and (for threaded operation)
  * "u2 inst".
+ *
+ * TODO: remove "switch" version.
  */
 #ifdef THREADED_INTERP
 # define H(_op)             &&op_##_op
@@ -28,12 +30,16 @@
         inst = FETCH(0);                                                    \
         CHECK_DEBUG_AND_PROF();                                             \
         CHECK_TRACKED_REFS();                                               \
-        CHECK_JIT();                                                        \
+        if (CHECK_JIT()) GOTO_bail_switch();                                \
         goto *handlerTable[INST_INST(inst)];                                \
     }
+# define FINISH_BKPT(_opcode) {                                             \
+        goto *handlerTable[_opcode];                                        \
+    }
 #else
 # define HANDLE_OPCODE(_op) case _op:
 # define FINISH(_offset)    { ADJUST_PC(_offset); break; }
+# define FINISH_BKPT(opcode) { > not implemented < }
 #endif
 
 #define OP_END
@@ -88,4 +94,3 @@
             GOTO_bail_switch();                                             \
         }                                                                   \
     }
-
diff --git a/vm/mterp/rebuild.sh b/vm/mterp/rebuild.sh
index 32c9007..841546f 100755
--- a/vm/mterp/rebuild.sh
+++ b/vm/mterp/rebuild.sh
@@ -19,7 +19,8 @@
 # generated as part of the build.
 #
 set -e
-for arch in portstd portdbg allstubs armv4t armv5te armv5te-vfp armv7-a x86; do TARGET_ARCH_EXT=$arch make -f Makefile-mterp; done
+
+for arch in portstd portdbg allstubs armv4t armv5te armv5te-vfp armv7-a armv7-a-neon x86 x86-atom; do TARGET_ARCH_EXT=$arch make -f Makefile-mterp; done
 
 # These aren't actually used, so just go ahead and remove them.  The correct
 # approach is to prevent them from being generated in the first place, but
diff --git a/vm/mterp/x86/OP_MONITOR_EXIT.S b/vm/mterp/x86/OP_MONITOR_EXIT.S
index b360a3e..788b7a7 100644
--- a/vm/mterp/x86/OP_MONITOR_EXIT.S
+++ b/vm/mterp/x86/OP_MONITOR_EXIT.S
@@ -14,7 +14,7 @@
     GET_GLUE(%ecx)
     EXPORT_PC()
     testl   %eax,%eax                   # null object?
-    je      common_errNullObject        # go if so
+    je      .L${opcode}_errNullObject   # go if so
     movl    offGlue_self(%ecx),%ecx     # ecx<- glue->self
     movl    %eax,OUT_ARG1(%esp)
     SPILL(rPC)
@@ -27,6 +27,9 @@
     UNSPILL(rPC)
     FETCH_INST_WORD(1)
     testl   %eax,%eax                   # success?
-    je      common_exceptionThrown      # no, exception pending
     ADVANCE_PC(1)
+    je      common_exceptionThrown      # no, exception pending
     GOTO_NEXT
+.L${opcode}_errNullObject:
+    ADVANCE_PC(1)                       # advance before throw
+    jmp     common_errNullObject
diff --git a/vm/mterp/x86/OP_UNUSED_EC.S b/vm/mterp/x86/OP_UNUSED_EC.S
deleted file mode 100644
index 31d98c1..0000000
--- a/vm/mterp/x86/OP_UNUSED_EC.S
+++ /dev/null
@@ -1 +0,0 @@
-%include "x86/unused.S"
diff --git a/vm/mterp/x86/OP_UNUSED_EF.S b/vm/mterp/x86/OP_UNUSED_EF.S
deleted file mode 100644
index 31d98c1..0000000
--- a/vm/mterp/x86/OP_UNUSED_EF.S
+++ /dev/null
@@ -1 +0,0 @@
-%include "x86/unused.S"
diff --git a/vm/mterp/x86/footer.S b/vm/mterp/x86/footer.S
index c39fa16..8ed6c66 100644
--- a/vm/mterp/x86/footer.S
+++ b/vm/mterp/x86/footer.S
@@ -227,11 +227,12 @@ common_invokeMethodNoRange:
     ADVANCE_PC(3)
     GOTO_NEXT                           # jump to next instruction
 
-.LstackOverflow:
+.LstackOverflow:    # eax=methodToCall
+    movl        %eax, OUT_ARG1(%esp)    # push parameter methodToCall
     GET_GLUE(%eax)                      # %eax<- pMterpGlue
     movl        offGlue_self(%eax), %eax # %eax<- glue->self
     movl        %eax, OUT_ARG0(%esp)    # push parameter self
-    call        dvmHandleStackOverflow  # call: (Thread* self)
+    call        dvmHandleStackOverflow  # call: (Thread* self, Method* meth)
     UNSPILL(rPC)                        # return: void
     jmp         common_exceptionThrown  # handle exception
 
diff --git a/vm/native/InternalNative.c b/vm/native/InternalNative.c
index 735cf96..9b6b801 100644
--- a/vm/native/InternalNative.c
+++ b/vm/native/InternalNative.c
@@ -285,15 +285,6 @@ u4 dvmFixMethodFlags(u4 flags)
 }
 
 
-/*
- * Return the hash code for the specified object.
- */
-u4 dvmGetObjectHashCode(Object* obj)
-{
-    return (u4) obj;
-}
-
-
 #define NUM_DOPRIV_FUNCS    4
 
 /*
diff --git a/vm/native/InternalNativePriv.h b/vm/native/InternalNativePriv.h
index abfda6c..bcd9119 100644
--- a/vm/native/InternalNativePriv.h
+++ b/vm/native/InternalNativePriv.h
@@ -77,11 +77,6 @@ void dvmFreeDexOrJar(void* vptr);
  */
 bool dvmIsPrivilegedMethod(const Method* method);
 
-/*
- * Return the hash code for the specified object.
- */
-u4 dvmGetObjectHashCode(Object* obj);
-
 
 /*
  * Tables of methods.
diff --git a/vm/native/dalvik_system_VMDebug.c b/vm/native/dalvik_system_VMDebug.c
index 8aa371d..34398d0 100644
--- a/vm/native/dalvik_system_VMDebug.c
+++ b/vm/native/dalvik_system_VMDebug.c
@@ -20,16 +20,103 @@
 #include "Dalvik.h"
 #include "native/InternalNativePriv.h"
 
+#include <errno.h>
+
+
+/*
+ * Convert an array of char* into a String[].
+ *
+ * Returns NULL on failure, with an exception raised.
+ */
+static ArrayObject* convertStringArray(char** strings, size_t count)
+{
+    /*
+     * Allocate an array to hold the String objects.
+     */
+    ClassObject* stringArrayClass =
+        dvmFindArrayClass("[Ljava/lang/String;", NULL);
+    if (stringArrayClass == NULL) {
+        /* shouldn't happen */
+        LOGE("Unable to find [Ljava/lang/String;\n");
+        dvmAbort();
+    }
+
+    ArrayObject* stringArray =
+        dvmAllocArrayByClass(stringArrayClass, count, ALLOC_DEFAULT);
+    if (stringArray == NULL) {
+        /* probably OOM */
+        LOGD("Failed allocating array of %d strings\n", count);
+        return NULL;
+    }
+
+    Thread* self = dvmThreadSelf();
+
+    /*
+     * Create the individual String objects and add them to the array.
+     */
+    StringObject** contents = (StringObject**) stringArray->contents;
+    size_t i;
+    for (i = 0; i < count; i++) {
+        contents[i] = dvmCreateStringFromCstr(strings[i], ALLOC_DEFAULT);
+        if (contents[i] == NULL) {
+            /* probably OOM; drop out now */
+            assert(dvmCheckException(dvmThreadSelf()));
+            dvmReleaseTrackedAlloc((Object*)stringArray, self);
+            return NULL;
+        }
+
+        /* stored in tracked array, okay to release */
+        dvmReleaseTrackedAlloc((Object*)contents[i], self);
+    }
+
+    dvmReleaseTrackedAlloc((Object*)stringArray, self);
+    return stringArray;
+}
+
+/*
+ * static String[] getVmFeatureList()
+ *
+ * Return a set of strings describing available VM features (this is chiefly
+ * of interest to DDMS).  Some features may be controlled by compile-time
+ * or command-line flags.
+ */
+static void Dalvik_dalvik_system_VMDebug_getVmFeatureList(const u4* args,
+    JValue* pResult)
+{
+    static const int MAX_FEATURE_COUNT = 10;
+    char* features[MAX_FEATURE_COUNT];
+    int idx = 0;
+
+#ifdef WITH_PROFILER
+    /* VM responds to DDMS method profiling requests */
+    features[idx++] = "method-trace-profiling";
+    features[idx++] = "method-trace-profiling-streaming";
+#endif
+#ifdef WITH_HPROF
+    /* VM responds to DDMS heap dump requests */
+    features[idx++] = "hprof-heap-dump";
+    features[idx++] = "hprof-heap-dump-streaming";
+#endif
+
+    assert(idx <= MAX_FEATURE_COUNT);
+
+    LOGV("+++ sending up %d features\n", idx);
+    ArrayObject* arrayObj = convertStringArray(features, idx);
+    RETURN_PTR(arrayObj);       /* will be null on OOM */
+}
+
 
 #ifdef WITH_PROFILER
 /* These must match the values in dalvik.system.VMDebug.
  */
 enum {
-    KIND_ALLOCATED_OBJECTS = 1<<0,
-    KIND_ALLOCATED_BYTES   = 1<<1,
-    KIND_FREED_OBJECTS     = 1<<2,
-    KIND_FREED_BYTES       = 1<<3,
-    KIND_GC_INVOCATIONS    = 1<<4,
+    KIND_ALLOCATED_OBJECTS      = 1<<0,
+    KIND_ALLOCATED_BYTES        = 1<<1,
+    KIND_FREED_OBJECTS          = 1<<2,
+    KIND_FREED_BYTES            = 1<<3,
+    KIND_GC_INVOCATIONS         = 1<<4,
+    KIND_CLASS_INIT_COUNT       = 1<<5,
+    KIND_CLASS_INIT_TIME        = 1<<6,
 #if PROFILE_EXTERNAL_ALLOCATIONS
     KIND_EXT_ALLOCATED_OBJECTS = 1<<12,
     KIND_EXT_ALLOCATED_BYTES   = 1<<13,
@@ -42,6 +129,8 @@ enum {
     KIND_GLOBAL_FREED_OBJECTS       = KIND_FREED_OBJECTS,
     KIND_GLOBAL_FREED_BYTES         = KIND_FREED_BYTES,
     KIND_GLOBAL_GC_INVOCATIONS      = KIND_GC_INVOCATIONS,
+    KIND_GLOBAL_CLASS_INIT_COUNT    = KIND_CLASS_INIT_COUNT,
+    KIND_GLOBAL_CLASS_INIT_TIME     = KIND_CLASS_INIT_TIME,
 #if PROFILE_EXTERNAL_ALLOCATIONS
     KIND_GLOBAL_EXT_ALLOCATED_OBJECTS = KIND_EXT_ALLOCATED_OBJECTS,
     KIND_GLOBAL_EXT_ALLOCATED_BYTES = KIND_EXT_ALLOCATED_BYTES,
@@ -87,6 +176,12 @@ static void clearAllocProfStateFields(AllocProfState *allocProf,
     if (kinds & KIND_GC_INVOCATIONS) {
         allocProf->gcCount = 0;
     }
+    if (kinds & KIND_CLASS_INIT_COUNT) {
+        allocProf->classInitCount = 0;
+    }
+    if (kinds & KIND_CLASS_INIT_TIME) {
+        allocProf->classInitTime = 0;
+    }
 #if PROFILE_EXTERNAL_ALLOCATIONS
     if (kinds & KIND_EXT_ALLOCATED_OBJECTS) {
         allocProf->externalAllocCount = 0;
@@ -171,6 +266,13 @@ static void Dalvik_dalvik_system_VMDebug_getAllocCount(const u4* args,
     case KIND_GC_INVOCATIONS:
         pResult->i = allocProf->gcCount;
         break;
+    case KIND_CLASS_INIT_COUNT:
+        pResult->i = allocProf->classInitCount;
+        break;
+    case KIND_CLASS_INIT_TIME:
+        /* convert nsec to usec, reduce to 32 bits */
+        pResult->i = (int) (allocProf->classInitTime / 1000);
+        break;
 #if PROFILE_EXTERNAL_ALLOCATIONS
     case KIND_EXT_ALLOCATED_OBJECTS:
         pResult->i = allocProf->externalAllocCount;
@@ -209,12 +311,16 @@ static void Dalvik_dalvik_system_VMDebug_resetAllocCount(const u4* args,
 }
 
 /*
- * static void startMethodTracing(String traceFileName, java.io.FileDescriptor,
- *     int bufferSize, int flags)
+ * static void startMethodTracingNative(String traceFileName,
+ *     FileDescriptor fd, int bufferSize, int flags)
  *
  * Start method trace profiling.
+ *
+ * If both "traceFileName" and "fd" are null, the result will be sent
+ * directly to DDMS.  (The non-DDMS versions of the calls are expected
+ * to enforce non-NULL filenames.)
  */
-static void Dalvik_dalvik_system_VMDebug_startMethodTracing(const u4* args,
+static void Dalvik_dalvik_system_VMDebug_startMethodTracingNative(const u4* args,
     JValue* pResult)
 {
 #ifdef WITH_PROFILER
@@ -222,32 +328,40 @@ static void Dalvik_dalvik_system_VMDebug_startMethodTracing(const u4* args,
     DataObject* traceFd = (DataObject*) args[1];
     int bufferSize = args[2];
     int flags = args[3];
-    char* traceFileName;
 
     if (bufferSize == 0) {
         // Default to 8MB per the documentation.
         bufferSize = 8 * 1024 * 1024;
     }
 
-    if (traceFileStr == NULL || bufferSize < 1024) {
+    if (bufferSize < 1024) {
         dvmThrowException("Ljava/lang/IllegalArgumentException;", NULL);
         RETURN_VOID();
     }
 
-    traceFileName = dvmCreateCstrFromString(traceFileStr);
+    char* traceFileName = NULL;
+    if (traceFileStr != NULL)
+        traceFileName = dvmCreateCstrFromString(traceFileStr);
 
     int fd = -1;
     if (traceFd != NULL) {
-        InstField* field = dvmFindInstanceField(traceFd->obj.clazz, "descriptor", "I");
+        InstField* field =
+            dvmFindInstanceField(traceFd->obj.clazz, "descriptor", "I");
         if (field == NULL) {
             dvmThrowException("Ljava/lang/NoSuchFieldException;",
                 "No FileDescriptor.descriptor field");
             RETURN_VOID();
         }
         fd = dup(dvmGetFieldInt(&traceFd->obj, field->byteOffset));
+        if (fd < 0) {
+            dvmThrowExceptionFmt("Ljava/lang/RuntimeException;",
+                "dup() failed: %s", strerror(errno));
+            RETURN_VOID();
+        }
     }
     
-    dvmMethodTraceStart(traceFileName, fd, bufferSize, flags);
+    dvmMethodTraceStart(traceFileName != NULL ? traceFileName : "[DDMS]",
+        fd, bufferSize, flags, (traceFileName == NULL && fd == -1));
     free(traceFileName);
 #else
     // throw exception?
@@ -570,7 +684,7 @@ static void Dalvik_dalvik_system_VMDebug_dumpHprofData(const u4* args,
         RETURN_VOID();
     }
 
-    result = hprofDumpHeap(fileName);
+    result = hprofDumpHeap(fileName, false);
     free(fileName);
 
     if (result != 0) {
@@ -587,6 +701,32 @@ static void Dalvik_dalvik_system_VMDebug_dumpHprofData(const u4* args,
 }
 
 /*
+ * static void dumpHprofDataDdms()
+ *
+ * Cause "hprof" data to be computed and sent directly to DDMS.
+ */
+static void Dalvik_dalvik_system_VMDebug_dumpHprofDataDdms(const u4* args,
+    JValue* pResult)
+{
+#ifdef WITH_HPROF
+    int result;
+
+    result = hprofDumpHeap("[DDMS]", true);
+
+    if (result != 0) {
+        /* ideally we'd throw something more specific based on actual failure */
+        dvmThrowException("Ljava/lang/RuntimeException;",
+            "Failure during heap dump -- check log output for details");
+        RETURN_VOID();
+    }
+#else
+    dvmThrowException("Ljava/lang/UnsupportedOperationException;", NULL);
+#endif
+
+    RETURN_VOID();
+}
+
+/*
  * static boolean cacheRegisterMap(String classAndMethodDescr)
  *
  * If the specified class is loaded, and the named method exists, ensure
@@ -699,6 +839,23 @@ bail:
 }
 
 /*
+ * static void dumpReferenceTables()
+ */
+static void Dalvik_dalvik_system_VMDebug_dumpReferenceTables(const u4* args,
+    JValue* pResult)
+{
+    UNUSED_PARAMETER(args);
+    UNUSED_PARAMETER(pResult);
+
+    LOGI("--- reference table dump ---\n");
+    dvmDumpJniReferenceTables();
+    // could dump thread's internalLocalRefTable, probably not useful
+    // ditto for thread's jniMonitorRefTable
+    LOGI("---\n");
+    RETURN_VOID();
+}
+
+/*
  * static void crash()
  *
  * Dump the current thread's interpreted stack and abort the VM.  Useful
@@ -718,7 +875,26 @@ static void Dalvik_dalvik_system_VMDebug_crash(const u4* args,
     dvmAbort();
 }
 
+/*
+ * static void infopoint(int id)
+ *
+ * Provide a hook for gdb to hang to so that the VM can be stopped when
+ * user-tagged source locations are being executed.
+ */
+static void Dalvik_dalvik_system_VMDebug_infopoint(const u4* args,
+    JValue* pResult)
+{
+    gDvm.nativeDebuggerActive = true;
+
+    LOGD("VMDebug infopoint %d hit", args[0]);
+
+    gDvm.nativeDebuggerActive = false;
+    RETURN_VOID();
+}
+
 const DalvikNativeMethod dvm_dalvik_system_VMDebug[] = {
+    { "getVmFeatureList",           "()[Ljava/lang/String;",
+        Dalvik_dalvik_system_VMDebug_getVmFeatureList },
     { "getAllocCount",              "(I)I",
         Dalvik_dalvik_system_VMDebug_getAllocCount },
     { "resetAllocCount",            "(I)V",
@@ -727,8 +903,8 @@ const DalvikNativeMethod dvm_dalvik_system_VMDebug[] = {
         Dalvik_dalvik_system_VMDebug_startAllocCounting },
     { "stopAllocCounting",          "()V",
         Dalvik_dalvik_system_VMDebug_stopAllocCounting },
-    { "startMethodTracing",         "(Ljava/lang/String;Ljava/io/FileDescriptor;II)V",
-        Dalvik_dalvik_system_VMDebug_startMethodTracing },
+    { "startMethodTracingNative",   "(Ljava/lang/String;Ljava/io/FileDescriptor;II)V",
+        Dalvik_dalvik_system_VMDebug_startMethodTracingNative },
     { "isMethodTracingActive",      "()Z",
         Dalvik_dalvik_system_VMDebug_isMethodTracingActive },
     { "stopMethodTracing",          "()V",
@@ -763,10 +939,15 @@ const DalvikNativeMethod dvm_dalvik_system_VMDebug[] = {
         Dalvik_dalvik_system_VMDebug_threadCpuTimeNanos },
     { "dumpHprofData",              "(Ljava/lang/String;)V",
         Dalvik_dalvik_system_VMDebug_dumpHprofData },
+    { "dumpHprofDataDdms",          "()V",
+        Dalvik_dalvik_system_VMDebug_dumpHprofDataDdms },
     { "cacheRegisterMap",           "(Ljava/lang/String;)Z",
         Dalvik_dalvik_system_VMDebug_cacheRegisterMap },
+    { "dumpReferenceTables",        "()V",
+        Dalvik_dalvik_system_VMDebug_dumpReferenceTables },
     { "crash",                      "()V",
         Dalvik_dalvik_system_VMDebug_crash },
+    { "infopoint",                 "(I)V",
+        Dalvik_dalvik_system_VMDebug_infopoint },
     { NULL, NULL, NULL },
 };
-
diff --git a/vm/native/dalvik_system_VMRuntime.c b/vm/native/dalvik_system_VMRuntime.c
index 0ec3ced..c020f8a 100644
--- a/vm/native/dalvik_system_VMRuntime.c
+++ b/vm/native/dalvik_system_VMRuntime.c
@@ -178,6 +178,45 @@ static void Dalvik_dalvik_system_VMRuntime_getExternalBytesAllocated(
     RETURN_LONG((s8)dvmGetExternalBytesAllocated());
 }
 
+/*
+ * public native void startJitCompilation()
+ *
+ * Callback function from the framework to indicate that an app has gone
+ * through the startup phase and it is time to enable the JIT compiler.
+ */
+static void Dalvik_dalvik_system_VMRuntime_startJitCompilation(const u4* args,
+    JValue* pResult)
+{
+#if defined(WITH_JIT)
+    if (gDvm.executionMode == kExecutionModeJit &&
+        gDvmJit.disableJit == false) {
+        dvmLockMutex(&gDvmJit.compilerLock);
+        gDvmJit.alreadyEnabledViaFramework = true;
+        pthread_cond_signal(&gDvmJit.compilerQueueActivity);
+        dvmUnlockMutex(&gDvmJit.compilerLock);
+    }
+#endif
+    RETURN_VOID();
+}
+
+/*
+ * public native void disableJitCompilation()
+ *
+ * Callback function from the framework to indicate that a VM instance wants to
+ * permanently disable the JIT compiler. Currently only the system server uses
+ * this interface when it detects system-wide safe mode is enabled.
+ */
+static void Dalvik_dalvik_system_VMRuntime_disableJitCompilation(const u4* args,
+    JValue* pResult)
+{
+#if defined(WITH_JIT)
+    if (gDvm.executionMode == kExecutionModeJit) {
+        gDvmJit.disableJit = true;
+    }
+#endif
+    RETURN_VOID();
+}
+
 const DalvikNativeMethod dvm_dalvik_system_VMRuntime[] = {
     { "getTargetHeapUtilization", "()F",
         Dalvik_dalvik_system_VMRuntime_getTargetHeapUtilization },
@@ -195,6 +234,9 @@ const DalvikNativeMethod dvm_dalvik_system_VMRuntime[] = {
         Dalvik_dalvik_system_VMRuntime_trackExternalFree },
     { "getExternalBytesAllocated", "()J",
         Dalvik_dalvik_system_VMRuntime_getExternalBytesAllocated },
+    { "startJitCompilation", "()V",
+        Dalvik_dalvik_system_VMRuntime_startJitCompilation },
+    { "disableJitCompilation", "()V",
+        Dalvik_dalvik_system_VMRuntime_disableJitCompilation },
     { NULL, NULL, NULL },
 };
-
diff --git a/vm/native/dalvik_system_Zygote.c b/vm/native/dalvik_system_Zygote.c
index fe2b5c0..f8e8250 100644
--- a/vm/native/dalvik_system_Zygote.c
+++ b/vm/native/dalvik_system_Zygote.c
@@ -37,6 +37,7 @@ enum {
     DEBUG_ENABLE_DEBUGGER           = 1,
     DEBUG_ENABLE_CHECKJNI           = 1 << 1,
     DEBUG_ENABLE_ASSERT             = 1 << 2,
+    DEBUG_ENABLE_SAFEMODE           = 1 << 3,
 };
 
 /*
@@ -269,6 +270,11 @@ static void Dalvik_dalvik_system_Zygote_fork(const u4* args, JValue* pResult)
  *   If set, make sure assertions are enabled.  This gets fairly weird,
  *   because it affects the result of a method called by class initializers,
  *   and hence can't affect pre-loaded/initialized classes.
+ * safemode
+ *   If set, operates the VM in the safe mode. The definition of "safe mode" is
+ *   implementation dependent and currently only the JIT compiler is disabled.
+ *   This is easy to handle because the compiler thread and associated resources
+ *   are not requested until we call dvmInitAfterZygote().
  */
 static void enableDebugFeatures(u4 debugFlags)
 {
@@ -285,6 +291,37 @@ static void enableDebugFeatures(u4 debugFlags)
         /* turn it on if it's not already enabled */
         dvmLateEnableAssertions();
     }
+
+    if ((debugFlags & DEBUG_ENABLE_SAFEMODE) != 0) {
+#if defined(WITH_JIT)
+        /* turn off the jit if it is explicitly requested by the app */
+        if (gDvm.executionMode == kExecutionModeJit)
+            gDvm.executionMode = kExecutionModeInterpFast;
+#endif
+    }
+
+#if HAVE_ANDROID_OS
+    if ((debugFlags & DEBUG_ENABLE_DEBUGGER) != 0) {
+        /* To let a non-privileged gdbserver attach to this
+         * process, we must set its dumpable bit flag. However
+         * we are not interested in generating a coredump in
+         * case of a crash, so also set the coredump size to 0
+         * to disable that
+         */
+        if (prctl(PR_SET_DUMPABLE, 1, 0, 0, 0) < 0) {
+            LOGE("could not set dumpable bit flag for pid %d, errno=%d",
+                 getpid(), errno);
+        } else {
+            struct rlimit rl;
+            rl.rlim_cur = 0;
+            rl.rlim_max = RLIM_INFINITY;
+            if (setrlimit(RLIMIT_CORE, &rl) < 0) {
+                LOGE("could not disable core file generation "
+                     "for pid %d, errno=%d", getpid(), errno);
+            }
+        }
+    }
+#endif
 }
 
 /* 
@@ -429,4 +466,3 @@ const DalvikNativeMethod dvm_dalvik_system_Zygote[] = {
         Dalvik_dalvik_system_Zygote_forkSystemServer },
     { NULL, NULL, NULL },
 };
-
diff --git a/vm/native/java_lang_Object.c b/vm/native/java_lang_Object.c
index 89e219f..44f581e 100644
--- a/vm/native/java_lang_Object.c
+++ b/vm/native/java_lang_Object.c
@@ -42,7 +42,7 @@ static void Dalvik_java_lang_Object_internalClone(const u4* args,
 static void Dalvik_java_lang_Object_hashCode(const u4* args, JValue* pResult)
 {
     Object* thisPtr = (Object*) args[0];
-    RETURN_INT(dvmGetObjectHashCode(thisPtr));
+    RETURN_INT(dvmIdentityHashCode(thisPtr));
 }
 
 /*
diff --git a/vm/native/java_lang_Runtime.c b/vm/native/java_lang_Runtime.c
index 1278f03..5170eed 100644
--- a/vm/native/java_lang_Runtime.c
+++ b/vm/native/java_lang_Runtime.c
@@ -19,7 +19,8 @@
  */
 #include "Dalvik.h"
 #include "native/InternalNativePriv.h"
-
+#include <unistd.h>
+#include <limits.h>
 
 /*
  * public void gc()
@@ -53,6 +54,9 @@ static void Dalvik_java_lang_Runtime_nativeExit(const u4* args,
         LOGW("JNI exit hook returned\n");
     }
     LOGD("Calling exit(%d)\n", status);
+#if defined(WITH_JIT) && defined(WITH_JIT_TUNING)
+    dvmCompilerDumpStats();
+#endif
     exit(status);
 }
 
@@ -106,6 +110,26 @@ static void Dalvik_java_lang_Runtime_runFinalization(const u4* args,
 }
 
 /*
+ * public int availableProcessors()
+ *
+ * Returns the number of online processors, at least one.
+ *
+ */
+static void Dalvik_java_lang_Runtime_availableProcessors(const u4* args,
+    JValue* pResult)
+{
+    long result = 1;
+#ifdef _SC_NPROCESSORS_ONLN
+    result = sysconf(_SC_NPROCESSORS_ONLN);
+    if (result > INT_MAX) {
+        result = INT_MAX;
+    } else if (result < 1 ) {
+        result = 1;
+    }
+#endif
+    RETURN_INT((int)result);
+}
+/*
  * public void maxMemory()
  *
  * Returns GC heap max memory in bytes.
@@ -149,6 +173,8 @@ const DalvikNativeMethod dvm_java_lang_Runtime[] = {
         Dalvik_java_lang_Runtime_freeMemory },
     { "gc",                 "()V",
         Dalvik_java_lang_Runtime_gc },
+    { "availableProcessors", "()I",
+        Dalvik_java_lang_Runtime_availableProcessors },
     { "maxMemory",          "()J",
         Dalvik_java_lang_Runtime_maxMemory },
     { "nativeExit",         "(IZ)V",
@@ -161,4 +187,3 @@ const DalvikNativeMethod dvm_java_lang_Runtime[] = {
         Dalvik_java_lang_Runtime_totalMemory },
     { NULL, NULL, NULL },
 };
-
diff --git a/vm/native/java_lang_System.c b/vm/native/java_lang_System.c
index b26a368..e2533ed 100644
--- a/vm/native/java_lang_System.c
+++ b/vm/native/java_lang_System.c
@@ -245,7 +245,7 @@ static void Dalvik_java_lang_System_identityHashCode(const u4* args,
     JValue* pResult)
 {
     Object* thisPtr = (Object*) args[0];
-    RETURN_INT(dvmGetObjectHashCode(thisPtr));
+    RETURN_INT(dvmIdentityHashCode(thisPtr));
 }
 
 /*
diff --git a/vm/native/java_lang_Throwable.c b/vm/native/java_lang_Throwable.c
index 3cd0e2e..cb94f0f 100644
--- a/vm/native/java_lang_Throwable.c
+++ b/vm/native/java_lang_Throwable.c
@@ -47,6 +47,11 @@ static void Dalvik_java_lang_Throwable_nativeGetStackTrace(const u4* args,
     Object* stackState = (Object*) args[0];
     ArrayObject* elements = NULL;
 
+    if (stackState == NULL) {
+        LOGW("getStackTrace() called but no trace available\n");
+        RETURN_PTR(NULL);   /* could throw NPE; currently caller will do so */
+    }
+
     elements = dvmGetStackTrace(stackState);
     RETURN_PTR(elements);
 }
diff --git a/vm/oo/Array.c b/vm/oo/Array.c
index 19a0f96..4af03a9 100644
--- a/vm/oo/Array.c
+++ b/vm/oo/Array.c
@@ -104,21 +104,13 @@ ClassObject* dvmFindArrayClassForElement(ClassObject* elemClassObj)
 
     assert(elemClassObj != NULL);
 
-    if (elemClassObj->arrayClass != NULL) {
-        arrayClass = elemClassObj->arrayClass;
-        LOGVV("using cached '%s' class for '%s'\n",
-            arrayClass->descriptor, elemClassObj->descriptor);
-    } else {
-        /* Simply prepend "[" to the descriptor. */
-        int nameLen = strlen(elemClassObj->descriptor);
-        char className[nameLen + 2];
-
-        className[0] = '[';
-        memcpy(className+1, elemClassObj->descriptor, nameLen+1);
-        arrayClass = dvmFindArrayClass(className, elemClassObj->classLoader);
-        if (arrayClass != NULL)
-            elemClassObj->arrayClass = arrayClass;
-    }
+    /* Simply prepend "[" to the descriptor. */
+    int nameLen = strlen(elemClassObj->descriptor);
+    char className[nameLen + 2];
+
+    className[0] = '[';
+    memcpy(className+1, elemClassObj->descriptor, nameLen+1);
+    arrayClass = dvmFindArrayClass(className, elemClassObj->classLoader);
 
     return arrayClass;
 }
@@ -697,6 +689,82 @@ bool dvmCopyObjectArray(ArrayObject* dstArray, const ArrayObject* srcArray,
 }
 
 /*
+ * Copy the entire contents of an array of boxed primitives into an
+ * array of primitives.  The boxed value must fit in the primitive (i.e.
+ * narrowing conversions are not allowed).
+ */
+bool dvmUnboxObjectArray(ArrayObject* dstArray, const ArrayObject* srcArray,
+    ClassObject* dstElemClass)
+{
+    Object** src = (Object**)srcArray->contents;
+    void* dst = (void*)dstArray->contents;
+    u4 count = dstArray->length;
+    PrimitiveType typeIndex = dstElemClass->primitiveType;
+
+    assert(typeIndex != PRIM_NOT);
+    assert(srcArray->length == dstArray->length);
+
+    while (count--) {
+        JValue result;
+
+        /*
+         * This will perform widening conversions as appropriate.  It
+         * might make sense to be more restrictive and require that the
+         * primitive type exactly matches the box class, but it's not
+         * necessary for correctness.
+         */
+        if (!dvmUnwrapPrimitive(*src, dstElemClass, &result)) {
+            LOGW("dvmCopyObjectArray: can't store %s in %s\n",
+                (*src)->clazz->descriptor, dstElemClass->descriptor);
+            return false;
+        }
+
+        /* would be faster with 4 loops, but speed not crucial here */
+        switch (typeIndex) {
+        case PRIM_BOOLEAN:
+        case PRIM_BYTE:
+            {
+                u1* tmp = dst;
+                *tmp++ = result.b;
+                dst = tmp;
+            }
+            break;
+        case PRIM_CHAR:
+        case PRIM_SHORT:
+            {
+                u2* tmp = dst;
+                *tmp++ = result.s;
+                dst = tmp;
+            }
+            break;
+        case PRIM_FLOAT:
+        case PRIM_INT:
+            {
+                u4* tmp = dst;
+                *tmp++ = result.i;
+                dst = tmp;
+            }
+            break;
+        case PRIM_DOUBLE:
+        case PRIM_LONG:
+            {
+                u8* tmp = dst;
+                *tmp++ = result.j;
+                dst = tmp;
+            }
+            break;
+        default:
+            /* should not be possible to get here */
+            dvmAbort();
+        }
+
+        src++;
+    }
+
+    return true;
+}
+
+/*
  * Add all primitive classes to the root set of objects.
 TODO: do these belong to the root class loader?
  */
@@ -708,4 +776,3 @@ void dvmGcScanPrimitiveClasses()
         dvmMarkObject((Object *)gDvm.primitiveClass[i]);    // may be NULL
     }
 }
-
diff --git a/vm/oo/Array.h b/vm/oo/Array.h
index 868e48a..161b1c6 100644
--- a/vm/oo/Array.h
+++ b/vm/oo/Array.h
@@ -113,6 +113,18 @@ INLINE bool dvmIsArray(const ArrayObject* arrayObj)
 }
 
 /*
+ * Verify that the array is an object array and not a primitive array.
+ *
+ * Does not verify that the object is actually a non-NULL object.
+ */
+INLINE bool dvmIsObjectArray(const ArrayObject* arrayObj)
+{
+    const char* descriptor = arrayObj->obj.clazz->descriptor;
+    return descriptor[0] == '[' && (descriptor[1] == 'L' ||
+                                    descriptor[1] == '[');
+}
+
+/*
  * Verify that the class is an array class.
  *
  * TODO: there may be some performance advantage to setting a flag in
@@ -132,4 +144,12 @@ INLINE bool dvmIsArrayClass(const ClassObject* clazz)
 bool dvmCopyObjectArray(ArrayObject* dstArray, const ArrayObject* srcArray,
     ClassObject* dstElemClass);
 
+/*
+ * Copy the entire contents of an array of boxed primitives into an
+ * array of primitives.  The boxed value must fit in the primitive (i.e.
+ * narrowing conversions are not allowed).
+ */
+bool dvmUnboxObjectArray(ArrayObject* dstArray, const ArrayObject* srcArray,
+    ClassObject* dstElemClass);
+
 #endif /*_DALVIK_OO_ARRAY*/
diff --git a/vm/oo/Class.c b/vm/oo/Class.c
index 1bde718..625a572 100644
--- a/vm/oo/Class.c
+++ b/vm/oo/Class.c
@@ -180,6 +180,8 @@ static void loadSFieldFromDex(ClassObject* clazz,
     const DexField* pDexSField, StaticField* sfield);
 static void loadIFieldFromDex(ClassObject* clazz,
     const DexField* pDexIField, InstField* field);
+static bool precacheReferenceOffsets(ClassObject* clazz);
+static void computeRefOffsets(ClassObject* clazz);
 static void freeMethodInnards(Method* meth);
 static bool createVtable(ClassObject* clazz);
 static bool createIftable(ClassObject* clazz);
@@ -1393,7 +1395,7 @@ static ClassObject* findClassNoInit(const char* descriptor, Object* loader,
      * making it an informative abort rather than an assert).
      */
     if (dvmCheckException(self)) {
-        LOGE("Class lookup %s attemped while exception %s pending\n",
+        LOGE("Class lookup %s attempted while exception %s pending\n",
             descriptor, dvmGetException(self)->clazz->descriptor);
         dvmDumpAllThreads(false);
         dvmAbort();
@@ -2115,6 +2117,7 @@ static void loadMethodFromDex(ClassObject* clazz, const DexMethod* pDexMethod,
     }
 }
 
+#if 0       /* replaced with private/read-write mapping */
 /*
  * We usually map bytecode directly out of the DEX file, which is mapped
  * shared read-only.  If we want to be able to modify it, we have to make
@@ -2162,6 +2165,7 @@ void dvmMakeCodeReadOnly(Method* meth)
     LOGV("+++ marking %p read-only\n", methodDexCode);
     dvmLinearReadOnly(meth->clazz->classLoader, methodDexCode);
 }
+#endif
 
 
 /*
@@ -2289,7 +2293,7 @@ static void loadIFieldFromDex(ClassObject* clazz,
 /*
  * Cache java.lang.ref.Reference fields and methods.
  */
-static bool precacheReferenceOffsets(ClassObject *clazz)
+static bool precacheReferenceOffsets(ClassObject* clazz)
 {
     Method *meth;
     int i;
@@ -2368,26 +2372,61 @@ static bool precacheReferenceOffsets(ClassObject *clazz)
                 "vmData", "I");
     assert(gDvm.offJavaLangRefReference_vmData >= 0);
 
-#if FANCY_REFERENCE_SUBCLASS
-    meth = dvmFindVirtualMethodByDescriptor(clazz, "clear", "()V");
-    assert(meth != NULL);
-    gDvm.voffJavaLangRefReference_clear = meth->methodIndex;
-
-    meth = dvmFindVirtualMethodByDescriptor(clazz, "enqueue", "()Z");
-    assert(meth != NULL);
-    gDvm.voffJavaLangRefReference_enqueue = meth->methodIndex;
-#else
     /* enqueueInternal() is private and thus a direct method. */
     meth = dvmFindDirectMethodByDescriptor(clazz, "enqueueInternal", "()Z");
     assert(meth != NULL);
     gDvm.methJavaLangRefReference_enqueueInternal = meth;
-#endif
 
     return true;
 }
 
 
 /*
+ * Set the bitmap of reference offsets, refOffsets, from the ifields
+ * list.
+ */
+static void computeRefOffsets(ClassObject* clazz)
+{
+    if (clazz->super != NULL) {
+        clazz->refOffsets = clazz->super->refOffsets;
+    } else {
+        clazz->refOffsets = 0;
+    }
+    /*
+     * If our superclass overflowed, we don't stand a chance.
+     */
+    if (clazz->refOffsets != CLASS_WALK_SUPER) {
+        InstField *f;
+        int i;
+
+        /* All of the fields that contain object references
+         * are guaranteed to be at the beginning of the ifields list.
+         */
+        f = clazz->ifields;
+        const int ifieldRefCount = clazz->ifieldRefCount;
+        for (i = 0; i < ifieldRefCount; i++) {
+          /*
+           * Note that, per the comment on struct InstField,
+           * f->byteOffset is the offset from the beginning of
+           * obj, not the offset into obj->instanceData.
+           */
+          assert(f->byteOffset >= (int) CLASS_SMALLEST_OFFSET);
+          assert((f->byteOffset & (CLASS_OFFSET_ALIGNMENT - 1)) == 0);
+          if (CLASS_CAN_ENCODE_OFFSET(f->byteOffset)) {
+              u4 newBit = CLASS_BIT_FROM_OFFSET(f->byteOffset);
+              assert(newBit != 0);
+              clazz->refOffsets |= newBit;
+          } else {
+              clazz->refOffsets = CLASS_WALK_SUPER;
+              break;
+          }
+          f++;
+        }
+    }
+}
+
+
+/*
  * Link (prepare and resolve).  Verification is deferred until later.
  *
  * This converts symbolic references into pointers.  It's independent of
@@ -2755,6 +2794,13 @@ bail_during_resolve:
     }
 
     /*
+     * Compact the offsets the GC has to examine into a bitmap, if
+     * possible.  (This has to happen after Reference.referent is
+     * massaged in precacheReferenceOffsets.)
+     */
+    computeRefOffsets(clazz);
+
+    /*
      * Done!
      */
     if (IS_CLASS_FLAG_SET(clazz, CLASS_ISPREVERIFIED))
@@ -4240,8 +4286,10 @@ bool dvmInitClass(ClassObject* clazz)
             (gDvm.classVerifyMode == VERIFY_MODE_REMOTE &&
              clazz->classLoader == NULL))
         {
+            /* advance to "verified" state */
             LOGV("+++ not verifying class %s (cl=%p)\n",
                 clazz->descriptor, clazz->classLoader);
+            clazz->status = CLASS_VERIFIED;
             goto noverify;
         }
 
@@ -4275,6 +4323,11 @@ verify_failed:
     }
 noverify:
 
+#ifdef WITH_DEBUGGER
+    /* update instruction stream now that the verifier is done */
+    dvmFlushBreakpoints(clazz);
+#endif
+
     if (clazz->status == CLASS_INITIALIZED)
         goto bail_unlock;
 
@@ -4348,6 +4401,11 @@ noverify:
         return false;
     }
 
+    u8 startWhen = 0;
+    if (gDvm.allocProf.enabled) {
+        startWhen = dvmGetRelativeTimeNsec();
+    }
+
     /*
      * We're ready to go, and have exclusive access to the class.
      *
@@ -4421,44 +4479,6 @@ noverify:
         dvmCallMethod(self, method, NULL, &unused);
     }
 
-    /* Set the bitmap of reference offsets. Except for class Object,
-     * start with the superclass offsets.
-     */
-    if (clazz->super != NULL) {
-        clazz->refOffsets = clazz->super->refOffsets;
-    } else {
-        clazz->refOffsets = 0;
-    }
-    /*
-     * If our superclass overflowed, we don't stand a chance.
-     */
-    if (clazz->refOffsets != CLASS_WALK_SUPER) {
-        InstField *f;
-        int i;
-
-        /* All of the fields that contain object references
-         * are guaranteed to be at the beginning of the ifields list.
-         */
-        f = clazz->ifields;
-        for (i = 0; i < clazz->ifieldRefCount; i++) {
-            /*
-             * Note that, per the comment on struct InstField,
-             * f->byteOffset is the offset from the beginning of
-             * obj, not the offset into obj->instanceData.
-             */
-            assert(f->byteOffset >= (int) CLASS_SMALLEST_OFFSET);
-            assert((f->byteOffset & (CLASS_OFFSET_ALIGNMENT - 1)) == 0);
-            u4 newBit = CLASS_BIT_FROM_OFFSET(f->byteOffset);
-            if (newBit != 0) {
-                clazz->refOffsets |= newBit;
-            } else {
-                clazz->refOffsets = CLASS_WALK_SUPER;
-                break;
-            }
-            f++;
-        }
-    }
-
     if (dvmCheckException(self)) {
         /*
          * We've had an exception thrown during static initialization.  We
@@ -4477,6 +4497,17 @@ noverify:
         dvmLockObject(self, (Object*) clazz);
         clazz->status = CLASS_INITIALIZED;
         LOGVV("Initialized class: %s\n", clazz->descriptor);
+
+        /*
+         * Update alloc counters.  TODO: guard with mutex.
+         */
+        if (gDvm.allocProf.enabled && startWhen != 0) {
+            u8 initDuration = dvmGetRelativeTimeNsec() - startWhen;
+            gDvm.allocProf.classInitTime += initDuration;
+            self->allocProf.classInitTime += initDuration;
+            gDvm.allocProf.classInitCount++;
+            self->allocProf.classInitCount++;
+        }
     }
 
 bail_notify:
diff --git a/vm/oo/Object.c b/vm/oo/Object.c
index b3a9ffa..de86d6e 100644
--- a/vm/oo/Object.c
+++ b/vm/oo/Object.c
@@ -216,11 +216,12 @@ static inline int compareMethodHelper(Method* method, const char* methodName,
     }
 
     proto = &method->prototype;
+
     if( proto->dexFile == NULL ) {
       LOGW("WARNING: Comparing method '%s' by name only because of hack found a match\n",methodName);
       return 0;
     }
-
+        
     if (strcmp(returnType, dexProtoGetReturnType(proto)) != 0) {
         return 1;
     }
diff --git a/vm/oo/Object.h b/vm/oo/Object.h
index 3e724f4..df167d5 100644
--- a/vm/oo/Object.h
+++ b/vm/oo/Object.h
@@ -187,12 +187,23 @@ typedef enum PrimitiveType {
 #define CLASS_OFFSET_ALIGNMENT 4
 #define CLASS_HIGH_BIT ((unsigned int)1 << (CLASS_BITS_PER_WORD - 1))
 /*
- * Return a single bit, or zero if the encoding can't encode the offset.
+ * Given an offset, return the bit number which would encode that offset.
+ * Local use only.
+ */
+#define _CLASS_BIT_NUMBER_FROM_OFFSET(byteOffset) \
+    (((unsigned int)(byteOffset) - CLASS_SMALLEST_OFFSET) / \
+     CLASS_OFFSET_ALIGNMENT)
+/*
+ * Is the given offset too large to be encoded?
+ */
+#define CLASS_CAN_ENCODE_OFFSET(byteOffset) \
+    (_CLASS_BIT_NUMBER_FROM_OFFSET(byteOffset) < CLASS_BITS_PER_WORD)
+/*
+ * Return a single bit, encoding the offset.
+ * Undefined if the offset is too large, as defined above.
  */
 #define CLASS_BIT_FROM_OFFSET(byteOffset) \
-    (CLASS_HIGH_BIT >> \
-      (((unsigned int)(byteOffset) - CLASS_SMALLEST_OFFSET) / \
-       CLASS_OFFSET_ALIGNMENT))
+    (CLASS_HIGH_BIT >> _CLASS_BIT_NUMBER_FROM_OFFSET(byteOffset))
 /*
  * Return an offset, given a bit number as returned from CLZ.
  */
@@ -232,8 +243,11 @@ typedef struct Object {
     /* ptr to class object */
     ClassObject*    clazz;
 
-    /* thin lock or "fat" monitor */
-    Lock            lock;
+    /*
+     * A word containing either a "thin" lock or a "fat" monitor.  See
+     * the comments in Sync.c for a description of its layout.
+     */
+    u4              lock;
 } Object;
 
 /*
@@ -277,7 +291,7 @@ struct StringObject {
  *
  * We don't currently store the size of each element.  Usually it's implied
  * by the instruction.  If necessary, the width can be derived from
- * the first char of obj->clazz->name.
+ * the first char of obj->clazz->descriptor.
  */
 struct ArrayObject {
     Object          obj;                /* MUST be first item */
@@ -367,9 +381,6 @@ struct ClassObject {
        (for String[][][], this will be String) */
     ClassObject*    elementClass;
 
-    /* class object representing an array of this class; set on first use */
-    ClassObject*    arrayClass;
-
     /* arrays only: number of dimensions, e.g. int[][] is 2 */
     int             arrayDim;
 
@@ -542,9 +553,6 @@ struct Method {
 #ifdef WITH_PROFILER
     bool            inProfile;
 #endif
-#ifdef WITH_DEBUGGER
-    short           debugBreakpointCount;
-#endif
 };
 
 /*
diff --git a/vm/reflect/Annotation.c b/vm/reflect/Annotation.c
index 109c7fb..fb4b83f 100644
--- a/vm/reflect/Annotation.c
+++ b/vm/reflect/Annotation.c
@@ -617,10 +617,8 @@ static bool processAnnotationValue(const ClassObject* clazz,
  *
  * For an array annotation, the type of the extracted object will always
  * be java.lang.Object[], but we want it to match the type that the
- * annotation member is expected to return.  In theory we can just stomp
- * the object's class to have the correct type, but this strikes me as a
- * risky proposition (at the very least we would need to call instanceof()
- * on every element).
+ * annotation member is expected to return.  In some cases this may
+ * involve un-boxing primitive values.
  *
  * We allocate a second array with the correct type, then copy the data
  * over.  This releases the tracked allocation on "valueObj" and returns
@@ -642,18 +640,26 @@ static Object* convertReturnType(Object* valueObj, ClassObject* methodReturn)
     ClassObject* dstElemClass;
 
     /*
-     * Strip off one '[' to get element class.  Note this is not the
-     * same as clazz->elementClass.
+     * We always extract kDexAnnotationArray into Object[], so we expect to
+     * find that here.  This means we can skip the FindClass on
+     * (valueObj->clazz->descriptor+1, valueObj->clazz->classLoader).
      */
-    srcElemClass = dvmFindClass(valueObj->clazz->descriptor+1,
-        valueObj->clazz->classLoader);
-    dstElemClass = dvmFindClass(methodReturn->descriptor+1,
-        methodReturn->classLoader);
-    if (srcElemClass->primitiveType != PRIM_NOT ||
-        dstElemClass->primitiveType != PRIM_NOT)
-    {
-        LOGE("ERROR: array of primitives not expected here\n");
-        dvmAbort();
+    if (strcmp(valueObj->clazz->descriptor, "[Ljava/lang/Object;") != 0) {
+        LOGE("Unexpected src type class (%s)\n", valueObj->clazz->descriptor);
+        return NULL;
+    }
+    srcElemClass = gDvm.classJavaLangObject;
+
+    /*
+     * Skip past the '[' to get element class name.  Note this is not always
+     * the same as methodReturn->elementClass.
+     */
+    char firstChar = methodReturn->descriptor[1];
+    if (firstChar == 'L' || firstChar == '[') {
+        dstElemClass = dvmFindClass(methodReturn->descriptor+1,
+            methodReturn->classLoader);
+    } else {
+        dstElemClass = dvmFindPrimitiveClass(firstChar);
     }
     LOGV("HEY: converting valueObj from [%s to [%s\n",
         srcElemClass->descriptor, dstElemClass->descriptor);
@@ -669,7 +675,13 @@ static Object* convertReturnType(Object* valueObj, ClassObject* methodReturn)
         goto bail;
     }
 
-    if (!dvmCopyObjectArray(newArray, srcArray, dstElemClass)) {
+    bool success;
+    if (dstElemClass->primitiveType == PRIM_NOT) {
+        success = dvmCopyObjectArray(newArray, srcArray, dstElemClass);
+    } else {
+        success = dvmUnboxObjectArray(newArray, srcArray, dstElemClass);
+    }
+    if (!success) {
         LOGE("Annotation array copy failed\n");
         dvmReleaseTrackedAlloc((Object*)newArray, self);
         newArray = NULL;
diff --git a/vm/test/TestIndirectRefTable.c b/vm/test/TestIndirectRefTable.c
index 64d843c..25f1dd1 100644
--- a/vm/test/TestIndirectRefTable.c
+++ b/vm/test/TestIndirectRefTable.c
@@ -245,12 +245,15 @@ static bool basicTest(void)
     dvmRemoveFromIndirectRefTable(&irt, cookie, iref0);
     iref1 = dvmAddToIndirectRefTable(&irt, cookie, obj0);
     if (iref0 != iref1) {
+        /* try 0, should not work */
         if (dvmRemoveFromIndirectRefTable(&irt, cookie, iref0)) {
             LOGE("temporal del succeeded (%p vs %p)\n", iref0, iref1);
             goto bail;
         }
-    } else {
-        dvmRemoveFromIndirectRefTable(&irt, cookie, iref1);
+    }
+    if (!dvmRemoveFromIndirectRefTable(&irt, cookie, iref1)) {
+        LOGE("temporal cleanup failed\n");
+        goto bail;
     }
     if (dvmIndirectRefTableEntries(&irt) != 0) {
         LOGE("temporal del not empty\n");
